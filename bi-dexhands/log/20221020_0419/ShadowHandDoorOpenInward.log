Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-16c7vyah
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandDoorOpenInward_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/16c7vyah
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:4
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Using VHACD cache directory '/data/zihan/.isaacgym/vhacd'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj'
Started convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj'
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj': 1 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj': 4 hulls
Finished convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj': 4 hulls
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-1.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-10.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-11.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-12.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-2.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-5.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-6.obj'
Found existing convex decomposition for mesh '../assets/mjcf/door/textured_objs/original-7.obj'
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=4, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=4, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:4', seed=None, sim_device='cuda:4', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandDoorOpenInward', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_door_open_inward', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 250, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.5, 'orientation_scale': 0.5, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandDoorOpenInward', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandDoorOpenInward_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 2254
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:4
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=417, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 640 steps/s (collection: 24.106s, learning 1.455s)
               Value function loss: 12.0272
                    Surrogate loss: 0.0483
             Mean action noise std: 0.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 25.56s
                        Total time: 25.56s
                               ETA: 2556057.8s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 749 steps/s (collection: 21.181s, learning 0.666s)
               Value function loss: 0.7859
                    Surrogate loss: -0.0234
             Mean action noise std: 0.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 21.85s
                        Total time: 47.41s
                               ETA: 2370360.2s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 719 steps/s (collection: 22.565s, learning 0.207s)
               Value function loss: 0.3096
                    Surrogate loss: -0.0390
             Mean action noise std: 0.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 22.77s
                        Total time: 70.18s
                               ETA: 2339281.3s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 708 steps/s (collection: 22.964s, learning 0.166s)
               Value function loss: 0.2186
                    Surrogate loss: -0.0394
             Mean action noise std: 0.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 23.13s
                        Total time: 93.31s
                               ETA: 2332690.3s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 728 steps/s (collection: 22.335s, learning 0.167s)
               Value function loss: 0.2195
                    Surrogate loss: -0.0194
             Mean action noise std: 0.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 22.50s
                        Total time: 115.81s
                               ETA: 2316158.4s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 722 steps/s (collection: 22.506s, learning 0.159s)
               Value function loss: 0.3558
                    Surrogate loss: -0.0062
             Mean action noise std: 0.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 22.67s
                        Total time: 138.48s
                               ETA: 2307846.3s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 713 steps/s (collection: 22.499s, learning 0.479s)
               Value function loss: 0.5482
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                  Mean reward/step: 0.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 22.98s
                        Total time: 161.46s
                               ETA: 2306379.4s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 725 steps/s (collection: 22.403s, learning 0.189s)
               Value function loss: 1.1175
                    Surrogate loss: 0.0000
             Mean action noise std: 0.80
                  Mean reward/step: 0.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 22.59s
                        Total time: 184.05s
                               ETA: 2300446.1s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 720 steps/s (collection: 22.509s, learning 0.225s)
               Value function loss: 1.5639
                    Surrogate loss: 0.0077
             Mean action noise std: 0.80
                  Mean reward/step: 0.31
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 22.73s
                        Total time: 206.78s
                               ETA: 2297389.8s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 715 steps/s (collection: 22.716s, learning 0.191s)
               Value function loss: 2.0988
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                  Mean reward/step: 0.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 22.91s
                        Total time: 229.69s
                               ETA: 2296678.8s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 719 steps/s (collection: 22.613s, learning 0.174s)
               Value function loss: 2.6330
                    Surrogate loss: -0.0005
             Mean action noise std: 0.80
                  Mean reward/step: 0.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 22.79s
                        Total time: 252.48s
                               ETA: 2294998.9s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 723 steps/s (collection: 22.449s, learning 0.198s)
               Value function loss: 3.7185
                    Surrogate loss: -0.0138
             Mean action noise std: 0.80
                  Mean reward/step: 0.72
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 22.65s
                        Total time: 275.12s
                               ETA: 2292435.9s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 729 steps/s (collection: 22.295s, learning 0.178s)
               Value function loss: 3.2802
                    Surrogate loss: 0.0311
             Mean action noise std: 0.80
                  Mean reward/step: 0.87
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 22.47s
                        Total time: 297.60s
                               ETA: 2288922.5s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 696 steps/s (collection: 23.334s, learning 0.197s)
               Value function loss: 4.5544
                    Surrogate loss: -0.0041
             Mean action noise std: 0.80
                  Mean reward/step: 1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 23.53s
                        Total time: 321.13s
                               ETA: 2293462.1s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 723 steps/s (collection: 22.482s, learning 0.167s)
               Value function loss: 3.8924
                    Surrogate loss: 0.0073
             Mean action noise std: 0.80
                  Mean reward/step: 1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 22.65s
                        Total time: 343.78s
                               ETA: 2291515.1s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 713 steps/s (collection: 22.670s, learning 0.300s)
               Value function loss: 5.7663
                    Surrogate loss: 0.0082
             Mean action noise std: 0.80
                  Mean reward/step: 1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 22.97s
                        Total time: 366.75s
                               ETA: 2291816.3s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 714 steps/s (collection: 22.773s, learning 0.172s)
               Value function loss: 9.6680
                    Surrogate loss: 0.0170
             Mean action noise std: 0.80
                  Mean reward/step: 1.20
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 22.94s
                        Total time: 389.69s
                               ETA: 2291930.3s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.361s, learning 0.164s)
               Value function loss: 7.0214
                    Surrogate loss: -0.0058
             Mean action noise std: 0.80
                  Mean reward/step: 1.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 22.53s
                        Total time: 412.22s
                               ETA: 2289700.4s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 720 steps/s (collection: 22.580s, learning 0.170s)
               Value function loss: 3.3556
                    Surrogate loss: 0.0438
             Mean action noise std: 0.80
                  Mean reward/step: 1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 22.75s
                        Total time: 434.97s
                               ETA: 2288878.6s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 724 steps/s (collection: 22.435s, learning 0.169s)
               Value function loss: 3.9191
                    Surrogate loss: 0.0007
             Mean action noise std: 0.80
                  Mean reward/step: 1.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 22.60s
                        Total time: 457.57s
                               ETA: 2287409.3s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 734 steps/s (collection: 22.128s, learning 0.165s)
               Value function loss: 2.6230
                    Surrogate loss: 0.0055
             Mean action noise std: 0.80
                  Mean reward/step: 1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 22.29s
                        Total time: 479.86s
                               ETA: 2284602.4s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 719 steps/s (collection: 22.594s, learning 0.169s)
               Value function loss: 2.3758
                    Surrogate loss: 0.0246
             Mean action noise std: 0.80
                  Mean reward/step: 1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 22.76s
                        Total time: 502.63s
                               ETA: 2284183.7s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 726 steps/s (collection: 22.365s, learning 0.183s)
               Value function loss: 2.6230
                    Surrogate loss: -0.0093
             Mean action noise std: 0.80
                  Mean reward/step: 1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 22.55s
                        Total time: 525.17s
                               ETA: 2282864.6s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 701 steps/s (collection: 23.192s, learning 0.175s)
               Value function loss: 1.9274
                    Surrogate loss: 0.2398
             Mean action noise std: 0.80
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 23.37s
                        Total time: 548.54s
                               ETA: 2285059.8s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 712 steps/s (collection: 22.829s, learning 0.164s)
               Value function loss: 2.4700
                    Surrogate loss: 0.0121
             Mean action noise std: 0.80
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 22.99s
                        Total time: 571.53s
                               ETA: 2285584.8s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.310s, learning 0.164s)
               Value function loss: 1.5209
                    Surrogate loss: 0.0539
             Mean action noise std: 0.80
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 23.47s
                        Total time: 595.01s
                               ETA: 2287919.6s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 715 steps/s (collection: 22.718s, learning 0.167s)
               Value function loss: 1.0302
                    Surrogate loss: 0.0183
             Mean action noise std: 0.80
                  Mean reward/step: 1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 22.89s
                        Total time: 617.89s
                               ETA: 2287899.5s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 726 steps/s (collection: 22.402s, learning 0.158s)
               Value function loss: 0.7535
                    Surrogate loss: 0.0091
             Mean action noise std: 0.80
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 22.56s
                        Total time: 640.45s
                               ETA: 2286715.9s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.429s, learning 0.176s)
               Value function loss: 0.7586
                    Surrogate loss: 0.0046
             Mean action noise std: 0.80
                  Mean reward/step: 1.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 23.60s
                        Total time: 664.06s
                               ETA: 2289214.3s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 728 steps/s (collection: 22.311s, learning 0.164s)
               Value function loss: 1.1434
                    Surrogate loss: -0.0083
             Mean action noise std: 0.80
                       Mean reward: 222.04
               Mean episode length: 237.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 22.47s
                        Total time: 686.53s
                               ETA: 2287779.2s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 720 steps/s (collection: 22.535s, learning 0.202s)
               Value function loss: 1.3227
                    Surrogate loss: 0.0086
             Mean action noise std: 0.80
                       Mean reward: 222.04
               Mean episode length: 237.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 22.74s
                        Total time: 709.27s
                               ETA: 2287278.4s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 719 steps/s (collection: 22.564s, learning 0.198s)
               Value function loss: 124.0450
                    Surrogate loss: 0.0339
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 0.26
       Mean episode length/episode: 4.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 22.76s
                        Total time: 732.03s
                               ETA: 2286886.7s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 729 steps/s (collection: 22.302s, learning 0.165s)
               Value function loss: 3.1293
                    Surrogate loss: 0.0078
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 0.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 22.47s
                        Total time: 754.50s
                               ETA: 2285625.3s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 748 steps/s (collection: 21.727s, learning 0.176s)
               Value function loss: 1.6818
                    Surrogate loss: 0.0044
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 21.90s
                        Total time: 776.40s
                               ETA: 2282776.6s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 749 steps/s (collection: 21.659s, learning 0.198s)
               Value function loss: 0.6094
                    Surrogate loss: 0.0073
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 21.86s
                        Total time: 798.26s
                               ETA: 2279962.1s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 736 steps/s (collection: 22.086s, learning 0.161s)
               Value function loss: 0.5992
                    Surrogate loss: 0.0040
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 22.25s
                        Total time: 820.51s
                               ETA: 2278383.6s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 736 steps/s (collection: 22.076s, learning 0.175s)
               Value function loss: 0.8766
                    Surrogate loss: -0.0074
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 22.25s
                        Total time: 842.76s
                               ETA: 2276898.3s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 977 steps/s (collection: 16.592s, learning 0.173s)
               Value function loss: 1.3801
                    Surrogate loss: 0.0738
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 16.77s
                        Total time: 859.52s
                               ETA: 2261060.3s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.063s, learning 0.170s)
               Value function loss: 0.7885
                    Surrogate loss: 0.0224
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 11.23s
                        Total time: 870.75s
                               ETA: 2231854.7s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.576s, learning 0.168s)
               Value function loss: 0.6119
                    Surrogate loss: 0.0085
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 11.74s
                        Total time: 882.50s
                               ETA: 2205386.4s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.788s, learning 0.160s)
               Value function loss: 1.1599
                    Surrogate loss: 0.0182
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 11.95s
                        Total time: 894.45s
                               ETA: 2180704.5s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.385s, learning 0.176s)
               Value function loss: 0.9646
                    Surrogate loss: 0.0065
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 11.56s
                        Total time: 906.01s
                               ETA: 2156277.5s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.433s, learning 0.194s)
               Value function loss: 0.6034
                    Surrogate loss: 0.0106
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 11.63s
                        Total time: 917.64s
                               ETA: 2133139.3s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.470s, learning 0.217s)
               Value function loss: 0.7251
                    Surrogate loss: -0.0125
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 11.69s
                        Total time: 929.32s
                               ETA: 2111189.2s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.296s, learning 0.180s)
               Value function loss: 1.1708
                    Surrogate loss: 0.0143
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 11.48s
                        Total time: 940.80s
                               ETA: 2089744.4s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.602s, learning 0.177s)
               Value function loss: 1.7565
                    Surrogate loss: 0.0068
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 11.78s
                        Total time: 952.58s
                               ETA: 2069890.1s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.140s, learning 0.200s)
               Value function loss: 2.4662
                    Surrogate loss: 0.0113
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 11.34s
                        Total time: 963.92s
                               ETA: 2049947.6s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.402s, learning 0.185s)
               Value function loss: 2.8110
                    Surrogate loss: 0.0014
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 0.70
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 11.59s
                        Total time: 975.51s
                               ETA: 2031348.5s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.116s, learning 0.191s)
               Value function loss: 2.8352
                    Surrogate loss: 0.0106
             Mean action noise std: 0.80
                       Mean reward: 222.35
               Mean episode length: 249.00
                  Mean reward/step: 0.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 11.31s
                        Total time: 986.81s
                               ETA: 2012937.5s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.017s, learning 0.169s)
               Value function loss: 4.5040
                    Surrogate loss: 0.0304
             Mean action noise std: 0.80
                       Mean reward: 219.23
               Mean episode length: 245.04
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 11.19s
                        Total time: 998.00s
                               ETA: 1995020.1s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.196s, learning 0.189s)
               Value function loss: 5.2525
                    Surrogate loss: 0.0257
             Mean action noise std: 0.80
                       Mean reward: 216.16
               Mean episode length: 242.31
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 11.38s
                        Total time: 1009.38s
                               ETA: 1978194.8s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.162s)
               Value function loss: 7.9823
                    Surrogate loss: 0.0109
             Mean action noise std: 0.80
                       Mean reward: 197.32
               Mean episode length: 223.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 11.40s
                        Total time: 1020.78s
                               ETA: 1962041.2s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.805s, learning 0.178s)
               Value function loss: 10.3118
                    Surrogate loss: -0.0039
             Mean action noise std: 0.80
                       Mean reward: 134.98
               Mean episode length: 172.91
                  Mean reward/step: -0.11
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 11.98s
                        Total time: 1032.76s
                               ETA: 1947599.9s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.289s, learning 0.213s)
               Value function loss: 16.8546
                    Surrogate loss: 0.0145
             Mean action noise std: 0.80
                       Mean reward: 134.13
               Mean episode length: 181.94
                  Mean reward/step: -0.21
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 11.50s
                        Total time: 1044.27s
                               ETA: 1932802.5s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.124s, learning 0.166s)
               Value function loss: 18.1011
                    Surrogate loss: -0.0100
             Mean action noise std: 0.80
                       Mean reward: 141.84
               Mean episode length: 190.54
                  Mean reward/step: -0.20
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 11.29s
                        Total time: 1055.56s
                               ETA: 1918158.4s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.570s, learning 0.321s)
               Value function loss: 15.5813
                    Surrogate loss: 0.0089
             Mean action noise std: 0.80
                       Mean reward: 150.33
               Mean episode length: 198.37
                  Mean reward/step: -0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 11.89s
                        Total time: 1067.45s
                               ETA: 1905108.9s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.311s, learning 0.163s)
               Value function loss: 9.4200
                    Surrogate loss: -0.0089
             Mean action noise std: 0.80
                       Mean reward: 154.62
               Mean episode length: 206.14
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 11.47s
                        Total time: 1078.92s
                               ETA: 1891784.8s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.207s, learning 0.163s)
               Value function loss: 7.9742
                    Surrogate loss: -0.0129
             Mean action noise std: 0.80
                       Mean reward: 155.26
               Mean episode length: 213.91
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 11.37s
                        Total time: 1090.29s
                               ETA: 1878741.4s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.200s, learning 0.227s)
               Value function loss: 5.7052
                    Surrogate loss: -0.0045
             Mean action noise std: 0.80
                       Mean reward: 167.01
               Mean episode length: 221.07
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 11.43s
                        Total time: 1101.72s
                               ETA: 1866235.0s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.297s, learning 0.155s)
               Value function loss: 3.6517
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 172.03
               Mean episode length: 225.27
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 11.45s
                        Total time: 1113.17s
                               ETA: 1854189.3s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.518s, learning 0.162s)
               Value function loss: 5.6415
                    Surrogate loss: 0.0103
             Mean action noise std: 0.80
                       Mean reward: 178.06
               Mean episode length: 230.65
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 11.68s
                        Total time: 1124.85s
                               ETA: 1842910.5s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.387s, learning 0.182s)
               Value function loss: 4.3207
                    Surrogate loss: 0.0091
             Mean action noise std: 0.80
                       Mean reward: 181.00
               Mean episode length: 234.32
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 11.57s
                        Total time: 1136.42s
                               ETA: 1831815.6s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.772s, learning 0.165s)
               Value function loss: 4.6257
                    Surrogate loss: -0.0069
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 10.94s
                        Total time: 1147.36s
                               ETA: 1820071.5s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.169s)
               Value function loss: 4.3967
                    Surrogate loss: 0.0008
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 11.43s
                        Total time: 1158.79s
                               ETA: 1809462.4s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.855s, learning 0.201s)
               Value function loss: 1.7514
                    Surrogate loss: 0.0381
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 12.06s
                        Total time: 1170.84s
                               ETA: 1800142.1s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.971s, learning 0.180s)
               Value function loss: 1.7516
                    Surrogate loss: 0.0149
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 11.15s
                        Total time: 1181.99s
                               ETA: 1789734.0s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.154s, learning 0.167s)
               Value function loss: 1.3519
                    Surrogate loss: 0.0122
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 11.32s
                        Total time: 1193.31s
                               ETA: 1779889.6s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.401s, learning 0.200s)
               Value function loss: 1.3182
                    Surrogate loss: 0.0260
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 11.60s
                        Total time: 1204.91s
                               ETA: 1770746.0s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.437s, learning 0.194s)
               Value function loss: 1.3427
                    Surrogate loss: -0.0065
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 11.63s
                        Total time: 1216.55s
                               ETA: 1761909.9s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.169s, learning 0.157s)
               Value function loss: 1.4727
                    Surrogate loss: -0.0121
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.36
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 11.33s
                        Total time: 1227.87s
                               ETA: 1752891.2s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.278s, learning 0.160s)
               Value function loss: 1.5640
                    Surrogate loss: 0.0215
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.29
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 11.44s
                        Total time: 1239.31s
                               ETA: 1744284.3s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.661s, learning 0.191s)
               Value function loss: 1.7186
                    Surrogate loss: 0.0071
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.21
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 11.85s
                        Total time: 1251.16s
                               ETA: 1736490.2s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.723s, learning 0.181s)
               Value function loss: 1.9227
                    Surrogate loss: -0.0140
             Mean action noise std: 0.80
                       Mean reward: 185.44
               Mean episode length: 239.64
                  Mean reward/step: 1.13
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 11.90s
                        Total time: 1263.06s
                               ETA: 1728980.1s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.534s, learning 0.173s)
               Value function loss: 2.2780
                    Surrogate loss: -0.0290
             Mean action noise std: 0.80
                       Mean reward: 184.82
               Mean episode length: 238.23
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 11.71s
                        Total time: 1274.77s
                               ETA: 1721408.0s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.532s, learning 0.179s)
               Value function loss: 2.3399
                    Surrogate loss: -0.0191
             Mean action noise std: 0.80
                       Mean reward: 184.59
               Mean episode length: 237.15
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 11.71s
                        Total time: 1286.48s
                               ETA: 1714042.8s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.514s, learning 0.163s)
               Value function loss: 2.8870
                    Surrogate loss: -0.0227
             Mean action noise std: 0.80
                       Mean reward: 182.25
               Mean episode length: 233.81
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 11.68s
                        Total time: 1298.16s
                               ETA: 1706825.5s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.731s, learning 0.159s)
               Value function loss: 4.0643
                    Surrogate loss: -0.0229
             Mean action noise std: 0.80
                       Mean reward: 181.71
               Mean episode length: 232.10
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 11.89s
                        Total time: 1310.05s
                               ETA: 1700071.2s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.229s, learning 0.167s)
               Value function loss: 5.2196
                    Surrogate loss: -0.0278
             Mean action noise std: 0.80
                       Mean reward: 180.59
               Mean episode length: 228.74
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 11.40s
                        Total time: 1321.45s
                               ETA: 1692858.4s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.559s, learning 0.228s)
               Value function loss: 3.6851
                    Surrogate loss: 0.0029
             Mean action noise std: 0.80
                       Mean reward: 180.73
               Mean episode length: 227.61
                  Mean reward/step: 0.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 11.79s
                        Total time: 1333.23s
                               ETA: 1686320.7s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.234s, learning 0.159s)
               Value function loss: 3.7262
                    Surrogate loss: -0.0040
             Mean action noise std: 0.80
                       Mean reward: 178.00
               Mean episode length: 221.06
                  Mean reward/step: 0.52
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 11.39s
                        Total time: 1344.63s
                               ETA: 1679455.1s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.811s, learning 0.166s)
               Value function loss: 3.7924
                    Surrogate loss: -0.0072
             Mean action noise std: 0.80
                       Mean reward: 175.14
               Mean episode length: 212.62
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 11.98s
                        Total time: 1356.60s
                               ETA: 1673479.3s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.313s, learning 0.160s)
               Value function loss: 5.3294
                    Surrogate loss: -0.0139
             Mean action noise std: 0.80
                       Mean reward: 176.28
               Mean episode length: 213.43
                  Mean reward/step: 0.39
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 11.47s
                        Total time: 1368.08s
                               ETA: 1667034.4s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.165s)
               Value function loss: 4.6126
                    Surrogate loss: -0.0215
             Mean action noise std: 0.80
                       Mean reward: 184.45
               Mean episode length: 223.31
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 11.46s
                        Total time: 1379.54s
                               ETA: 1660732.0s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.483s, learning 0.186s)
               Value function loss: 5.9380
                    Surrogate loss: -0.0230
             Mean action noise std: 0.80
                       Mean reward: 193.29
               Mean episode length: 238.03
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 11.67s
                        Total time: 1391.21s
                               ETA: 1654824.2s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.140s, learning 0.163s)
               Value function loss: 9.3096
                    Surrogate loss: 0.0002
             Mean action noise std: 0.80
                       Mean reward: 213.00
               Mean episode length: 246.18
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 11.30s
                        Total time: 1402.51s
                               ETA: 1648626.2s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.237s, learning 0.176s)
               Value function loss: 11.2667
                    Surrogate loss: -0.0084
             Mean action noise std: 0.80
                       Mean reward: 228.35
               Mean episode length: 248.72
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 11.41s
                        Total time: 1413.92s
                               ETA: 1642699.3s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.341s, learning 0.160s)
               Value function loss: 10.8588
                    Surrogate loss: -0.0190
             Mean action noise std: 0.80
                       Mean reward: 231.63
               Mean episode length: 249.36
                  Mean reward/step: 0.39
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 11.50s
                        Total time: 1425.42s
                               ETA: 1637009.9s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.172s)
               Value function loss: 8.3120
                    Surrogate loss: -0.0216
             Mean action noise std: 0.80
                       Mean reward: 244.45
               Mean episode length: 249.71
                  Mean reward/step: 0.49
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 11.50s
                        Total time: 1436.92s
                               ETA: 1631445.5s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.032s, learning 0.187s)
               Value function loss: 6.4221
                    Surrogate loss: -0.0110
             Mean action noise std: 0.80
                       Mean reward: 250.06
               Mean episode length: 249.56
                  Mean reward/step: 0.62
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 11.22s
                        Total time: 1448.14s
                               ETA: 1625692.7s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.475s, learning 0.168s)
               Value function loss: 5.0063
                    Surrogate loss: -0.0145
             Mean action noise std: 0.80
                       Mean reward: 253.05
               Mean episode length: 249.82
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 11.64s
                        Total time: 1459.78s
                               ETA: 1620538.6s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.168s, learning 0.160s)
               Value function loss: 2.6635
                    Surrogate loss: -0.0244
             Mean action noise std: 0.80
                       Mean reward: 257.19
               Mean episode length: 249.49
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 11.33s
                        Total time: 1471.11s
                               ETA: 1615152.6s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.202s, learning 0.159s)
               Value function loss: 1.6341
                    Surrogate loss: -0.0211
             Mean action noise std: 0.80
                       Mean reward: 256.55
               Mean episode length: 249.48
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 11.36s
                        Total time: 1482.47s
                               ETA: 1609917.7s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1459 steps/s (collection: 10.949s, learning 0.277s)
               Value function loss: 1.1909
                    Surrogate loss: -0.0201
             Mean action noise std: 0.80
                       Mean reward: 257.84
               Mean episode length: 249.38
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 11.23s
                        Total time: 1493.70s
                               ETA: 1604650.1s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.243s, learning 0.159s)
               Value function loss: 1.1800
                    Surrogate loss: -0.0177
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 11.40s
                        Total time: 1505.10s
                               ETA: 1599682.4s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.575s, learning 0.162s)
               Value function loss: 0.7412
                    Surrogate loss: -0.0057
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.17
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 11.74s
                        Total time: 1516.84s
                               ETA: 1595171.3s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.726s, learning 0.180s)
               Value function loss: 0.7513
                    Surrogate loss: -0.0247
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 11.91s
                        Total time: 1528.74s
                               ETA: 1590929.4s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.595s, learning 0.210s)
               Value function loss: 0.7568
                    Surrogate loss: -0.0104
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.19
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 11.80s
                        Total time: 1540.55s
                               ETA: 1586669.9s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.389s, learning 0.198s)
               Value function loss: 0.7945
                    Surrogate loss: -0.0118
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.18
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 11.59s
                        Total time: 1552.14s
                               ETA: 1582275.4s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.200s, learning 0.189s)
               Value function loss: 0.9616
                    Surrogate loss: -0.0081
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.16
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 11.39s
                        Total time: 1563.52s
                               ETA: 1577769.7s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.391s, learning 0.185s)
               Value function loss: 0.9101
                    Surrogate loss: 0.0968
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.14
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 11.58s
                        Total time: 1575.10s
                               ETA: 1573540.9s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.587s, learning 0.164s)
               Value function loss: 0.8132
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 262.31
               Mean episode length: 249.45
                  Mean reward/step: 1.12
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 11.75s
                        Total time: 1586.85s
                               ETA: 1569568.1s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.465s, learning 0.171s)
               Value function loss: 0.8419
                    Surrogate loss: -0.0088
             Mean action noise std: 0.80
                       Mean reward: 259.33
               Mean episode length: 248.50
                  Mean reward/step: 1.09
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 11.64s
                        Total time: 1598.49s
                               ETA: 1565560.8s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.371s, learning 0.165s)
               Value function loss: 0.8150
                    Surrogate loss: -0.0254
             Mean action noise std: 0.80
                       Mean reward: 255.66
               Mean episode length: 246.31
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 11.54s
                        Total time: 1610.02s
                               ETA: 1561534.3s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.389s, learning 0.206s)
               Value function loss: 0.7333
                    Surrogate loss: -0.0143
             Mean action noise std: 0.80
                       Mean reward: 253.95
               Mean episode length: 245.55
                  Mean reward/step: 1.06
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 11.60s
                        Total time: 1621.62s
                               ETA: 1557641.8s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.165s)
               Value function loss: 0.7887
                    Surrogate loss: -0.0242
             Mean action noise std: 0.80
                       Mean reward: 251.19
               Mean episode length: 243.06
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 11.46s
                        Total time: 1633.07s
                               ETA: 1553689.9s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.434s, learning 0.189s)
               Value function loss: 0.8591
                    Surrogate loss: -0.0237
             Mean action noise std: 0.80
                       Mean reward: 248.06
               Mean episode length: 240.73
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 11.62s
                        Total time: 1644.70s
                               ETA: 1549970.2s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.337s, learning 0.198s)
               Value function loss: 0.9402
                    Surrogate loss: -0.0228
             Mean action noise std: 0.80
                       Mean reward: 244.72
               Mean episode length: 240.07
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 11.53s
                        Total time: 1656.23s
                               ETA: 1546237.8s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.994s, learning 0.161s)
               Value function loss: 0.9134
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 239.65
               Mean episode length: 238.59
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 11.15s
                        Total time: 1667.38s
                               ETA: 1542222.6s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.025s, learning 0.167s)
               Value function loss: 1.1394
                    Surrogate loss: -0.0202
             Mean action noise std: 0.80
                       Mean reward: 236.36
               Mean episode length: 238.27
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 11.19s
                        Total time: 1678.58s
                               ETA: 1538315.0s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.115s, learning 0.260s)
               Value function loss: 0.9986
                    Surrogate loss: -0.0018
             Mean action noise std: 0.80
                       Mean reward: 233.93
               Mean episode length: 237.70
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 11.38s
                        Total time: 1689.95s
                               ETA: 1534644.9s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.253s, learning 0.168s)
               Value function loss: 1.5675
                    Surrogate loss: -0.0102
             Mean action noise std: 0.80
                       Mean reward: 228.52
               Mean episode length: 236.84
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 11.42s
                        Total time: 1701.37s
                               ETA: 1531082.3s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.304s, learning 0.221s)
               Value function loss: 1.9435
                    Surrogate loss: -0.0183
             Mean action noise std: 0.80
                       Mean reward: 216.37
               Mean episode length: 236.34
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 11.53s
                        Total time: 1712.90s
                               ETA: 1527675.7s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.201s, learning 0.159s)
               Value function loss: 3.3235
                    Surrogate loss: -0.0174
             Mean action noise std: 0.80
                       Mean reward: 230.16
               Mean episode length: 246.73
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 11.36s
                        Total time: 1724.26s
                               ETA: 1524183.7s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.496s, learning 0.215s)
               Value function loss: 3.4905
                    Surrogate loss: -0.0064
             Mean action noise std: 0.80
                       Mean reward: 235.23
               Mean episode length: 249.11
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 11.71s
                        Total time: 1735.97s
                               ETA: 1521059.2s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.487s, learning 0.177s)
               Value function loss: 5.1363
                    Surrogate loss: 0.0047
             Mean action noise std: 0.80
                       Mean reward: 238.31
               Mean episode length: 249.22
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 11.66s
                        Total time: 1747.63s
                               ETA: 1517948.1s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.911s, learning 0.173s)
               Value function loss: 11.5422
                    Surrogate loss: 0.0087
             Mean action noise std: 0.80
                       Mean reward: 239.85
               Mean episode length: 250.00
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 12.08s
                        Total time: 1759.72s
                               ETA: 1515252.4s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.590s, learning 0.169s)
               Value function loss: 19.0561
                    Surrogate loss: -0.0038
             Mean action noise std: 0.80
                       Mean reward: 244.87
               Mean episode length: 250.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 11.76s
                        Total time: 1771.48s
                               ETA: 1512325.7s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.532s, learning 0.171s)
               Value function loss: 22.3960
                    Surrogate loss: -0.0111
             Mean action noise std: 0.80
                       Mean reward: 244.86
               Mean episode length: 249.92
                  Mean reward/step: 0.75
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 11.70s
                        Total time: 1783.18s
                               ETA: 1509401.0s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.562s, learning 0.197s)
               Value function loss: 20.9768
                    Surrogate loss: 0.0003
             Mean action noise std: 0.80
                       Mean reward: 251.17
               Mean episode length: 250.00
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 11.76s
                        Total time: 1794.94s
                               ETA: 1506572.2s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.046s, learning 0.157s)
               Value function loss: 14.2932
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                       Mean reward: 249.28
               Mean episode length: 250.00
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 11.20s
                        Total time: 1806.14s
                               ETA: 1503327.5s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.911s, learning 0.187s)
               Value function loss: 10.5980
                    Surrogate loss: -0.0143
             Mean action noise std: 0.80
                       Mean reward: 248.72
               Mean episode length: 250.00
                  Mean reward/step: 0.88
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 12.10s
                        Total time: 1818.24s
                               ETA: 1500874.5s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.624s, learning 0.161s)
               Value function loss: 4.6182
                    Surrogate loss: -0.0179
             Mean action noise std: 0.80
                       Mean reward: 254.79
               Mean episode length: 250.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 11.79s
                        Total time: 1830.03s
                               ETA: 1498206.0s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.451s, learning 0.230s)
               Value function loss: 2.0194
                    Surrogate loss: -0.0238
             Mean action noise std: 0.80
                       Mean reward: 256.44
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 11.68s
                        Total time: 1841.71s
                               ETA: 1495495.5s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.014s, learning 0.164s)
               Value function loss: 1.5531
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 258.89
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 11.18s
                        Total time: 1852.88s
                               ETA: 1492423.2s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.983s, learning 0.162s)
               Value function loss: 1.5252
                    Surrogate loss: 0.0045
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 11.15s
                        Total time: 1864.03s
                               ETA: 1489374.3s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.953s, learning 0.165s)
               Value function loss: 0.6712
                    Surrogate loss: -0.0049
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 11.12s
                        Total time: 1875.15s
                               ETA: 1486351.9s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.273s, learning 0.159s)
               Value function loss: 0.7085
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 11.43s
                        Total time: 1886.58s
                               ETA: 1483623.9s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.158s, learning 0.158s)
               Value function loss: 0.6320
                    Surrogate loss: 0.0174
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 11.32s
                        Total time: 1897.90s
                               ETA: 1480847.7s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.230s, learning 0.259s)
               Value function loss: 0.6729
                    Surrogate loss: 0.0066
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 11.49s
                        Total time: 1909.38s
                               ETA: 1478248.0s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.363s, learning 0.162s)
               Value function loss: 1.2717
                    Surrogate loss: 0.0162
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 11.52s
                        Total time: 1920.91s
                               ETA: 1475715.7s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.163s)
               Value function loss: 0.9164
                    Surrogate loss: 0.0065
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 11.40s
                        Total time: 1932.31s
                               ETA: 1473130.5s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.114s, learning 0.251s)
               Value function loss: 0.7858
                    Surrogate loss: -0.0118
             Mean action noise std: 0.80
                       Mean reward: 260.31
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 11.36s
                        Total time: 1943.68s
                               ETA: 1470553.6s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.649s, learning 0.157s)
               Value function loss: 0.9171
                    Surrogate loss: 0.0050
             Mean action noise std: 0.80
                       Mean reward: 258.84
               Mean episode length: 248.64
                  Mean reward/step: 1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 11.81s
                        Total time: 1955.48s
                               ETA: 1468347.1s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.518s, learning 0.161s)
               Value function loss: 1.0268
                    Surrogate loss: -0.0073
             Mean action noise std: 0.80
                       Mean reward: 259.19
               Mean episode length: 248.64
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 11.68s
                        Total time: 1967.16s
                               ETA: 1466078.4s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.433s, learning 0.189s)
               Value function loss: 0.9807
                    Surrogate loss: -0.0123
             Mean action noise std: 0.80
                       Mean reward: 259.19
               Mean episode length: 248.64
                  Mean reward/step: 1.37
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 11.62s
                        Total time: 1978.78s
                               ETA: 1463801.7s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.355s, learning 0.163s)
               Value function loss: 1.3056
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 261.44
               Mean episode length: 248.64
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 11.52s
                        Total time: 1990.30s
                               ETA: 1461481.4s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.332s, learning 0.204s)
               Value function loss: 1.3957
                    Surrogate loss: 0.0019
             Mean action noise std: 0.80
                       Mean reward: 262.60
               Mean episode length: 248.64
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 11.54s
                        Total time: 2001.84s
                               ETA: 1459208.0s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.381s, learning 0.191s)
               Value function loss: 1.5172
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 263.64
               Mean episode length: 248.64
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 11.57s
                        Total time: 2013.41s
                               ETA: 1456993.3s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.220s, learning 0.286s)
               Value function loss: 1.1919
                    Surrogate loss: -0.0069
             Mean action noise std: 0.80
                       Mean reward: 264.15
               Mean episode length: 248.64
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 11.51s
                        Total time: 2024.91s
                               ETA: 1454763.0s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.009s, learning 0.166s)
               Value function loss: 1.7752
                    Surrogate loss: -0.0102
             Mean action noise std: 0.80
                       Mean reward: 267.04
               Mean episode length: 248.64
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 11.17s
                        Total time: 2036.09s
                               ETA: 1452328.4s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.704s, learning 0.230s)
               Value function loss: 1.4710
                    Surrogate loss: 0.0083
             Mean action noise std: 0.80
                       Mean reward: 269.58
               Mean episode length: 248.64
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 11.93s
                        Total time: 2048.02s
                               ETA: 1450465.5s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1337 steps/s (collection: 11.956s, learning 0.296s)
               Value function loss: 2.1511
                    Surrogate loss: -0.0135
             Mean action noise std: 0.80
                       Mean reward: 274.27
               Mean episode length: 248.64
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 12.25s
                        Total time: 2060.28s
                               ETA: 1448852.4s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.335s, learning 0.162s)
               Value function loss: 3.1349
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 283.03
               Mean episode length: 248.64
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 11.50s
                        Total time: 2071.77s
                               ETA: 1446734.1s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.262s, learning 0.169s)
               Value function loss: 4.4852
                    Surrogate loss: -0.0060
             Mean action noise std: 0.80
                       Mean reward: 295.55
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 11.43s
                        Total time: 2083.20s
                               ETA: 1444599.9s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.297s, learning 0.176s)
               Value function loss: 6.4325
                    Surrogate loss: -0.0080
             Mean action noise std: 0.80
                       Mean reward: 299.00
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 11.47s
                        Total time: 2094.68s
                               ETA: 1442523.8s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.809s, learning 0.181s)
               Value function loss: 8.6164
                    Surrogate loss: 0.0038
             Mean action noise std: 0.80
                       Mean reward: 298.10
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 11.99s
                        Total time: 2106.67s
                               ETA: 1440829.3s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.174s)
               Value function loss: 16.6443
                    Surrogate loss: 0.0005
             Mean action noise std: 0.80
                       Mean reward: 298.51
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 11.50s
                        Total time: 2118.17s
                               ETA: 1438825.1s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.141s, learning 0.157s)
               Value function loss: 24.1747
                    Surrogate loss: -0.0042
             Mean action noise std: 0.80
                       Mean reward: 302.69
               Mean episode length: 250.00
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 11.30s
                        Total time: 2129.46s
                               ETA: 1436711.2s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.157s, learning 0.196s)
               Value function loss: 29.6547
                    Surrogate loss: 0.0012
             Mean action noise std: 0.80
                       Mean reward: 299.00
               Mean episode length: 250.00
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 11.35s
                        Total time: 2140.82s
                               ETA: 1434662.3s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.471s, learning 0.192s)
               Value function loss: 22.3221
                    Surrogate loss: -0.0114
             Mean action noise std: 0.80
                       Mean reward: 305.38
               Mean episode length: 250.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 11.66s
                        Total time: 2152.48s
                               ETA: 1432846.9s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.725s, learning 0.161s)
               Value function loss: 16.8642
                    Surrogate loss: -0.0021
             Mean action noise std: 0.80
                       Mean reward: 304.08
               Mean episode length: 250.00
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 11.89s
                        Total time: 2164.36s
                               ETA: 1431203.0s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.162s)
               Value function loss: 10.7127
                    Surrogate loss: -0.0135
             Mean action noise std: 0.80
                       Mean reward: 302.32
               Mean episode length: 250.00
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 11.36s
                        Total time: 2175.72s
                               ETA: 1429233.4s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.460s, learning 0.160s)
               Value function loss: 4.9925
                    Surrogate loss: -0.0245
             Mean action noise std: 0.80
                       Mean reward: 301.59
               Mean episode length: 250.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 11.62s
                        Total time: 2187.34s
                               ETA: 1427460.8s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.781s, learning 0.227s)
               Value function loss: 2.2529
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: 305.51
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 11.01s
                        Total time: 2198.35s
                               ETA: 1425314.6s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.507s, learning 0.173s)
               Value function loss: 1.5215
                    Surrogate loss: -0.0064
             Mean action noise std: 0.80
                       Mean reward: 303.97
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 11.68s
                        Total time: 2210.03s
                               ETA: 1423628.8s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.159s)
               Value function loss: 1.2891
                    Surrogate loss: 0.0164
             Mean action noise std: 0.80
                       Mean reward: 304.46
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 11.63s
                        Total time: 2221.66s
                               ETA: 1421933.2s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.329s, learning 0.184s)
               Value function loss: 1.0363
                    Surrogate loss: -0.0006
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 11.51s
                        Total time: 2233.17s
                               ETA: 1420184.0s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.497s, learning 0.230s)
               Value function loss: 0.6755
                    Surrogate loss: 0.0879
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 11.73s
                        Total time: 2244.90s
                               ETA: 1418592.3s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.788s, learning 0.169s)
               Value function loss: 0.6400
                    Surrogate loss: 0.0098
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 11.96s
                        Total time: 2256.86s
                               ETA: 1417164.3s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.635s, learning 0.161s)
               Value function loss: 0.6272
                    Surrogate loss: 0.0202
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 11.80s
                        Total time: 2268.65s
                               ETA: 1415654.1s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.668s, learning 0.165s)
               Value function loss: 0.6751
                    Surrogate loss: 0.0220
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 11.83s
                        Total time: 2280.49s
                               ETA: 1414184.9s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.132s, learning 0.164s)
               Value function loss: 0.6619
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 11.30s
                        Total time: 2291.78s
                               ETA: 1412402.7s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.195s, learning 0.214s)
               Value function loss: 0.7269
                    Surrogate loss: 0.0013
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 11.41s
                        Total time: 2303.19s
                               ETA: 1410711.7s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.978s, learning 0.193s)
               Value function loss: 0.8053
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 302.72
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 12.17s
                        Total time: 2315.36s
                               ETA: 1409505.2s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.680s, learning 0.225s)
               Value function loss: 1.3052
                    Surrogate loss: 0.0140
             Mean action noise std: 0.80
                       Mean reward: 302.70
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 11.91s
                        Total time: 2327.27s
                               ETA: 1408152.2s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.382s, learning 0.188s)
               Value function loss: 0.7864
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 302.70
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 11.57s
                        Total time: 2338.84s
                               ETA: 1406613.6s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.676s, learning 0.160s)
               Value function loss: 1.2184
                    Surrogate loss: -0.0182
             Mean action noise std: 0.79
                       Mean reward: 301.77
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 11.84s
                        Total time: 2350.67s
                               ETA: 1405252.1s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.592s, learning 0.183s)
               Value function loss: 1.6440
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 301.20
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 11.77s
                        Total time: 2362.45s
                               ETA: 1403870.5s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.370s, learning 0.237s)
               Value function loss: 1.4330
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 300.61
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 11.61s
                        Total time: 2374.05s
                               ETA: 1402405.8s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.205s, learning 0.202s)
               Value function loss: 1.4451
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 300.06
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 11.41s
                        Total time: 2385.46s
                               ETA: 1400841.4s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.177s, learning 0.213s)
               Value function loss: 2.3962
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 298.12
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 11.39s
                        Total time: 2396.85s
                               ETA: 1399284.9s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.359s, learning 0.159s)
               Value function loss: 3.0322
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 296.95
               Mean episode length: 249.86
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 11.52s
                        Total time: 2408.37s
                               ETA: 1397820.7s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.516s, learning 0.198s)
               Value function loss: 2.0269
                    Surrogate loss: 0.0090
             Mean action noise std: 0.79
                       Mean reward: 294.98
               Mean episode length: 249.33
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 11.71s
                        Total time: 2420.08s
                               ETA: 1396486.7s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.162s)
               Value function loss: 2.3418
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 290.44
               Mean episode length: 248.69
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 11.55s
                        Total time: 2431.63s
                               ETA: 1395072.3s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.307s, learning 0.194s)
               Value function loss: 2.8902
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 285.72
               Mean episode length: 248.69
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 11.50s
                        Total time: 2443.13s
                               ETA: 1393647.5s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.349s, learning 0.170s)
               Value function loss: 4.4672
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 286.07
               Mean episode length: 249.84
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 11.52s
                        Total time: 2454.65s
                               ETA: 1392248.6s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.243s, learning 0.169s)
               Value function loss: 3.9410
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 283.71
               Mean episode length: 249.84
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 11.41s
                        Total time: 2466.07s
                               ETA: 1390805.4s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.475s, learning 0.162s)
               Value function loss: 7.5040
                    Surrogate loss: -0.0202
             Mean action noise std: 0.79
                       Mean reward: 281.53
               Mean episode length: 249.75
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 11.64s
                        Total time: 2477.70s
                               ETA: 1389503.7s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.403s, learning 0.219s)
               Value function loss: 9.7695
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 281.79
               Mean episode length: 249.74
                  Mean reward/step: 0.64
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 11.62s
                        Total time: 2489.32s
                               ETA: 1388208.1s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.481s, learning 0.165s)
               Value function loss: 10.2063
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 273.51
               Mean episode length: 249.70
                  Mean reward/step: 0.60
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 11.65s
                        Total time: 2500.97s
                               ETA: 1386940.8s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.709s, learning 0.197s)
               Value function loss: 9.7492
                    Surrogate loss: -0.0193
             Mean action noise std: 0.79
                       Mean reward: 274.32
               Mean episode length: 249.81
                  Mean reward/step: 0.61
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 11.91s
                        Total time: 2512.88s
                               ETA: 1385830.3s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.434s, learning 0.164s)
               Value function loss: 7.1107
                    Surrogate loss: -0.0239
             Mean action noise std: 0.79
                       Mean reward: 273.01
               Mean episode length: 249.66
                  Mean reward/step: 0.66
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 11.60s
                        Total time: 2524.47s
                               ETA: 1384562.7s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.776s, learning 0.156s)
               Value function loss: 5.5201
                    Surrogate loss: -0.0295
             Mean action noise std: 0.79
                       Mean reward: 265.03
               Mean episode length: 249.68
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 11.93s
                        Total time: 2536.41s
                               ETA: 1383491.5s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.177s)
               Value function loss: 3.6868
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 271.13
               Mean episode length: 249.57
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 11.50s
                        Total time: 2547.91s
                               ETA: 1382199.1s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.622s, learning 0.165s)
               Value function loss: 2.0965
                    Surrogate loss: -0.0004
             Mean action noise std: 0.79
                       Mean reward: 271.14
               Mean episode length: 249.13
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 11.79s
                        Total time: 2559.70s
                               ETA: 1381073.7s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.631s, learning 0.159s)
               Value function loss: 1.4894
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 264.18
               Mean episode length: 248.99
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 11.79s
                        Total time: 2571.49s
                               ETA: 1379961.7s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.583s, learning 0.167s)
               Value function loss: 1.1717
                    Surrogate loss: 0.0084
             Mean action noise std: 0.79
                       Mean reward: 263.07
               Mean episode length: 249.11
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 11.75s
                        Total time: 2583.24s
                               ETA: 1378839.8s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1492 steps/s (collection: 10.785s, learning 0.192s)
               Value function loss: 1.0431
                    Surrogate loss: 0.0189
             Mean action noise std: 0.79
                       Mean reward: 262.58
               Mean episode length: 249.33
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 10.98s
                        Total time: 2594.21s
                               ETA: 1377319.7s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.484s, learning 0.165s)
               Value function loss: 0.8358
                    Surrogate loss: 0.0053
             Mean action noise std: 0.79
                       Mean reward: 262.58
               Mean episode length: 249.33
                  Mean reward/step: 1.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 11.65s
                        Total time: 2605.86s
                               ETA: 1376170.8s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.335s, learning 0.167s)
               Value function loss: 0.6886
                    Surrogate loss: 0.0013
             Mean action noise std: 0.79
                       Mean reward: 262.58
               Mean episode length: 249.33
                  Mean reward/step: 1.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 11.50s
                        Total time: 2617.36s
                               ETA: 1374956.1s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.757s, learning 0.166s)
               Value function loss: 0.6074
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 262.58
               Mean episode length: 249.33
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 11.92s
                        Total time: 2629.29s
                               ETA: 1373974.6s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.376s, learning 0.323s)
               Value function loss: 0.6893
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 262.58
               Mean episode length: 249.33
                  Mean reward/step: 1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 11.70s
                        Total time: 2640.99s
                               ETA: 1372886.5s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1339 steps/s (collection: 11.983s, learning 0.249s)
               Value function loss: 0.7344
                    Surrogate loss: -0.0181
             Mean action noise std: 0.79
                       Mean reward: 262.34
               Mean episode length: 248.91
                  Mean reward/step: 1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 12.23s
                        Total time: 2653.22s
                               ETA: 1372085.0s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.161s)
               Value function loss: 0.7712
                    Surrogate loss: -0.0053
             Mean action noise std: 0.79
                       Mean reward: 261.91
               Mean episode length: 248.44
                  Mean reward/step: 1.30
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 11.45s
                        Total time: 2664.67s
                               ETA: 1370888.5s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.520s, learning 0.190s)
               Value function loss: 1.1679
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 258.37
               Mean episode length: 245.45
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 11.71s
                        Total time: 2676.38s
                               ETA: 1369838.1s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.163s)
               Value function loss: 1.5515
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 259.41
               Mean episode length: 245.51
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 11.55s
                        Total time: 2687.92s
                               ETA: 1368715.6s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.500s, learning 0.197s)
               Value function loss: 0.9469
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 259.23
               Mean episode length: 245.51
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 11.70s
                        Total time: 2699.62s
                               ETA: 1367679.7s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.170s, learning 0.198s)
               Value function loss: 0.9658
                    Surrogate loss: -0.0230
             Mean action noise std: 0.79
                       Mean reward: 257.52
               Mean episode length: 244.50
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 11.37s
                        Total time: 2710.99s
                               ETA: 1366488.6s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.524s, learning 0.190s)
               Value function loss: 0.9300
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 254.21
               Mean episode length: 242.34
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 11.71s
                        Total time: 2722.70s
                               ETA: 1365482.9s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.911s, learning 0.165s)
               Value function loss: 1.0517
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 251.31
               Mean episode length: 240.09
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 12.08s
                        Total time: 2734.78s
                               ETA: 1364668.3s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.930s, learning 0.162s)
               Value function loss: 0.9237
                    Surrogate loss: -0.0314
             Mean action noise std: 0.79
                       Mean reward: 249.55
               Mean episode length: 238.52
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 11.09s
                        Total time: 2745.87s
                               ETA: 1363372.6s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.892s, learning 0.170s)
               Value function loss: 1.1768
                    Surrogate loss: -0.0218
             Mean action noise std: 0.79
                       Mean reward: 243.90
               Mean episode length: 235.03
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 12.06s
                        Total time: 2757.93s
                               ETA: 1362568.9s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.318s, learning 0.163s)
               Value function loss: 0.9097
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 244.18
               Mean episode length: 234.91
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 11.48s
                        Total time: 2769.41s
                               ETA: 1361487.4s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.363s, learning 0.163s)
               Value function loss: 1.0193
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 238.55
               Mean episode length: 233.01
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 11.53s
                        Total time: 2780.94s
                               ETA: 1360438.2s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.346s, learning 0.164s)
               Value function loss: 1.4977
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 230.54
               Mean episode length: 230.13
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 11.51s
                        Total time: 2792.45s
                               ETA: 1359391.4s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.320s, learning 0.158s)
               Value function loss: 1.6337
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 232.97
               Mean episode length: 236.53
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 11.48s
                        Total time: 2803.93s
                               ETA: 1358339.4s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.304s, learning 0.223s)
               Value function loss: 2.0483
                    Surrogate loss: -0.0167
             Mean action noise std: 0.79
                       Mean reward: 238.05
               Mean episode length: 242.49
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 11.53s
                        Total time: 2815.45s
                               ETA: 1357321.2s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.328s, learning 0.197s)
               Value function loss: 2.4379
                    Surrogate loss: -0.0171
             Mean action noise std: 0.79
                       Mean reward: 244.38
               Mean episode length: 245.74
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 11.52s
                        Total time: 2826.98s
                               ETA: 1356311.2s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.185s, learning 0.178s)
               Value function loss: 3.7983
                    Surrogate loss: -0.0239
             Mean action noise std: 0.79
                       Mean reward: 252.20
               Mean episode length: 249.67
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 11.36s
                        Total time: 2838.34s
                               ETA: 1355233.5s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.101s, learning 0.172s)
               Value function loss: 7.8031
                    Surrogate loss: -0.0227
             Mean action noise std: 0.79
                       Mean reward: 258.30
               Mean episode length: 249.79
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 11.27s
                        Total time: 2849.61s
                               ETA: 1354123.2s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.470s, learning 0.188s)
               Value function loss: 11.6604
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 264.81
               Mean episode length: 250.00
                  Mean reward/step: 0.77
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 11.66s
                        Total time: 2861.27s
                               ETA: 1353205.5s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.116s, learning 0.167s)
               Value function loss: 14.1450
                    Surrogate loss: 0.0001
             Mean action noise std: 0.79
                       Mean reward: 265.16
               Mean episode length: 249.80
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 11.28s
                        Total time: 2872.56s
                               ETA: 1352120.0s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.585s, learning 0.177s)
               Value function loss: 13.1885
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 272.38
               Mean episode length: 250.00
                  Mean reward/step: 0.78
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 11.76s
                        Total time: 2884.32s
                               ETA: 1351269.1s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.296s, learning 0.190s)
               Value function loss: 8.8691
                    Surrogate loss: -0.0266
             Mean action noise std: 0.79
                       Mean reward: 277.46
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 11.49s
                        Total time: 2895.80s
                               ETA: 1350297.1s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.009s, learning 0.206s)
               Value function loss: 5.6792
                    Surrogate loss: -0.0260
             Mean action noise std: 0.79
                       Mean reward: 275.24
               Mean episode length: 250.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 12.22s
                        Total time: 2908.02s
                               ETA: 1349672.7s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.348s, learning 0.160s)
               Value function loss: 2.6959
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 283.59
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 11.51s
                        Total time: 2919.53s
                               ETA: 1348727.2s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.152s, learning 0.199s)
               Value function loss: 1.5784
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 283.03
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 11.35s
                        Total time: 2930.88s
                               ETA: 1347717.9s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.635s, learning 0.162s)
               Value function loss: 1.3040
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 287.91
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 11.80s
                        Total time: 2942.67s
                               ETA: 1346921.7s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.430s, learning 0.158s)
               Value function loss: 0.9651
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 291.37
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 11.59s
                        Total time: 2954.26s
                               ETA: 1346037.7s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.708s, learning 0.285s)
               Value function loss: 0.7137
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 291.37
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 11.99s
                        Total time: 2966.26s
                               ETA: 1345345.4s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.332s, learning 0.175s)
               Value function loss: 1.0392
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 291.37
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 11.51s
                        Total time: 2977.76s
                               ETA: 1344439.8s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.276s, learning 0.361s)
               Value function loss: 0.5207
                    Surrogate loss: 0.1766
             Mean action noise std: 0.79
                       Mean reward: 291.37
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 11.64s
                        Total time: 2989.40s
                               ETA: 1343600.2s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.182s, learning 0.259s)
               Value function loss: 0.3407
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: 291.37
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 11.44s
                        Total time: 3000.84s
                               ETA: 1342680.8s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.483s, learning 0.173s)
               Value function loss: 0.4506
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 292.01
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 11.66s
                        Total time: 3012.50s
                               ETA: 1341865.2s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.571s, learning 0.167s)
               Value function loss: 0.7073
                    Surrogate loss: 0.0390
             Mean action noise std: 0.79
                       Mean reward: 292.57
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 11.74s
                        Total time: 3024.23s
                               ETA: 1341093.0s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.334s, learning 0.198s)
               Value function loss: 0.6983
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: 294.22
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 11.53s
                        Total time: 3035.77s
                               ETA: 1340236.7s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.109s, learning 0.174s)
               Value function loss: 0.8392
                    Surrogate loss: 0.0096
             Mean action noise std: 0.79
                       Mean reward: 297.01
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 11.28s
                        Total time: 3047.05s
                               ETA: 1339278.4s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.259s, learning 0.201s)
               Value function loss: 0.7807
                    Surrogate loss: -0.0001
             Mean action noise std: 0.79
                       Mean reward: 299.51
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 11.46s
                        Total time: 3058.51s
                               ETA: 1338406.0s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.434s, learning 0.193s)
               Value function loss: 0.6277
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 299.30
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 11.63s
                        Total time: 3070.13s
                               ETA: 1337613.5s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.333s, learning 0.180s)
               Value function loss: 1.0219
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 302.81
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 11.51s
                        Total time: 3081.65s
                               ETA: 1336778.7s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.252s, learning 0.204s)
               Value function loss: 0.7868
                    Surrogate loss: 0.0042
             Mean action noise std: 0.79
                       Mean reward: 303.49
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 11.46s
                        Total time: 3093.10s
                               ETA: 1335926.3s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.481s, learning 0.169s)
               Value function loss: 1.3549
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 308.68
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 11.65s
                        Total time: 3104.75s
                               ETA: 1335164.9s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.366s, learning 0.215s)
               Value function loss: 1.2784
                    Surrogate loss: -0.0194
             Mean action noise std: 0.79
                       Mean reward: 310.89
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 11.58s
                        Total time: 3116.34s
                               ETA: 1334380.3s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.669s, learning 0.198s)
               Value function loss: 1.5484
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 315.11
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 11.87s
                        Total time: 3128.20s
                               ETA: 1333724.0s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.622s, learning 0.167s)
               Value function loss: 1.4476
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 317.71
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 11.79s
                        Total time: 3139.99s
                               ETA: 1333040.2s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.649s, learning 0.217s)
               Value function loss: 3.2011
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 324.61
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 11.87s
                        Total time: 3151.86s
                               ETA: 1332394.5s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.672s, learning 0.189s)
               Value function loss: 4.1351
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 326.87
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 11.86s
                        Total time: 3163.72s
                               ETA: 1331752.2s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.389s, learning 0.200s)
               Value function loss: 5.6194
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 329.07
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 11.59s
                        Total time: 3175.31s
                               ETA: 1331001.3s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.746s, learning 0.178s)
               Value function loss: 6.6505
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 329.39
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 11.92s
                        Total time: 3187.23s
                               ETA: 1330396.2s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.167s)
               Value function loss: 8.8308
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 328.51
               Mean episode length: 249.67
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 11.27s
                        Total time: 3198.50s
                               ETA: 1329524.8s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.708s, learning 0.195s)
               Value function loss: 22.9520
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 332.19
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 11.90s
                        Total time: 3210.41s
                               ETA: 1328922.1s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.730s, learning 0.168s)
               Value function loss: 35.5010
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 332.86
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 11.90s
                        Total time: 3222.31s
                               ETA: 1328322.3s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.404s, learning 0.159s)
               Value function loss: 42.6317
                    Surrogate loss: 0.0037
             Mean action noise std: 0.79
                       Mean reward: 335.18
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 11.56s
                        Total time: 3233.87s
                               ETA: 1327589.5s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.377s, learning 0.182s)
               Value function loss: 33.3133
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 332.77
               Mean episode length: 250.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 11.56s
                        Total time: 3245.43s
                               ETA: 1326861.0s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.432s, learning 0.163s)
               Value function loss: 29.0773
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 335.68
               Mean episode length: 250.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 11.60s
                        Total time: 3257.02s
                               ETA: 1326153.1s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.529s, learning 0.164s)
               Value function loss: 12.3004
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 333.88
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 11.69s
                        Total time: 3268.72s
                               ETA: 1325490.9s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.268s, learning 0.181s)
               Value function loss: 6.3152
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 334.97
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 11.45s
                        Total time: 3280.17s
                               ETA: 1324735.2s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.591s, learning 0.169s)
               Value function loss: 2.9169
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 335.65
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 11.76s
                        Total time: 3291.93s
                               ETA: 1324110.8s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.049s, learning 0.233s)
               Value function loss: 2.2743
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 336.11
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 11.28s
                        Total time: 3303.21s
                               ETA: 1323299.4s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.369s, learning 0.171s)
               Value function loss: 1.7746
                    Surrogate loss: -0.0169
             Mean action noise std: 0.79
                       Mean reward: 336.52
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 11.54s
                        Total time: 3314.75s
                               ETA: 1322597.5s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.203s, learning 0.176s)
               Value function loss: 0.7034
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 336.52
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 11.38s
                        Total time: 3326.13s
                               ETA: 1321836.9s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.525s, learning 0.192s)
               Value function loss: 0.4744
                    Surrogate loss: 0.0023
             Mean action noise std: 0.79
                       Mean reward: 336.52
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 11.72s
                        Total time: 3337.84s
                               ETA: 1321216.3s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.638s, learning 0.157s)
               Value function loss: 0.4477
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 336.52
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 11.80s
                        Total time: 3349.64s
                               ETA: 1320631.2s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.320s, learning 0.232s)
               Value function loss: 0.3627
                    Surrogate loss: 0.0040
             Mean action noise std: 0.79
                       Mean reward: 336.52
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 11.55s
                        Total time: 3361.19s
                               ETA: 1319955.2s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.277s, learning 0.193s)
               Value function loss: 0.4275
                    Surrogate loss: 0.0181
             Mean action noise std: 0.79
                       Mean reward: 336.52
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 11.47s
                        Total time: 3372.66s
                               ETA: 1319252.2s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.571s, learning 0.192s)
               Value function loss: 0.7788
                    Surrogate loss: -0.0047
             Mean action noise std: 0.79
                       Mean reward: 335.26
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 11.76s
                        Total time: 3384.42s
                               ETA: 1318668.9s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1469 steps/s (collection: 10.986s, learning 0.162s)
               Value function loss: 0.5629
                    Surrogate loss: 0.0133
             Mean action noise std: 0.79
                       Mean reward: 336.39
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 11.15s
                        Total time: 3395.57s
                               ETA: 1317851.2s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.041s, learning 0.200s)
               Value function loss: 0.9057
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 336.83
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 11.24s
                        Total time: 3406.81s
                               ETA: 1317075.7s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.559s, learning 0.206s)
               Value function loss: 0.9031
                    Surrogate loss: 0.0006
             Mean action noise std: 0.79
                       Mean reward: 336.99
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 11.77s
                        Total time: 3418.58s
                               ETA: 1316508.2s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.046s, learning 0.160s)
               Value function loss: 0.5447
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 337.23
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 11.21s
                        Total time: 3429.78s
                               ETA: 1315730.3s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.254s, learning 0.171s)
               Value function loss: 1.4575
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 338.03
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 11.42s
                        Total time: 3441.21s
                               ETA: 1315041.8s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.931s, learning 0.198s)
               Value function loss: 1.1683
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 337.96
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 11.13s
                        Total time: 3452.34s
                               ETA: 1314246.1s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.593s, learning 0.224s)
               Value function loss: 1.8417
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 336.91
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 11.82s
                        Total time: 3464.15s
                               ETA: 1313717.4s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.916s, learning 0.161s)
               Value function loss: 1.3980
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 335.21
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 11.08s
                        Total time: 3475.23s
                               ETA: 1312912.9s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.434s, learning 0.188s)
               Value function loss: 1.6007
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 330.80
               Mean episode length: 249.25
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 11.62s
                        Total time: 3486.85s
                               ETA: 1312319.6s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.663s, learning 0.203s)
               Value function loss: 1.6842
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 328.23
               Mean episode length: 249.25
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 11.87s
                        Total time: 3498.72s
                               ETA: 1311821.8s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.221s, learning 0.158s)
               Value function loss: 2.8887
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 323.44
               Mean episode length: 249.25
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 11.38s
                        Total time: 3510.10s
                               ETA: 1311146.0s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.197s, learning 0.164s)
               Value function loss: 4.3995
                    Surrogate loss: -0.0030
             Mean action noise std: 0.79
                       Mean reward: 322.15
               Mean episode length: 249.25
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 11.36s
                        Total time: 3521.46s
                               ETA: 1310468.6s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1534 steps/s (collection: 10.514s, learning 0.162s)
               Value function loss: 5.0637
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 318.07
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 10.68s
                        Total time: 3532.13s
                               ETA: 1309542.2s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.829s, learning 0.164s)
               Value function loss: 5.4180
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 320.86
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 11.99s
                        Total time: 3544.13s
                               ETA: 1309108.9s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.289s, learning 0.164s)
               Value function loss: 7.6011
                    Surrogate loss: -0.0095
             Mean action noise std: 0.79
                       Mean reward: 319.61
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 11.45s
                        Total time: 3555.58s
                               ETA: 1308480.0s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.356s, learning 0.236s)
               Value function loss: 18.8172
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 327.50
               Mean episode length: 249.83
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 11.59s
                        Total time: 3567.17s
                               ETA: 1307906.6s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.491s, learning 0.186s)
               Value function loss: 30.2326
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 326.78
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 11.68s
                        Total time: 3578.85s
                               ETA: 1307368.2s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.026s, learning 0.163s)
               Value function loss: 45.3608
                    Surrogate loss: 0.0014
             Mean action noise std: 0.79
                       Mean reward: 335.26
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 12.19s
                        Total time: 3591.04s
                               ETA: 1307020.0s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.312s, learning 0.184s)
               Value function loss: 34.4249
                    Surrogate loss: 0.0043
             Mean action noise std: 0.79
                       Mean reward: 336.67
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 11.50s
                        Total time: 3602.53s
                               ETA: 1306422.9s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.182s, learning 0.171s)
               Value function loss: 26.2488
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 340.98
               Mean episode length: 250.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 11.35s
                        Total time: 3613.89s
                               ETA: 1305778.7s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.209s, learning 0.184s)
               Value function loss: 19.4039
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 344.53
               Mean episode length: 250.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 11.39s
                        Total time: 3625.28s
                               ETA: 1305153.5s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.449s, learning 0.171s)
               Value function loss: 7.7609
                    Surrogate loss: -0.0183
             Mean action noise std: 0.79
                       Mean reward: 346.31
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 11.62s
                        Total time: 3636.90s
                               ETA: 1304613.9s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.972s, learning 0.209s)
               Value function loss: 2.9471
                    Surrogate loss: -0.0163
             Mean action noise std: 0.79
                       Mean reward: 345.24
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 11.18s
                        Total time: 3648.08s
                               ETA: 1303921.3s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.160s, learning 0.164s)
               Value function loss: 1.6965
                    Surrogate loss: 0.0018
             Mean action noise std: 0.79
                       Mean reward: 344.74
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 11.32s
                        Total time: 3659.41s
                               ETA: 1303284.6s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.408s, learning 0.187s)
               Value function loss: 1.1635
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 343.33
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 11.60s
                        Total time: 3671.00s
                               ETA: 1302748.4s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.683s, learning 0.169s)
               Value function loss: 0.6570
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 343.50
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 11.85s
                        Total time: 3682.85s
                               ETA: 1302306.7s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.664s, learning 0.163s)
               Value function loss: 0.6917
                    Surrogate loss: 0.0139
             Mean action noise std: 0.79
                       Mean reward: 343.50
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 11.83s
                        Total time: 3694.68s
                               ETA: 1301859.0s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.035s, learning 0.166s)
               Value function loss: 0.3954
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 343.50
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 11.20s
                        Total time: 3705.88s
                               ETA: 1301195.0s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.816s, learning 0.204s)
               Value function loss: 0.3557
                    Surrogate loss: 0.0078
             Mean action noise std: 0.79
                       Mean reward: 343.50
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 12.02s
                        Total time: 3717.90s
                               ETA: 1300822.2s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.900s, learning 0.268s)
               Value function loss: 0.3771
                    Surrogate loss: 0.0137
             Mean action noise std: 0.79
                       Mean reward: 343.50
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 12.17s
                        Total time: 3730.07s
                               ETA: 1300503.5s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.961s, learning 0.202s)
               Value function loss: 0.4406
                    Surrogate loss: 0.0216
             Mean action noise std: 0.79
                       Mean reward: 343.28
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 11.16s
                        Total time: 3741.23s
                               ETA: 1299837.7s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.811s, learning 0.227s)
               Value function loss: 0.5275
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: 343.49
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 12.04s
                        Total time: 3753.27s
                               ETA: 1299479.0s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.480s, learning 0.192s)
               Value function loss: 0.9371
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 342.77
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 11.67s
                        Total time: 3764.94s
                               ETA: 1298996.7s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.962s, learning 0.170s)
               Value function loss: 1.5771
                    Surrogate loss: 0.0028
             Mean action noise std: 0.79
                       Mean reward: 344.86
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 11.13s
                        Total time: 3776.08s
                               ETA: 1298331.8s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.371s, learning 0.196s)
               Value function loss: 0.4686
                    Surrogate loss: -0.0056
             Mean action noise std: 0.79
                       Mean reward: 344.86
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 11.57s
                        Total time: 3787.64s
                               ETA: 1297820.5s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.423s, learning 0.204s)
               Value function loss: 1.6922
                    Surrogate loss: 0.0024
             Mean action noise std: 0.79
                       Mean reward: 346.06
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 11.63s
                        Total time: 3799.27s
                               ETA: 1297333.2s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.190s, learning 0.194s)
               Value function loss: 1.3908
                    Surrogate loss: 0.0062
             Mean action noise std: 0.79
                       Mean reward: 346.66
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 11.38s
                        Total time: 3810.65s
                               ETA: 1296766.6s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.414s, learning 0.292s)
               Value function loss: 2.0518
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 347.59
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 11.71s
                        Total time: 3822.36s
                               ETA: 1296312.9s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.447s, learning 0.174s)
               Value function loss: 1.8049
                    Surrogate loss: -0.0058
             Mean action noise std: 0.79
                       Mean reward: 346.84
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 11.62s
                        Total time: 3833.98s
                               ETA: 1295833.2s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1471 steps/s (collection: 10.938s, learning 0.196s)
               Value function loss: 2.2335
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 346.48
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 11.13s
                        Total time: 3845.11s
                               ETA: 1295193.0s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.474s, learning 0.162s)
               Value function loss: 2.1907
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 346.19
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 11.64s
                        Total time: 3856.75s
                               ETA: 1294725.2s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.541s, learning 0.168s)
               Value function loss: 2.6308
                    Surrogate loss: 0.0003
             Mean action noise std: 0.79
                       Mean reward: 347.44
               Mean episode length: 249.60
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 11.71s
                        Total time: 3868.46s
                               ETA: 1294285.1s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.174s, learning 0.198s)
               Value function loss: 4.1232
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 346.21
               Mean episode length: 249.60
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 11.37s
                        Total time: 3879.83s
                               ETA: 1293735.5s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.523s, learning 0.192s)
               Value function loss: 4.7668
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 338.60
               Mean episode length: 249.60
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 11.71s
                        Total time: 3891.55s
                               ETA: 1293303.2s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.651s, learning 0.185s)
               Value function loss: 7.4534
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 337.16
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 11.84s
                        Total time: 3903.38s
                               ETA: 1292914.2s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.295s, learning 0.164s)
               Value function loss: 7.8777
                    Surrogate loss: -0.0004
             Mean action noise std: 0.79
                       Mean reward: 337.34
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 11.46s
                        Total time: 3914.84s
                               ETA: 1292403.0s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.460s, learning 0.192s)
               Value function loss: 16.6924
                    Surrogate loss: 0.0070
             Mean action noise std: 0.79
                       Mean reward: 333.59
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 11.65s
                        Total time: 3926.49s
                               ETA: 1291958.3s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.584s, learning 0.167s)
               Value function loss: 31.7700
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 339.65
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 11.75s
                        Total time: 3938.24s
                               ETA: 1291549.3s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.212s, learning 0.170s)
               Value function loss: 40.4052
                    Surrogate loss: -0.0010
             Mean action noise std: 0.79
                       Mean reward: 340.85
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 11.38s
                        Total time: 3949.62s
                               ETA: 1291022.3s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.538s, learning 0.193s)
               Value function loss: 41.5479
                    Surrogate loss: 0.0041
             Mean action noise std: 0.79
                       Mean reward: 346.42
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 11.73s
                        Total time: 3961.36s
                               ETA: 1290612.5s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.302s, learning 0.169s)
               Value function loss: 33.3342
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 344.84
               Mean episode length: 250.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 11.47s
                        Total time: 3972.83s
                               ETA: 1290120.7s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.375s, learning 0.187s)
               Value function loss: 20.9642
                    Surrogate loss: -0.0170
             Mean action noise std: 0.79
                       Mean reward: 347.29
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 11.56s
                        Total time: 3984.39s
                               ETA: 1289661.3s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.306s, learning 0.197s)
               Value function loss: 11.4411
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 348.53
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 11.50s
                        Total time: 3995.89s
                               ETA: 1289185.9s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.457s, learning 0.227s)
               Value function loss: 3.8744
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 351.40
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 11.68s
                        Total time: 4007.58s
                               ETA: 1288771.8s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.785s, learning 0.162s)
               Value function loss: 2.4700
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 352.87
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 11.95s
                        Total time: 4019.52s
                               ETA: 1288444.4s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.069s, learning 0.159s)
               Value function loss: 1.3362
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 352.93
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 11.23s
                        Total time: 4030.75s
                               ETA: 1287889.2s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.196s, learning 0.195s)
               Value function loss: 1.1029
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 353.92
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 11.39s
                        Total time: 4042.14s
                               ETA: 1287389.7s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.601s, learning 0.157s)
               Value function loss: 0.3414
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 353.92
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 11.76s
                        Total time: 4053.90s
                               ETA: 1287009.7s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.027s, learning 0.177s)
               Value function loss: 0.2991
                    Surrogate loss: 0.0025
             Mean action noise std: 0.79
                       Mean reward: 353.92
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 11.20s
                        Total time: 4065.10s
                               ETA: 1286456.8s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.103s, learning 0.176s)
               Value function loss: 0.2602
                    Surrogate loss: -0.0024
             Mean action noise std: 0.79
                       Mean reward: 353.92
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 11.28s
                        Total time: 4076.38s
                               ETA: 1285930.8s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.537s, learning 0.166s)
               Value function loss: 0.2702
                    Surrogate loss: 0.0078
             Mean action noise std: 0.79
                       Mean reward: 353.92
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 11.70s
                        Total time: 4088.08s
                               ETA: 1285541.5s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.220s, learning 0.265s)
               Value function loss: 0.3986
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 353.99
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 11.48s
                        Total time: 4099.57s
                               ETA: 1285086.1s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.649s, learning 0.260s)
               Value function loss: 0.3750
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 354.20
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 11.91s
                        Total time: 4111.48s
                               ETA: 1284766.0s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.453s, learning 0.204s)
               Value function loss: 1.1161
                    Surrogate loss: -0.0024
             Mean action noise std: 0.79
                       Mean reward: 354.17
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 11.66s
                        Total time: 4123.14s
                               ETA: 1284369.5s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.587s, learning 0.164s)
               Value function loss: 1.4255
                    Surrogate loss: -0.0001
             Mean action noise std: 0.79
                       Mean reward: 354.57
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 11.75s
                        Total time: 4134.89s
                               ETA: 1284004.4s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.273s, learning 0.217s)
               Value function loss: 0.4656
                    Surrogate loss: 0.0027
             Mean action noise std: 0.79
                       Mean reward: 355.00
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 11.49s
                        Total time: 4146.38s
                               ETA: 1283560.9s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.159s)
               Value function loss: 1.5397
                    Surrogate loss: -0.0007
             Mean action noise std: 0.79
                       Mean reward: 354.79
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 11.37s
                        Total time: 4157.74s
                               ETA: 1283081.5s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.293s, learning 0.167s)
               Value function loss: 0.9568
                    Surrogate loss: -0.0014
             Mean action noise std: 0.79
                       Mean reward: 355.79
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 11.46s
                        Total time: 4169.20s
                               ETA: 1282634.2s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.088s, learning 0.193s)
               Value function loss: 2.3458
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 357.03
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 11.28s
                        Total time: 4180.48s
                               ETA: 1282134.7s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.354s, learning 0.168s)
               Value function loss: 1.6583
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 357.59
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 11.52s
                        Total time: 4192.00s
                               ETA: 1281711.9s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.513s, learning 0.168s)
               Value function loss: 3.4559
                    Surrogate loss: 0.0004
             Mean action noise std: 0.79
                       Mean reward: 358.46
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 11.68s
                        Total time: 4203.69s
                               ETA: 1281339.8s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.364s, learning 0.245s)
               Value function loss: 1.8148
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 359.66
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 11.61s
                        Total time: 4215.29s
                               ETA: 1280948.3s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.401s, learning 0.157s)
               Value function loss: 2.8738
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 358.48
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 11.56s
                        Total time: 4226.85s
                               ETA: 1280543.6s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.160s, learning 0.204s)
               Value function loss: 5.1517
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 358.82
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 11.36s
                        Total time: 4238.22s
                               ETA: 1280082.7s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.522s, learning 0.317s)
               Value function loss: 7.4002
                    Surrogate loss: -0.0064
             Mean action noise std: 0.79
                       Mean reward: 352.84
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 11.84s
                        Total time: 4250.05s
                               ETA: 1279767.3s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.205s, learning 0.159s)
               Value function loss: 10.4491
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 350.03
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 11.36s
                        Total time: 4261.42s
                               ETA: 1279311.1s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.164s)
               Value function loss: 11.0853
                    Surrogate loss: 0.0002
             Mean action noise std: 0.79
                       Mean reward: 352.23
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 11.22s
                        Total time: 4272.64s
                               ETA: 1278816.0s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.316s, learning 0.224s)
               Value function loss: 18.6720
                    Surrogate loss: 0.0044
             Mean action noise std: 0.79
                       Mean reward: 346.67
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 11.54s
                        Total time: 4284.18s
                               ETA: 1278417.8s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.906s, learning 0.191s)
               Value function loss: 35.4025
                    Surrogate loss: 0.0010
             Mean action noise std: 0.79
                       Mean reward: 347.61
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 12.10s
                        Total time: 4296.28s
                               ETA: 1278187.9s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.271s, learning 0.162s)
               Value function loss: 45.9340
                    Surrogate loss: 0.0108
             Mean action noise std: 0.79
                       Mean reward: 345.53
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 11.43s
                        Total time: 4307.71s
                               ETA: 1277762.3s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.346s, learning 0.177s)
               Value function loss: 45.6437
                    Surrogate loss: 0.0015
             Mean action noise std: 0.79
                       Mean reward: 343.12
               Mean episode length: 250.00
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 11.52s
                        Total time: 4319.23s
                               ETA: 1277365.7s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.087s, learning 0.164s)
               Value function loss: 40.8847
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 343.88
               Mean episode length: 250.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 11.25s
                        Total time: 4330.49s
                               ETA: 1276891.2s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.283s, learning 0.184s)
               Value function loss: 31.2993
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 342.81
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 11.47s
                        Total time: 4341.95s
                               ETA: 1276482.8s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1472 steps/s (collection: 10.948s, learning 0.177s)
               Value function loss: 14.6124
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 342.10
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 11.12s
                        Total time: 4353.08s
                               ETA: 1275976.5s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.127s, learning 0.158s)
               Value function loss: 5.7845
                    Surrogate loss: -0.0117
             Mean action noise std: 0.79
                       Mean reward: 342.55
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 11.28s
                        Total time: 4364.36s
                               ETA: 1275519.9s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.327s, learning 0.162s)
               Value function loss: 3.3892
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 343.34
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 11.49s
                        Total time: 4375.85s
                               ETA: 1275125.7s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.108s, learning 0.267s)
               Value function loss: 3.1579
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 340.50
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 11.37s
                        Total time: 4387.23s
                               ETA: 1274700.1s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.168s, learning 0.165s)
               Value function loss: 1.7760
                    Surrogate loss: -0.0152
             Mean action noise std: 0.79
                       Mean reward: 337.87
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 11.33s
                        Total time: 4398.56s
                               ETA: 1274265.0s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.424s, learning 0.173s)
               Value function loss: 0.7945
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 337.87
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 11.60s
                        Total time: 4410.16s
                               ETA: 1273908.6s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.164s)
               Value function loss: 0.7018
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 337.87
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 11.42s
                        Total time: 4421.58s
                               ETA: 1273503.2s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.179s, learning 0.190s)
               Value function loss: 0.5398
                    Surrogate loss: -0.0052
             Mean action noise std: 0.79
                       Mean reward: 337.87
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 11.37s
                        Total time: 4432.95s
                               ETA: 1273085.6s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.595s, learning 0.164s)
               Value function loss: 0.5863
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 337.87
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 11.76s
                        Total time: 4444.70s
                               ETA: 1272782.1s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.094s, learning 0.195s)
               Value function loss: 0.5952
                    Surrogate loss: 0.0260
             Mean action noise std: 0.79
                       Mean reward: 338.36
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 11.29s
                        Total time: 4455.99s
                               ETA: 1272346.0s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.416s, learning 0.160s)
               Value function loss: 0.6246
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 11.58s
                        Total time: 4467.57s
                               ETA: 1271994.0s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1477 steps/s (collection: 10.911s, learning 0.175s)
               Value function loss: 0.8562
                    Surrogate loss: 0.0101
             Mean action noise std: 0.79
                       Mean reward: 336.49
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 11.09s
                        Total time: 4478.66s
                               ETA: 1271504.7s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.280s, learning 0.186s)
               Value function loss: 1.0197
                    Surrogate loss: -0.0019
             Mean action noise std: 0.79
                       Mean reward: 334.96
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 11.47s
                        Total time: 4490.12s
                               ETA: 1271125.8s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.277s, learning 0.165s)
               Value function loss: 0.8707
                    Surrogate loss: -0.0029
             Mean action noise std: 0.79
                       Mean reward: 334.04
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 11.44s
                        Total time: 4501.57s
                               ETA: 1270742.2s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.170s, learning 0.176s)
               Value function loss: 0.8693
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 332.59
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 11.35s
                        Total time: 4512.91s
                               ETA: 1270333.6s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.157s, learning 0.187s)
               Value function loss: 1.2914
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 330.19
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 11.34s
                        Total time: 4524.26s
                               ETA: 1269926.6s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.538s, learning 0.164s)
               Value function loss: 1.2978
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 330.76
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 11.70s
                        Total time: 4535.96s
                               ETA: 1269622.1s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.918s, learning 0.198s)
               Value function loss: 1.5403
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 325.62
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 11.12s
                        Total time: 4547.07s
                               ETA: 1269155.7s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.403s, learning 0.159s)
               Value function loss: 1.8339
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 321.94
               Mean episode length: 249.43
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 11.56s
                        Total time: 4558.64s
                               ETA: 1268816.0s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.290s, learning 0.179s)
               Value function loss: 2.0079
                    Surrogate loss: -0.0202
             Mean action noise std: 0.78
                       Mean reward: 314.81
               Mean episode length: 248.61
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 11.47s
                        Total time: 4570.11s
                               ETA: 1268452.5s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.167s)
               Value function loss: 2.5298
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 314.03
               Mean episode length: 248.24
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 11.24s
                        Total time: 4581.35s
                               ETA: 1268028.1s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.278s, learning 0.163s)
               Value function loss: 3.6772
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 308.99
               Mean episode length: 248.24
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 11.44s
                        Total time: 4592.79s
                               ETA: 1267660.9s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.123s, learning 0.159s)
               Value function loss: 5.1509
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 310.88
               Mean episode length: 248.24
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 11.28s
                        Total time: 4604.07s
                               ETA: 1267251.7s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.465s, learning 0.261s)
               Value function loss: 7.1620
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 317.45
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 11.73s
                        Total time: 4615.80s
                               ETA: 1266966.7s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1467 steps/s (collection: 10.984s, learning 0.179s)
               Value function loss: 6.7965
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 319.27
               Mean episode length: 249.55
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 11.16s
                        Total time: 4626.96s
                               ETA: 1266529.1s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.041s, learning 0.171s)
               Value function loss: 10.1895
                    Surrogate loss: 0.0062
             Mean action noise std: 0.78
                       Mean reward: 315.88
               Mean episode length: 249.55
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 11.21s
                        Total time: 4638.17s
                               ETA: 1266107.2s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.288s, learning 0.201s)
               Value function loss: 22.9355
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: 316.78
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 11.49s
                        Total time: 4649.66s
                               ETA: 1265763.1s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.324s, learning 0.180s)
               Value function loss: 25.3833
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 313.32
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 11.50s
                        Total time: 4661.17s
                               ETA: 1265424.6s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.248s, learning 0.161s)
               Value function loss: 30.0588
                    Surrogate loss: -0.0012
             Mean action noise std: 0.78
                       Mean reward: 315.10
               Mean episode length: 250.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 11.41s
                        Total time: 4672.58s
                               ETA: 1265062.0s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.454s, learning 0.163s)
               Value function loss: 20.3689
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 320.31
               Mean episode length: 250.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 11.62s
                        Total time: 4684.19s
                               ETA: 1264757.7s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.310s, learning 0.171s)
               Value function loss: 16.0839
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 320.98
               Mean episode length: 250.00
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 11.48s
                        Total time: 4695.67s
                               ETA: 1264418.2s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.280s, learning 0.274s)
               Value function loss: 9.6252
                    Surrogate loss: -0.0232
             Mean action noise std: 0.78
                       Mean reward: 320.03
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 11.55s
                        Total time: 4707.23s
                               ETA: 1264100.1s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.621s, learning 0.170s)
               Value function loss: 4.8128
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 323.98
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 11.79s
                        Total time: 4719.02s
                               ETA: 1263847.3s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1522 steps/s (collection: 10.541s, learning 0.222s)
               Value function loss: 2.3625
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 324.09
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 10.76s
                        Total time: 4729.78s
                               ETA: 1263321.1s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.313s, learning 0.192s)
               Value function loss: 1.6133
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 324.01
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 11.50s
                        Total time: 4741.29s
                               ETA: 1262995.3s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1464 steps/s (collection: 11.033s, learning 0.156s)
               Value function loss: 1.4492
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 323.46
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 11.19s
                        Total time: 4752.48s
                               ETA: 1262587.2s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.070s, learning 0.162s)
               Value function loss: 0.4314
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 323.46
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 11.23s
                        Total time: 4763.71s
                               ETA: 1262192.7s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.264s, learning 0.225s)
               Value function loss: 0.3832
                    Surrogate loss: -0.0032
             Mean action noise std: 0.78
                       Mean reward: 323.46
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 11.49s
                        Total time: 4775.20s
                               ETA: 1261867.9s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.098s, learning 0.165s)
               Value function loss: 0.3657
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 323.46
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 11.26s
                        Total time: 4786.46s
                               ETA: 1261485.5s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.112s, learning 0.188s)
               Value function loss: 0.3671
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 323.46
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 11.30s
                        Total time: 4797.76s
                               ETA: 1261114.6s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.345s, learning 0.157s)
               Value function loss: 0.2665
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 323.46
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 11.50s
                        Total time: 4809.26s
                               ETA: 1260798.6s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.761s, learning 0.201s)
               Value function loss: 0.5073
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 321.66
               Mean episode length: 248.39
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 11.96s
                        Total time: 4821.22s
                               ETA: 1260604.3s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.285s, learning 0.184s)
               Value function loss: 0.3037
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 321.85
               Mean episode length: 248.39
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 11.47s
                        Total time: 4832.69s
                               ETA: 1260282.8s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.685s, learning 0.159s)
               Value function loss: 0.7839
                    Surrogate loss: 0.0057
             Mean action noise std: 0.78
                       Mean reward: 320.90
               Mean episode length: 248.39
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 11.84s
                        Total time: 4844.54s
                               ETA: 1260060.1s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.086s, learning 0.193s)
               Value function loss: 0.9777
                    Surrogate loss: 0.0021
             Mean action noise std: 0.78
                       Mean reward: 321.82
               Mean episode length: 248.39
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 11.28s
                        Total time: 4855.82s
                               ETA: 1259692.0s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.453s, learning 0.190s)
               Value function loss: 0.4405
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 322.77
               Mean episode length: 248.39
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 11.64s
                        Total time: 4867.46s
                               ETA: 1259420.2s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.417s, learning 0.160s)
               Value function loss: 1.2007
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 321.87
               Mean episode length: 248.39
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 11.58s
                        Total time: 4879.04s
                               ETA: 1259132.5s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.474s, learning 0.182s)
               Value function loss: 1.2375
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 322.24
               Mean episode length: 248.39
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 11.66s
                        Total time: 4890.69s
                               ETA: 1258866.6s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.274s, learning 0.162s)
               Value function loss: 2.0129
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 323.09
               Mean episode length: 248.39
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 11.44s
                        Total time: 4902.13s
                               ETA: 1258545.5s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.191s, learning 0.175s)
               Value function loss: 1.8965
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 324.04
               Mean episode length: 248.39
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 11.37s
                        Total time: 4913.49s
                               ETA: 1258208.2s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.755s, learning 0.173s)
               Value function loss: 2.8883
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 324.20
               Mean episode length: 247.48
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 11.93s
                        Total time: 4925.42s
                               ETA: 1258016.1s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.341s, learning 0.194s)
               Value function loss: 2.4500
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 325.79
               Mean episode length: 247.30
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 11.53s
                        Total time: 4936.96s
                               ETA: 1257724.5s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.298s, learning 0.192s)
               Value function loss: 4.5866
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 327.74
               Mean episode length: 247.30
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 11.49s
                        Total time: 4948.45s
                               ETA: 1257423.0s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.669s, learning 0.251s)
               Value function loss: 5.9072
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 329.08
               Mean episode length: 248.38
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 11.92s
                        Total time: 4960.37s
                               ETA: 1257232.1s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.459s, learning 0.186s)
               Value function loss: 7.4044
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 330.29
               Mean episode length: 249.29
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 11.64s
                        Total time: 4972.01s
                               ETA: 1256972.5s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.495s, learning 0.164s)
               Value function loss: 9.4505
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 334.87
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 11.66s
                        Total time: 4983.67s
                               ETA: 1256717.8s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.071s, learning 0.166s)
               Value function loss: 11.4140
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 338.50
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 11.24s
                        Total time: 4994.91s
                               ETA: 1256358.1s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.166s)
               Value function loss: 23.4968
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 333.86
               Mean episode length: 249.51
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 11.55s
                        Total time: 5006.46s
                               ETA: 1256079.2s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.547s, learning 0.161s)
               Value function loss: 33.9199
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 332.46
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 11.71s
                        Total time: 5018.17s
                               ETA: 1255840.8s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.320s, learning 0.195s)
               Value function loss: 39.2472
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 329.22
               Mean episode length: 249.84
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 11.52s
                        Total time: 5029.68s
                               ETA: 1255555.3s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.266s, learning 0.164s)
               Value function loss: 30.0967
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 328.96
               Mean episode length: 250.00
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 11.43s
                        Total time: 5041.11s
                               ETA: 1255249.9s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.235s, learning 0.164s)
               Value function loss: 20.4644
                    Surrogate loss: -0.0195
             Mean action noise std: 0.78
                       Mean reward: 330.87
               Mean episode length: 250.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 11.40s
                        Total time: 5052.51s
                               ETA: 1254938.4s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.292s, learning 0.159s)
               Value function loss: 11.8145
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 327.73
               Mean episode length: 249.68
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 11.45s
                        Total time: 5063.96s
                               ETA: 1254641.1s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.754s, learning 0.197s)
               Value function loss: 5.7371
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 329.21
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 11.95s
                        Total time: 5075.91s
                               ETA: 1254468.8s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.348s, learning 0.196s)
               Value function loss: 2.2316
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 327.88
               Mean episode length: 249.85
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 11.54s
                        Total time: 5087.46s
                               ETA: 1254197.1s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.286s, learning 0.192s)
               Value function loss: 2.0450
                    Surrogate loss: -0.0039
             Mean action noise std: 0.78
                       Mean reward: 327.22
               Mean episode length: 249.85
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 11.48s
                        Total time: 5098.94s
                               ETA: 1253910.3s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.378s, learning 0.156s)
               Value function loss: 1.1035
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 327.41
               Mean episode length: 249.85
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 11.53s
                        Total time: 5110.47s
                               ETA: 1253638.9s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.568s, learning 0.191s)
               Value function loss: 0.6928
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 327.27
               Mean episode length: 249.85
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 11.76s
                        Total time: 5122.23s
                               ETA: 1253423.5s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.057s, learning 0.179s)
               Value function loss: 0.4678
                    Surrogate loss: 0.0192
             Mean action noise std: 0.78
                       Mean reward: 327.27
               Mean episode length: 249.85
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 11.24s
                        Total time: 5133.47s
                               ETA: 1253081.6s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.546s, learning 0.189s)
               Value function loss: 0.4520
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 327.27
               Mean episode length: 249.85
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 11.74s
                        Total time: 5145.20s
                               ETA: 1252862.9s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.164s)
               Value function loss: 0.4872
                    Surrogate loss: 0.0162
             Mean action noise std: 0.78
                       Mean reward: 327.27
               Mean episode length: 249.85
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 11.38s
                        Total time: 5156.58s
                               ETA: 1252557.8s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.569s, learning 0.177s)
               Value function loss: 0.4829
                    Surrogate loss: 0.0120
             Mean action noise std: 0.78
                       Mean reward: 327.27
               Mean episode length: 249.85
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 11.75s
                        Total time: 5168.32s
                               ETA: 1252343.7s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.458s, learning 0.290s)
               Value function loss: 0.6519
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: 326.21
               Mean episode length: 249.85
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 11.75s
                        Total time: 5180.07s
                               ETA: 1252131.2s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.207s, learning 0.193s)
               Value function loss: 0.6593
                    Surrogate loss: 0.0071
             Mean action noise std: 0.78
                       Mean reward: 326.08
               Mean episode length: 249.85
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 11.40s
                        Total time: 5191.47s
                               ETA: 1251835.7s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.774s, learning 0.259s)
               Value function loss: 0.9690
                    Surrogate loss: 0.0193
             Mean action noise std: 0.78
                       Mean reward: 325.59
               Mean episode length: 249.85
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 12.03s
                        Total time: 5203.50s
                               ETA: 1251693.7s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.809s, learning 0.166s)
               Value function loss: 1.0852
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 324.41
               Mean episode length: 249.85
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 11.97s
                        Total time: 5215.48s
                               ETA: 1251538.6s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1488 steps/s (collection: 10.839s, learning 0.169s)
               Value function loss: 0.6298
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 324.41
               Mean episode length: 249.85
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 11.01s
                        Total time: 5226.49s
                               ETA: 1251152.7s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.198s, learning 0.164s)
               Value function loss: 1.2853
                    Surrogate loss: 0.0216
             Mean action noise std: 0.78
                       Mean reward: 326.23
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 11.36s
                        Total time: 5237.85s
                               ETA: 1250853.0s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.037s, learning 0.194s)
               Value function loss: 1.2947
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 326.40
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 11.23s
                        Total time: 5249.08s
                               ETA: 1250523.7s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.599s, learning 0.173s)
               Value function loss: 1.6996
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 326.43
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 11.77s
                        Total time: 5260.85s
                               ETA: 1250324.3s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.321s, learning 0.167s)
               Value function loss: 1.5109
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 326.17
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 11.49s
                        Total time: 5272.34s
                               ETA: 1250058.5s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.459s, learning 0.198s)
               Value function loss: 2.8864
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 325.55
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 11.66s
                        Total time: 5283.99s
                               ETA: 1249834.0s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.815s, learning 0.161s)
               Value function loss: 2.6749
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 325.83
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 11.98s
                        Total time: 5295.97s
                               ETA: 1249685.9s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1491 steps/s (collection: 10.818s, learning 0.166s)
               Value function loss: 3.3735
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 325.23
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 10.98s
                        Total time: 5306.95s
                               ETA: 1249304.8s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.905s, learning 0.190s)
               Value function loss: 4.1657
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 328.87
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 12.09s
                        Total time: 5319.05s
                               ETA: 1249186.2s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.632s, learning 0.224s)
               Value function loss: 6.7304
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 327.82
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 11.86s
                        Total time: 5330.90s
                               ETA: 1249012.2s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.760s, learning 0.181s)
               Value function loss: 8.4059
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 330.26
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 11.94s
                        Total time: 5342.85s
                               ETA: 1248859.0s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.820s, learning 0.161s)
               Value function loss: 9.7004
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 331.48
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 11.98s
                        Total time: 5354.83s
                               ETA: 1248715.6s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.209s, learning 0.177s)
               Value function loss: 18.8505
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 335.62
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 11.39s
                        Total time: 5366.21s
                               ETA: 1248434.5s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1463 steps/s (collection: 11.032s, learning 0.159s)
               Value function loss: 33.0654
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 336.46
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 11.19s
                        Total time: 5377.40s
                               ETA: 1248109.4s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.381s, learning 0.186s)
               Value function loss: 39.4344
                    Surrogate loss: 0.0211
             Mean action noise std: 0.78
                       Mean reward: 331.68
               Mean episode length: 250.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 11.57s
                        Total time: 5388.97s
                               ETA: 1247872.8s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.907s, learning 0.265s)
               Value function loss: 34.6139
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 330.13
               Mean episode length: 250.00
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 12.17s
                        Total time: 5401.14s
                               ETA: 1247776.9s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.231s, learning 0.166s)
               Value function loss: 27.7548
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 331.96
               Mean episode length: 250.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 11.40s
                        Total time: 5412.54s
                               ETA: 1247502.9s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.682s, learning 0.167s)
               Value function loss: 25.0146
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 336.27
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 11.85s
                        Total time: 5424.39s
                               ETA: 1247333.9s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.236s, learning 0.181s)
               Value function loss: 10.9218
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 334.27
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 11.42s
                        Total time: 5435.81s
                               ETA: 1247066.6s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.100s, learning 0.165s)
               Value function loss: 3.1617
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 332.85
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 11.27s
                        Total time: 5447.07s
                               ETA: 1246765.8s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.427s, learning 0.220s)
               Value function loss: 2.4206
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 330.14
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 11.65s
                        Total time: 5458.72s
                               ETA: 1246553.6s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.766s, learning 0.264s)
               Value function loss: 1.1689
                    Surrogate loss: 0.0157
             Mean action noise std: 0.78
                       Mean reward: 328.81
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 12.03s
                        Total time: 5470.75s
                               ETA: 1246429.3s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.700s, learning 0.160s)
               Value function loss: 0.9302
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 328.67
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 11.86s
                        Total time: 5482.61s
                               ETA: 1246267.2s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.051s, learning 0.168s)
               Value function loss: 0.3682
                    Surrogate loss: -0.0276
             Mean action noise std: 0.78
                       Mean reward: 328.67
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 11.22s
                        Total time: 5493.83s
                               ETA: 1245960.3s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.855s, learning 0.173s)
               Value function loss: 0.3085
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 328.67
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 12.03s
                        Total time: 5505.86s
                               ETA: 1245837.7s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.236s, learning 0.167s)
               Value function loss: 0.3065
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 328.67
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 11.40s
                        Total time: 5517.26s
                               ETA: 1245574.5s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.559s, learning 0.158s)
               Value function loss: 0.2956
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 328.67
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 11.72s
                        Total time: 5528.98s
                               ETA: 1245383.1s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1462 steps/s (collection: 11.012s, learning 0.193s)
               Value function loss: 0.4941
                    Surrogate loss: -0.0188
             Mean action noise std: 0.78
                       Mean reward: 328.17
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 11.20s
                        Total time: 5540.18s
                               ETA: 1245077.4s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.263s, learning 0.171s)
               Value function loss: 0.4154
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 328.12
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 11.43s
                        Total time: 5551.61s
                               ETA: 1244824.5s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.392s, learning 0.159s)
               Value function loss: 0.5972
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 326.62
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 11.55s
                        Total time: 5563.17s
                               ETA: 1244598.9s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.527s, learning 0.219s)
               Value function loss: 0.8224
                    Surrogate loss: 0.0010
             Mean action noise std: 0.78
                       Mean reward: 327.58
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 11.75s
                        Total time: 5574.91s
                               ETA: 1244417.7s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.366s, learning 0.269s)
               Value function loss: 0.6670
                    Surrogate loss: 0.0020
             Mean action noise std: 0.78
                       Mean reward: 327.97
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 11.63s
                        Total time: 5586.55s
                               ETA: 1244212.6s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.782s, learning 0.199s)
               Value function loss: 1.2704
                    Surrogate loss: 0.0067
             Mean action noise std: 0.78
                       Mean reward: 328.06
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 11.98s
                        Total time: 5598.53s
                               ETA: 1244085.2s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.421s, learning 0.162s)
               Value function loss: 1.1573
                    Surrogate loss: 0.0253
             Mean action noise std: 0.78
                       Mean reward: 326.81
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 11.58s
                        Total time: 5610.11s
                               ETA: 1243870.1s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.095s, learning 0.156s)
               Value function loss: 2.4142
                    Surrogate loss: -0.0021
             Mean action noise std: 0.78
                       Mean reward: 326.47
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 11.25s
                        Total time: 5621.36s
                               ETA: 1243582.6s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.578s, learning 0.156s)
               Value function loss: 2.7533
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 325.77
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 11.73s
                        Total time: 5633.10s
                               ETA: 1243402.9s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.295s, learning 0.163s)
               Value function loss: 3.7468
                    Surrogate loss: 0.0000
             Mean action noise std: 0.78
                       Mean reward: 323.95
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 11.46s
                        Total time: 5644.55s
                               ETA: 1243163.2s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.335s, learning 0.176s)
               Value function loss: 2.6023
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 324.68
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 11.51s
                        Total time: 5656.07s
                               ETA: 1242936.1s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.163s)
               Value function loss: 3.3411
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 323.94
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 11.45s
                        Total time: 5667.52s
                               ETA: 1242697.3s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.184s, learning 0.195s)
               Value function loss: 3.1557
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 321.85
               Mean episode length: 250.00
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 11.38s
                        Total time: 5678.90s
                               ETA: 1242443.1s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.213s, learning 0.274s)
               Value function loss: 4.2020
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 321.30
               Mean episode length: 250.00
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 11.49s
                        Total time: 5690.39s
                               ETA: 1242213.5s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.304s, learning 0.174s)
               Value function loss: 4.2583
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 317.19
               Mean episode length: 250.00
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 11.48s
                        Total time: 5701.86s
                               ETA: 1241983.2s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.643s, learning 0.243s)
               Value function loss: 5.0496
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 316.17
               Mean episode length: 250.00
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 11.89s
                        Total time: 5713.75s
                               ETA: 1241842.1s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.270s, learning 0.199s)
               Value function loss: 8.8691
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 312.81
               Mean episode length: 250.00
                  Mean reward/step: 0.81
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 11.47s
                        Total time: 5725.22s
                               ETA: 1241611.5s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.936s, learning 0.205s)
               Value function loss: 12.8790
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 313.33
               Mean episode length: 250.00
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 11.14s
                        Total time: 5736.36s
                               ETA: 1241310.7s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.605s, learning 0.185s)
               Value function loss: 14.9212
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 308.07
               Mean episode length: 250.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 11.79s
                        Total time: 5748.15s
                               ETA: 1241151.3s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.146s, learning 0.157s)
               Value function loss: 12.7097
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 303.84
               Mean episode length: 250.00
                  Mean reward/step: 0.73
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 11.30s
                        Total time: 5759.45s
                               ETA: 1240887.7s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.426s, learning 0.277s)
               Value function loss: 11.1105
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 298.09
               Mean episode length: 250.00
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 11.70s
                        Total time: 5771.16s
                               ETA: 1240711.1s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.321s, learning 0.165s)
               Value function loss: 8.1118
                    Surrogate loss: -0.0198
             Mean action noise std: 0.78
                       Mean reward: 294.28
               Mean episode length: 250.00
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 11.49s
                        Total time: 5782.64s
                               ETA: 1240488.7s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.332s, learning 0.182s)
               Value function loss: 5.7884
                    Surrogate loss: -0.0222
             Mean action noise std: 0.78
                       Mean reward: 292.24
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 11.51s
                        Total time: 5794.16s
                               ETA: 1240273.2s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1474 steps/s (collection: 10.958s, learning 0.156s)
               Value function loss: 2.7095
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 289.78
               Mean episode length: 249.74
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 11.11s
                        Total time: 5805.27s
                               ETA: 1239973.2s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.060s, learning 0.192s)
               Value function loss: 1.4958
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 292.24
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 11.25s
                        Total time: 5816.52s
                               ETA: 1239703.8s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.409s, learning 0.168s)
               Value function loss: 1.0529
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 289.78
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 11.58s
                        Total time: 5828.10s
                               ETA: 1239504.5s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.209s, learning 0.195s)
               Value function loss: 1.0913
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 288.51
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 11.40s
                        Total time: 5839.50s
                               ETA: 1239269.3s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1490 steps/s (collection: 10.830s, learning 0.165s)
               Value function loss: 0.5503
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 288.51
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 10.99s
                        Total time: 5850.50s
                               ETA: 1238948.4s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.227s, learning 0.207s)
               Value function loss: 0.5943
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 288.51
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 11.43s
                        Total time: 5861.93s
                               ETA: 1238721.7s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1446 steps/s (collection: 11.161s, learning 0.167s)
               Value function loss: 0.7165
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 288.51
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 11.33s
                        Total time: 5873.26s
                               ETA: 1238473.6s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.092s, learning 0.182s)
               Value function loss: 0.7991
                    Surrogate loss: 0.0010
             Mean action noise std: 0.78
                       Mean reward: 288.51
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 11.27s
                        Total time: 5884.53s
                               ETA: 1238214.9s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.199s, learning 0.159s)
               Value function loss: 0.8833
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 288.25
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 11.36s
                        Total time: 5895.89s
                               ETA: 1237975.0s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1478 steps/s (collection: 10.928s, learning 0.156s)
               Value function loss: 0.9438
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 288.20
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 11.08s
                        Total time: 5906.97s
                               ETA: 1237678.7s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.346s, learning 0.213s)
               Value function loss: 0.9866
                    Surrogate loss: -0.0247
             Mean action noise std: 0.78
                       Mean reward: 287.91
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 11.56s
                        Total time: 5918.53s
                               ETA: 1237482.9s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.481s, learning 0.175s)
               Value function loss: 1.2860
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 285.46
               Mean episode length: 249.81
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 11.66s
                        Total time: 5930.19s
                               ETA: 1237308.1s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.977s, learning 0.158s)
               Value function loss: 1.1127
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 284.15
               Mean episode length: 249.81
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 12.14s
                        Total time: 5942.32s
                               ETA: 1237233.8s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.370s, learning 0.163s)
               Value function loss: 1.1678
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 283.54
               Mean episode length: 249.81
                  Mean reward/step: 1.15
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 11.53s
                        Total time: 5953.86s
                               ETA: 1237034.6s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.342s, learning 0.240s)
               Value function loss: 1.3875
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 282.27
               Mean episode length: 249.81
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 11.58s
                        Total time: 5965.44s
                               ETA: 1236846.5s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.401s, learning 0.178s)
               Value function loss: 1.4007
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 276.80
               Mean episode length: 248.43
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 11.58s
                        Total time: 5977.02s
                               ETA: 1236658.5s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1476 steps/s (collection: 10.872s, learning 0.226s)
               Value function loss: 1.3905
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 273.50
               Mean episode length: 247.87
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 11.10s
                        Total time: 5988.11s
                               ETA: 1236371.6s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.245s, learning 0.196s)
               Value function loss: 1.3970
                    Surrogate loss: -0.0214
             Mean action noise std: 0.78
                       Mean reward: 268.55
               Mean episode length: 247.27
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 11.44s
                        Total time: 5999.56s
                               ETA: 1236156.8s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.579s, learning 0.188s)
               Value function loss: 1.6163
                    Surrogate loss: -0.0223
             Mean action noise std: 0.78
                       Mean reward: 261.50
               Mean episode length: 245.88
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 11.77s
                        Total time: 6011.32s
                               ETA: 1236009.8s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.282s, learning 0.180s)
               Value function loss: 1.5837
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: 257.05
               Mean episode length: 244.82
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 11.46s
                        Total time: 6022.78s
                               ETA: 1235800.7s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.327s, learning 0.170s)
               Value function loss: 2.0194
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 251.82
               Mean episode length: 243.56
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 11.50s
                        Total time: 6034.28s
                               ETA: 1235599.8s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.504s, learning 0.181s)
               Value function loss: 2.1666
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 249.81
               Mean episode length: 244.11
                  Mean reward/step: 0.82
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 11.68s
                        Total time: 6045.97s
                               ETA: 1235437.9s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.101s, learning 0.164s)
               Value function loss: 2.9154
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 252.26
               Mean episode length: 246.46
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 11.26s
                        Total time: 6057.23s
                               ETA: 1235190.9s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.202s, learning 0.271s)
               Value function loss: 2.8127
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 253.85
               Mean episode length: 245.06
                  Mean reward/step: 0.76
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 11.47s
                        Total time: 6068.70s
                               ETA: 1234987.4s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.302s, learning 0.160s)
               Value function loss: 3.8726
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 257.21
               Mean episode length: 247.45
                  Mean reward/step: 0.75
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 11.46s
                        Total time: 6080.17s
                               ETA: 1234782.5s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.625s, learning 0.182s)
               Value function loss: 7.8972
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 264.28
               Mean episode length: 249.32
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 11.81s
                        Total time: 6091.97s
                               ETA: 1234648.2s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.240s, learning 0.165s)
               Value function loss: 10.0581
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 270.99
               Mean episode length: 249.75
                  Mean reward/step: 0.71
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 11.40s
                        Total time: 6103.38s
                               ETA: 1234433.0s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.522s, learning 0.209s)
               Value function loss: 12.1206
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 270.58
               Mean episode length: 249.74
                  Mean reward/step: 0.72
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 11.73s
                        Total time: 6115.11s
                               ETA: 1234284.6s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.108s, learning 0.166s)
               Value function loss: 10.9293
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 275.06
               Mean episode length: 249.89
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 11.27s
                        Total time: 6126.38s
                               ETA: 1234044.6s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.408s, learning 0.162s)
               Value function loss: 9.6809
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 283.62
               Mean episode length: 249.98
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 11.57s
                        Total time: 6137.95s
                               ETA: 1233865.0s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.187s, learning 0.187s)
               Value function loss: 6.1217
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 275.28
               Mean episode length: 249.95
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 11.37s
                        Total time: 6149.33s
                               ETA: 1233646.7s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.558s, learning 0.226s)
               Value function loss: 3.3115
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 280.99
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 11.78s
                        Total time: 6161.11s
                               ETA: 1233511.3s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.259s, learning 0.225s)
               Value function loss: 1.4805
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 284.50
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 11.48s
                        Total time: 6172.59s
                               ETA: 1233316.7s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.268s, learning 0.213s)
               Value function loss: 1.0873
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 289.01
               Mean episode length: 249.91
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 11.48s
                        Total time: 6184.07s
                               ETA: 1233121.9s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.968s, learning 0.171s)
               Value function loss: 0.8964
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 291.30
               Mean episode length: 249.89
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 11.14s
                        Total time: 6195.21s
                               ETA: 1232860.0s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.358s, learning 0.167s)
               Value function loss: 0.3717
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 291.30
               Mean episode length: 249.89
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 11.52s
                        Total time: 6206.74s
                               ETA: 1232675.7s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.561s, learning 0.266s)
               Value function loss: 0.3641
                    Surrogate loss: -0.0203
             Mean action noise std: 0.78
                       Mean reward: 291.30
               Mean episode length: 249.89
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 11.83s
                        Total time: 6218.57s
                               ETA: 1232552.1s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.163s)
               Value function loss: 0.3981
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 291.30
               Mean episode length: 249.89
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 11.39s
                        Total time: 6229.95s
                               ETA: 1232341.6s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.266s, learning 0.167s)
               Value function loss: 0.3842
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 291.30
               Mean episode length: 249.89
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 11.43s
                        Total time: 6241.39s
                               ETA: 1232141.3s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.127s, learning 0.206s)
               Value function loss: 0.4340
                    Surrogate loss: 0.0008
             Mean action noise std: 0.78
                       Mean reward: 291.30
               Mean episode length: 249.89
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 11.33s
                        Total time: 6252.72s
                               ETA: 1231922.1s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.265s, learning 0.203s)
               Value function loss: 0.5394
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 290.54
               Mean episode length: 249.89
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 11.47s
                        Total time: 6264.19s
                               ETA: 1231730.1s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.359s, learning 0.166s)
               Value function loss: 0.6427
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 290.48
               Mean episode length: 249.82
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 11.53s
                        Total time: 6275.71s
                               ETA: 1231550.0s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.830s, learning 0.246s)
               Value function loss: 0.7993
                    Surrogate loss: 0.0035
             Mean action noise std: 0.78
                       Mean reward: 290.67
               Mean episode length: 249.82
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 12.08s
                        Total time: 6287.79s
                               ETA: 1231478.4s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.234s, learning 0.200s)
               Value function loss: 0.8024
                    Surrogate loss: 0.0000
             Mean action noise std: 0.78
                       Mean reward: 289.14
               Mean episode length: 249.82
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 11.43s
                        Total time: 6299.22s
                               ETA: 1231281.7s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.384s, learning 0.173s)
               Value function loss: 0.7244
                    Surrogate loss: -0.0043
             Mean action noise std: 0.78
                       Mean reward: 288.22
               Mean episode length: 249.72
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 11.56s
                        Total time: 6310.78s
                               ETA: 1231109.6s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.327s, learning 0.171s)
               Value function loss: 0.8803
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 289.39
               Mean episode length: 249.72
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 11.50s
                        Total time: 6322.28s
                               ETA: 1230926.5s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.820s, learning 0.178s)
               Value function loss: 1.3297
                    Surrogate loss: 0.0760
             Mean action noise std: 0.78
                       Mean reward: 288.13
               Mean episode length: 249.72
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 12.00s
                        Total time: 6334.28s
                               ETA: 1230841.4s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.426s, learning 0.193s)
               Value function loss: 1.1556
                    Surrogate loss: 0.0092
             Mean action noise std: 0.78
                       Mean reward: 289.01
               Mean episode length: 249.72
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 11.62s
                        Total time: 6345.89s
                               ETA: 1230683.0s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.625s, learning 0.168s)
               Value function loss: 1.3407
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 289.15
               Mean episode length: 249.72
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 11.79s
                        Total time: 6357.69s
                               ETA: 1230558.7s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.839s, learning 0.194s)
               Value function loss: 1.4302
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 286.26
               Mean episode length: 249.72
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 12.03s
                        Total time: 6369.72s
                               ETA: 1230481.4s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.601s, learning 0.203s)
               Value function loss: 1.5932
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 284.56
               Mean episode length: 249.81
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 11.80s
                        Total time: 6381.52s
                               ETA: 1230360.3s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.434s, learning 0.183s)
               Value function loss: 2.1079
                    Surrogate loss: 0.0079
             Mean action noise std: 0.78
                       Mean reward: 288.59
               Mean episode length: 249.90
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 11.62s
                        Total time: 6393.14s
                               ETA: 1230203.5s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.525s, learning 0.165s)
               Value function loss: 2.5041
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 293.49
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 11.69s
                        Total time: 6404.83s
                               ETA: 1230061.4s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.764s, learning 0.165s)
               Value function loss: 2.8758
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: 296.26
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 11.93s
                        Total time: 6416.76s
                               ETA: 1229965.4s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.609s, learning 0.280s)
               Value function loss: 4.2119
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 299.74
               Mean episode length: 249.47
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 11.89s
                        Total time: 6428.65s
                               ETA: 1229862.2s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.264s, learning 0.176s)
               Value function loss: 4.1105
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 301.77
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 11.44s
                        Total time: 6440.09s
                               ETA: 1229673.6s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.508s, learning 0.167s)
               Value function loss: 9.2612
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 308.92
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 11.68s
                        Total time: 6451.76s
                               ETA: 1229530.6s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.416s, learning 0.174s)
               Value function loss: 17.7354
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 314.50
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 11.59s
                        Total time: 6463.35s
                               ETA: 1229371.8s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.300s, learning 0.180s)
               Value function loss: 24.6235
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 325.95
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 11.48s
                        Total time: 6474.83s
                               ETA: 1229192.7s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.449s, learning 0.159s)
               Value function loss: 24.7228
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 330.13
               Mean episode length: 250.00
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 11.61s
                        Total time: 6486.44s
                               ETA: 1229038.5s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.751s, learning 0.187s)
               Value function loss: 11.8846
                    Surrogate loss: -0.0299
             Mean action noise std: 0.78
                       Mean reward: 335.82
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 11.94s
                        Total time: 6498.38s
                               ETA: 1228947.3s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.148s, learning 0.186s)
               Value function loss: 8.5794
                    Surrogate loss: -0.0265
             Mean action noise std: 0.78
                       Mean reward: 339.95
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 11.33s
                        Total time: 6509.71s
                               ETA: 1228742.2s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.413s, learning 0.201s)
               Value function loss: 4.9579
                    Surrogate loss: -0.0236
             Mean action noise std: 0.78
                       Mean reward: 341.17
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 11.61s
                        Total time: 6521.33s
                               ETA: 1228590.8s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.305s, learning 0.181s)
               Value function loss: 2.2182
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 344.22
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 11.49s
                        Total time: 6532.81s
                               ETA: 1228415.8s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.837s, learning 0.195s)
               Value function loss: 1.4790
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 346.34
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 12.03s
                        Total time: 6544.85s
                               ETA: 1228344.0s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.791s, learning 0.157s)
               Value function loss: 0.7480
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 346.88
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 11.95s
                        Total time: 6556.79s
                               ETA: 1228256.5s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.755s, learning 0.183s)
               Value function loss: 0.3936
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 347.57
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 11.94s
                        Total time: 6568.73s
                               ETA: 1228167.6s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.428s, learning 0.164s)
               Value function loss: 0.3630
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: 347.57
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 11.59s
                        Total time: 6580.32s
                               ETA: 1228014.3s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.549s, learning 0.202s)
               Value function loss: 0.3250
                    Surrogate loss: 0.0790
             Mean action noise std: 0.78
                       Mean reward: 347.57
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 11.75s
                        Total time: 6592.07s
                               ETA: 1227891.1s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.633s, learning 0.186s)
               Value function loss: 0.1907
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 347.57
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 11.82s
                        Total time: 6603.89s
                               ETA: 1227781.0s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.222s, learning 0.176s)
               Value function loss: 0.1866
                    Surrogate loss: -0.0023
             Mean action noise std: 0.78
                       Mean reward: 347.57
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 11.40s
                        Total time: 6615.29s
                               ETA: 1227593.3s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.387s, learning 0.158s)
               Value function loss: 0.3296
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 348.22
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 11.55s
                        Total time: 6626.84s
                               ETA: 1227433.4s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.350s, learning 0.193s)
               Value function loss: 0.4531
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 348.88
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 11.54s
                        Total time: 6638.38s
                               ETA: 1227273.6s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.659s, learning 0.203s)
               Value function loss: 0.4619
                    Surrogate loss: 0.0021
             Mean action noise std: 0.77
                       Mean reward: 349.86
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 11.86s
                        Total time: 6650.24s
                               ETA: 1227173.2s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.501s, learning 0.185s)
               Value function loss: 1.5730
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 351.92
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 11.69s
                        Total time: 6661.93s
                               ETA: 1227040.9s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.629s, learning 0.159s)
               Value function loss: 0.2929
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 351.82
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 11.79s
                        Total time: 6673.72s
                               ETA: 1226927.5s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.329s, learning 0.188s)
               Value function loss: 0.6659
                    Surrogate loss: 0.0052
             Mean action noise std: 0.77
                       Mean reward: 352.18
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 11.52s
                        Total time: 6685.23s
                               ETA: 1226764.9s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.868s, learning 0.178s)
               Value function loss: 1.3302
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 352.74
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 12.05s
                        Total time: 6697.28s
                               ETA: 1226699.7s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.836s, learning 0.164s)
               Value function loss: 2.0007
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 353.53
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 12.00s
                        Total time: 6709.28s
                               ETA: 1226626.3s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.477s, learning 0.167s)
               Value function loss: 1.6187
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 354.40
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 11.64s
                        Total time: 6720.92s
                               ETA: 1226488.3s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.790s, learning 0.164s)
               Value function loss: 2.5943
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 356.17
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 11.95s
                        Total time: 6732.88s
                               ETA: 1226407.1s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.356s, learning 0.164s)
               Value function loss: 2.8973
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 357.81
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 11.52s
                        Total time: 6744.40s
                               ETA: 1226247.3s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.464s, learning 0.268s)
               Value function loss: 3.1776
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 359.34
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 11.73s
                        Total time: 6756.13s
                               ETA: 1226126.5s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.164s, learning 0.182s)
               Value function loss: 3.8512
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 359.83
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 11.35s
                        Total time: 6767.48s
                               ETA: 1225936.2s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.502s, learning 0.209s)
               Value function loss: 6.6861
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 361.90
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 11.71s
                        Total time: 6779.19s
                               ETA: 1225812.5s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.014s, learning 0.184s)
               Value function loss: 11.5387
                    Surrogate loss: 0.0016
             Mean action noise std: 0.77
                       Mean reward: 362.73
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 12.20s
                        Total time: 6791.38s
                               ETA: 1225777.1s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.425s, learning 0.163s)
               Value function loss: 9.9105
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 365.48
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 11.59s
                        Total time: 6802.97s
                               ETA: 1225631.9s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.621s, learning 0.196s)
               Value function loss: 23.4175
                    Surrogate loss: 0.0094
             Mean action noise std: 0.77
                       Mean reward: 365.35
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 11.82s
                        Total time: 6814.79s
                               ETA: 1225528.4s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.444s, learning 0.164s)
               Value function loss: 35.7862
                    Surrogate loss: 0.0051
             Mean action noise std: 0.77
                       Mean reward: 366.02
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 11.61s
                        Total time: 6826.40s
                               ETA: 1225387.8s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.446s, learning 0.175s)
               Value function loss: 44.4142
                    Surrogate loss: 0.0064
             Mean action noise std: 0.77
                       Mean reward: 365.45
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 11.62s
                        Total time: 6838.02s
                               ETA: 1225249.8s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.514s, learning 0.175s)
               Value function loss: 38.4558
                    Surrogate loss: -0.0014
             Mean action noise std: 0.77
                       Mean reward: 365.37
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 11.69s
                        Total time: 6849.71s
                               ETA: 1225124.4s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.209s, learning 0.168s)
               Value function loss: 24.5704
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 364.65
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 11.38s
                        Total time: 6861.09s
                               ETA: 1224943.9s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.046s, learning 0.175s)
               Value function loss: 19.5639
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 365.75
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 11.22s
                        Total time: 6872.31s
                               ETA: 1224736.2s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.660s, learning 0.181s)
               Value function loss: 8.0145
                    Surrogate loss: -0.0219
             Mean action noise std: 0.77
                       Mean reward: 364.44
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 11.84s
                        Total time: 6884.15s
                               ETA: 1224639.2s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.278s, learning 0.171s)
               Value function loss: 2.7567
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 365.10
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 11.45s
                        Total time: 6895.60s
                               ETA: 1224473.0s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.497s, learning 0.165s)
               Value function loss: 2.0242
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 365.07
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 11.66s
                        Total time: 6907.26s
                               ETA: 1224345.2s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.379s, learning 0.169s)
               Value function loss: 1.0394
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 365.32
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 11.55s
                        Total time: 6918.81s
                               ETA: 1224197.7s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.629s, learning 0.185s)
               Value function loss: 0.6937
                    Surrogate loss: 0.0173
             Mean action noise std: 0.77
                       Mean reward: 365.53
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 11.81s
                        Total time: 6930.62s
                               ETA: 1224097.5s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.080s, learning 0.171s)
               Value function loss: 0.2419
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 365.53
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 11.25s
                        Total time: 6941.87s
                               ETA: 1223898.5s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.770s, learning 0.241s)
               Value function loss: 0.1906
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 365.53
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 12.01s
                        Total time: 6953.88s
                               ETA: 1223833.9s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.601s, learning 0.179s)
               Value function loss: 0.2279
                    Surrogate loss: 0.0059
             Mean action noise std: 0.77
                       Mean reward: 365.53
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 11.78s
                        Total time: 6965.66s
                               ETA: 1223728.8s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.690s, learning 0.211s)
               Value function loss: 0.2553
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 365.53
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 11.90s
                        Total time: 6977.56s
                               ETA: 1223645.3s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.299s, learning 0.182s)
               Value function loss: 0.3964
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 365.30
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 11.48s
                        Total time: 6989.04s
                               ETA: 1223488.5s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1454 steps/s (collection: 11.079s, learning 0.186s)
               Value function loss: 0.2978
                    Surrogate loss: 0.0318
             Mean action noise std: 0.77
                       Mean reward: 365.32
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 11.26s
                        Total time: 7000.31s
                               ETA: 1223294.5s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.123s, learning 0.183s)
               Value function loss: 0.6939
                    Surrogate loss: 0.0070
             Mean action noise std: 0.77
                       Mean reward: 365.89
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 11.31s
                        Total time: 7011.61s
                               ETA: 1223108.3s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.443s, learning 0.159s)
               Value function loss: 0.6200
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 365.77
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 11.60s
                        Total time: 7023.22s
                               ETA: 1222974.3s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.363s, learning 0.158s)
               Value function loss: 0.5276
                    Surrogate loss: 0.0063
             Mean action noise std: 0.77
                       Mean reward: 365.93
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 11.52s
                        Total time: 7034.74s
                               ETA: 1222826.6s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.352s, learning 0.165s)
               Value function loss: 0.7941
                    Surrogate loss: 0.0035
             Mean action noise std: 0.77
                       Mean reward: 366.40
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 11.52s
                        Total time: 7046.25s
                               ETA: 1222678.7s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.822s, learning 0.154s)
               Value function loss: 0.9867
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 366.82
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 11.98s
                        Total time: 7058.23s
                               ETA: 1222610.8s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.799s, learning 0.166s)
               Value function loss: 1.4982
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 367.23
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 11.97s
                        Total time: 7070.20s
                               ETA: 1222541.2s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.123s, learning 0.162s)
               Value function loss: 1.2982
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 367.35
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 11.29s
                        Total time: 7081.48s
                               ETA: 1222354.6s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.138s, learning 0.175s)
               Value function loss: 2.2637
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 367.49
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 11.31s
                        Total time: 7092.79s
                               ETA: 1222173.2s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.496s, learning 0.197s)
               Value function loss: 2.4583
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 367.10
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 11.69s
                        Total time: 7104.49s
                               ETA: 1222057.8s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.626s, learning 0.178s)
               Value function loss: 2.9426
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 367.95
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 11.80s
                        Total time: 7116.29s
                               ETA: 1221961.8s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.698s, learning 0.199s)
               Value function loss: 4.1583
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 368.32
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 11.90s
                        Total time: 7128.19s
                               ETA: 1221881.9s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.580s, learning 0.163s)
               Value function loss: 5.6148
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 367.09
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 11.74s
                        Total time: 7139.93s
                               ETA: 1221776.1s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.521s, learning 0.174s)
               Value function loss: 9.6920
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 369.06
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 11.69s
                        Total time: 7151.63s
                               ETA: 1221662.3s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.549s, learning 0.161s)
               Value function loss: 9.6051
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 370.46
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 11.71s
                        Total time: 7163.34s
                               ETA: 1221551.5s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.414s, learning 0.165s)
               Value function loss: 16.5102
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 368.92
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 11.58s
                        Total time: 7174.91s
                               ETA: 1221418.7s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.453s, learning 0.200s)
               Value function loss: 32.6779
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 368.53
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 11.65s
                        Total time: 7186.57s
                               ETA: 1221298.8s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.509s, learning 0.211s)
               Value function loss: 43.2859
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 370.35
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 11.72s
                        Total time: 7198.29s
                               ETA: 1221190.7s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.180s, learning 0.166s)
               Value function loss: 47.5551
                    Surrogate loss: -0.0014
             Mean action noise std: 0.77
                       Mean reward: 370.48
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 11.35s
                        Total time: 7209.63s
                               ETA: 1221019.7s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.635s, learning 0.169s)
               Value function loss: 35.2263
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 368.67
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 11.80s
                        Total time: 7221.44s
                               ETA: 1220926.6s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.115s, learning 0.244s)
               Value function loss: 24.9817
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 370.84
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 12.36s
                        Total time: 7233.80s
                               ETA: 1220927.4s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.376s, learning 0.179s)
               Value function loss: 14.8063
                    Surrogate loss: -0.0201
             Mean action noise std: 0.77
                       Mean reward: 370.41
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 11.56s
                        Total time: 7245.35s
                               ETA: 1220792.8s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.517s, learning 0.173s)
               Value function loss: 5.4855
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 369.87
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 11.69s
                        Total time: 7257.04s
                               ETA: 1220681.2s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.365s, learning 0.200s)
               Value function loss: 2.2969
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 370.76
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 11.56s
                        Total time: 7268.61s
                               ETA: 1220548.9s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.949s, learning 0.251s)
               Value function loss: 1.6883
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 369.71
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 12.20s
                        Total time: 7280.81s
                               ETA: 1220523.7s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.996s, learning 0.181s)
               Value function loss: 0.9688
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 370.81
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 11.18s
                        Total time: 7291.98s
                               ETA: 1220327.1s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.142s, learning 0.172s)
               Value function loss: 0.3153
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 370.81
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 11.31s
                        Total time: 7303.30s
                               ETA: 1220154.1s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.124s, learning 0.232s)
               Value function loss: 0.2449
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 370.81
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 11.36s
                        Total time: 7314.65s
                               ETA: 1219988.5s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.494s, learning 0.197s)
               Value function loss: 0.2486
                    Surrogate loss: -0.0226
             Mean action noise std: 0.77
                       Mean reward: 370.81
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 11.69s
                        Total time: 7326.34s
                               ETA: 1219879.3s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.710s, learning 0.193s)
               Value function loss: 0.2128
                    Surrogate loss: -0.0261
             Mean action noise std: 0.77
                       Mean reward: 370.81
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 11.90s
                        Total time: 7338.25s
                               ETA: 1219805.6s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.882s, learning 0.166s)
               Value function loss: 0.5208
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 370.65
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 12.05s
                        Total time: 7350.29s
                               ETA: 1219756.3s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.507s, learning 0.319s)
               Value function loss: 0.3088
                    Surrogate loss: -0.0223
             Mean action noise std: 0.77
                       Mean reward: 370.41
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 11.83s
                        Total time: 7362.12s
                               ETA: 1219670.2s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.182s, learning 0.196s)
               Value function loss: 0.6795
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 369.58
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 11.38s
                        Total time: 7373.50s
                               ETA: 1219510.4s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.654s, learning 0.166s)
               Value function loss: 1.1091
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 369.26
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 11.82s
                        Total time: 7385.32s
                               ETA: 1219424.0s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.700s, learning 0.292s)
               Value function loss: 0.6661
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 368.19
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 11.99s
                        Total time: 7397.31s
                               ETA: 1219366.2s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.716s, learning 0.172s)
               Value function loss: 0.7633
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 368.13
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 11.89s
                        Total time: 7409.20s
                               ETA: 1219291.6s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.595s, learning 0.191s)
               Value function loss: 0.8251
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 366.81
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 11.79s
                        Total time: 7420.98s
                               ETA: 1219200.3s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1489 steps/s (collection: 10.840s, learning 0.158s)
               Value function loss: 1.2025
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 365.92
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 11.00s
                        Total time: 7431.98s
                               ETA: 1218980.0s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.301s, learning 0.197s)
               Value function loss: 1.9490
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 363.18
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 11.50s
                        Total time: 7443.48s
                               ETA: 1218842.4s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.865s, learning 0.188s)
               Value function loss: 1.6941
                    Surrogate loss: 0.0039
             Mean action noise std: 0.77
                       Mean reward: 361.32
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 12.05s
                        Total time: 7455.53s
                               ETA: 1218795.9s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1460 steps/s (collection: 10.992s, learning 0.226s)
               Value function loss: 2.3376
                    Surrogate loss: 0.0019
             Mean action noise std: 0.77
                       Mean reward: 358.22
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 11.22s
                        Total time: 7466.75s
                               ETA: 1218613.2s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.062s, learning 0.170s)
               Value function loss: 3.6719
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 357.80
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 11.23s
                        Total time: 7477.98s
                               ETA: 1218433.4s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.335s, learning 0.219s)
               Value function loss: 4.8406
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 351.61
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 11.55s
                        Total time: 7489.54s
                               ETA: 1218306.6s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.780s, learning 0.176s)
               Value function loss: 5.3762
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 349.62
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 11.96s
                        Total time: 7501.50s
                               ETA: 1218245.3s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.149s, learning 0.327s)
               Value function loss: 8.7823
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 346.38
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 11.48s
                        Total time: 7512.97s
                               ETA: 1218106.4s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.536s, learning 0.176s)
               Value function loss: 9.4251
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 350.17
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 11.71s
                        Total time: 7524.68s
                               ETA: 1218005.9s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.162s)
               Value function loss: 10.9491
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 347.58
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 11.45s
                        Total time: 7536.13s
                               ETA: 1217863.3s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.227s, learning 0.214s)
               Value function loss: 26.2471
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 350.35
               Mean episode length: 249.86
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 11.44s
                        Total time: 7547.57s
                               ETA: 1217719.9s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.461s, learning 0.192s)
               Value function loss: 34.2948
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 344.95
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 11.65s
                        Total time: 7559.23s
                               ETA: 1217611.2s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.432s, learning 0.238s)
               Value function loss: 29.2258
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 341.36
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 11.67s
                        Total time: 7570.90s
                               ETA: 1217505.3s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.349s, learning 0.179s)
               Value function loss: 23.5286
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 339.18
               Mean episode length: 250.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 11.53s
                        Total time: 7582.42s
                               ETA: 1217377.1s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.617s, learning 0.164s)
               Value function loss: 16.2738
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 347.55
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 11.78s
                        Total time: 7594.21s
                               ETA: 1217289.8s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.135s, learning 0.167s)
               Value function loss: 10.5913
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 340.60
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 11.30s
                        Total time: 7605.51s
                               ETA: 1217126.2s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1447 steps/s (collection: 11.026s, learning 0.296s)
               Value function loss: 4.3748
                    Surrogate loss: -0.0228
             Mean action noise std: 0.77
                       Mean reward: 338.06
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 11.32s
                        Total time: 7616.83s
                               ETA: 1216966.1s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.048s, learning 0.202s)
               Value function loss: 1.8180
                    Surrogate loss: 0.0135
             Mean action noise std: 0.77
                       Mean reward: 341.33
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 11.25s
                        Total time: 7628.08s
                               ETA: 1216795.1s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.266s, learning 0.219s)
               Value function loss: 1.4589
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 337.81
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 11.48s
                        Total time: 7639.56s
                               ETA: 1216661.9s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.426s, learning 0.161s)
               Value function loss: 1.1265
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 11.59s
                        Total time: 7651.15s
                               ETA: 1216545.3s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.205s, learning 0.226s)
               Value function loss: 0.4697
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 11.43s
                        Total time: 7662.58s
                               ETA: 1216404.4s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.209s, learning 0.159s)
               Value function loss: 0.5454
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 11.37s
                        Total time: 7673.95s
                               ETA: 1216254.0s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.951s, learning 0.231s)
               Value function loss: 0.5091
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 11.18s
                        Total time: 7685.13s
                               ETA: 1216074.5s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.580s, learning 0.189s)
               Value function loss: 0.5173
                    Surrogate loss: 0.0559
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 11.77s
                        Total time: 7696.90s
                               ETA: 1215988.2s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.326s, learning 0.164s)
               Value function loss: 0.5656
                    Surrogate loss: 0.0107
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 11.49s
                        Total time: 7708.39s
                               ETA: 1215858.2s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.417s, learning 0.197s)
               Value function loss: 0.6392
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 337.80
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 11.61s
                        Total time: 7720.01s
                               ETA: 1215748.0s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.015s, learning 0.212s)
               Value function loss: 1.0568
                    Surrogate loss: 0.0174
             Mean action noise std: 0.77
                       Mean reward: 337.81
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 11.23s
                        Total time: 7731.23s
                               ETA: 1215577.4s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.631s, learning 0.168s)
               Value function loss: 0.5911
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 336.32
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 11.80s
                        Total time: 7743.03s
                               ETA: 1215497.0s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.295s, learning 0.165s)
               Value function loss: 1.1281
                    Surrogate loss: 0.0049
             Mean action noise std: 0.77
                       Mean reward: 336.10
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 11.46s
                        Total time: 7754.49s
                               ETA: 1215363.6s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.181s, learning 0.185s)
               Value function loss: 0.5269
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 334.67
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 11.37s
                        Total time: 7765.86s
                               ETA: 1215215.9s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.530s, learning 0.173s)
               Value function loss: 0.8593
                    Surrogate loss: -0.0005
             Mean action noise std: 0.77
                       Mean reward: 333.00
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 11.70s
                        Total time: 7777.56s
                               ETA: 1215121.4s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.412s, learning 0.193s)
               Value function loss: 1.4048
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 331.35
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 11.61s
                        Total time: 7789.17s
                               ETA: 1215012.0s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.194s, learning 0.216s)
               Value function loss: 2.0308
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 331.21
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 11.41s
                        Total time: 7800.57s
                               ETA: 1214872.2s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.605s, learning 0.186s)
               Value function loss: 1.6260
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 330.88
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 11.79s
                        Total time: 7812.37s
                               ETA: 1214792.2s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.343s, learning 0.166s)
               Value function loss: 2.4781
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 329.63
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 11.51s
                        Total time: 7823.87s
                               ETA: 1214668.6s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.400s, learning 0.194s)
               Value function loss: 2.3600
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 330.01
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 11.59s
                        Total time: 7835.47s
                               ETA: 1214558.5s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.406s, learning 0.190s)
               Value function loss: 4.3331
                    Surrogate loss: -0.0057
             Mean action noise std: 0.77
                       Mean reward: 329.04
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 11.60s
                        Total time: 7847.06s
                               ETA: 1214449.1s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.687s, learning 0.169s)
               Value function loss: 4.8040
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 331.66
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 11.86s
                        Total time: 7858.92s
                               ETA: 1214380.1s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.162s, learning 0.186s)
               Value function loss: 7.2522
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 333.73
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 11.35s
                        Total time: 7870.27s
                               ETA: 1214233.0s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.407s, learning 0.167s)
               Value function loss: 8.6827
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 334.73
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 11.57s
                        Total time: 7881.84s
                               ETA: 1214121.1s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.602s, learning 0.192s)
               Value function loss: 8.0712
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 333.66
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 11.79s
                        Total time: 7893.63s
                               ETA: 1214043.4s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.160s)
               Value function loss: 17.8747
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 331.75
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 11.33s
                        Total time: 7904.97s
                               ETA: 1213895.2s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.618s, learning 0.207s)
               Value function loss: 23.8450
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 336.75
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 11.82s
                        Total time: 7916.79s
                               ETA: 1213822.7s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.098s, learning 0.162s)
               Value function loss: 31.2111
                    Surrogate loss: 0.0124
             Mean action noise std: 0.77
                       Mean reward: 331.82
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 11.26s
                        Total time: 7928.05s
                               ETA: 1213663.9s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.692s, learning 0.165s)
               Value function loss: 32.7382
                    Surrogate loss: -0.0010
             Mean action noise std: 0.77
                       Mean reward: 340.56
               Mean episode length: 250.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 11.86s
                        Total time: 7939.91s
                               ETA: 1213596.9s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.165s)
               Value function loss: 24.8218
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 341.02
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 11.57s
                        Total time: 7951.48s
                               ETA: 1213486.2s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.887s, learning 0.186s)
               Value function loss: 15.8686
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 344.19
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 11.07s
                        Total time: 7962.55s
                               ETA: 1213300.1s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.061s, learning 0.164s)
               Value function loss: 7.2846
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 347.87
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 12.23s
                        Total time: 7974.78s
                               ETA: 1213289.8s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.446s, learning 0.159s)
               Value function loss: 3.2751
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 350.26
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 11.60s
                        Total time: 7986.38s
                               ETA: 1213185.3s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.394s, learning 0.221s)
               Value function loss: 1.8207
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 349.49
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 11.61s
                        Total time: 7998.00s
                               ETA: 1213082.6s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.571s, learning 0.184s)
               Value function loss: 1.1466
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 350.31
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 11.75s
                        Total time: 8009.75s
                               ETA: 1213001.3s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.187s, learning 0.216s)
               Value function loss: 0.5621
                    Surrogate loss: 0.0117
             Mean action noise std: 0.77
                       Mean reward: 349.98
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 11.40s
                        Total time: 8021.16s
                               ETA: 1212867.1s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.266s, learning 0.161s)
               Value function loss: 0.3221
                    Surrogate loss: 0.0003
             Mean action noise std: 0.77
                       Mean reward: 349.98
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 11.43s
                        Total time: 8032.58s
                               ETA: 1212737.0s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.605s, learning 0.198s)
               Value function loss: 0.3400
                    Surrogate loss: 0.0040
             Mean action noise std: 0.77
                       Mean reward: 349.98
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 11.80s
                        Total time: 8044.39s
                               ETA: 1212663.8s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.858s, learning 0.179s)
               Value function loss: 0.3221
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 349.98
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 12.04s
                        Total time: 8056.42s
                               ETA: 1212626.0s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.331s, learning 0.193s)
               Value function loss: 0.2924
                    Surrogate loss: 0.0310
             Mean action noise std: 0.77
                       Mean reward: 349.98
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 11.52s
                        Total time: 8067.95s
                               ETA: 1212511.2s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.366s, learning 0.168s)
               Value function loss: 0.5301
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 349.49
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 11.53s
                        Total time: 8079.48s
                               ETA: 1212398.2s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.789s, learning 0.172s)
               Value function loss: 0.5210
                    Surrogate loss: 0.0032
             Mean action noise std: 0.77
                       Mean reward: 349.72
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 11.96s
                        Total time: 8091.44s
                               ETA: 1212349.5s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.701s, learning 0.176s)
               Value function loss: 0.7034
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: 351.40
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 11.88s
                        Total time: 8103.32s
                               ETA: 1212288.4s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.092s, learning 0.163s)
               Value function loss: 1.5905
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 351.90
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 11.25s
                        Total time: 8114.57s
                               ETA: 1212134.4s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.331s, learning 0.160s)
               Value function loss: 0.4741
                    Surrogate loss: -0.0050
             Mean action noise std: 0.77
                       Mean reward: 352.82
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 11.49s
                        Total time: 8126.07s
                               ETA: 1212016.1s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.269s, learning 0.168s)
               Value function loss: 0.9752
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 354.10
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 11.44s
                        Total time: 8137.50s
                               ETA: 1211890.0s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.694s, learning 0.188s)
               Value function loss: 1.9147
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 355.80
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 11.88s
                        Total time: 8149.38s
                               ETA: 1211830.5s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.200s, learning 0.160s)
               Value function loss: 2.2273
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 356.80
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 11.36s
                        Total time: 8160.74s
                               ETA: 1211693.6s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.629s, learning 0.196s)
               Value function loss: 1.8990
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 359.31
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 11.83s
                        Total time: 8172.57s
                               ETA: 1211626.1s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.411s, learning 0.198s)
               Value function loss: 3.6311
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 362.45
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 11.61s
                        Total time: 8184.18s
                               ETA: 1211526.7s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.439s, learning 0.194s)
               Value function loss: 3.2858
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 364.02
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 11.63s
                        Total time: 8195.81s
                               ETA: 1211431.1s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.260s, learning 0.189s)
               Value function loss: 4.0567
                    Surrogate loss: 0.0035
             Mean action noise std: 0.77
                       Mean reward: 365.87
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 11.45s
                        Total time: 8207.26s
                               ETA: 1211308.6s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.384s, learning 0.191s)
               Value function loss: 4.7494
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 362.52
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 11.58s
                        Total time: 8218.83s
                               ETA: 1211205.0s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.241s, learning 0.157s)
               Value function loss: 7.6820
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 359.87
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 11.40s
                        Total time: 8230.23s
                               ETA: 1211075.6s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.289s, learning 0.213s)
               Value function loss: 11.7956
                    Surrogate loss: 0.0114
             Mean action noise std: 0.77
                       Mean reward: 357.76
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 11.50s
                        Total time: 8241.73s
                               ETA: 1210961.9s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.489s, learning 0.194s)
               Value function loss: 10.4416
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 358.28
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 11.68s
                        Total time: 8253.42s
                               ETA: 1210875.0s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.176s, learning 0.198s)
               Value function loss: 21.8261
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 358.50
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 11.37s
                        Total time: 8264.79s
                               ETA: 1210743.1s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.419s, learning 0.158s)
               Value function loss: 34.5278
                    Surrogate loss: 0.0500
             Mean action noise std: 0.77
                       Mean reward: 355.71
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 11.58s
                        Total time: 8276.37s
                               ETA: 1210641.1s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.389s, learning 0.175s)
               Value function loss: 37.4847
                    Surrogate loss: 0.0001
             Mean action noise std: 0.77
                       Mean reward: 353.02
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 11.56s
                        Total time: 8287.93s
                               ETA: 1210537.6s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.534s, learning 0.162s)
               Value function loss: 33.8209
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 351.65
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 11.70s
                        Total time: 8299.63s
                               ETA: 1210453.7s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.312s, learning 0.228s)
               Value function loss: 23.0494
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 349.13
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 11.54s
                        Total time: 8311.17s
                               ETA: 1210347.3s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.507s, learning 0.231s)
               Value function loss: 17.8029
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 347.52
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 11.74s
                        Total time: 8322.91s
                               ETA: 1210270.0s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.512s, learning 0.191s)
               Value function loss: 7.4294
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 345.85
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 11.70s
                        Total time: 8334.61s
                               ETA: 1210187.7s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.208s, learning 0.214s)
               Value function loss: 3.2330
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 348.48
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 11.42s
                        Total time: 8346.03s
                               ETA: 1210064.9s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.196s, learning 0.177s)
               Value function loss: 2.4323
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 346.78
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 11.37s
                        Total time: 8357.40s
                               ETA: 1209935.4s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.544s, learning 0.171s)
               Value function loss: 1.6345
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 344.86
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 11.72s
                        Total time: 8369.12s
                               ETA: 1209855.7s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.476s, learning 0.175s)
               Value function loss: 1.1980
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 344.22
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 11.65s
                        Total time: 8380.77s
                               ETA: 1209766.8s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.321s, learning 0.259s)
               Value function loss: 0.5357
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 344.22
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 11.58s
                        Total time: 8392.35s
                               ETA: 1209667.9s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.491s, learning 0.185s)
               Value function loss: 0.5212
                    Surrogate loss: 0.0024
             Mean action noise std: 0.77
                       Mean reward: 344.22
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 11.68s
                        Total time: 8404.03s
                               ETA: 1209583.2s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.501s, learning 0.196s)
               Value function loss: 0.5302
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 344.22
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 11.70s
                        Total time: 8415.72s
                               ETA: 1209501.5s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.737s, learning 0.175s)
               Value function loss: 0.4614
                    Surrogate loss: 0.0056
             Mean action noise std: 0.77
                       Mean reward: 344.22
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 11.91s
                        Total time: 8427.64s
                               ETA: 1209451.0s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.700s, learning 0.207s)
               Value function loss: 0.7690
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 343.76
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 11.91s
                        Total time: 8439.54s
                               ETA: 1209399.9s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.468s, learning 0.171s)
               Value function loss: 0.4440
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 343.81
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 11.64s
                        Total time: 8451.18s
                               ETA: 1209310.7s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.440s, learning 0.162s)
               Value function loss: 1.1161
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 343.99
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 11.60s
                        Total time: 8462.78s
                               ETA: 1209216.3s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.057s, learning 0.179s)
               Value function loss: 1.6649
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 344.11
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 11.24s
                        Total time: 8474.02s
                               ETA: 1209069.8s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.013s, learning 0.201s)
               Value function loss: 0.7462
                    Surrogate loss: 0.0055
             Mean action noise std: 0.77
                       Mean reward: 343.46
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 11.21s
                        Total time: 8485.23s
                               ETA: 1208920.6s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.138s, learning 0.239s)
               Value function loss: 1.1534
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 343.14
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 11.38s
                        Total time: 8496.61s
                               ETA: 1208795.1s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.985s, learning 0.214s)
               Value function loss: 0.9847
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 343.04
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 12.20s
                        Total time: 8508.81s
                               ETA: 1208786.7s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.961s, learning 0.205s)
               Value function loss: 2.2137
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 342.28
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 12.17s
                        Total time: 8520.98s
                               ETA: 1208773.5s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.425s, learning 0.290s)
               Value function loss: 1.8079
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 341.28
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 11.72s
                        Total time: 8532.69s
                               ETA: 1208696.6s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.527s, learning 0.260s)
               Value function loss: 3.1560
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 342.07
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 11.79s
                        Total time: 8544.48s
                               ETA: 1208630.0s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1475 steps/s (collection: 10.911s, learning 0.190s)
               Value function loss: 3.2571
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 343.20
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 11.10s
                        Total time: 8555.58s
                               ETA: 1208466.5s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.110s, learning 0.173s)
               Value function loss: 3.7843
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 342.30
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 11.28s
                        Total time: 8566.86s
                               ETA: 1208329.2s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.205s, learning 0.204s)
               Value function loss: 5.7481
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 342.33
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 11.41s
                        Total time: 8578.27s
                               ETA: 1208210.0s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.302s, learning 0.278s)
               Value function loss: 7.5355
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 342.29
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 11.58s
                        Total time: 8589.85s
                               ETA: 1208115.2s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.548s, learning 0.210s)
               Value function loss: 11.0450
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 338.73
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 11.76s
                        Total time: 8601.61s
                               ETA: 1208045.7s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.401s, learning 0.223s)
               Value function loss: 9.6097
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 336.68
               Mean episode length: 249.67
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 11.62s
                        Total time: 8613.23s
                               ETA: 1207957.4s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.007s, learning 0.171s)
               Value function loss: 13.5233
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 337.31
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 11.18s
                        Total time: 8624.41s
                               ETA: 1207807.0s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.544s, learning 0.210s)
               Value function loss: 24.6790
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 333.76
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 11.75s
                        Total time: 8636.17s
                               ETA: 1207737.5s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.273s, learning 0.293s)
               Value function loss: 30.8523
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 335.93
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 11.57s
                        Total time: 8647.73s
                               ETA: 1207641.7s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.971s, learning 0.182s)
               Value function loss: 32.5366
                    Surrogate loss: 0.0069
             Mean action noise std: 0.77
                       Mean reward: 332.16
               Mean episode length: 249.95
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 11.15s
                        Total time: 8658.88s
                               ETA: 1207488.8s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.336s, learning 0.185s)
               Value function loss: 23.6138
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 332.03
               Mean episode length: 250.00
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 11.52s
                        Total time: 8670.41s
                               ETA: 1207387.4s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.435s, learning 0.224s)
               Value function loss: 15.8009
                    Surrogate loss: -0.0250
             Mean action noise std: 0.77
                       Mean reward: 329.90
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 11.66s
                        Total time: 8682.06s
                               ETA: 1207305.5s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.559s, learning 0.267s)
               Value function loss: 9.7434
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 331.52
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 11.83s
                        Total time: 8693.89s
                               ETA: 1207247.0s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.233s, learning 0.196s)
               Value function loss: 3.9549
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 334.18
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 11.43s
                        Total time: 8705.32s
                               ETA: 1207133.4s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.284s, learning 0.256s)
               Value function loss: 2.6138
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 332.96
               Mean episode length: 249.91
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 11.54s
                        Total time: 8716.86s
                               ETA: 1207035.7s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1498 steps/s (collection: 10.775s, learning 0.159s)
               Value function loss: 1.5750
                    Surrogate loss: 0.0287
             Mean action noise std: 0.77
                       Mean reward: 332.57
               Mean episode length: 249.91
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 10.93s
                        Total time: 8727.79s
                               ETA: 1206854.4s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1460 steps/s (collection: 11.061s, learning 0.161s)
               Value function loss: 1.1889
                    Surrogate loss: 0.0098
             Mean action noise std: 0.77
                       Mean reward: 332.27
               Mean episode length: 249.91
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 11.22s
                        Total time: 8739.01s
                               ETA: 1206713.2s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.410s, learning 0.181s)
               Value function loss: 0.7612
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 332.27
               Mean episode length: 249.91
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 11.59s
                        Total time: 8750.61s
                               ETA: 1206623.4s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.232s, learning 0.214s)
               Value function loss: 0.6789
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 332.27
               Mean episode length: 249.91
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 11.45s
                        Total time: 8762.05s
                               ETA: 1206513.8s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1458 steps/s (collection: 11.072s, learning 0.161s)
               Value function loss: 0.7651
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 332.27
               Mean episode length: 249.91
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 11.23s
                        Total time: 8773.28s
                               ETA: 1206375.2s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.731s, learning 0.202s)
               Value function loss: 0.7717
                    Surrogate loss: 0.0067
             Mean action noise std: 0.77
                       Mean reward: 332.27
               Mean episode length: 249.91
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 11.93s
                        Total time: 8785.22s
                               ETA: 1206333.1s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.168s, learning 0.163s)
               Value function loss: 1.2346
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 332.87
               Mean episode length: 249.91
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 11.33s
                        Total time: 8796.55s
                               ETA: 1206208.5s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.068s, learning 0.183s)
               Value function loss: 1.0628
                    Surrogate loss: 0.0602
             Mean action noise std: 0.77
                       Mean reward: 332.35
               Mean episode length: 249.91
                  Mean reward/step: 1.48
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 11.25s
                        Total time: 8807.80s
                               ETA: 1206073.3s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.748s, learning 0.162s)
               Value function loss: 1.0410
                    Surrogate loss: -0.0015
             Mean action noise std: 0.77
                       Mean reward: 331.94
               Mean episode length: 249.91
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 11.91s
                        Total time: 8819.71s
                               ETA: 1206028.6s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.294s, learning 0.155s)
               Value function loss: 1.0977
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 330.24
               Mean episode length: 249.91
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 11.45s
                        Total time: 8831.16s
                               ETA: 1205921.0s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.735s, learning 0.191s)
               Value function loss: 1.1695
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 329.43
               Mean episode length: 249.91
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 11.93s
                        Total time: 8843.09s
                               ETA: 1205878.7s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.503s, learning 0.161s)
               Value function loss: 1.0173
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 329.95
               Mean episode length: 249.91
                  Mean reward/step: 1.42
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 11.66s
                        Total time: 8854.75s
                               ETA: 1205800.8s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.325s, learning 0.163s)
               Value function loss: 1.4320
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 329.39
               Mean episode length: 249.91
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 11.49s
                        Total time: 8866.24s
                               ETA: 1205699.1s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.703s, learning 0.159s)
               Value function loss: 1.5849
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 331.03
               Mean episode length: 249.91
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 11.86s
                        Total time: 8878.10s
                               ETA: 1205648.4s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.095s, learning 0.214s)
               Value function loss: 2.8542
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 331.49
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 11.31s
                        Total time: 8889.41s
                               ETA: 1205522.8s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.663s, learning 0.158s)
               Value function loss: 3.0655
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 330.40
               Mean episode length: 248.92
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 11.82s
                        Total time: 8901.23s
                               ETA: 1205467.0s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.206s, learning 0.190s)
               Value function loss: 2.8326
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 332.70
               Mean episode length: 248.70
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 11.40s
                        Total time: 8912.63s
                               ETA: 1205353.8s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.295s, learning 0.160s)
               Value function loss: 2.5662
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 330.24
               Mean episode length: 248.70
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 11.45s
                        Total time: 8924.08s
                               ETA: 1205248.7s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.246s, learning 0.162s)
               Value function loss: 4.0060
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 331.83
               Mean episode length: 248.67
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 11.41s
                        Total time: 8935.49s
                               ETA: 1205137.7s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.261s, learning 0.173s)
               Value function loss: 4.7426
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 329.13
               Mean episode length: 247.84
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 11.43s
                        Total time: 8946.92s
                               ETA: 1205030.5s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.180s, learning 0.177s)
               Value function loss: 6.8482
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 325.57
               Mean episode length: 248.69
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 11.36s
                        Total time: 8958.28s
                               ETA: 1204913.0s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.521s, learning 0.200s)
               Value function loss: 7.3107
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 327.41
               Mean episode length: 249.52
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 11.72s
                        Total time: 8970.00s
                               ETA: 1204844.7s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.270s, learning 0.218s)
               Value function loss: 9.8806
                    Surrogate loss: 0.0019
             Mean action noise std: 0.77
                       Mean reward: 332.57
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 11.49s
                        Total time: 8981.49s
                               ETA: 1204745.5s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.888s, learning 0.178s)
               Value function loss: 23.7643
                    Surrogate loss: 0.0167
             Mean action noise std: 0.77
                       Mean reward: 330.85
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 12.07s
                        Total time: 8993.56s
                               ETA: 1204723.8s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.035s, learning 0.176s)
               Value function loss: 32.2110
                    Surrogate loss: 0.0044
             Mean action noise std: 0.77
                       Mean reward: 331.01
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 11.21s
                        Total time: 9004.77s
                               ETA: 1204587.9s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.679s, learning 0.175s)
               Value function loss: 41.6165
                    Surrogate loss: 0.0054
             Mean action noise std: 0.77
                       Mean reward: 337.59
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 11.85s
                        Total time: 9016.62s
                               ETA: 1204538.1s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.729s, learning 0.160s)
               Value function loss: 38.8463
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 339.60
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 11.89s
                        Total time: 9028.51s
                               ETA: 1204493.1s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1468 steps/s (collection: 10.992s, learning 0.166s)
               Value function loss: 27.5556
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 340.40
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 11.16s
                        Total time: 9039.67s
                               ETA: 1204350.8s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.047s, learning 0.257s)
               Value function loss: 16.9000
                    Surrogate loss: -0.0204
             Mean action noise std: 0.77
                       Mean reward: 343.96
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 11.30s
                        Total time: 9050.97s
                               ETA: 1204228.2s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.298s, learning 0.176s)
               Value function loss: 6.6692
                    Surrogate loss: -0.0234
             Mean action noise std: 0.77
                       Mean reward: 340.72
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 11.47s
                        Total time: 9062.45s
                               ETA: 1204128.6s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.168s)
               Value function loss: 3.4714
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 343.75
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 11.52s
                        Total time: 9073.97s
                               ETA: 1204035.5s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.615s, learning 0.168s)
               Value function loss: 2.6630
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 347.52
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 11.78s
                        Total time: 9085.75s
                               ETA: 1203977.2s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.555s, learning 0.160s)
               Value function loss: 2.1613
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 347.91
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 11.72s
                        Total time: 9097.47s
                               ETA: 1203910.2s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.163s)
               Value function loss: 0.8179
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 347.91
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 11.54s
                        Total time: 9109.01s
                               ETA: 1203820.3s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.789s, learning 0.201s)
               Value function loss: 0.6811
                    Surrogate loss: 0.0152
             Mean action noise std: 0.76
                       Mean reward: 347.91
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 11.99s
                        Total time: 9121.00s
                               ETA: 1203789.8s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.575s, learning 0.188s)
               Value function loss: 0.5553
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 347.91
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 11.76s
                        Total time: 9132.76s
                               ETA: 1203729.5s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.268s, learning 0.234s)
               Value function loss: 1.3200
                    Surrogate loss: 0.0047
             Mean action noise std: 0.76
                       Mean reward: 344.02
               Mean episode length: 247.19
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 11.50s
                        Total time: 9144.26s
                               ETA: 1203634.8s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.552s, learning 0.196s)
               Value function loss: 0.4627
                    Surrogate loss: -0.0025
             Mean action noise std: 0.76
                       Mean reward: 344.02
               Mean episode length: 247.19
                  Mean reward/step: 1.56
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 11.75s
                        Total time: 9156.01s
                               ETA: 1203572.8s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.180s, learning 0.163s)
               Value function loss: 0.7546
                    Surrogate loss: -0.0063
             Mean action noise std: 0.76
                       Mean reward: 343.69
               Mean episode length: 247.19
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 11.34s
                        Total time: 9167.35s
                               ETA: 1203457.8s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.188s, learning 0.170s)
               Value function loss: 1.3103
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 341.06
               Mean episode length: 244.82
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 11.36s
                        Total time: 9178.71s
                               ETA: 1203345.0s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.805s, learning 0.223s)
               Value function loss: 0.8971
                    Surrogate loss: 0.0026
             Mean action noise std: 0.76
                       Mean reward: 341.83
               Mean episode length: 244.82
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 12.03s
                        Total time: 9190.74s
                               ETA: 1203320.2s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.070s, learning 0.158s)
               Value function loss: 1.9179
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 342.81
               Mean episode length: 243.73
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 11.23s
                        Total time: 9201.97s
                               ETA: 1203190.8s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.146s, learning 0.168s)
               Value function loss: 0.8763
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 341.69
               Mean episode length: 242.33
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 11.31s
                        Total time: 9213.28s
                               ETA: 1203073.0s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.341s, learning 0.169s)
               Value function loss: 1.3618
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 342.85
               Mean episode length: 242.33
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 11.51s
                        Total time: 9224.79s
                               ETA: 1202981.0s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.603s, learning 0.163s)
               Value function loss: 1.8470
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 344.50
               Mean episode length: 242.33
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 11.77s
                        Total time: 9236.56s
                               ETA: 1202922.6s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.666s, learning 0.192s)
               Value function loss: 2.9989
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 345.56
               Mean episode length: 242.33
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 11.86s
                        Total time: 9248.42s
                               ETA: 1202876.3s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.183s, learning 0.160s)
               Value function loss: 2.9545
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 346.31
               Mean episode length: 242.33
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 11.34s
                        Total time: 9259.76s
                               ETA: 1202763.1s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.212s, learning 0.160s)
               Value function loss: 4.1487
                    Surrogate loss: 0.0015
             Mean action noise std: 0.76
                       Mean reward: 348.41
               Mean episode length: 242.33
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 11.37s
                        Total time: 9271.13s
                               ETA: 1202653.9s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.314s, learning 0.287s)
               Value function loss: 2.8205
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 349.48
               Mean episode length: 242.33
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 11.60s
                        Total time: 9282.74s
                               ETA: 1202574.7s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.326s, learning 0.193s)
               Value function loss: 5.8588
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 355.89
               Mean episode length: 246.71
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 11.52s
                        Total time: 9294.25s
                               ETA: 1202485.1s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.641s, learning 0.191s)
               Value function loss: 5.7539
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 357.70
               Mean episode length: 249.20
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 11.83s
                        Total time: 9306.09s
                               ETA: 1202436.0s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.438s, learning 0.176s)
               Value function loss: 9.2776
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 356.52
               Mean episode length: 249.20
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 11.61s
                        Total time: 9317.70s
                               ETA: 1202358.9s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1465 steps/s (collection: 11.016s, learning 0.165s)
               Value function loss: 11.8212
                    Surrogate loss: -0.0023
             Mean action noise std: 0.76
                       Mean reward: 358.54
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 11.18s
                        Total time: 9328.88s
                               ETA: 1202226.2s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.461s, learning 0.202s)
               Value function loss: 13.0290
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 357.03
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 11.66s
                        Total time: 9340.54s
                               ETA: 1202155.8s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.551s, learning 0.198s)
               Value function loss: 27.1501
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 358.56
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 11.75s
                        Total time: 9352.29s
                               ETA: 1202096.7s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.797s, learning 0.181s)
               Value function loss: 42.1302
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 358.09
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 11.98s
                        Total time: 9364.27s
                               ETA: 1202067.1s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.768s, learning 0.163s)
               Value function loss: 37.2746
                    Surrogate loss: 0.0092
             Mean action noise std: 0.76
                       Mean reward: 358.89
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 11.93s
                        Total time: 9376.20s
                               ETA: 1202031.4s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.275s, learning 0.166s)
               Value function loss: 29.1456
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 357.91
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 11.44s
                        Total time: 9387.64s
                               ETA: 1201933.2s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.659s, learning 0.195s)
               Value function loss: 21.9846
                    Surrogate loss: -0.0138
             Mean action noise std: 0.76
                       Mean reward: 361.20
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 11.85s
                        Total time: 9399.50s
                               ETA: 1201887.9s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1436 steps/s (collection: 11.245s, learning 0.158s)
               Value function loss: 15.5503
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 360.00
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 11.40s
                        Total time: 9410.90s
                               ETA: 1201785.2s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1550 steps/s (collection: 10.399s, learning 0.164s)
               Value function loss: 6.9165
                    Surrogate loss: -0.0227
             Mean action noise std: 0.76
                       Mean reward: 355.26
               Mean episode length: 248.54
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 10.56s
                        Total time: 9421.46s
                               ETA: 1201575.6s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.591s, learning 0.166s)
               Value function loss: 2.8222
                    Surrogate loss: -0.0161
             Mean action noise std: 0.76
                       Mean reward: 360.39
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 11.76s
                        Total time: 9433.22s
                               ETA: 1201518.6s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1456 steps/s (collection: 11.079s, learning 0.167s)
               Value function loss: 1.8557
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 361.49
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 11.25s
                        Total time: 9444.47s
                               ETA: 1201396.7s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.267s, learning 0.162s)
               Value function loss: 1.1196
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 360.82
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 11.43s
                        Total time: 9455.90s
                               ETA: 1201298.2s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.058s, learning 0.185s)
               Value function loss: 0.5449
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 360.92
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 11.24s
                        Total time: 9467.14s
                               ETA: 1201176.4s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.451s, learning 0.172s)
               Value function loss: 0.4566
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 358.68
               Mean episode length: 248.54
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 11.62s
                        Total time: 9478.76s
                               ETA: 1201103.1s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.248s, learning 0.163s)
               Value function loss: 0.2923
                    Surrogate loss: -0.0119
             Mean action noise std: 0.76
                       Mean reward: 358.68
               Mean episode length: 248.54
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 11.41s
                        Total time: 9490.17s
                               ETA: 1201003.1s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.575s, learning 0.159s)
               Value function loss: 0.6027
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 358.82
               Mean episode length: 248.54
                  Mean reward/step: 1.53
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 11.73s
                        Total time: 9501.91s
                               ETA: 1200944.1s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.227s, learning 0.234s)
               Value function loss: 0.2813
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 357.50
               Mean episode length: 247.57
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 11.46s
                        Total time: 9513.37s
                               ETA: 1200850.9s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.732s, learning 0.163s)
               Value function loss: 0.5748
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 357.66
               Mean episode length: 247.57
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 11.89s
                        Total time: 9525.26s
                               ETA: 1200812.5s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.346s, learning 0.214s)
               Value function loss: 1.2277
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 353.37
               Mean episode length: 244.65
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 11.56s
                        Total time: 9536.82s
                               ETA: 1200732.0s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.177s, learning 0.184s)
               Value function loss: 0.7282
                    Surrogate loss: 0.0103
             Mean action noise std: 0.76
                       Mean reward: 353.66
               Mean episode length: 244.65
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 11.36s
                        Total time: 9548.18s
                               ETA: 1200626.7s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.644s, learning 0.180s)
               Value function loss: 1.5487
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 353.12
               Mean episode length: 244.65
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 11.82s
                        Total time: 9560.01s
                               ETA: 1200579.7s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.505s, learning 0.159s)
               Value function loss: 0.6490
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 353.09
               Mean episode length: 244.65
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 11.66s
                        Total time: 9571.67s
                               ETA: 1200512.8s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.098s, learning 0.179s)
               Value function loss: 1.0026
                    Surrogate loss: -0.0048
             Mean action noise std: 0.76
                       Mean reward: 353.10
               Mean episode length: 244.65
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 11.28s
                        Total time: 9582.95s
                               ETA: 1200397.6s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.110s, learning 0.222s)
               Value function loss: 2.1680
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 353.77
               Mean episode length: 244.65
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 11.33s
                        Total time: 9594.28s
                               ETA: 1200289.4s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.196s, learning 0.171s)
               Value function loss: 2.6444
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 353.24
               Mean episode length: 244.65
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 11.37s
                        Total time: 9605.65s
                               ETA: 1200185.9s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.510s, learning 0.157s)
               Value function loss: 2.0634
                    Surrogate loss: -0.0090
             Mean action noise std: 0.76
                       Mean reward: 353.72
               Mean episode length: 244.65
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 11.67s
                        Total time: 9617.31s
                               ETA: 1200119.9s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.045s, learning 0.228s)
               Value function loss: 3.9415
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 353.81
               Mean episode length: 244.65
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 11.27s
                        Total time: 9628.59s
                               ETA: 1200005.1s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.525s, learning 0.157s)
               Value function loss: 4.1429
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 353.60
               Mean episode length: 244.04
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 11.68s
                        Total time: 9640.27s
                               ETA: 1199941.4s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.501s, learning 0.301s)
               Value function loss: 5.3408
                    Surrogate loss: -0.0070
             Mean action noise std: 0.76
                       Mean reward: 358.46
               Mean episode length: 248.77
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 11.80s
                        Total time: 9652.07s
                               ETA: 1199892.8s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.559s, learning 0.212s)
               Value function loss: 5.2986
                    Surrogate loss: -0.0111
             Mean action noise std: 0.76
                       Mean reward: 358.37
               Mean episode length: 248.77
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 11.77s
                        Total time: 9663.84s
                               ETA: 1199840.5s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.322s, learning 0.181s)
               Value function loss: 9.3702
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 357.12
               Mean episode length: 248.77
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 11.50s
                        Total time: 9675.35s
                               ETA: 1199754.9s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.263s, learning 0.187s)
               Value function loss: 13.9064
                    Surrogate loss: -0.0107
             Mean action noise std: 0.76
                       Mean reward: 358.00
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 11.45s
                        Total time: 9686.79s
                               ETA: 1199663.0s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.599s, learning 0.160s)
               Value function loss: 11.7190
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 356.80
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 11.76s
                        Total time: 9698.55s
                               ETA: 1199609.5s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.848s, learning 0.171s)
               Value function loss: 27.3599
                    Surrogate loss: 0.0092
             Mean action noise std: 0.76
                       Mean reward: 354.93
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 12.02s
                        Total time: 9710.57s
                               ETA: 1199588.3s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.800s, learning 0.160s)
               Value function loss: 41.9412
                    Surrogate loss: 0.0051
             Mean action noise std: 0.76
                       Mean reward: 357.27
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 11.96s
                        Total time: 9722.53s
                               ETA: 1199559.9s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.331s, learning 0.180s)
               Value function loss: 50.4228
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 356.56
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 11.51s
                        Total time: 9734.05s
                               ETA: 1199476.2s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.310s, learning 0.160s)
               Value function loss: 39.7206
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 357.84
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 11.47s
                        Total time: 9745.52s
                               ETA: 1199387.6s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.145s, learning 0.165s)
               Value function loss: 32.5566
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 352.97
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 11.31s
                        Total time: 9756.82s
                               ETA: 1199279.4s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.220s, learning 0.266s)
               Value function loss: 15.0650
                    Surrogate loss: -0.0169
             Mean action noise std: 0.76
                       Mean reward: 350.32
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 11.49s
                        Total time: 9768.31s
                               ETA: 1199193.1s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.781s, learning 0.167s)
               Value function loss: 7.1824
                    Surrogate loss: -0.0205
             Mean action noise std: 0.76
                       Mean reward: 349.62
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 11.95s
                        Total time: 9780.26s
                               ETA: 1199163.7s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.475s, learning 0.156s)
               Value function loss: 3.2793
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 350.38
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 11.63s
                        Total time: 9791.89s
                               ETA: 1199095.4s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.566s, learning 0.266s)
               Value function loss: 2.7736
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 348.78
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 11.83s
                        Total time: 9803.72s
                               ETA: 1199051.9s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.110s, learning 0.175s)
               Value function loss: 1.3748
                    Surrogate loss: 0.0232
             Mean action noise std: 0.76
                       Mean reward: 347.74
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 11.28s
                        Total time: 9815.01s
                               ETA: 1198941.6s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.161s)
               Value function loss: 1.0030
                    Surrogate loss: 0.0059
             Mean action noise std: 0.76
                       Mean reward: 347.57
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 11.22s
                        Total time: 9826.23s
                               ETA: 1198824.0s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.736s, learning 0.166s)
               Value function loss: 0.5884
                    Surrogate loss: 0.0277
             Mean action noise std: 0.76
                       Mean reward: 347.18
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 11.90s
                        Total time: 9838.13s
                               ETA: 1198789.4s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.349s, learning 0.185s)
               Value function loss: 0.5574
                    Surrogate loss: 0.0306
             Mean action noise std: 0.76
                       Mean reward: 347.18
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 11.53s
                        Total time: 9849.66s
                               ETA: 1198710.1s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.316s, learning 0.165s)
               Value function loss: 0.4665
                    Surrogate loss: -0.0010
             Mean action noise std: 0.76
                       Mean reward: 345.50
               Mean episode length: 248.74
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 11.48s
                        Total time: 9861.14s
                               ETA: 1198624.5s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.293s, learning 0.200s)
               Value function loss: 0.6077
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 345.08
               Mean episode length: 248.74
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 11.49s
                        Total time: 9872.64s
                               ETA: 1198540.5s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.616s, learning 0.179s)
               Value function loss: 1.0429
                    Surrogate loss: 0.0387
             Mean action noise std: 0.76
                       Mean reward: 341.51
               Mean episode length: 246.38
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 11.80s
                        Total time: 9884.43s
                               ETA: 1198493.5s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.699s, learning 0.190s)
               Value function loss: 0.8128
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 340.26
               Mean episode length: 246.38
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 11.89s
                        Total time: 9896.32s
                               ETA: 1198457.8s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.481s, learning 0.170s)
               Value function loss: 1.0517
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 338.67
               Mean episode length: 246.38
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 11.65s
                        Total time: 9907.97s
                               ETA: 1198393.4s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.422s, learning 0.253s)
               Value function loss: 2.2185
                    Surrogate loss: -0.0033
             Mean action noise std: 0.76
                       Mean reward: 328.23
               Mean episode length: 239.92
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 11.67s
                        Total time: 9919.65s
                               ETA: 1198332.0s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.643s, learning 0.258s)
               Value function loss: 1.9217
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 320.59
               Mean episode length: 235.37
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 11.90s
                        Total time: 9931.55s
                               ETA: 1198298.0s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.249s, learning 0.168s)
               Value function loss: 1.7877
                    Surrogate loss: -0.0019
             Mean action noise std: 0.76
                       Mean reward: 313.94
               Mean episode length: 230.33
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 11.42s
                        Total time: 9942.96s
                               ETA: 1198205.8s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1455 steps/s (collection: 11.084s, learning 0.169s)
               Value function loss: 1.2016
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 312.62
               Mean episode length: 229.40
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 11.25s
                        Total time: 9954.22s
                               ETA: 1198094.1s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1465 steps/s (collection: 10.981s, learning 0.200s)
               Value function loss: 2.0095
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 309.66
               Mean episode length: 226.95
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 11.18s
                        Total time: 9965.40s
                               ETA: 1197973.9s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1466 steps/s (collection: 11.012s, learning 0.159s)
               Value function loss: 2.4171
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 305.98
               Mean episode length: 225.54
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 11.17s
                        Total time: 9976.57s
                               ETA: 1197852.8s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.165s)
               Value function loss: 3.3831
                    Surrogate loss: -0.0086
             Mean action noise std: 0.76
                       Mean reward: 305.47
               Mean episode length: 225.54
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 11.40s
                        Total time: 9987.97s
                               ETA: 1197759.0s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.826s, learning 0.212s)
               Value function loss: 2.9638
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 308.79
               Mean episode length: 228.49
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 12.04s
                        Total time: 10000.01s
                               ETA: 1197742.2s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.255s, learning 0.188s)
               Value function loss: 3.6552
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 317.32
               Mean episode length: 232.66
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 11.44s
                        Total time: 10011.45s
                               ETA: 1197654.2s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.396s, learning 0.163s)
               Value function loss: 4.9293
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 335.01
               Mean episode length: 245.14
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 11.56s
                        Total time: 10023.01s
                               ETA: 1197580.4s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.565s, learning 0.162s)
               Value function loss: 6.1141
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 339.89
               Mean episode length: 247.69
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 11.73s
                        Total time: 10034.74s
                               ETA: 1197526.7s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.283s, learning 0.164s)
               Value function loss: 10.7150
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 345.72
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 11.45s
                        Total time: 10046.18s
                               ETA: 1197439.8s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.564s, learning 0.179s)
               Value function loss: 10.8089
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 347.54
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 11.74s
                        Total time: 10057.93s
                               ETA: 1197388.2s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.370s, learning 0.167s)
               Value function loss: 16.9896
                    Surrogate loss: 0.0007
             Mean action noise std: 0.76
                       Mean reward: 345.95
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 11.54s
                        Total time: 10069.46s
                               ETA: 1197312.1s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.303s, learning 0.170s)
               Value function loss: 33.7890
                    Surrogate loss: 0.0031
             Mean action noise std: 0.76
                       Mean reward: 348.80
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 11.47s
                        Total time: 10080.93s
                               ETA: 1197228.7s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.722s, learning 0.175s)
               Value function loss: 40.2002
                    Surrogate loss: 0.0097
             Mean action noise std: 0.76
                       Mean reward: 349.14
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 11.90s
                        Total time: 10092.83s
                               ETA: 1197195.7s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.474s, learning 0.166s)
               Value function loss: 41.3312
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 345.84
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 11.64s
                        Total time: 10104.47s
                               ETA: 1197132.3s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.451s, learning 0.165s)
               Value function loss: 28.0975
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 349.58
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 11.62s
                        Total time: 10116.09s
                               ETA: 1197066.2s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.168s)
               Value function loss: 23.3396
                    Surrogate loss: 0.0015
             Mean action noise std: 0.76
                       Mean reward: 347.97
               Mean episode length: 249.79
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 11.55s
                        Total time: 10127.64s
                               ETA: 1196992.9s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.889s, learning 0.191s)
               Value function loss: 11.6127
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 347.01
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 12.08s
                        Total time: 10139.72s
                               ETA: 1196981.8s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.298s, learning 0.234s)
               Value function loss: 4.3714
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 346.59
               Mean episode length: 248.52
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 11.53s
                        Total time: 10151.25s
                               ETA: 1196906.2s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.165s)
               Value function loss: 2.1219
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 346.21
               Mean episode length: 248.52
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 11.55s
                        Total time: 10162.80s
                               ETA: 1196832.8s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.435s, learning 0.172s)
               Value function loss: 1.1573
                    Surrogate loss: -0.0160
             Mean action noise std: 0.76
                       Mean reward: 346.61
               Mean episode length: 248.52
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 11.61s
                        Total time: 10174.41s
                               ETA: 1196766.3s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1459 steps/s (collection: 11.054s, learning 0.172s)
               Value function loss: 0.7537
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 346.88
               Mean episode length: 248.52
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 11.23s
                        Total time: 10185.63s
                               ETA: 1196655.2s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.129s, learning 0.167s)
               Value function loss: 0.3301
                    Surrogate loss: -0.0165
             Mean action noise std: 0.76
                       Mean reward: 346.91
               Mean episode length: 248.52
                  Mean reward/step: 1.51
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 11.30s
                        Total time: 10196.93s
                               ETA: 1196552.5s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.462s, learning 0.194s)
               Value function loss: 0.2643
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 346.91
               Mean episode length: 248.52
                  Mean reward/step: 1.52
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 11.66s
                        Total time: 10208.59s
                               ETA: 1196492.2s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.715s, learning 0.159s)
               Value function loss: 0.7811
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 343.27
               Mean episode length: 245.53
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 11.87s
                        Total time: 10220.46s
                               ETA: 1196457.7s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.347s, learning 0.162s)
               Value function loss: 0.5197
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 343.56
               Mean episode length: 245.53
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 11.51s
                        Total time: 10231.97s
                               ETA: 1196380.4s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.276s, learning 0.202s)
               Value function loss: 1.0577
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 344.07
               Mean episode length: 245.53
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 11.48s
                        Total time: 10243.45s
                               ETA: 1196299.7s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.508s, learning 0.180s)
               Value function loss: 0.9987
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 343.03
               Mean episode length: 244.22
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 11.69s
                        Total time: 10255.14s
                               ETA: 1196243.5s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1440 steps/s (collection: 11.147s, learning 0.228s)
               Value function loss: 1.8898
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 335.40
               Mean episode length: 238.73
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 11.38s
                        Total time: 10266.51s
                               ETA: 1196151.1s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1428 steps/s (collection: 11.282s, learning 0.184s)
               Value function loss: 1.5851
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 334.39
               Mean episode length: 237.40
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 11.47s
                        Total time: 10277.98s
                               ETA: 1196069.4s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1457 steps/s (collection: 11.080s, learning 0.164s)
               Value function loss: 1.9830
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 323.70
               Mean episode length: 228.68
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 11.24s
                        Total time: 10289.22s
                               ETA: 1195962.1s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.480s, learning 0.204s)
               Value function loss: 1.4273
                    Surrogate loss: 0.0025
             Mean action noise std: 0.76
                       Mean reward: 319.72
               Mean episode length: 225.21
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 11.68s
                        Total time: 10300.91s
                               ETA: 1195906.1s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.367s, learning 0.162s)
               Value function loss: 1.3363
                    Surrogate loss: -0.0036
             Mean action noise std: 0.76
                       Mean reward: 312.05
               Mean episode length: 219.84
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 11.53s
                        Total time: 10312.43s
                               ETA: 1195832.3s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.194s, learning 0.164s)
               Value function loss: 1.8683
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 308.05
               Mean episode length: 217.68
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 11.36s
                        Total time: 10323.79s
                               ETA: 1195738.7s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.488s, learning 0.167s)
               Value function loss: 2.3047
                    Surrogate loss: -0.0044
             Mean action noise std: 0.76
                       Mean reward: 307.69
               Mean episode length: 216.61
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 11.65s
                        Total time: 10335.45s
                               ETA: 1195679.7s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.143s, learning 0.154s)
               Value function loss: 2.8649
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 311.62
               Mean episode length: 220.02
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 11.30s
                        Total time: 10346.74s
                               ETA: 1195579.4s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.108s, learning 0.170s)
               Value function loss: 3.2520
                    Surrogate loss: -0.0117
             Mean action noise std: 0.76
                       Mean reward: 321.54
               Mean episode length: 227.36
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 11.28s
                        Total time: 10358.02s
                               ETA: 1195477.3s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1442 steps/s (collection: 11.187s, learning 0.169s)
               Value function loss: 3.4361
                    Surrogate loss: -0.0066
             Mean action noise std: 0.76
                       Mean reward: 332.37
               Mean episode length: 234.84
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 11.36s
                        Total time: 10369.38s
                               ETA: 1195384.2s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.372s, learning 0.194s)
               Value function loss: 5.5407
                    Surrogate loss: -0.0125
             Mean action noise std: 0.76
                       Mean reward: 346.72
               Mean episode length: 244.92
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 11.57s
                        Total time: 10380.94s
                               ETA: 1195315.5s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.568s, learning 0.188s)
               Value function loss: 7.1294
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 355.28
               Mean episode length: 249.04
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 11.76s
                        Total time: 10392.70s
                               ETA: 1195268.8s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.212s, learning 0.172s)
               Value function loss: 11.3029
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 355.99
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 11.38s
                        Total time: 10404.08s
                               ETA: 1195179.4s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.743s, learning 0.158s)
               Value function loss: 10.2960
                    Surrogate loss: -0.0109
             Mean action noise std: 0.76
                       Mean reward: 357.22
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 11.90s
                        Total time: 10415.98s
                               ETA: 1195149.6s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1449 steps/s (collection: 11.144s, learning 0.163s)
               Value function loss: 14.9824
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 358.05
               Mean episode length: 248.56
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 11.31s
                        Total time: 10427.29s
                               ETA: 1195051.7s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1452 steps/s (collection: 11.115s, learning 0.167s)
               Value function loss: 33.1297
                    Surrogate loss: 0.0108
             Mean action noise std: 0.76
                       Mean reward: 357.79
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 11.28s
                        Total time: 10438.57s
                               ETA: 1194951.2s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1460 steps/s (collection: 10.980s, learning 0.236s)
               Value function loss: 42.8429
                    Surrogate loss: -0.0004
             Mean action noise std: 0.76
                       Mean reward: 357.81
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 11.22s
                        Total time: 10449.79s
                               ETA: 1194843.4s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.492s, learning 0.159s)
               Value function loss: 48.5835
                    Surrogate loss: 0.0022
             Mean action noise std: 0.76
                       Mean reward: 359.65
               Mean episode length: 249.78
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 11.65s
                        Total time: 10461.44s
                               ETA: 1194785.4s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.485s, learning 0.167s)
               Value function loss: 46.4608
                    Surrogate loss: 0.0010
             Mean action noise std: 0.76
                       Mean reward: 359.38
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 11.65s
                        Total time: 10473.09s
                               ETA: 1194727.7s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.417s, learning 0.202s)
               Value function loss: 27.0695
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 358.69
               Mean episode length: 249.98
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 11.62s
                        Total time: 10484.71s
                               ETA: 1194666.2s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.754s, learning 0.224s)
               Value function loss: 13.2687
                    Surrogate loss: 0.0045
             Mean action noise std: 0.76
                       Mean reward: 361.13
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 11.98s
                        Total time: 10496.69s
                               ETA: 1194645.8s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.477s, learning 0.159s)
               Value function loss: 6.0510
                    Surrogate loss: -0.0171
             Mean action noise std: 0.76
                       Mean reward: 359.59
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 11.64s
                        Total time: 10508.32s
                               ETA: 1194586.5s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1470 steps/s (collection: 10.977s, learning 0.165s)
               Value function loss: 2.1890
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 359.63
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 11.14s
                        Total time: 10519.46s
                               ETA: 1194471.3s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1453 steps/s (collection: 11.070s, learning 0.203s)
               Value function loss: 2.1137
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 360.22
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 11.27s
                        Total time: 10530.74s
                               ETA: 1194371.1s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.155s, learning 0.244s)
               Value function loss: 1.2999
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 359.63
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 11.40s
                        Total time: 10542.14s
                               ETA: 1194285.4s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.611s, learning 0.194s)
               Value function loss: 0.4517
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 359.63
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 11.80s
                        Total time: 10553.94s
                               ETA: 1194245.8s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.164s)
               Value function loss: 0.5813
                    Surrogate loss: 0.0028
             Mean action noise std: 0.76
                       Mean reward: 359.19
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 11.12s
                        Total time: 10565.06s
                               ETA: 1194128.9s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1443 steps/s (collection: 11.164s, learning 0.187s)
               Value function loss: 0.9703
                    Surrogate loss: 0.0155
             Mean action noise std: 0.76
                       Mean reward: 359.14
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 11.35s
                        Total time: 10576.41s
                               ETA: 1194038.3s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.133s, learning 0.165s)
               Value function loss: 0.8984
                    Surrogate loss: 0.0071
             Mean action noise std: 0.76
                       Mean reward: 357.41
               Mean episode length: 248.65
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 11.30s
                        Total time: 10587.71s
                               ETA: 1193942.0s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1479 steps/s (collection: 10.910s, learning 0.165s)
               Value function loss: 0.5003
                    Surrogate loss: -0.0052
             Mean action noise std: 0.76
                       Mean reward: 357.60
               Mean episode length: 248.65
                  Mean reward/step: 1.50
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 11.08s
                        Total time: 10598.79s
                               ETA: 1193820.7s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.485s, learning 0.157s)
               Value function loss: 1.1053
                    Surrogate loss: -0.0026
             Mean action noise std: 0.76
                       Mean reward: 351.27
               Mean episode length: 244.11
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 11.64s
                        Total time: 10610.43s
                               ETA: 1193763.4s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.370s, learning 0.157s)
               Value function loss: 1.7850
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 350.64
               Mean episode length: 244.11
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 11.53s
                        Total time: 10621.95s
                               ETA: 1193693.2s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.111s, learning 0.186s)
               Value function loss: 1.9336
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 348.97
               Mean episode length: 243.18
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 11.30s
                        Total time: 10633.25s
                               ETA: 1193597.4s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.655s, learning 0.207s)
               Value function loss: 2.5142
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 349.22
               Mean episode length: 243.18
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 11.86s
                        Total time: 10645.11s
                               ETA: 1193565.0s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.295s, learning 0.165s)
               Value function loss: 2.8066
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 345.14
               Mean episode length: 240.70
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 11.46s
                        Total time: 10656.57s
                               ETA: 1193487.8s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.574s, learning 0.197s)
               Value function loss: 2.5360
                    Surrogate loss: -0.0133
             Mean action noise std: 0.76
                       Mean reward: 337.87
               Mean episode length: 235.07
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 11.77s
                        Total time: 10668.34s
                               ETA: 1193445.6s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.460s, learning 0.184s)
               Value function loss: 2.4949
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 335.58
               Mean episode length: 233.83
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 11.64s
                        Total time: 10679.99s
                               ETA: 1193389.1s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.457s, learning 0.180s)
               Value function loss: 2.7759
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 338.48
               Mean episode length: 235.18
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 11.64s
                        Total time: 10691.62s
                               ETA: 1193332.1s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.483s, learning 0.161s)
               Value function loss: 2.9894
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 343.58
               Mean episode length: 238.77
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 11.64s
                        Total time: 10703.27s
                               ETA: 1193276.0s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.428s, learning 0.191s)
               Value function loss: 4.2069
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 344.74
               Mean episode length: 239.70
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 11.62s
                        Total time: 10714.89s
                               ETA: 1193217.1s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.372s, learning 0.190s)
               Value function loss: 2.9544
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 346.54
               Mean episode length: 240.85
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 11.56s
                        Total time: 10726.45s
                               ETA: 1193151.9s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.778s, learning 0.173s)
               Value function loss: 5.8279
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 353.85
               Mean episode length: 247.81
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 11.95s
                        Total time: 10738.40s
                               ETA: 1193130.1s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.737s, learning 0.160s)
               Value function loss: 5.2924
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 352.28
               Mean episode length: 247.62
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 11.90s
                        Total time: 10750.30s
                               ETA: 1193102.4s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.412s, learning 0.163s)
               Value function loss: 8.3246
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 351.16
               Mean episode length: 248.29
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 11.58s
                        Total time: 10761.87s
                               ETA: 1193039.0s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.161s)
               Value function loss: 10.4226
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 353.81
               Mean episode length: 249.53
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 11.39s
                        Total time: 10773.27s
                               ETA: 1192955.7s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.363s, learning 0.161s)
               Value function loss: 13.9216
                    Surrogate loss: -0.0089
             Mean action noise std: 0.76
                       Mean reward: 350.94
               Mean episode length: 249.61
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 11.52s
                        Total time: 10784.79s
                               ETA: 1192887.0s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.786s, learning 0.182s)
               Value function loss: 28.5464
                    Surrogate loss: 0.0007
             Mean action noise std: 0.76
                       Mean reward: 347.34
               Mean episode length: 249.76
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 11.97s
                        Total time: 10796.76s
                               ETA: 1192867.4s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.405s, learning 0.207s)
               Value function loss: 41.2061
                    Surrogate loss: -0.0028
             Mean action noise std: 0.76
                       Mean reward: 345.69
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 11.61s
                        Total time: 10808.37s
                               ETA: 1192808.4s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.534s, learning 0.175s)
               Value function loss: 47.0506
                    Surrogate loss: -0.0058
             Mean action noise std: 0.76
                       Mean reward: 346.39
               Mean episode length: 249.78
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 11.71s
                        Total time: 10820.08s
                               ETA: 1192760.3s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.624s, learning 0.209s)
               Value function loss: 38.5120
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 346.54
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 11.83s
                        Total time: 10831.91s
                               ETA: 1192725.9s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.495s, learning 0.160s)
               Value function loss: 26.2286
                    Surrogate loss: -0.0023
             Mean action noise std: 0.76
                       Mean reward: 343.81
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 11.65s
                        Total time: 10843.57s
                               ETA: 1192672.0s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.221s, learning 0.215s)
               Value function loss: 21.8150
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 340.57
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 11.44s
                        Total time: 10855.00s
                               ETA: 1192594.1s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.489s, learning 0.208s)
               Value function loss: 7.3627
                    Surrogate loss: -0.0153
             Mean action noise std: 0.76
                       Mean reward: 335.68
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 11.70s
                        Total time: 10866.70s
                               ETA: 1192545.0s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.707s, learning 0.205s)
               Value function loss: 3.0570
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 338.87
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 11.91s
                        Total time: 10878.61s
                               ETA: 1192519.6s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1437 steps/s (collection: 11.211s, learning 0.190s)
               Value function loss: 2.6541
                    Surrogate loss: -0.0085
             Mean action noise std: 0.76
                       Mean reward: 339.88
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 11.40s
                        Total time: 10890.01s
                               ETA: 1192438.3s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.439s, learning 0.207s)
               Value function loss: 1.3593
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 340.93
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 11.65s
                        Total time: 10901.66s
                               ETA: 1192383.9s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.499s, learning 0.192s)
               Value function loss: 0.7390
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 340.62
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 11.69s
                        Total time: 10913.35s
                               ETA: 1192334.6s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.577s, learning 0.186s)
               Value function loss: 0.5615
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 340.73
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 11.76s
                        Total time: 10925.11s
                               ETA: 1192293.2s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.425s, learning 0.180s)
               Value function loss: 0.6872
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 340.72
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 11.60s
                        Total time: 10936.72s
                               ETA: 1192234.6s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.177s, learning 0.157s)
               Value function loss: 0.6494
                    Surrogate loss: 0.0068
             Mean action noise std: 0.76
                       Mean reward: 340.03
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 11.33s
                        Total time: 10948.05s
                               ETA: 1192146.6s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.402s, learning 0.238s)
               Value function loss: 0.4565
                    Surrogate loss: 0.0088
             Mean action noise std: 0.76
                       Mean reward: 339.69
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 11.64s
                        Total time: 10959.69s
                               ETA: 1192092.1s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.709s, learning 0.234s)
               Value function loss: 1.4219
                    Surrogate loss: 0.0126
             Mean action noise std: 0.76
                       Mean reward: 339.92
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 11.94s
                        Total time: 10971.63s
                               ETA: 1192070.5s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.292s, learning 0.194s)
               Value function loss: 1.7210
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 339.67
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 11.49s
                        Total time: 10983.12s
                               ETA: 1191999.3s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1450 steps/s (collection: 11.122s, learning 0.171s)
               Value function loss: 1.4191
                    Surrogate loss: 0.0017
             Mean action noise std: 0.76
                       Mean reward: 339.87
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 11.29s
                        Total time: 10994.41s
                               ETA: 1191907.3s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.198s, learning 0.166s)
               Value function loss: 3.5930
                    Surrogate loss: -0.0069
             Mean action noise std: 0.76
                       Mean reward: 338.09
               Mean episode length: 249.18
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 11.36s
                        Total time: 11005.78s
                               ETA: 1191823.3s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.822s, learning 0.233s)
               Value function loss: 3.1728
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 337.76
               Mean episode length: 249.18
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 12.06s
                        Total time: 11017.83s
                               ETA: 1191814.2s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.582s, learning 0.192s)
               Value function loss: 2.8925
                    Surrogate loss: -0.0040
             Mean action noise std: 0.76
                       Mean reward: 337.10
               Mean episode length: 249.18
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 11.77s
                        Total time: 11029.60s
                               ETA: 1191774.7s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.560s, learning 0.163s)
               Value function loss: 3.2555
                    Surrogate loss: -0.0068
             Mean action noise std: 0.76
                       Mean reward: 337.56
               Mean episode length: 249.18
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 11.72s
                        Total time: 11041.33s
                               ETA: 1191729.8s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.515s, learning 0.194s)
               Value function loss: 3.4879
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 337.52
               Mean episode length: 248.15
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 11.71s
                        Total time: 11053.04s
                               ETA: 1191683.3s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.439s, learning 0.189s)
               Value function loss: 2.2701
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 337.53
               Mean episode length: 248.15
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 11.63s
                        Total time: 11064.66s
                               ETA: 1191628.3s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.485s, learning 0.206s)
               Value function loss: 4.1258
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 338.68
               Mean episode length: 248.97
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 11.69s
                        Total time: 11076.36s
                               ETA: 1191580.2s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.462s, learning 0.206s)
               Value function loss: 4.0427
                    Surrogate loss: -0.0056
             Mean action noise std: 0.76
                       Mean reward: 337.78
               Mean episode length: 248.18
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 11.67s
                        Total time: 11088.02s
                               ETA: 1191529.6s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1444 steps/s (collection: 11.183s, learning 0.162s)
               Value function loss: 4.9843
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 339.14
               Mean episode length: 247.58
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 11.34s
                        Total time: 11099.37s
                               ETA: 1191444.4s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1445 steps/s (collection: 11.127s, learning 0.207s)
               Value function loss: 4.6649
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 338.91
               Mean episode length: 247.58
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 11.33s
                        Total time: 11110.70s
                               ETA: 1191358.2s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.153s, learning 0.156s)
               Value function loss: 7.9196
                    Surrogate loss: -0.0092
             Mean action noise std: 0.76
                       Mean reward: 341.72
               Mean episode length: 248.89
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 11.31s
                        Total time: 11122.01s
                               ETA: 1191269.6s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.616s, learning 0.267s)
               Value function loss: 12.4965
                    Surrogate loss: -0.0064
             Mean action noise std: 0.76
                       Mean reward: 340.76
               Mean episode length: 249.49
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 11.88s
                        Total time: 11133.89s
                               ETA: 1191242.5s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.743s, learning 0.203s)
               Value function loss: 10.6866
                    Surrogate loss: -0.0038
             Mean action noise std: 0.76
                       Mean reward: 342.63
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 11.95s
                        Total time: 11145.84s
                               ETA: 1191222.2s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.351s, learning 0.205s)
               Value function loss: 23.9846
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 343.70
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 11.56s
                        Total time: 11157.40s
                               ETA: 1191160.2s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.118s, learning 0.169s)
               Value function loss: 37.1900
                    Surrogate loss: 0.0026
             Mean action noise std: 0.76
                       Mean reward: 342.05
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 11.29s
                        Total time: 11168.68s
                               ETA: 1191069.6s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.624s, learning 0.219s)
               Value function loss: 53.8085
                    Surrogate loss: 0.0140
             Mean action noise std: 0.76
                       Mean reward: 344.71
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 11.84s
                        Total time: 11180.53s
                               ETA: 1191038.5s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.533s, learning 0.170s)
               Value function loss: 35.7651
                    Surrogate loss: -0.0011
             Mean action noise std: 0.76
                       Mean reward: 345.33
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 11.70s
                        Total time: 11192.23s
                               ETA: 1190992.6s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.355s, learning 0.290s)
               Value function loss: 26.4034
                    Surrogate loss: -0.0009
             Mean action noise std: 0.76
                       Mean reward: 342.16
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 11.65s
                        Total time: 11203.87s
                               ETA: 1190940.5s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1015 steps/s (collection: 15.967s, learning 0.166s)
               Value function loss: 21.1208
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 342.38
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 16.13s
                        Total time: 11220.01s
                               ETA: 1191365.1s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.967s, learning 0.167s)
               Value function loss: 10.6321
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 343.97
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 22.13s
                        Total time: 11242.14s
                               ETA: 1192425.2s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 730 steps/s (collection: 22.144s, learning 0.271s)
               Value function loss: 4.0329
                    Surrogate loss: 0.0144
             Mean action noise std: 0.76
                       Mean reward: 345.96
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 22.42s
                        Total time: 11264.56s
                               ETA: 1193512.9s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 751 steps/s (collection: 21.642s, learning 0.169s)
               Value function loss: 3.4597
                    Surrogate loss: -0.0042
             Mean action noise std: 0.76
                       Mean reward: 346.04
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 21.81s
                        Total time: 11286.37s
                               ETA: 1194534.2s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 729 steps/s (collection: 22.201s, learning 0.246s)
               Value function loss: 2.0724
                    Surrogate loss: 0.0140
             Mean action noise std: 0.76
                       Mean reward: 345.18
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 22.45s
                        Total time: 11308.81s
                               ETA: 1195620.5s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.897s, learning 0.215s)
               Value function loss: 1.6242
                    Surrogate loss: 0.0048
             Mean action noise std: 0.76
                       Mean reward: 345.39
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 22.11s
                        Total time: 11330.93s
                               ETA: 1196669.0s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 714 steps/s (collection: 22.750s, learning 0.173s)
               Value function loss: 1.1408
                    Surrogate loss: 0.0445
             Mean action noise std: 0.76
                       Mean reward: 345.11
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 22.92s
                        Total time: 11353.85s
                               ETA: 1197801.0s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 747 steps/s (collection: 21.735s, learning 0.197s)
               Value function loss: 1.2512
                    Surrogate loss: 0.0322
             Mean action noise std: 0.76
                       Mean reward: 342.78
               Mean episode length: 248.34
                  Mean reward/step: 1.47
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 21.93s
                        Total time: 11375.78s
                               ETA: 1198825.9s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.894s, learning 0.225s)
               Value function loss: 0.8668
                    Surrogate loss: -0.0020
             Mean action noise std: 0.76
                       Mean reward: 343.22
               Mean episode length: 248.34
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 22.12s
                        Total time: 11397.90s
                               ETA: 1199868.3s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 728 steps/s (collection: 22.334s, learning 0.171s)
               Value function loss: 0.8834
                    Surrogate loss: 0.0113
             Mean action noise std: 0.76
                       Mean reward: 340.89
               Mean episode length: 247.08
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 22.50s
                        Total time: 11420.41s
                               ETA: 1200949.0s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 740 steps/s (collection: 21.966s, learning 0.164s)
               Value function loss: 1.3221
                    Surrogate loss: -0.0001
             Mean action noise std: 0.76
                       Mean reward: 341.40
               Mean episode length: 247.08
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 22.13s
                        Total time: 11442.54s
                               ETA: 1201988.0s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 728 steps/s (collection: 22.312s, learning 0.177s)
               Value function loss: 1.3844
                    Surrogate loss: -0.0043
             Mean action noise std: 0.76
                       Mean reward: 338.82
               Mean episode length: 245.53
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 22.49s
                        Total time: 11465.03s
                               ETA: 1203062.5s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.779s, learning 0.194s)
               Value function loss: 1.7686
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 335.82
               Mean episode length: 243.14
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 21.97s
                        Total time: 11487.00s
                               ETA: 1204080.5s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 721 steps/s (collection: 22.416s, learning 0.304s)
               Value function loss: 4.1225
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 327.87
               Mean episode length: 238.29
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 22.72s
                        Total time: 11509.72s
                               ETA: 1205174.6s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 720 steps/s (collection: 22.559s, learning 0.173s)
               Value function loss: 3.4664
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 323.04
               Mean episode length: 235.06
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 22.73s
                        Total time: 11532.45s
                               ETA: 1206267.5s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 730 steps/s (collection: 22.184s, learning 0.236s)
               Value function loss: 2.6891
                    Surrogate loss: 0.0026
             Mean action noise std: 0.76
                       Mean reward: 317.31
               Mean episode length: 232.61
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 22.42s
                        Total time: 11554.87s
                               ETA: 1207325.5s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 739 steps/s (collection: 21.941s, learning 0.202s)
               Value function loss: 2.4524
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 315.23
               Mean episode length: 232.61
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 22.14s
                        Total time: 11577.01s
                               ETA: 1208352.4s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.350s, learning 0.169s)
               Value function loss: 3.6794
                    Surrogate loss: -0.0061
             Mean action noise std: 0.76
                       Mean reward: 317.84
               Mean episode length: 234.88
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 22.52s
                        Total time: 11599.53s
                               ETA: 1209416.2s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 733 steps/s (collection: 22.112s, learning 0.237s)
               Value function loss: 2.5811
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 320.09
               Mean episode length: 236.48
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 22.35s
                        Total time: 11621.88s
                               ETA: 1210459.9s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 737 steps/s (collection: 22.040s, learning 0.164s)
               Value function loss: 3.6989
                    Surrogate loss: -0.0037
             Mean action noise std: 0.76
                       Mean reward: 324.09
               Mean episode length: 239.03
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 22.20s
                        Total time: 11644.09s
                               ETA: 1211486.3s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 732 steps/s (collection: 22.199s, learning 0.166s)
               Value function loss: 3.2820
                    Surrogate loss: -0.0124
             Mean action noise std: 0.76
                       Mean reward: 327.17
               Mean episode length: 240.83
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 22.37s
                        Total time: 11666.45s
                               ETA: 1212527.4s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 719 steps/s (collection: 22.621s, learning 0.162s)
               Value function loss: 4.5795
                    Surrogate loss: -0.0054
             Mean action noise std: 0.76
                       Mean reward: 332.00
               Mean episode length: 242.75
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 22.78s
                        Total time: 11689.23s
                               ETA: 1213609.5s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.318s, learning 0.190s)
               Value function loss: 5.4299
                    Surrogate loss: -0.0047
             Mean action noise std: 0.76
                       Mean reward: 334.00
               Mean episode length: 244.95
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 22.51s
                        Total time: 11711.74s
                               ETA: 1214660.8s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 730 steps/s (collection: 22.233s, learning 0.188s)
               Value function loss: 7.3505
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 337.86
               Mean episode length: 248.57
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 22.42s
                        Total time: 11734.16s
                               ETA: 1215700.9s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 739 steps/s (collection: 21.990s, learning 0.170s)
               Value function loss: 11.5451
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 341.04
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 22.16s
                        Total time: 11756.32s
                               ETA: 1216711.7s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 734 steps/s (collection: 22.122s, learning 0.188s)
               Value function loss: 11.3536
                    Surrogate loss: -0.0099
             Mean action noise std: 0.76
                       Mean reward: 340.86
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 22.31s
                        Total time: 11778.63s
                               ETA: 1217735.9s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 739 steps/s (collection: 21.968s, learning 0.178s)
               Value function loss: 16.7398
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 336.70
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 22.15s
                        Total time: 11800.78s
                               ETA: 1218740.9s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 716 steps/s (collection: 22.701s, learning 0.174s)
               Value function loss: 32.4458
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 339.40
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 22.88s
                        Total time: 11823.65s
                               ETA: 1219819.1s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.345s, learning 0.163s)
               Value function loss: 37.5435
                    Surrogate loss: -0.0014
             Mean action noise std: 0.76
                       Mean reward: 337.47
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 22.51s
                        Total time: 11846.16s
                               ETA: 1220857.2s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 745 steps/s (collection: 21.774s, learning 0.201s)
               Value function loss: 38.1624
                    Surrogate loss: -0.0075
             Mean action noise std: 0.76
                       Mean reward: 338.04
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 21.98s
                        Total time: 11868.14s
                               ETA: 1221838.2s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 714 steps/s (collection: 22.780s, learning 0.162s)
               Value function loss: 27.2131
                    Surrogate loss: -0.0035
             Mean action noise std: 0.76
                       Mean reward: 338.77
               Mean episode length: 250.00
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 22.94s
                        Total time: 11891.08s
                               ETA: 1222916.5s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 752 steps/s (collection: 21.603s, learning 0.170s)
               Value function loss: 20.6816
                    Surrogate loss: -0.0136
             Mean action noise std: 0.76
                       Mean reward: 339.15
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 21.77s
                        Total time: 11912.85s
                               ETA: 1223872.5s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.343s, learning 0.184s)
               Value function loss: 17.0332
                    Surrogate loss: -0.0147
             Mean action noise std: 0.76
                       Mean reward: 337.62
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 22.53s
                        Total time: 11935.38s
                               ETA: 1224903.8s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 728 steps/s (collection: 22.308s, learning 0.188s)
               Value function loss: 5.8647
                    Surrogate loss: 0.0069
             Mean action noise std: 0.76
                       Mean reward: 338.41
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 22.50s
                        Total time: 11957.88s
                               ETA: 1225929.8s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 727 steps/s (collection: 22.347s, learning 0.188s)
               Value function loss: 3.2265
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 337.66
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 22.54s
                        Total time: 11980.41s
                               ETA: 1226957.5s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 733 steps/s (collection: 22.150s, learning 0.194s)
               Value function loss: 2.1658
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 336.42
               Mean episode length: 249.40
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 22.34s
                        Total time: 12002.75s
                               ETA: 1227963.6s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 722 steps/s (collection: 22.509s, learning 0.159s)
               Value function loss: 1.8070
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 337.41
               Mean episode length: 249.40
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 22.67s
                        Total time: 12025.42s
                               ETA: 1229000.6s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 728 steps/s (collection: 22.285s, learning 0.200s)
               Value function loss: 0.5418
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 337.36
               Mean episode length: 249.40
                  Mean reward/step: 1.44
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 22.48s
                        Total time: 12047.91s
                               ETA: 1230016.8s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1134 steps/s (collection: 14.239s, learning 0.201s)
               Value function loss: 0.5051
                    Surrogate loss: 0.0036
             Mean action noise std: 0.76
                       Mean reward: 337.95
               Mean episode length: 249.40
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 14.44s
                        Total time: 12062.35s
                               ETA: 1230210.3s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.600s, learning 0.166s)
               Value function loss: 0.6977
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: 338.89
               Mean episode length: 249.40
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 11.77s
                        Total time: 12074.11s
                               ETA: 1230131.0s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.430s, learning 0.225s)
               Value function loss: 0.7923
                    Surrogate loss: 0.0003
             Mean action noise std: 0.76
                       Mean reward: 339.22
               Mean episode length: 249.40
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 11.65s
                        Total time: 12085.77s
                               ETA: 1230040.5s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.620s, learning 0.166s)
               Value function loss: 0.9490
                    Surrogate loss: -0.0009
             Mean action noise std: 0.76
                       Mean reward: 339.26
               Mean episode length: 249.40
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 11.79s
                        Total time: 12097.55s
                               ETA: 1229963.5s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.428s, learning 0.159s)
               Value function loss: 1.6969
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 338.33
               Mean episode length: 247.98
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 11.59s
                        Total time: 12109.14s
                               ETA: 1229866.4s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.267s, learning 0.210s)
               Value function loss: 1.7600
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 336.07
               Mean episode length: 246.70
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 11.48s
                        Total time: 12120.62s
                               ETA: 1229758.3s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.611s, learning 0.162s)
               Value function loss: 3.1925
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 335.83
               Mean episode length: 245.36
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 11.77s
                        Total time: 12132.39s
                               ETA: 1229680.4s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.364s, learning 0.206s)
               Value function loss: 3.2859
                    Surrogate loss: -0.0123
             Mean action noise std: 0.76
                       Mean reward: 334.51
               Mean episode length: 245.36
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 11.57s
                        Total time: 12143.96s
                               ETA: 1229582.1s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1448 steps/s (collection: 11.147s, learning 0.162s)
               Value function loss: 2.5630
                    Surrogate loss: -0.0113
             Mean action noise std: 0.76
                       Mean reward: 332.43
               Mean episode length: 244.16
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 11.31s
                        Total time: 12155.27s
                               ETA: 1229457.7s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1461 steps/s (collection: 11.054s, learning 0.157s)
               Value function loss: 2.4987
                    Surrogate loss: -0.0100
             Mean action noise std: 0.76
                       Mean reward: 326.68
               Mean episode length: 238.99
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 11.21s
                        Total time: 12166.48s
                               ETA: 1229323.5s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.657s, learning 0.165s)
               Value function loss: 2.8389
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 326.40
               Mean episode length: 238.45
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 11.82s
                        Total time: 12178.30s
                               ETA: 1229251.2s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1441 steps/s (collection: 11.193s, learning 0.176s)
               Value function loss: 2.9262
                    Surrogate loss: -0.0041
             Mean action noise std: 0.76
                       Mean reward: 326.13
               Mean episode length: 238.69
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 11.37s
                        Total time: 12189.67s
                               ETA: 1229133.5s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.161s, learning 0.237s)
               Value function loss: 3.4001
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 327.83
               Mean episode length: 240.03
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 12.40s
                        Total time: 12202.07s
                               ETA: 1229119.5s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.240s, learning 0.182s)
               Value function loss: 4.2154
                    Surrogate loss: -0.0080
             Mean action noise std: 0.76
                       Mean reward: 328.41
               Mean episode length: 240.03
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 11.42s
                        Total time: 12213.49s
                               ETA: 1229007.4s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.458s, learning 0.229s)
               Value function loss: 3.3593
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 333.40
               Mean episode length: 243.54
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 11.69s
                        Total time: 12225.18s
                               ETA: 1228922.1s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.566s, learning 0.204s)
               Value function loss: 5.7404
                    Surrogate loss: -0.0021
             Mean action noise std: 0.76
                       Mean reward: 335.79
               Mean episode length: 248.27
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 11.77s
                        Total time: 12236.95s
                               ETA: 1228845.2s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.423s, learning 0.204s)
               Value function loss: 6.8449
                    Surrogate loss: -0.0053
             Mean action noise std: 0.76
                       Mean reward: 330.91
               Mean episode length: 247.55
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 11.63s
                        Total time: 12248.57s
                               ETA: 1228754.1s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.589s, learning 0.212s)
               Value function loss: 10.7094
                    Surrogate loss: 0.0020
             Mean action noise std: 0.76
                       Mean reward: 326.26
               Mean episode length: 247.55
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 11.80s
                        Total time: 12260.37s
                               ETA: 1228680.7s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.345s, learning 0.183s)
               Value function loss: 9.8671
                    Surrogate loss: -0.0030
             Mean action noise std: 0.76
                       Mean reward: 333.10
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 11.53s
                        Total time: 12271.90s
                               ETA: 1228580.0s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.516s, learning 0.195s)
               Value function loss: 13.4073
                    Surrogate loss: -0.0012
             Mean action noise std: 0.76
                       Mean reward: 331.42
               Mean episode length: 249.81
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 11.71s
                        Total time: 12283.61s
                               ETA: 1228497.8s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.519s, learning 0.176s)
               Value function loss: 29.5706
                    Surrogate loss: 0.0153
             Mean action noise std: 0.76
                       Mean reward: 330.20
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 11.69s
                        Total time: 12295.31s
                               ETA: 1228414.1s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.338s, learning 0.196s)
               Value function loss: 36.8993
                    Surrogate loss: -0.0012
             Mean action noise std: 0.76
                       Mean reward: 323.95
               Mean episode length: 247.29
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 11.53s
                        Total time: 12306.84s
                               ETA: 1228314.6s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.319s, learning 0.173s)
               Value function loss: 43.6498
                    Surrogate loss: 0.0230
             Mean action noise std: 0.76
                       Mean reward: 323.33
               Mean episode length: 248.78
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 11.49s
                        Total time: 12318.33s
                               ETA: 1228211.0s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.323s, learning 0.163s)
               Value function loss: 30.3452
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 328.14
               Mean episode length: 250.00
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 11.49s
                        Total time: 12329.82s
                               ETA: 1228107.1s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1434 steps/s (collection: 11.261s, learning 0.164s)
               Value function loss: 25.2848
                    Surrogate loss: -0.0071
             Mean action noise std: 0.76
                       Mean reward: 324.85
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 11.42s
                        Total time: 12341.24s
                               ETA: 1227997.2s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.410s, learning 0.159s)
               Value function loss: 16.9439
                    Surrogate loss: -0.0128
             Mean action noise std: 0.76
                       Mean reward: 323.87
               Mean episode length: 248.89
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 11.57s
                        Total time: 12352.81s
                               ETA: 1227901.9s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1451 steps/s (collection: 11.108s, learning 0.178s)
               Value function loss: 7.2449
                    Surrogate loss: -0.0065
             Mean action noise std: 0.76
                       Mean reward: 325.38
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 11.29s
                        Total time: 12364.10s
                               ETA: 1227778.7s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1429 steps/s (collection: 11.269s, learning 0.192s)
               Value function loss: 3.5041
                    Surrogate loss: -0.0177
             Mean action noise std: 0.76
                       Mean reward: 325.98
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 11.46s
                        Total time: 12375.56s
                               ETA: 1227673.1s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.449s, learning 0.255s)
               Value function loss: 2.8482
                    Surrogate loss: 0.0034
             Mean action noise std: 0.76
                       Mean reward: 328.12
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 11.70s
                        Total time: 12387.27s
                               ETA: 1227591.7s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.866s, learning 0.206s)
               Value function loss: 2.3490
                    Surrogate loss: 0.0116
             Mean action noise std: 0.76
                       Mean reward: 327.11
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 12.07s
                        Total time: 12399.34s
                               ETA: 1227546.9s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.196s, learning 0.249s)
               Value function loss: 0.9863
                    Surrogate loss: 0.0054
             Mean action noise std: 0.76
                       Mean reward: 326.68
               Mean episode length: 249.46
                  Mean reward/step: 1.43
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 11.45s
                        Total time: 12410.78s
                               ETA: 1227440.1s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.676s, learning 0.167s)
               Value function loss: 0.9539
                    Surrogate loss: 0.0504
             Mean action noise std: 0.76
                       Mean reward: 326.02
               Mean episode length: 249.46
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 11.84s
                        Total time: 12422.63s
                               ETA: 1227372.8s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.442s, learning 0.201s)
               Value function loss: 1.1395
                    Surrogate loss: -0.0059
             Mean action noise std: 0.76
                       Mean reward: 326.06
               Mean episode length: 249.46
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 11.64s
                        Total time: 12434.27s
                               ETA: 1227285.9s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.628s, learning 0.175s)
               Value function loss: 1.6552
                    Surrogate loss: 0.0345
             Mean action noise std: 0.76
                       Mean reward: 326.14
               Mean episode length: 249.46
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 11.80s
                        Total time: 12446.07s
                               ETA: 1227215.0s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.338s, learning 0.222s)
               Value function loss: 0.8197
                    Surrogate loss: 0.0255
             Mean action noise std: 0.76
                       Mean reward: 326.14
               Mean episode length: 249.46
                  Mean reward/step: 1.41
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 11.56s
                        Total time: 12457.63s
                               ETA: 1227120.2s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.736s, learning 0.167s)
               Value function loss: 1.7383
                    Surrogate loss: 0.0023
             Mean action noise std: 0.76
                       Mean reward: 323.97
               Mean episode length: 247.69
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 11.90s
                        Total time: 12469.53s
                               ETA: 1227059.2s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.532s, learning 0.200s)
               Value function loss: 1.5209
                    Surrogate loss: -0.0114
             Mean action noise std: 0.76
                       Mean reward: 323.68
               Mean episode length: 247.69
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 11.73s
                        Total time: 12481.27s
                               ETA: 1226981.7s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.287s, learning 0.164s)
               Value function loss: 1.9089
                    Surrogate loss: 0.0091
             Mean action noise std: 0.76
                       Mean reward: 321.67
               Mean episode length: 246.84
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 11.45s
                        Total time: 12492.72s
                               ETA: 1226876.7s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.394s, learning 0.166s)
               Value function loss: 3.5690
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 320.12
               Mean episode length: 245.21
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 11.56s
                        Total time: 12504.28s
                               ETA: 1226782.6s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.391s, learning 0.164s)
               Value function loss: 2.6697
                    Surrogate loss: -0.0126
             Mean action noise std: 0.76
                       Mean reward: 314.49
               Mean episode length: 242.49
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 11.56s
                        Total time: 12515.83s
                               ETA: 1226688.1s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.498s, learning 0.158s)
               Value function loss: 3.0912
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 313.34
               Mean episode length: 243.03
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 11.66s
                        Total time: 12527.49s
                               ETA: 1226603.7s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.002s, learning 0.159s)
               Value function loss: 3.8332
                    Surrogate loss: 0.0038
             Mean action noise std: 0.76
                       Mean reward: 312.97
               Mean episode length: 241.57
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 11.16s
                        Total time: 12538.65s
                               ETA: 1226471.0s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.347s, learning 0.192s)
               Value function loss: 3.7220
                    Surrogate loss: -0.0104
             Mean action noise std: 0.76
                       Mean reward: 311.69
               Mean episode length: 239.01
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 11.54s
                        Total time: 12550.19s
                               ETA: 1226375.4s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.559s, learning 0.165s)
               Value function loss: 3.0789
                    Surrogate loss: -0.0062
             Mean action noise std: 0.76
                       Mean reward: 309.15
               Mean episode length: 238.60
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 11.72s
                        Total time: 12561.92s
                               ETA: 1226298.1s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.431s, learning 0.158s)
               Value function loss: 5.1875
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 315.13
               Mean episode length: 239.47
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 11.59s
                        Total time: 12573.50s
                               ETA: 1226207.8s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.061s, learning 0.187s)
               Value function loss: 3.5268
                    Surrogate loss: -0.0096
             Mean action noise std: 0.76
                       Mean reward: 316.67
               Mean episode length: 239.47
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 11.25s
                        Total time: 12584.75s
                               ETA: 1226084.4s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.438s, learning 0.174s)
               Value function loss: 6.1556
                    Surrogate loss: -0.0115
             Mean action noise std: 0.76
                       Mean reward: 322.12
               Mean episode length: 243.45
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 11.61s
                        Total time: 12596.36s
                               ETA: 1225996.6s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.542s, learning 0.162s)
               Value function loss: 6.4924
                    Surrogate loss: -0.0150
             Mean action noise std: 0.76
                       Mean reward: 326.80
               Mean episode length: 247.33
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 11.70s
                        Total time: 12608.07s
                               ETA: 1225917.9s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.744s, learning 0.166s)
               Value function loss: 8.8345
                    Surrogate loss: -0.0135
             Mean action noise std: 0.76
                       Mean reward: 330.53
               Mean episode length: 248.88
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 11.91s
                        Total time: 12619.98s
                               ETA: 1225859.4s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.382s, learning 0.196s)
               Value function loss: 11.5341
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 330.72
               Mean episode length: 249.70
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 11.58s
                        Total time: 12631.56s
                               ETA: 1225768.8s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.212s, learning 0.165s)
               Value function loss: 13.8462
                    Surrogate loss: -0.0095
             Mean action noise std: 0.76
                       Mean reward: 327.69
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 11.38s
                        Total time: 12642.93s
                               ETA: 1225658.8s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.442s, learning 0.174s)
               Value function loss: 28.9498
                    Surrogate loss: -0.0106
             Mean action noise std: 0.76
                       Mean reward: 336.87
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 11.62s
                        Total time: 12654.55s
                               ETA: 1225572.1s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.680s, learning 0.164s)
               Value function loss: 52.0628
                    Surrogate loss: 0.0058
             Mean action noise std: 0.76
                       Mean reward: 334.60
               Mean episode length: 249.76
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 11.84s
                        Total time: 12666.39s
                               ETA: 1225507.7s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.555s, learning 0.172s)
               Value function loss: 56.6889
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 335.14
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 11.73s
                        Total time: 12678.12s
                               ETA: 1225432.0s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.130s, learning 0.208s)
               Value function loss: 44.2338
                    Surrogate loss: -0.0032
             Mean action noise std: 0.76
                       Mean reward: 338.02
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 11.34s
                        Total time: 12689.46s
                               ETA: 1225318.9s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.180s, learning 0.181s)
               Value function loss: 35.8355
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 344.26
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 11.36s
                        Total time: 12700.82s
                               ETA: 1225208.2s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.563s, learning 0.277s)
               Value function loss: 22.5113
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 343.48
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 11.84s
                        Total time: 12712.66s
                               ETA: 1225144.0s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.356s, learning 0.196s)
               Value function loss: 12.5026
                    Surrogate loss: -0.0151
             Mean action noise std: 0.76
                       Mean reward: 347.12
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 11.55s
                        Total time: 12724.21s
                               ETA: 1225052.0s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.578s, learning 0.163s)
               Value function loss: 5.1636
                    Surrogate loss: -0.0093
             Mean action noise std: 0.76
                       Mean reward: 348.00
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 11.74s
                        Total time: 12735.95s
                               ETA: 1224978.5s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.439s, learning 0.201s)
               Value function loss: 3.2716
                    Surrogate loss: -0.0127
             Mean action noise std: 0.76
                       Mean reward: 349.14
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 11.64s
                        Total time: 12747.59s
                               ETA: 1224895.3s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.222s, learning 0.159s)
               Value function loss: 2.4642
                    Surrogate loss: -0.0016
             Mean action noise std: 0.76
                       Mean reward: 349.06
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 11.38s
                        Total time: 12758.98s
                               ETA: 1224787.4s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.433s, learning 0.171s)
               Value function loss: 1.1650
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 350.22
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 11.60s
                        Total time: 12770.58s
                               ETA: 1224701.1s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.462s, learning 0.201s)
               Value function loss: 0.9396
                    Surrogate loss: -0.0039
             Mean action noise std: 0.76
                       Mean reward: 350.74
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 11.66s
                        Total time: 12782.24s
                               ETA: 1224620.6s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.083s, learning 0.167s)
               Value function loss: 1.2673
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 350.24
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 11.25s
                        Total time: 12793.49s
                               ETA: 1224500.6s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.327s, learning 0.265s)
               Value function loss: 1.1577
                    Surrogate loss: 0.0002
             Mean action noise std: 0.76
                       Mean reward: 351.98
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 11.59s
                        Total time: 12805.08s
                               ETA: 1224413.6s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.006s, learning 0.199s)
               Value function loss: 0.8425
                    Surrogate loss: 0.0001
             Mean action noise std: 0.76
                       Mean reward: 351.27
               Mean episode length: 249.54
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 12.21s
                        Total time: 12817.29s
                               ETA: 1224385.2s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.647s, learning 0.204s)
               Value function loss: 2.2376
                    Surrogate loss: -0.0122
             Mean action noise std: 0.76
                       Mean reward: 350.23
               Mean episode length: 248.87
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 11.85s
                        Total time: 12829.14s
                               ETA: 1224323.1s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.741s, learning 0.193s)
               Value function loss: 2.2401
                    Surrogate loss: -0.0060
             Mean action noise std: 0.76
                       Mean reward: 345.69
               Mean episode length: 247.67
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 11.93s
                        Total time: 12841.07s
                               ETA: 1224269.1s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.337s, learning 0.274s)
               Value function loss: 2.0822
                    Surrogate loss: -0.0146
             Mean action noise std: 0.76
                       Mean reward: 345.33
               Mean episode length: 247.67
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 11.61s
                        Total time: 12852.69s
                               ETA: 1224184.3s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.821s, learning 0.196s)
               Value function loss: 4.8155
                    Surrogate loss: -0.0024
             Mean action noise std: 0.76
                       Mean reward: 339.85
               Mean episode length: 246.81
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 12.02s
                        Total time: 12864.70s
                               ETA: 1224138.3s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.711s, learning 0.203s)
               Value function loss: 4.2366
                    Surrogate loss: -0.0084
             Mean action noise std: 0.76
                       Mean reward: 333.91
               Mean episode length: 246.24
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 11.91s
                        Total time: 12876.62s
                               ETA: 1224082.6s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.011s, learning 0.161s)
               Value function loss: 4.0904
                    Surrogate loss: -0.0091
             Mean action noise std: 0.76
                       Mean reward: 326.70
               Mean episode length: 244.99
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 11.17s
                        Total time: 12887.79s
                               ETA: 1223956.5s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.962s, learning 0.200s)
               Value function loss: 4.8247
                    Surrogate loss: -0.0074
             Mean action noise std: 0.76
                       Mean reward: 327.72
               Mean episode length: 246.19
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 11.16s
                        Total time: 12898.95s
                               ETA: 1223829.6s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.258s, learning 0.163s)
               Value function loss: 4.5650
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 323.78
               Mean episode length: 245.67
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 11.42s
                        Total time: 12910.37s
                               ETA: 1223727.6s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.259s, learning 0.284s)
               Value function loss: 3.3548
                    Surrogate loss: 0.0015
             Mean action noise std: 0.76
                       Mean reward: 316.74
               Mean episode length: 244.79
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 11.54s
                        Total time: 12921.91s
                               ETA: 1223637.2s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.524s, learning 0.202s)
               Value function loss: 4.5110
                    Surrogate loss: 0.0089
             Mean action noise std: 0.76
                       Mean reward: 314.55
               Mean episode length: 243.66
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 11.73s
                        Total time: 12933.64s
                               ETA: 1223564.3s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.575s, learning 0.165s)
               Value function loss: 4.4747
                    Surrogate loss: -0.0103
             Mean action noise std: 0.76
                       Mean reward: 310.40
               Mean episode length: 242.39
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 11.74s
                        Total time: 12945.38s
                               ETA: 1223492.8s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.397s, learning 0.181s)
               Value function loss: 5.6718
                    Surrogate loss: -0.0057
             Mean action noise std: 0.76
                       Mean reward: 303.83
               Mean episode length: 240.83
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 11.58s
                        Total time: 12956.96s
                               ETA: 1223406.3s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.205s, learning 0.161s)
               Value function loss: 4.6564
                    Surrogate loss: -0.0102
             Mean action noise std: 0.76
                       Mean reward: 309.52
               Mean episode length: 241.94
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 11.37s
                        Total time: 12968.32s
                               ETA: 1223299.8s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.076s, learning 0.174s)
               Value function loss: 6.8133
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 305.88
               Mean episode length: 242.80
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 11.25s
                        Total time: 12979.57s
                               ETA: 1223182.6s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.375s, learning 0.171s)
               Value function loss: 8.9672
                    Surrogate loss: -0.0055
             Mean action noise std: 0.76
                       Mean reward: 308.44
               Mean episode length: 244.18
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 11.55s
                        Total time: 12991.12s
                               ETA: 1223093.5s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.772s, learning 0.181s)
               Value function loss: 7.2168
                    Surrogate loss: -0.0081
             Mean action noise std: 0.76
                       Mean reward: 313.13
               Mean episode length: 247.38
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 11.95s
                        Total time: 13003.07s
                               ETA: 1223042.7s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.448s, learning 0.165s)
               Value function loss: 17.4943
                    Surrogate loss: -0.0083
             Mean action noise std: 0.76
                       Mean reward: 326.99
               Mean episode length: 248.22
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 11.61s
                        Total time: 13014.68s
                               ETA: 1222960.1s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.588s, learning 0.275s)
               Value function loss: 29.3433
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 326.85
               Mean episode length: 248.58
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 11.86s
                        Total time: 13026.55s
                               ETA: 1222901.2s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.988s, learning 0.165s)
               Value function loss: 40.6354
                    Surrogate loss: 0.0075
             Mean action noise std: 0.76
                       Mean reward: 330.74
               Mean episode length: 249.98
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 11.15s
                        Total time: 13037.70s
                               ETA: 1222775.7s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.041s, learning 0.163s)
               Value function loss: 35.7162
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 333.19
               Mean episode length: 249.83
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 11.20s
                        Total time: 13048.91s
                               ETA: 1222655.2s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.462s, learning 0.204s)
               Value function loss: 22.4062
                    Surrogate loss: -0.0077
             Mean action noise std: 0.76
                       Mean reward: 336.37
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 11.67s
                        Total time: 13060.57s
                               ETA: 1222578.2s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.259s, learning 0.164s)
               Value function loss: 16.0798
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 339.21
               Mean episode length: 249.87
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 11.42s
                        Total time: 13071.99s
                               ETA: 1222478.5s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.971s, learning 0.175s)
               Value function loss: 8.3124
                    Surrogate loss: -0.0191
             Mean action noise std: 0.76
                       Mean reward: 341.97
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 11.15s
                        Total time: 13083.14s
                               ETA: 1222353.1s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.709s, learning 0.173s)
               Value function loss: 3.9777
                    Surrogate loss: -0.0137
             Mean action noise std: 0.76
                       Mean reward: 349.29
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 11.88s
                        Total time: 13095.02s
                               ETA: 1222296.7s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.958s, learning 0.161s)
               Value function loss: 3.2959
                    Surrogate loss: -0.0145
             Mean action noise std: 0.76
                       Mean reward: 349.56
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 11.12s
                        Total time: 13106.14s
                               ETA: 1222169.2s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.597s, learning 0.200s)
               Value function loss: 2.1033
                    Surrogate loss: 0.0239
             Mean action noise std: 0.76
                       Mean reward: 351.31
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 11.80s
                        Total time: 13117.94s
                               ETA: 1222105.1s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.146s, learning 0.229s)
               Value function loss: 1.4810
                    Surrogate loss: -0.0046
             Mean action noise std: 0.76
                       Mean reward: 351.93
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 11.37s
                        Total time: 13129.31s
                               ETA: 1222001.7s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.842s, learning 0.166s)
               Value function loss: 1.1708
                    Surrogate loss: -0.0006
             Mean action noise std: 0.76
                       Mean reward: 350.03
               Mean episode length: 248.71
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 12.01s
                        Total time: 13141.32s
                               ETA: 1221957.4s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.006s, learning 0.206s)
               Value function loss: 1.1070
                    Surrogate loss: -0.0017
             Mean action noise std: 0.76
                       Mean reward: 349.90
               Mean episode length: 248.71
                  Mean reward/step: 1.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 11.21s
                        Total time: 13152.53s
                               ETA: 1221839.3s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.247s, learning 0.168s)
               Value function loss: 1.7385
                    Surrogate loss: 0.0038
             Mean action noise std: 0.76
                       Mean reward: 348.95
               Mean episode length: 247.44
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 11.41s
                        Total time: 13163.95s
                               ETA: 1221740.1s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.281s, learning 0.224s)
               Value function loss: 1.3807
                    Surrogate loss: 0.0018
             Mean action noise std: 0.76
                       Mean reward: 350.79
               Mean episode length: 247.44
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 11.51s
                        Total time: 13175.45s
                               ETA: 1221649.6s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.720s, learning 0.230s)
               Value function loss: 1.5217
                    Surrogate loss: -0.0087
             Mean action noise std: 0.76
                       Mean reward: 351.58
               Mean episode length: 247.44
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 11.95s
                        Total time: 13187.40s
                               ETA: 1221600.4s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.304s, learning 0.178s)
               Value function loss: 2.6820
                    Surrogate loss: 0.0022
             Mean action noise std: 0.76
                       Mean reward: 353.12
               Mean episode length: 247.44
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 11.48s
                        Total time: 13198.88s
                               ETA: 1221507.9s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.200s, learning 0.180s)
               Value function loss: 2.1028
                    Surrogate loss: -0.0120
             Mean action noise std: 0.76
                       Mean reward: 353.79
               Mean episode length: 247.44
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 11.38s
                        Total time: 13210.26s
                               ETA: 1221406.2s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.136s, learning 0.222s)
               Value function loss: 5.0673
                    Surrogate loss: 0.0082
             Mean action noise std: 0.76
                       Mean reward: 353.90
               Mean episode length: 247.16
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 11.36s
                        Total time: 13221.62s
                               ETA: 1221302.5s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.058s, learning 0.196s)
               Value function loss: 4.8075
                    Surrogate loss: 0.0054
             Mean action noise std: 0.76
                       Mean reward: 355.42
               Mean episode length: 246.05
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 12.25s
                        Total time: 13233.87s
                               ETA: 1221281.7s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.970s, learning 0.169s)
               Value function loss: 4.6427
                    Surrogate loss: 0.0034
             Mean action noise std: 0.76
                       Mean reward: 356.47
               Mean episode length: 248.61
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 11.14s
                        Total time: 13245.01s
                               ETA: 1221158.3s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.553s, learning 0.192s)
               Value function loss: 4.3059
                    Surrogate loss: -0.0031
             Mean action noise std: 0.76
                       Mean reward: 353.30
               Mean episode length: 247.09
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 11.74s
                        Total time: 13256.76s
                               ETA: 1221090.7s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.552s, learning 0.228s)
               Value function loss: 6.1177
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 349.86
               Mean episode length: 245.12
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 11.78s
                        Total time: 13268.54s
                               ETA: 1221026.5s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.994s, learning 0.197s)
               Value function loss: 5.5211
                    Surrogate loss: -0.0073
             Mean action noise std: 0.76
                       Mean reward: 347.00
               Mean episode length: 244.69
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 12.19s
                        Total time: 13280.73s
                               ETA: 1221000.2s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.194s, learning 0.185s)
               Value function loss: 7.7318
                    Surrogate loss: 0.0273
             Mean action noise std: 0.76
                       Mean reward: 344.50
               Mean episode length: 245.11
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 11.38s
                        Total time: 13292.11s
                               ETA: 1220899.3s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.213s, learning 0.199s)
               Value function loss: 5.7304
                    Surrogate loss: 0.0081
             Mean action noise std: 0.76
                       Mean reward: 340.54
               Mean episode length: 244.18
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 11.41s
                        Total time: 13303.52s
                               ETA: 1220801.7s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.357s, learning 0.180s)
               Value function loss: 7.5808
                    Surrogate loss: -0.0049
             Mean action noise std: 0.76
                       Mean reward: 335.17
               Mean episode length: 244.59
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 11.54s
                        Total time: 13315.06s
                               ETA: 1220715.6s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.149s, learning 0.201s)
               Value function loss: 8.4770
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 316.24
               Mean episode length: 239.11
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 11.35s
                        Total time: 13326.41s
                               ETA: 1220612.5s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.370s, learning 0.200s)
               Value function loss: 10.2552
                    Surrogate loss: 0.0160
             Mean action noise std: 0.76
                       Mean reward: 291.02
               Mean episode length: 231.02
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 11.57s
                        Total time: 13337.98s
                               ETA: 1220529.8s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.868s, learning 0.301s)
               Value function loss: 11.5828
                    Surrogate loss: -0.0029
             Mean action noise std: 0.76
                       Mean reward: 295.81
               Mean episode length: 234.30
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 12.17s
                        Total time: 13350.15s
                               ETA: 1220501.9s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.597s, learning 0.189s)
               Value function loss: 8.3088
                    Surrogate loss: 0.0029
             Mean action noise std: 0.76
                       Mean reward: 293.04
               Mean episode length: 235.59
                  Mean reward/step: 0.83
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 11.79s
                        Total time: 13361.93s
                               ETA: 1220439.1s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.357s, learning 0.162s)
               Value function loss: 10.0740
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 290.60
               Mean episode length: 239.22
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 11.52s
                        Total time: 13373.45s
                               ETA: 1220352.0s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.824s, learning 0.176s)
               Value function loss: 13.1451
                    Surrogate loss: -0.0094
             Mean action noise std: 0.76
                       Mean reward: 288.58
               Mean episode length: 240.85
                  Mean reward/step: 0.79
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 12.00s
                        Total time: 13385.45s
                               ETA: 1220309.0s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.304s, learning 0.212s)
               Value function loss: 17.8681
                    Surrogate loss: -0.0082
             Mean action noise std: 0.76
                       Mean reward: 296.64
               Mean episode length: 243.82
                  Mean reward/step: 0.80
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 11.52s
                        Total time: 13396.97s
                               ETA: 1220221.9s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.404s, learning 0.179s)
               Value function loss: 9.4306
                    Surrogate loss: -0.0079
             Mean action noise std: 0.76
                       Mean reward: 298.60
               Mean episode length: 246.22
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 11.58s
                        Total time: 13408.55s
                               ETA: 1220141.0s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.676s, learning 0.165s)
               Value function loss: 6.2405
                    Surrogate loss: -0.0179
             Mean action noise std: 0.76
                       Mean reward: 301.71
               Mean episode length: 247.59
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 11.84s
                        Total time: 13420.39s
                               ETA: 1220083.8s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.975s, learning 0.162s)
               Value function loss: 4.7017
                    Surrogate loss: -0.0198
             Mean action noise std: 0.76
                       Mean reward: 298.73
               Mean episode length: 247.52
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 11.14s
                        Total time: 13431.53s
                               ETA: 1219962.6s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.327s, learning 0.166s)
               Value function loss: 3.4731
                    Surrogate loss: -0.0158
             Mean action noise std: 0.76
                       Mean reward: 290.29
               Mean episode length: 247.42
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 11.49s
                        Total time: 13443.02s
                               ETA: 1219873.9s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.325s, learning 0.168s)
               Value function loss: 1.7190
                    Surrogate loss: -0.0034
             Mean action noise std: 0.76
                       Mean reward: 288.88
               Mean episode length: 247.37
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 11.49s
                        Total time: 13454.51s
                               ETA: 1219785.4s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.916s, learning 0.181s)
               Value function loss: 1.1398
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 289.28
               Mean episode length: 247.43
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 12.10s
                        Total time: 13466.61s
                               ETA: 1219751.8s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.113s, learning 0.178s)
               Value function loss: 0.8816
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 288.03
               Mean episode length: 247.47
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 11.29s
                        Total time: 13477.90s
                               ETA: 1219645.3s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.499s, learning 0.168s)
               Value function loss: 0.7861
                    Surrogate loss: 0.0008
             Mean action noise std: 0.76
                       Mean reward: 286.55
               Mean episode length: 246.88
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 11.67s
                        Total time: 13489.57s
                               ETA: 1219572.9s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.182s, learning 0.168s)
               Value function loss: 0.7296
                    Surrogate loss: 0.0052
             Mean action noise std: 0.76
                       Mean reward: 285.28
               Mean episode length: 246.55
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 11.35s
                        Total time: 13500.92s
                               ETA: 1219472.0s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.155s, learning 0.159s)
               Value function loss: 0.7824
                    Surrogate loss: -0.0072
             Mean action noise std: 0.76
                       Mean reward: 283.03
               Mean episode length: 245.44
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 11.31s
                        Total time: 13512.23s
                               ETA: 1219368.0s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.626s, learning 0.187s)
               Value function loss: 0.8741
                    Surrogate loss: -0.0116
             Mean action noise std: 0.76
                       Mean reward: 277.41
               Mean episode length: 243.48
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 11.81s
                        Total time: 13524.04s
                               ETA: 1219309.1s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.995s, learning 0.187s)
               Value function loss: 0.7889
                    Surrogate loss: 0.0051
             Mean action noise std: 0.76
                       Mean reward: 275.79
               Mean episode length: 243.20
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 12.18s
                        Total time: 13536.23s
                               ETA: 1219283.6s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.418s, learning 0.162s)
               Value function loss: 0.8748
                    Surrogate loss: -0.0140
             Mean action noise std: 0.76
                       Mean reward: 274.46
               Mean episode length: 242.96
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 11.58s
                        Total time: 13547.81s
                               ETA: 1219203.9s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.623s, learning 0.159s)
               Value function loss: 1.6061
                    Surrogate loss: -0.0097
             Mean action noise std: 0.76
                       Mean reward: 270.45
               Mean episode length: 241.54
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 11.78s
                        Total time: 13559.59s
                               ETA: 1219142.6s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.770s, learning 0.203s)
               Value function loss: 1.0743
                    Surrogate loss: -0.0105
             Mean action noise std: 0.76
                       Mean reward: 268.98
               Mean episode length: 241.77
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 11.97s
                        Total time: 13571.56s
                               ETA: 1219098.5s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.851s, learning 0.161s)
               Value function loss: 1.5380
                    Surrogate loss: -0.0192
             Mean action noise std: 0.76
                       Mean reward: 261.54
               Mean episode length: 241.40
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 12.01s
                        Total time: 13583.57s
                               ETA: 1219057.9s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.366s, learning 0.185s)
               Value function loss: 2.3500
                    Surrogate loss: 0.0004
             Mean action noise std: 0.76
                       Mean reward: 263.28
               Mean episode length: 241.93
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 11.55s
                        Total time: 13595.12s
                               ETA: 1218976.1s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.166s)
               Value function loss: 1.8015
                    Surrogate loss: -0.0143
             Mean action noise std: 0.76
                       Mean reward: 266.61
               Mean episode length: 242.82
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 11.52s
                        Total time: 13606.65s
                               ETA: 1218891.7s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.036s, learning 0.166s)
               Value function loss: 1.9903
                    Surrogate loss: -0.0088
             Mean action noise std: 0.76
                       Mean reward: 275.39
               Mean episode length: 246.80
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 11.20s
                        Total time: 13617.85s
                               ETA: 1218778.8s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.511s, learning 0.200s)
               Value function loss: 1.9979
                    Surrogate loss: -0.0098
             Mean action noise std: 0.76
                       Mean reward: 279.00
               Mean episode length: 248.69
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 11.71s
                        Total time: 13629.56s
                               ETA: 1218711.7s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.303s, learning 0.159s)
               Value function loss: 3.0922
                    Surrogate loss: 0.0009
             Mean action noise std: 0.76
                       Mean reward: 286.91
               Mean episode length: 249.86
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 11.46s
                        Total time: 13641.02s
                               ETA: 1218622.4s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.587s, learning 0.157s)
               Value function loss: 3.6848
                    Surrogate loss: -0.0005
             Mean action noise std: 0.76
                       Mean reward: 289.68
               Mean episode length: 249.92
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 11.74s
                        Total time: 13652.76s
                               ETA: 1218558.5s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.625s, learning 0.164s)
               Value function loss: 3.0429
                    Surrogate loss: -0.0067
             Mean action noise std: 0.76
                       Mean reward: 290.94
               Mean episode length: 249.92
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 11.79s
                        Total time: 13664.55s
                               ETA: 1218498.6s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.417s, learning 0.199s)
               Value function loss: 4.6483
                    Surrogate loss: -0.0101
             Mean action noise std: 0.76
                       Mean reward: 295.30
               Mean episode length: 249.98
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 11.62s
                        Total time: 13676.17s
                               ETA: 1218423.4s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.213s, learning 0.220s)
               Value function loss: 10.9245
                    Surrogate loss: 0.0011
             Mean action noise std: 0.76
                       Mean reward: 301.03
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 11.43s
                        Total time: 13687.60s
                               ETA: 1218332.1s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.450s, learning 0.262s)
               Value function loss: 9.4446
                    Surrogate loss: -0.0008
             Mean action noise std: 0.76
                       Mean reward: 309.41
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 11.71s
                        Total time: 13699.31s
                               ETA: 1218265.7s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.287s, learning 0.197s)
               Value function loss: 14.8224
                    Surrogate loss: -0.0112
             Mean action noise std: 0.76
                       Mean reward: 314.61
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 11.48s
                        Total time: 13710.80s
                               ETA: 1218179.2s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.286s, learning 0.187s)
               Value function loss: 11.6593
                    Surrogate loss: -0.0139
             Mean action noise std: 0.76
                       Mean reward: 314.67
               Mean episode length: 249.99
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 11.47s
                        Total time: 13722.27s
                               ETA: 1218091.7s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.521s, learning 0.173s)
               Value function loss: 17.4936
                    Surrogate loss: -0.0078
             Mean action noise std: 0.76
                       Mean reward: 320.08
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 11.69s
                        Total time: 13733.96s
                               ETA: 1218024.0s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.004s, learning 0.167s)
               Value function loss: 25.3615
                    Surrogate loss: -0.0051
             Mean action noise std: 0.76
                       Mean reward: 328.76
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 11.17s
                        Total time: 13745.13s
                               ETA: 1217910.1s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.079s, learning 0.203s)
               Value function loss: 39.9206
                    Surrogate loss: 0.0072
             Mean action noise std: 0.76
                       Mean reward: 328.33
               Mean episode length: 250.00
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 11.28s
                        Total time: 13756.42s
                               ETA: 1217806.3s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1452 steps/s (collection: 10.991s, learning 0.287s)
               Value function loss: 26.4285
                    Surrogate loss: -0.0013
             Mean action noise std: 0.76
                       Mean reward: 337.49
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 11.28s
                        Total time: 13767.70s
                               ETA: 1217702.3s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.286s, learning 0.185s)
               Value function loss: 16.4226
                    Surrogate loss: -0.0159
             Mean action noise std: 0.76
                       Mean reward: 339.04
               Mean episode length: 250.00
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 11.47s
                        Total time: 13779.17s
                               ETA: 1217615.3s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.735s, learning 0.189s)
               Value function loss: 8.3970
                    Surrogate loss: -0.0230
             Mean action noise std: 0.76
                       Mean reward: 343.03
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 11.92s
                        Total time: 13791.09s
                               ETA: 1217568.6s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.166s, learning 0.177s)
               Value function loss: 6.5111
                    Surrogate loss: -0.0195
             Mean action noise std: 0.76
                       Mean reward: 347.30
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 11.34s
                        Total time: 13802.43s
                               ETA: 1217470.7s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.924s, learning 0.169s)
               Value function loss: 3.2123
                    Surrogate loss: -0.0215
             Mean action noise std: 0.76
                       Mean reward: 351.14
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 11.09s
                        Total time: 13813.53s
                               ETA: 1217350.9s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.360s, learning 0.219s)
               Value function loss: 1.4942
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 352.56
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 11.58s
                        Total time: 13825.11s
                               ETA: 1217274.0s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.433s, learning 0.273s)
               Value function loss: 1.3300
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 354.15
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 11.71s
                        Total time: 13836.81s
                               ETA: 1217208.5s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.101s, learning 0.186s)
               Value function loss: 1.4868
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 354.96
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 11.29s
                        Total time: 13848.10s
                               ETA: 1217106.2s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.294s, learning 0.178s)
               Value function loss: 0.9306
                    Surrogate loss: -0.0137
             Mean action noise std: 0.75
                       Mean reward: 355.90
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 11.47s
                        Total time: 13859.57s
                               ETA: 1217020.4s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.691s, learning 0.170s)
               Value function loss: 0.9513
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 356.93
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 10.86s
                        Total time: 13870.43s
                               ETA: 1216881.1s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.266s, learning 0.197s)
               Value function loss: 1.3249
                    Surrogate loss: -0.0087
             Mean action noise std: 0.75
                       Mean reward: 358.36
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 11.46s
                        Total time: 13881.89s
                               ETA: 1216794.7s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.412s, learning 0.179s)
               Value function loss: 0.9053
                    Surrogate loss: 0.0066
             Mean action noise std: 0.75
                       Mean reward: 359.46
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 11.59s
                        Total time: 13893.48s
                               ETA: 1216719.7s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.260s, learning 0.176s)
               Value function loss: 0.9950
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 359.85
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 11.44s
                        Total time: 13904.92s
                               ETA: 1216631.3s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.568s, learning 0.170s)
               Value function loss: 2.9322
                    Surrogate loss: 0.0070
             Mean action noise std: 0.75
                       Mean reward: 362.77
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 11.74s
                        Total time: 13916.66s
                               ETA: 1216569.4s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.273s, learning 0.224s)
               Value function loss: 2.2929
                    Surrogate loss: -0.0015
             Mean action noise std: 0.75
                       Mean reward: 364.72
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 11.50s
                        Total time: 13928.16s
                               ETA: 1216486.6s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.723s, learning 0.194s)
               Value function loss: 2.7421
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: 365.92
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 11.92s
                        Total time: 13940.07s
                               ETA: 1216440.5s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.198s, learning 0.155s)
               Value function loss: 4.0011
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 368.06
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 11.35s
                        Total time: 13951.43s
                               ETA: 1216345.4s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.271s, learning 0.161s)
               Value function loss: 3.0762
                    Surrogate loss: 0.0065
             Mean action noise std: 0.75
                       Mean reward: 368.70
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 11.43s
                        Total time: 13962.86s
                               ETA: 1216257.3s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.476s, learning 0.169s)
               Value function loss: 3.8929
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 367.58
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 11.65s
                        Total time: 13974.50s
                               ETA: 1216187.8s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.352s, learning 0.162s)
               Value function loss: 3.3575
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 366.64
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 11.51s
                        Total time: 13986.02s
                               ETA: 1216107.1s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.541s, learning 0.191s)
               Value function loss: 5.0394
                    Surrogate loss: -0.0090
             Mean action noise std: 0.75
                       Mean reward: 366.15
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 11.73s
                        Total time: 13997.75s
                               ETA: 1216045.3s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.242s, learning 0.157s)
               Value function loss: 5.3594
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 366.09
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 11.40s
                        Total time: 14009.15s
                               ETA: 1215954.8s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.475s, learning 0.189s)
               Value function loss: 5.9908
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 364.16
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 11.66s
                        Total time: 14020.81s
                               ETA: 1215887.4s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.161s, learning 0.174s)
               Value function loss: 4.4468
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 364.45
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 11.33s
                        Total time: 14032.15s
                               ETA: 1215791.5s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.205s, learning 0.216s)
               Value function loss: 9.6234
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 361.04
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 11.42s
                        Total time: 14043.57s
                               ETA: 1215703.3s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.348s, learning 0.274s)
               Value function loss: 12.3018
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 359.74
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 11.62s
                        Total time: 14055.19s
                               ETA: 1215632.6s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.630s, learning 0.163s)
               Value function loss: 20.8559
                    Surrogate loss: -0.0042
             Mean action noise std: 0.75
                       Mean reward: 363.33
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 11.79s
                        Total time: 14066.98s
                               ETA: 1215576.8s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.093s, learning 0.228s)
               Value function loss: 19.3650
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 361.99
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 11.32s
                        Total time: 14078.31s
                               ETA: 1215480.3s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.500s, learning 0.165s)
               Value function loss: 24.9290
                    Surrogate loss: -0.0084
             Mean action noise std: 0.75
                       Mean reward: 358.86
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 11.66s
                        Total time: 14089.97s
                               ETA: 1215413.7s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.510s, learning 0.205s)
               Value function loss: 37.0537
                    Surrogate loss: 0.0024
             Mean action noise std: 0.75
                       Mean reward: 361.35
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 11.71s
                        Total time: 14101.69s
                               ETA: 1215351.3s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.269s, learning 0.226s)
               Value function loss: 49.6430
                    Surrogate loss: 0.0053
             Mean action noise std: 0.75
                       Mean reward: 360.83
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 11.50s
                        Total time: 14113.18s
                               ETA: 1215270.2s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.463s, learning 0.168s)
               Value function loss: 44.6379
                    Surrogate loss: -0.0009
             Mean action noise std: 0.75
                       Mean reward: 360.69
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 11.63s
                        Total time: 14124.81s
                               ETA: 1215201.0s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.269s, learning 0.162s)
               Value function loss: 26.4687
                    Surrogate loss: -0.0072
             Mean action noise std: 0.75
                       Mean reward: 363.35
               Mean episode length: 250.00
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 11.43s
                        Total time: 14136.24s
                               ETA: 1215114.6s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.798s, learning 0.159s)
               Value function loss: 17.2167
                    Surrogate loss: -0.0138
             Mean action noise std: 0.75
                       Mean reward: 362.46
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 11.96s
                        Total time: 14148.20s
                               ETA: 1215073.5s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.714s, learning 0.323s)
               Value function loss: 14.1901
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 361.95
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 12.04s
                        Total time: 14160.24s
                               ETA: 1215039.4s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.703s, learning 0.179s)
               Value function loss: 6.5901
                    Surrogate loss: -0.0154
             Mean action noise std: 0.75
                       Mean reward: 364.77
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 11.88s
                        Total time: 14172.12s
                               ETA: 1214991.9s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.153s, learning 0.158s)
               Value function loss: 2.1941
                    Surrogate loss: -0.0237
             Mean action noise std: 0.75
                       Mean reward: 364.63
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 11.31s
                        Total time: 14183.43s
                               ETA: 1214895.6s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.092s, learning 0.200s)
               Value function loss: 1.5155
                    Surrogate loss: -0.0169
             Mean action noise std: 0.75
                       Mean reward: 364.22
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 11.29s
                        Total time: 14194.72s
                               ETA: 1214797.9s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.362s, learning 0.303s)
               Value function loss: 1.6791
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 364.38
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 11.66s
                        Total time: 14206.39s
                               ETA: 1214732.2s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.741s, learning 0.258s)
               Value function loss: 0.9622
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 364.25
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 12.00s
                        Total time: 14218.39s
                               ETA: 1214695.1s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.442s, learning 0.338s)
               Value function loss: 0.9001
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 364.26
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 11.78s
                        Total time: 14230.17s
                               ETA: 1214639.3s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.142s, learning 0.217s)
               Value function loss: 1.5922
                    Surrogate loss: 0.0275
             Mean action noise std: 0.75
                       Mean reward: 364.78
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 11.36s
                        Total time: 14241.53s
                               ETA: 1214547.8s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.522s, learning 0.230s)
               Value function loss: 1.2270
                    Surrogate loss: 0.0002
             Mean action noise std: 0.75
                       Mean reward: 365.31
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 11.75s
                        Total time: 14253.28s
                               ETA: 1214489.9s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.193s, learning 0.178s)
               Value function loss: 1.3987
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 364.99
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 11.37s
                        Total time: 14264.65s
                               ETA: 1214399.6s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.497s, learning 0.219s)
               Value function loss: 1.9154
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 364.83
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 11.72s
                        Total time: 14276.36s
                               ETA: 1214338.7s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.346s, learning 0.169s)
               Value function loss: 2.4311
                    Surrogate loss: -0.0119
             Mean action noise std: 0.75
                       Mean reward: 366.18
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 11.52s
                        Total time: 14287.88s
                               ETA: 1214260.9s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.761s, learning 0.173s)
               Value function loss: 2.4596
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 366.38
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 11.93s
                        Total time: 14299.81s
                               ETA: 1214218.9s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.376s, learning 0.197s)
               Value function loss: 4.4943
                    Surrogate loss: -0.0129
             Mean action noise std: 0.75
                       Mean reward: 364.90
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 11.57s
                        Total time: 14311.39s
                               ETA: 1214146.1s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.366s, learning 0.167s)
               Value function loss: 3.8762
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 364.78
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 11.53s
                        Total time: 14322.92s
                               ETA: 1214070.1s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.302s, learning 0.169s)
               Value function loss: 3.9689
                    Surrogate loss: -0.0082
             Mean action noise std: 0.75
                       Mean reward: 363.88
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 11.47s
                        Total time: 14334.39s
                               ETA: 1213988.9s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.925s, learning 0.231s)
               Value function loss: 4.8207
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 362.90
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 11.16s
                        Total time: 14345.55s
                               ETA: 1213881.2s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.730s, learning 0.165s)
               Value function loss: 4.5608
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 361.75
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 11.89s
                        Total time: 14357.44s
                               ETA: 1213836.2s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.790s, learning 0.214s)
               Value function loss: 4.7765
                    Surrogate loss: -0.0183
             Mean action noise std: 0.75
                       Mean reward: 362.17
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 12.00s
                        Total time: 14369.44s
                               ETA: 1213800.4s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.090s, learning 0.161s)
               Value function loss: 6.0208
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 364.29
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 12.25s
                        Total time: 14381.70s
                               ETA: 1213785.6s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.487s, learning 0.176s)
               Value function loss: 5.3994
                    Surrogate loss: -0.0147
             Mean action noise std: 0.75
                       Mean reward: 365.25
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 11.66s
                        Total time: 14393.36s
                               ETA: 1213721.1s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.076s, learning 0.176s)
               Value function loss: 8.2128
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 366.53
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 11.25s
                        Total time: 14404.61s
                               ETA: 1213622.1s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.463s, learning 0.186s)
               Value function loss: 8.2641
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 365.02
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 11.65s
                        Total time: 14416.26s
                               ETA: 1213556.6s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.528s, learning 0.161s)
               Value function loss: 16.7800
                    Surrogate loss: 0.0010
             Mean action noise std: 0.75
                       Mean reward: 364.96
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 11.69s
                        Total time: 14427.95s
                               ETA: 1213494.7s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.068s, learning 0.272s)
               Value function loss: 19.0841
                    Surrogate loss: -0.0020
             Mean action noise std: 0.75
                       Mean reward: 364.95
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 11.34s
                        Total time: 14439.29s
                               ETA: 1213403.5s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.350s, learning 0.279s)
               Value function loss: 17.9358
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 363.65
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 11.63s
                        Total time: 14450.92s
                               ETA: 1213336.7s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.057s, learning 0.163s)
               Value function loss: 29.4918
                    Surrogate loss: 0.0089
             Mean action noise std: 0.75
                       Mean reward: 362.69
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 11.22s
                        Total time: 14462.14s
                               ETA: 1213235.7s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.596s, learning 0.172s)
               Value function loss: 38.7519
                    Surrogate loss: 0.0076
             Mean action noise std: 0.75
                       Mean reward: 361.80
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 11.77s
                        Total time: 14473.90s
                               ETA: 1213180.8s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.531s, learning 0.160s)
               Value function loss: 38.6072
                    Surrogate loss: 0.0016
             Mean action noise std: 0.75
                       Mean reward: 364.32
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 11.69s
                        Total time: 14485.60s
                               ETA: 1213119.5s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.080s, learning 0.195s)
               Value function loss: 28.3939
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 359.85
               Mean episode length: 250.00
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 11.27s
                        Total time: 14496.87s
                               ETA: 1213023.4s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.605s, learning 0.206s)
               Value function loss: 15.4373
                    Surrogate loss: -0.0197
             Mean action noise std: 0.75
                       Mean reward: 361.97
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 11.81s
                        Total time: 14508.68s
                               ETA: 1212972.3s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.521s, learning 0.160s)
               Value function loss: 10.4165
                    Surrogate loss: -0.0182
             Mean action noise std: 0.75
                       Mean reward: 361.88
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 11.68s
                        Total time: 14520.36s
                               ETA: 1212910.4s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.427s, learning 0.168s)
               Value function loss: 5.6438
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 362.44
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 11.59s
                        Total time: 14531.96s
                               ETA: 1212841.4s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.638s, learning 0.228s)
               Value function loss: 1.9102
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 361.94
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 11.87s
                        Total time: 14543.82s
                               ETA: 1212795.2s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.259s, learning 0.196s)
               Value function loss: 1.2041
                    Surrogate loss: -0.0163
             Mean action noise std: 0.75
                       Mean reward: 361.85
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 11.45s
                        Total time: 14555.28s
                               ETA: 1212714.7s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.331s, learning 0.191s)
               Value function loss: 1.1740
                    Surrogate loss: -0.0157
             Mean action noise std: 0.75
                       Mean reward: 362.05
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 11.52s
                        Total time: 14566.80s
                               ETA: 1212639.9s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.415s, learning 0.190s)
               Value function loss: 0.7340
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 362.36
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 11.61s
                        Total time: 14578.40s
                               ETA: 1212572.2s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.244s, learning 0.189s)
               Value function loss: 0.3643
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 362.24
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 11.43s
                        Total time: 14589.84s
                               ETA: 1212490.3s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.427s, learning 0.267s)
               Value function loss: 1.3585
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 362.24
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 11.69s
                        Total time: 14601.53s
                               ETA: 1212430.2s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.531s, learning 0.165s)
               Value function loss: 0.7209
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 362.67
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 11.70s
                        Total time: 14613.23s
                               ETA: 1212370.3s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.663s, learning 0.172s)
               Value function loss: 0.8066
                    Surrogate loss: -0.0120
             Mean action noise std: 0.75
                       Mean reward: 363.33
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 11.83s
                        Total time: 14625.06s
                               ETA: 1212322.0s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.289s, learning 0.168s)
               Value function loss: 2.0870
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 363.55
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 11.46s
                        Total time: 14636.52s
                               ETA: 1212242.5s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.213s, learning 0.224s)
               Value function loss: 1.9756
                    Surrogate loss: -0.0101
             Mean action noise std: 0.75
                       Mean reward: 365.27
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 11.44s
                        Total time: 14647.96s
                               ETA: 1212161.3s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.511s, learning 0.163s)
               Value function loss: 1.2789
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 367.52
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 11.67s
                        Total time: 14659.63s
                               ETA: 1212100.0s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.136s, learning 0.159s)
               Value function loss: 4.9487
                    Surrogate loss: -0.0055
             Mean action noise std: 0.75
                       Mean reward: 369.61
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 11.30s
                        Total time: 14670.93s
                               ETA: 1212007.4s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.973s, learning 0.165s)
               Value function loss: 3.1552
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 371.81
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 11.14s
                        Total time: 14682.06s
                               ETA: 1211902.0s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.073s, learning 0.155s)
               Value function loss: 3.0090
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 372.37
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 11.23s
                        Total time: 14693.29s
                               ETA: 1211804.2s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.380s, learning 0.173s)
               Value function loss: 3.5732
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 373.00
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 11.55s
                        Total time: 14704.85s
                               ETA: 1211733.3s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.986s, learning 0.195s)
               Value function loss: 4.1562
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 371.76
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 11.18s
                        Total time: 14716.03s
                               ETA: 1211631.9s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.479s, learning 0.260s)
               Value function loss: 5.5466
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 372.29
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 11.74s
                        Total time: 14727.77s
                               ETA: 1211576.5s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.563s, learning 0.190s)
               Value function loss: 4.9995
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 372.46
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 11.75s
                        Total time: 14739.52s
                               ETA: 1211522.3s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.052s, learning 0.170s)
               Value function loss: 4.2321
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 372.66
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 12.22s
                        Total time: 14751.74s
                               ETA: 1211506.7s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.446s, learning 0.195s)
               Value function loss: 7.4743
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 372.49
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 11.64s
                        Total time: 14763.38s
                               ETA: 1211443.5s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.911s, learning 0.252s)
               Value function loss: 10.2171
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 373.08
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 11.16s
                        Total time: 14774.55s
                               ETA: 1211341.2s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.390s, learning 0.166s)
               Value function loss: 15.1683
                    Surrogate loss: -0.0061
             Mean action noise std: 0.75
                       Mean reward: 372.54
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 11.56s
                        Total time: 14786.10s
                               ETA: 1211271.2s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.283s, learning 0.200s)
               Value function loss: 22.8978
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 373.73
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 11.48s
                        Total time: 14797.59s
                               ETA: 1211195.3s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.337s, learning 0.167s)
               Value function loss: 21.2578
                    Surrogate loss: -0.0128
             Mean action noise std: 0.75
                       Mean reward: 374.65
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 11.50s
                        Total time: 14809.09s
                               ETA: 1211121.2s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.394s, learning 0.294s)
               Value function loss: 33.9086
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 375.24
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 11.69s
                        Total time: 14820.78s
                               ETA: 1211062.3s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.371s, learning 0.188s)
               Value function loss: 48.6725
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 373.09
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 11.56s
                        Total time: 14832.34s
                               ETA: 1210992.8s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.051s, learning 0.198s)
               Value function loss: 55.3457
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 372.85
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 11.25s
                        Total time: 14843.59s
                               ETA: 1210898.3s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.509s, learning 0.165s)
               Value function loss: 36.9228
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 373.26
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 11.67s
                        Total time: 14855.26s
                               ETA: 1210838.5s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.109s, learning 0.259s)
               Value function loss: 21.8416
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 374.25
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 11.37s
                        Total time: 14866.63s
                               ETA: 1210753.8s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.340s, learning 0.168s)
               Value function loss: 17.6578
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 373.88
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 11.51s
                        Total time: 14878.14s
                               ETA: 1210680.7s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.166s)
               Value function loss: 7.1222
                    Surrogate loss: -0.0159
             Mean action noise std: 0.75
                       Mean reward: 375.08
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 11.73s
                        Total time: 14889.87s
                               ETA: 1210625.8s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.566s, learning 0.185s)
               Value function loss: 3.6778
                    Surrogate loss: -0.0175
             Mean action noise std: 0.75
                       Mean reward: 374.41
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 11.75s
                        Total time: 14901.62s
                               ETA: 1210572.6s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.544s, learning 0.159s)
               Value function loss: 1.7630
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 375.04
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 11.70s
                        Total time: 14913.32s
                               ETA: 1210515.5s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.582s, learning 0.166s)
               Value function loss: 1.4815
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 374.82
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 11.75s
                        Total time: 14925.07s
                               ETA: 1210462.2s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.535s, learning 0.230s)
               Value function loss: 0.9091
                    Surrogate loss: 0.0176
             Mean action noise std: 0.75
                       Mean reward: 375.45
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 11.76s
                        Total time: 14936.83s
                               ETA: 1210410.3s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.332s, learning 0.179s)
               Value function loss: 0.7774
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 375.37
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 11.51s
                        Total time: 14948.34s
                               ETA: 1210337.9s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.921s, learning 0.169s)
               Value function loss: 1.1047
                    Surrogate loss: -0.0031
             Mean action noise std: 0.75
                       Mean reward: 375.81
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 12.09s
                        Total time: 14960.43s
                               ETA: 1210312.6s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.287s, learning 0.233s)
               Value function loss: 1.2394
                    Surrogate loss: -0.0064
             Mean action noise std: 0.75
                       Mean reward: 376.09
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 11.52s
                        Total time: 14971.95s
                               ETA: 1210241.0s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.075s, learning 0.160s)
               Value function loss: 1.2385
                    Surrogate loss: -0.0028
             Mean action noise std: 0.75
                       Mean reward: 376.24
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 11.23s
                        Total time: 14983.19s
                               ETA: 1210146.6s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.221s, learning 0.190s)
               Value function loss: 1.1328
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 375.80
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 11.41s
                        Total time: 14994.60s
                               ETA: 1210066.6s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.159s, learning 0.199s)
               Value function loss: 3.1636
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 375.96
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 11.36s
                        Total time: 15005.96s
                               ETA: 1209982.3s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.520s, learning 0.175s)
               Value function loss: 1.7903
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 375.76
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 11.69s
                        Total time: 15017.65s
                               ETA: 1209925.4s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.129s, learning 0.215s)
               Value function loss: 4.3823
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 377.70
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 11.34s
                        Total time: 15029.00s
                               ETA: 1209840.2s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1496 steps/s (collection: 10.790s, learning 0.159s)
               Value function loss: 3.8439
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 377.28
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 10.95s
                        Total time: 15039.94s
                               ETA: 1209723.5s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.440s, learning 0.235s)
               Value function loss: 2.8137
                    Surrogate loss: -0.0096
             Mean action noise std: 0.75
                       Mean reward: 377.40
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 11.67s
                        Total time: 15051.62s
                               ETA: 1209665.2s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.543s, learning 0.210s)
               Value function loss: 3.5619
                    Surrogate loss: -0.0161
             Mean action noise std: 0.75
                       Mean reward: 376.62
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 11.75s
                        Total time: 15063.37s
                               ETA: 1209613.3s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.787s, learning 0.170s)
               Value function loss: 3.1075
                    Surrogate loss: -0.0134
             Mean action noise std: 0.75
                       Mean reward: 376.43
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 10.96s
                        Total time: 15074.33s
                               ETA: 1209497.5s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.279s, learning 0.301s)
               Value function loss: 4.7216
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 376.54
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 11.58s
                        Total time: 15085.91s
                               ETA: 1209431.9s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.092s, learning 0.177s)
               Value function loss: 4.5424
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 375.55
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 11.27s
                        Total time: 15097.18s
                               ETA: 1209341.5s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.052s, learning 0.190s)
               Value function loss: 4.4707
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 374.89
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 11.24s
                        Total time: 15108.42s
                               ETA: 1209249.1s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.270s, learning 0.193s)
               Value function loss: 5.2085
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 373.77
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 11.46s
                        Total time: 15119.88s
                               ETA: 1209174.4s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.226s, learning 0.166s)
               Value function loss: 10.2025
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 372.37
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 11.39s
                        Total time: 15131.28s
                               ETA: 1209094.2s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.130s, learning 0.193s)
               Value function loss: 12.0559
                    Surrogate loss: -0.0125
             Mean action noise std: 0.75
                       Mean reward: 369.97
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 11.32s
                        Total time: 15142.60s
                               ETA: 1209008.6s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.101s, learning 0.170s)
               Value function loss: 20.7214
                    Surrogate loss: -0.0085
             Mean action noise std: 0.75
                       Mean reward: 369.04
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 11.27s
                        Total time: 15153.87s
                               ETA: 1208918.9s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.150s, learning 0.201s)
               Value function loss: 16.9689
                    Surrogate loss: -0.0008
             Mean action noise std: 0.75
                       Mean reward: 366.74
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 11.35s
                        Total time: 15165.22s
                               ETA: 1208835.8s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.496s, learning 0.188s)
               Value function loss: 22.2662
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 366.62
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 11.68s
                        Total time: 15176.90s
                               ETA: 1208779.3s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.229s, learning 0.219s)
               Value function loss: 30.6446
                    Surrogate loss: -0.0100
             Mean action noise std: 0.75
                       Mean reward: 365.63
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 11.45s
                        Total time: 15188.35s
                               ETA: 1208704.0s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.257s, learning 0.163s)
               Value function loss: 31.1298
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 364.64
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 11.42s
                        Total time: 15199.77s
                               ETA: 1208626.7s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.112s, learning 0.161s)
               Value function loss: 28.9954
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 364.10
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 11.27s
                        Total time: 15211.05s
                               ETA: 1208537.8s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.261s, learning 0.272s)
               Value function loss: 16.4120
                    Surrogate loss: -0.0151
             Mean action noise std: 0.75
                       Mean reward: 362.98
               Mean episode length: 250.00
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 11.53s
                        Total time: 15222.58s
                               ETA: 1208469.6s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.077s, learning 0.162s)
               Value function loss: 10.4071
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 363.51
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 11.24s
                        Total time: 15233.82s
                               ETA: 1208378.2s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.457s, learning 0.227s)
               Value function loss: 7.1647
                    Surrogate loss: -0.0158
             Mean action noise std: 0.75
                       Mean reward: 364.81
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 11.68s
                        Total time: 15245.50s
                               ETA: 1208322.2s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.984s, learning 0.170s)
               Value function loss: 3.8261
                    Surrogate loss: -0.0110
             Mean action noise std: 0.75
                       Mean reward: 363.68
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 11.15s
                        Total time: 15256.66s
                               ETA: 1208224.3s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.422s, learning 0.194s)
               Value function loss: 1.6576
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 363.70
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 11.62s
                        Total time: 15268.27s
                               ETA: 1208163.1s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.492s, learning 0.187s)
               Value function loss: 1.0265
                    Surrogate loss: -0.0035
             Mean action noise std: 0.75
                       Mean reward: 363.83
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 11.68s
                        Total time: 15279.95s
                               ETA: 1208107.0s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.093s, learning 0.164s)
               Value function loss: 1.2245
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 363.93
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 11.26s
                        Total time: 15291.21s
                               ETA: 1208017.6s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.328s, learning 0.179s)
               Value function loss: 0.7920
                    Surrogate loss: -0.0083
             Mean action noise std: 0.75
                       Mean reward: 364.35
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 11.51s
                        Total time: 15302.71s
                               ETA: 1207948.1s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.890s, learning 0.193s)
               Value function loss: 1.0304
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 364.30
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 11.08s
                        Total time: 15313.80s
                               ETA: 1207845.2s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.164s, learning 0.180s)
               Value function loss: 0.9650
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 364.13
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 11.34s
                        Total time: 15325.14s
                               ETA: 1207763.0s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.267s, learning 0.188s)
               Value function loss: 0.7394
                    Surrogate loss: -0.0053
             Mean action noise std: 0.75
                       Mean reward: 364.39
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 11.45s
                        Total time: 15336.60s
                               ETA: 1207689.6s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.771s, learning 0.197s)
               Value function loss: 0.7405
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 364.52
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 11.97s
                        Total time: 15348.56s
                               ETA: 1207656.8s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.035s, learning 0.173s)
               Value function loss: 2.5409
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 364.47
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 11.21s
                        Total time: 15359.77s
                               ETA: 1207564.2s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.503s, learning 0.217s)
               Value function loss: 1.9857
                    Surrogate loss: -0.0065
             Mean action noise std: 0.75
                       Mean reward: 364.59
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 11.72s
                        Total time: 15371.49s
                               ETA: 1207512.0s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.450s, learning 0.223s)
               Value function loss: 2.9893
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 365.17
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 11.67s
                        Total time: 15383.17s
                               ETA: 1207456.2s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.488s, learning 0.216s)
               Value function loss: 3.7261
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 364.14
               Mean episode length: 249.09
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 11.70s
                        Total time: 15394.87s
                               ETA: 1207402.9s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.471s, learning 0.231s)
               Value function loss: 2.6862
                    Surrogate loss: -0.0059
             Mean action noise std: 0.75
                       Mean reward: 364.93
               Mean episode length: 249.09
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 11.70s
                        Total time: 15406.57s
                               ETA: 1207349.4s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.582s, learning 0.206s)
               Value function loss: 3.9141
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 365.45
               Mean episode length: 249.09
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 11.79s
                        Total time: 15418.36s
                               ETA: 1207302.8s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.540s, learning 0.207s)
               Value function loss: 3.2015
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 366.19
               Mean episode length: 249.09
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 11.75s
                        Total time: 15430.11s
                               ETA: 1207253.0s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.193s, learning 0.187s)
               Value function loss: 5.0650
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 366.21
               Mean episode length: 249.09
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 11.38s
                        Total time: 15441.49s
                               ETA: 1207174.7s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.424s, learning 0.198s)
               Value function loss: 5.3363
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 367.38
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 11.62s
                        Total time: 15453.11s
                               ETA: 1207115.3s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.238s, learning 0.224s)
               Value function loss: 6.0669
                    Surrogate loss: -0.0025
             Mean action noise std: 0.75
                       Mean reward: 367.60
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 11.46s
                        Total time: 15464.57s
                               ETA: 1207043.5s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.405s, learning 0.192s)
               Value function loss: 4.6092
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 368.52
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 11.60s
                        Total time: 15476.17s
                               ETA: 1206982.3s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.218s, learning 0.162s)
               Value function loss: 10.2783
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 371.09
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 11.38s
                        Total time: 15487.55s
                               ETA: 1206904.2s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.029s, learning 0.237s)
               Value function loss: 12.1962
                    Surrogate loss: -0.0050
             Mean action noise std: 0.75
                       Mean reward: 372.77
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 11.27s
                        Total time: 15498.81s
                               ETA: 1206817.4s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.636s, learning 0.161s)
               Value function loss: 21.5238
                    Surrogate loss: 0.0033
             Mean action noise std: 0.75
                       Mean reward: 373.15
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 11.80s
                        Total time: 15510.61s
                               ETA: 1206772.0s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.667s, learning 0.191s)
               Value function loss: 20.4927
                    Surrogate loss: -0.0014
             Mean action noise std: 0.75
                       Mean reward: 375.94
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 11.86s
                        Total time: 15522.47s
                               ETA: 1206731.4s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.271s, learning 0.173s)
               Value function loss: 27.3675
                    Surrogate loss: -0.0002
             Mean action noise std: 0.75
                       Mean reward: 376.26
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 11.44s
                        Total time: 15533.91s
                               ETA: 1206658.8s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.677s, learning 0.166s)
               Value function loss: 41.4654
                    Surrogate loss: 0.0032
             Mean action noise std: 0.75
                       Mean reward: 376.00
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 11.84s
                        Total time: 15545.76s
                               ETA: 1206617.2s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.215s, learning 0.163s)
               Value function loss: 55.3817
                    Surrogate loss: 0.0270
             Mean action noise std: 0.75
                       Mean reward: 374.44
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 11.38s
                        Total time: 15557.13s
                               ETA: 1206539.5s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.698s, learning 0.164s)
               Value function loss: 50.5236
                    Surrogate loss: 0.0018
             Mean action noise std: 0.75
                       Mean reward: 376.58
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 11.86s
                        Total time: 15569.00s
                               ETA: 1206499.5s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.401s, learning 0.169s)
               Value function loss: 30.1011
                    Surrogate loss: -0.0146
             Mean action noise std: 0.75
                       Mean reward: 375.81
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 11.57s
                        Total time: 15580.57s
                               ETA: 1206437.0s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.300s, learning 0.201s)
               Value function loss: 19.4095
                    Surrogate loss: -0.0143
             Mean action noise std: 0.75
                       Mean reward: 374.09
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 11.50s
                        Total time: 15592.07s
                               ETA: 1206369.1s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.482s, learning 0.167s)
               Value function loss: 13.2875
                    Surrogate loss: -0.0173
             Mean action noise std: 0.75
                       Mean reward: 374.28
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 11.65s
                        Total time: 15603.72s
                               ETA: 1206312.8s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.819s, learning 0.176s)
               Value function loss: 6.1585
                    Surrogate loss: -0.0195
             Mean action noise std: 0.75
                       Mean reward: 373.96
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 12.00s
                        Total time: 15615.71s
                               ETA: 1206283.3s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.236s, learning 0.174s)
               Value function loss: 2.7272
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 372.99
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 11.41s
                        Total time: 15627.12s
                               ETA: 1206208.6s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.572s, learning 0.185s)
               Value function loss: 1.4753
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 372.80
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 11.76s
                        Total time: 15638.88s
                               ETA: 1206160.8s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.389s, learning 0.200s)
               Value function loss: 1.5690
                    Surrogate loss: -0.0094
             Mean action noise std: 0.75
                       Mean reward: 370.80
               Mean episode length: 248.43
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 11.59s
                        Total time: 15650.47s
                               ETA: 1206100.1s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.949s, learning 0.189s)
               Value function loss: 1.2115
                    Surrogate loss: 0.0007
             Mean action noise std: 0.75
                       Mean reward: 370.73
               Mean episode length: 248.43
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 11.14s
                        Total time: 15661.61s
                               ETA: 1206004.8s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.330s, learning 0.172s)
               Value function loss: 0.6326
                    Surrogate loss: -0.0062
             Mean action noise std: 0.75
                       Mean reward: 370.72
               Mean episode length: 248.43
                  Mean reward/step: 1.59
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 11.50s
                        Total time: 15673.11s
                               ETA: 1205937.5s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.476s, learning 0.160s)
               Value function loss: 1.4587
                    Surrogate loss: -0.0005
             Mean action noise std: 0.75
                       Mean reward: 369.97
               Mean episode length: 248.43
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 11.64s
                        Total time: 15684.74s
                               ETA: 1205880.8s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.606s, learning 0.159s)
               Value function loss: 0.7027
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 369.44
               Mean episode length: 248.43
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 11.76s
                        Total time: 15696.51s
                               ETA: 1205834.0s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.161s)
               Value function loss: 1.4390
                    Surrogate loss: 0.0028
             Mean action noise std: 0.75
                       Mean reward: 367.21
               Mean episode length: 247.22
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 11.37s
                        Total time: 15707.88s
                               ETA: 1205756.6s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.229s, learning 0.184s)
               Value function loss: 1.7586
                    Surrogate loss: -0.0049
             Mean action noise std: 0.75
                       Mean reward: 366.75
               Mean episode length: 247.22
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 11.41s
                        Total time: 15719.29s
                               ETA: 1205682.9s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.611s, learning 0.185s)
               Value function loss: 2.3044
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 366.19
               Mean episode length: 247.22
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 11.80s
                        Total time: 15731.09s
                               ETA: 1205638.7s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.583s, learning 0.155s)
               Value function loss: 2.0894
                    Surrogate loss: -0.0066
             Mean action noise std: 0.75
                       Mean reward: 365.66
               Mean episode length: 247.22
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 11.74s
                        Total time: 15742.82s
                               ETA: 1205590.1s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.096s, learning 0.176s)
               Value function loss: 6.0201
                    Surrogate loss: -0.0117
             Mean action noise std: 0.75
                       Mean reward: 360.91
               Mean episode length: 244.88
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 11.27s
                        Total time: 15754.10s
                               ETA: 1205505.9s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.915s, learning 0.168s)
               Value function loss: 4.2120
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 360.28
               Mean episode length: 245.09
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 12.08s
                        Total time: 15766.18s
                               ETA: 1205483.7s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.249s, learning 0.157s)
               Value function loss: 3.1826
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 360.13
               Mean episode length: 244.01
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 11.41s
                        Total time: 15777.58s
                               ETA: 1205409.9s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.270s, learning 0.252s)
               Value function loss: 4.3105
                    Surrogate loss: -0.0063
             Mean action noise std: 0.75
                       Mean reward: 362.36
               Mean episode length: 245.22
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 11.52s
                        Total time: 15789.11s
                               ETA: 1205345.0s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.550s, learning 0.294s)
               Value function loss: 5.0270
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 363.80
               Mean episode length: 245.22
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 11.84s
                        Total time: 15800.95s
                               ETA: 1205304.8s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.166s)
               Value function loss: 5.9432
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 367.48
               Mean episode length: 246.83
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 11.34s
                        Total time: 15812.29s
                               ETA: 1205226.0s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.187s, learning 0.196s)
               Value function loss: 6.6747
                    Surrogate loss: -0.0142
             Mean action noise std: 0.75
                       Mean reward: 371.92
               Mean episode length: 249.27
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 11.38s
                        Total time: 15823.67s
                               ETA: 1205150.7s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.712s, learning 0.262s)
               Value function loss: 5.7043
                    Surrogate loss: -0.0126
             Mean action noise std: 0.75
                       Mean reward: 373.47
               Mean episode length: 249.27
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 11.97s
                        Total time: 15835.64s
                               ETA: 1205120.5s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.427s, learning 0.191s)
               Value function loss: 9.6921
                    Surrogate loss: 0.0092
             Mean action noise std: 0.75
                       Mean reward: 373.23
               Mean episode length: 249.27
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 11.62s
                        Total time: 15847.26s
                               ETA: 1205063.3s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.197s, learning 0.224s)
               Value function loss: 10.2003
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 373.73
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 11.42s
                        Total time: 15858.68s
                               ETA: 1204991.2s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.889s, learning 0.281s)
               Value function loss: 17.8604
                    Surrogate loss: -0.0098
             Mean action noise std: 0.75
                       Mean reward: 374.39
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 12.17s
                        Total time: 15870.85s
                               ETA: 1204976.1s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.963s, learning 0.166s)
               Value function loss: 23.6180
                    Surrogate loss: 0.0015
             Mean action noise std: 0.75
                       Mean reward: 374.18
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 12.13s
                        Total time: 15882.98s
                               ETA: 1204957.9s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.127s, learning 0.257s)
               Value function loss: 20.5748
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 374.05
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 11.38s
                        Total time: 15894.36s
                               ETA: 1204883.2s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.650s, learning 0.257s)
               Value function loss: 34.1253
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 374.01
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 11.91s
                        Total time: 15906.27s
                               ETA: 1204848.2s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.543s, learning 0.295s)
               Value function loss: 45.8815
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 372.52
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 11.84s
                        Total time: 15918.11s
                               ETA: 1204808.1s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.141s, learning 0.158s)
               Value function loss: 48.1436
                    Surrogate loss: -0.0021
             Mean action noise std: 0.75
                       Mean reward: 371.47
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 11.30s
                        Total time: 15929.41s
                               ETA: 1204727.1s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.586s, learning 0.186s)
               Value function loss: 43.1139
                    Surrogate loss: -0.0118
             Mean action noise std: 0.75
                       Mean reward: 371.99
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 11.77s
                        Total time: 15941.18s
                               ETA: 1204682.2s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.473s, learning 0.171s)
               Value function loss: 25.2734
                    Surrogate loss: -0.0140
             Mean action noise std: 0.75
                       Mean reward: 371.09
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 11.64s
                        Total time: 15952.82s
                               ETA: 1204627.5s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.213s, learning 0.169s)
               Value function loss: 14.3678
                    Surrogate loss: -0.0219
             Mean action noise std: 0.75
                       Mean reward: 370.40
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 11.38s
                        Total time: 15964.21s
                               ETA: 1204553.1s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.958s, learning 0.167s)
               Value function loss: 6.5562
                    Surrogate loss: -0.0176
             Mean action noise std: 0.75
                       Mean reward: 368.93
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 11.12s
                        Total time: 15975.33s
                               ETA: 1204459.4s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.181s, learning 0.159s)
               Value function loss: 2.2950
                    Surrogate loss: 0.0090
             Mean action noise std: 0.75
                       Mean reward: 368.50
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 11.34s
                        Total time: 15986.67s
                               ETA: 1204382.1s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1483 steps/s (collection: 10.799s, learning 0.243s)
               Value function loss: 1.5709
                    Surrogate loss: -0.0023
             Mean action noise std: 0.75
                       Mean reward: 368.11
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 11.04s
                        Total time: 15997.71s
                               ETA: 1204282.4s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.085s, learning 0.240s)
               Value function loss: 1.2939
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 368.54
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 11.32s
                        Total time: 16009.04s
                               ETA: 1204204.2s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.044s, learning 0.207s)
               Value function loss: 1.0668
                    Surrogate loss: 0.0106
             Mean action noise std: 0.75
                       Mean reward: 368.88
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 11.25s
                        Total time: 16020.29s
                               ETA: 1204120.4s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.576s, learning 0.264s)
               Value function loss: 0.5674
                    Surrogate loss: -0.0093
             Mean action noise std: 0.75
                       Mean reward: 368.84
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 11.84s
                        Total time: 16032.13s
                               ETA: 1204081.1s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.382s, learning 0.279s)
               Value function loss: 1.0048
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 369.32
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 11.66s
                        Total time: 16043.79s
                               ETA: 1204028.4s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.456s, learning 0.280s)
               Value function loss: 0.7742
                    Surrogate loss: -0.0075
             Mean action noise std: 0.75
                       Mean reward: 369.47
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 11.74s
                        Total time: 16055.52s
                               ETA: 1203981.3s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.346s, learning 0.242s)
               Value function loss: 0.6096
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 369.25
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 11.59s
                        Total time: 16067.11s
                               ETA: 1203923.2s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.110s, learning 0.304s)
               Value function loss: 1.5417
                    Surrogate loss: -0.0037
             Mean action noise std: 0.75
                       Mean reward: 368.47
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 11.41s
                        Total time: 16078.53s
                               ETA: 1203852.2s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.117s, learning 0.166s)
               Value function loss: 1.9910
                    Surrogate loss: -0.0095
             Mean action noise std: 0.75
                       Mean reward: 367.73
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 11.28s
                        Total time: 16089.81s
                               ETA: 1203771.4s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.137s, learning 0.168s)
               Value function loss: 1.1467
                    Surrogate loss: -0.0105
             Mean action noise std: 0.75
                       Mean reward: 366.82
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 11.31s
                        Total time: 16101.11s
                               ETA: 1203692.4s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.109s, learning 0.212s)
               Value function loss: 3.5005
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 364.70
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 11.32s
                        Total time: 16112.44s
                               ETA: 1203614.8s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.254s, learning 0.254s)
               Value function loss: 3.1479
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 363.57
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 11.51s
                        Total time: 16123.94s
                               ETA: 1203551.1s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.233s, learning 0.175s)
               Value function loss: 3.0625
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 362.26
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 11.41s
                        Total time: 16135.35s
                               ETA: 1203480.1s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.189s, learning 0.169s)
               Value function loss: 2.7753
                    Surrogate loss: -0.0131
             Mean action noise std: 0.75
                       Mean reward: 362.47
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 11.36s
                        Total time: 16146.71s
                               ETA: 1203405.4s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.594s, learning 0.342s)
               Value function loss: 3.3202
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 361.81
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 11.94s
                        Total time: 16158.64s
                               ETA: 1203373.9s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.053s, learning 0.372s)
               Value function loss: 3.3020
                    Surrogate loss: -0.0152
             Mean action noise std: 0.75
                       Mean reward: 361.23
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 12.42s
                        Total time: 16171.07s
                               ETA: 1203378.8s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.489s, learning 0.159s)
               Value function loss: 3.8257
                    Surrogate loss: -0.0171
             Mean action noise std: 0.75
                       Mean reward: 361.66
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 11.65s
                        Total time: 16182.72s
                               ETA: 1203325.9s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1476 steps/s (collection: 10.929s, learning 0.169s)
               Value function loss: 2.9861
                    Surrogate loss: 0.0124
             Mean action noise std: 0.75
                       Mean reward: 360.92
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 11.10s
                        Total time: 16193.82s
                               ETA: 1203232.3s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.088s, learning 0.164s)
               Value function loss: 5.2133
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 361.99
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 11.25s
                        Total time: 16205.07s
                               ETA: 1203150.1s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.016s, learning 0.158s)
               Value function loss: 8.0244
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 364.01
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 11.17s
                        Total time: 16216.24s
                               ETA: 1203062.3s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.521s, learning 0.168s)
               Value function loss: 13.4565
                    Surrogate loss: 0.0038
             Mean action noise std: 0.75
                       Mean reward: 367.09
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 11.69s
                        Total time: 16227.93s
                               ETA: 1203012.8s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.731s, learning 0.165s)
               Value function loss: 18.1497
                    Surrogate loss: -0.0003
             Mean action noise std: 0.75
                       Mean reward: 368.58
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 11.90s
                        Total time: 16239.83s
                               ETA: 1202978.6s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.414s, learning 0.278s)
               Value function loss: 16.8152
                    Surrogate loss: -0.0109
             Mean action noise std: 0.75
                       Mean reward: 368.66
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 11.69s
                        Total time: 16251.52s
                               ETA: 1202929.3s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.620s, learning 0.261s)
               Value function loss: 26.7994
                    Surrogate loss: 0.0090
             Mean action noise std: 0.75
                       Mean reward: 368.33
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 11.88s
                        Total time: 16263.40s
                               ETA: 1202894.1s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.507s, learning 0.274s)
               Value function loss: 38.4518
                    Surrogate loss: 0.0105
             Mean action noise std: 0.75
                       Mean reward: 371.80
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 11.78s
                        Total time: 16275.18s
                               ETA: 1202851.6s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.685s, learning 0.157s)
               Value function loss: 41.1936
                    Surrogate loss: 0.0027
             Mean action noise std: 0.75
                       Mean reward: 371.35
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 11.84s
                        Total time: 16287.02s
                               ETA: 1202813.6s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.208s, learning 0.185s)
               Value function loss: 36.7476
                    Surrogate loss: 0.0011
             Mean action noise std: 0.75
                       Mean reward: 373.14
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 11.39s
                        Total time: 16298.41s
                               ETA: 1202742.5s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.303s, learning 0.169s)
               Value function loss: 21.7965
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 372.21
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 11.47s
                        Total time: 16309.89s
                               ETA: 1202677.4s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.176s, learning 0.226s)
               Value function loss: 15.5654
                    Surrogate loss: -0.0116
             Mean action noise std: 0.75
                       Mean reward: 373.50
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 11.40s
                        Total time: 16321.29s
                               ETA: 1202607.2s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.695s, learning 0.190s)
               Value function loss: 10.4912
                    Surrogate loss: -0.0102
             Mean action noise std: 0.75
                       Mean reward: 375.11
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 11.88s
                        Total time: 16333.17s
                               ETA: 1202572.6s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.050s, learning 0.275s)
               Value function loss: 4.2860
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 375.91
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 11.32s
                        Total time: 16344.50s
                               ETA: 1202496.8s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.465s, learning 0.289s)
               Value function loss: 2.3751
                    Surrogate loss: -0.0184
             Mean action noise std: 0.75
                       Mean reward: 376.24
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 11.75s
                        Total time: 16356.25s
                               ETA: 1202452.6s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.729s, learning 0.251s)
               Value function loss: 2.1469
                    Surrogate loss: -0.0141
             Mean action noise std: 0.75
                       Mean reward: 377.07
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 11.98s
                        Total time: 16368.23s
                               ETA: 1202425.2s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.190s, learning 0.228s)
               Value function loss: 1.3049
                    Surrogate loss: 0.0122
             Mean action noise std: 0.75
                       Mean reward: 377.63
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 11.42s
                        Total time: 16379.65s
                               ETA: 1202356.5s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.248s, learning 0.294s)
               Value function loss: 0.8522
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 377.45
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 11.54s
                        Total time: 16391.19s
                               ETA: 1202297.0s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.327s, learning 0.239s)
               Value function loss: 1.3459
                    Surrogate loss: -0.0010
             Mean action noise std: 0.75
                       Mean reward: 375.77
               Mean episode length: 248.76
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 11.57s
                        Total time: 16402.76s
                               ETA: 1202239.4s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.297s, learning 0.173s)
               Value function loss: 1.4137
                    Surrogate loss: -0.0022
             Mean action noise std: 0.75
                       Mean reward: 376.85
               Mean episode length: 248.76
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 11.47s
                        Total time: 16414.23s
                               ETA: 1202174.7s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.699s, learning 0.262s)
               Value function loss: 1.0542
                    Surrogate loss: -0.0043
             Mean action noise std: 0.75
                       Mean reward: 377.02
               Mean episode length: 248.76
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 11.96s
                        Total time: 16426.19s
                               ETA: 1202146.0s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.334s, learning 0.266s)
               Value function loss: 1.2060
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 377.80
               Mean episode length: 248.76
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 11.60s
                        Total time: 16437.79s
                               ETA: 1202091.0s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.206s, learning 0.165s)
               Value function loss: 3.7408
                    Surrogate loss: -0.0104
             Mean action noise std: 0.75
                       Mean reward: 379.53
               Mean episode length: 248.76
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 11.37s
                        Total time: 16449.16s
                               ETA: 1202019.4s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.594s, learning 0.203s)
               Value function loss: 2.0850
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 378.54
               Mean episode length: 247.66
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 11.80s
                        Total time: 16460.96s
                               ETA: 1201978.9s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.168s)
               Value function loss: 5.6343
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 381.06
               Mean episode length: 247.66
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 11.38s
                        Total time: 16472.34s
                               ETA: 1201908.3s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.212s, learning 0.194s)
               Value function loss: 4.9318
                    Surrogate loss: -0.0127
             Mean action noise std: 0.75
                       Mean reward: 382.58
               Mean episode length: 247.66
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 11.41s
                        Total time: 16483.75s
                               ETA: 1201839.4s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.062s, learning 0.159s)
               Value function loss: 2.9655
                    Surrogate loss: -0.0112
             Mean action noise std: 0.75
                       Mean reward: 384.81
               Mean episode length: 248.90
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 11.22s
                        Total time: 16494.97s
                               ETA: 1201757.1s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.956s, learning 0.162s)
               Value function loss: 4.9894
                    Surrogate loss: -0.0057
             Mean action noise std: 0.75
                       Mean reward: 382.43
               Mean episode length: 247.53
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 11.12s
                        Total time: 16506.09s
                               ETA: 1201667.4s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.224s, learning 0.190s)
               Value function loss: 3.8952
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 383.82
               Mean episode length: 248.63
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 11.41s
                        Total time: 16517.50s
                               ETA: 1201599.4s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.427s, learning 0.283s)
               Value function loss: 5.9826
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 383.89
               Mean episode length: 248.63
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 11.71s
                        Total time: 16529.21s
                               ETA: 1201553.0s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.146s, learning 0.163s)
               Value function loss: 5.4610
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 384.12
               Mean episode length: 248.63
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 11.31s
                        Total time: 16540.52s
                               ETA: 1201477.4s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.643s, learning 0.198s)
               Value function loss: 6.1696
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 384.54
               Mean episode length: 248.63
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 11.84s
                        Total time: 16552.36s
                               ETA: 1201440.7s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.716s, learning 0.223s)
               Value function loss: 7.3809
                    Surrogate loss: -0.0074
             Mean action noise std: 0.75
                       Mean reward: 384.67
               Mean episode length: 248.94
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 10.94s
                        Total time: 16563.30s
                               ETA: 1201338.5s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.106s, learning 0.164s)
               Value function loss: 12.9929
                    Surrogate loss: -0.0070
             Mean action noise std: 0.75
                       Mean reward: 383.92
               Mean episode length: 248.65
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 11.27s
                        Total time: 16574.57s
                               ETA: 1201260.5s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.213s, learning 0.198s)
               Value function loss: 16.1758
                    Surrogate loss: -0.0111
             Mean action noise std: 0.75
                       Mean reward: 385.44
               Mean episode length: 249.71
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 11.41s
                        Total time: 16585.98s
                               ETA: 1201192.8s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.684s, learning 0.187s)
               Value function loss: 29.1448
                    Surrogate loss: -0.0089
             Mean action noise std: 0.75
                       Mean reward: 382.82
               Mean episode length: 248.57
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 11.87s
                        Total time: 16597.85s
                               ETA: 1201158.4s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.457s, learning 0.202s)
               Value function loss: 24.0934
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 381.71
               Mean episode length: 247.24
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 11.66s
                        Total time: 16609.51s
                               ETA: 1201108.7s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.170s, learning 0.171s)
               Value function loss: 33.9809
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 385.42
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 11.34s
                        Total time: 16620.85s
                               ETA: 1201036.2s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.541s, learning 0.260s)
               Value function loss: 59.3165
                    Surrogate loss: -0.0018
             Mean action noise std: 0.75
                       Mean reward: 382.16
               Mean episode length: 248.64
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 11.80s
                        Total time: 16632.65s
                               ETA: 1200996.9s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.296s, learning 0.225s)
               Value function loss: 64.6154
                    Surrogate loss: -0.0124
             Mean action noise std: 0.75
                       Mean reward: 382.59
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 11.52s
                        Total time: 16644.17s
                               ETA: 1200937.5s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.294s, learning 0.196s)
               Value function loss: 60.7470
                    Surrogate loss: -0.0135
             Mean action noise std: 0.75
                       Mean reward: 383.46
               Mean episode length: 249.77
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 11.49s
                        Total time: 16655.66s
                               ETA: 1200875.8s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.220s, learning 0.293s)
               Value function loss: 33.4665
                    Surrogate loss: -0.0106
             Mean action noise std: 0.75
                       Mean reward: 382.65
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 11.51s
                        Total time: 16667.18s
                               ETA: 1200815.9s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.383s, learning 0.287s)
               Value function loss: 21.4549
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 382.91
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 11.67s
                        Total time: 16678.85s
                               ETA: 1200767.4s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.768s, learning 0.174s)
               Value function loss: 14.8401
                    Surrogate loss: -0.0048
             Mean action noise std: 0.75
                       Mean reward: 381.94
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 10.94s
                        Total time: 16689.79s
                               ETA: 1200666.5s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.166s)
               Value function loss: 7.6422
                    Surrogate loss: -0.0130
             Mean action noise std: 0.75
                       Mean reward: 381.65
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 11.25s
                        Total time: 16701.04s
                               ETA: 1200588.1s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.906s, learning 0.170s)
               Value function loss: 3.1930
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 381.13
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 11.08s
                        Total time: 16712.11s
                               ETA: 1200497.1s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.302s, learning 0.157s)
               Value function loss: 2.9143
                    Surrogate loss: -0.0051
             Mean action noise std: 0.75
                       Mean reward: 379.01
               Mean episode length: 248.52
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 11.46s
                        Total time: 16723.57s
                               ETA: 1200433.7s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.324s, learning 0.155s)
               Value function loss: 3.0647
                    Surrogate loss: 0.0149
             Mean action noise std: 0.75
                       Mean reward: 379.31
               Mean episode length: 248.52
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 11.48s
                        Total time: 16735.05s
                               ETA: 1200371.8s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.214s, learning 0.209s)
               Value function loss: 1.5836
                    Surrogate loss: 0.0025
             Mean action noise std: 0.75
                       Mean reward: 379.00
               Mean episode length: 248.52
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 11.42s
                        Total time: 16746.47s
                               ETA: 1200306.0s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.292s, learning 0.190s)
               Value function loss: 2.6298
                    Surrogate loss: -0.0076
             Mean action noise std: 0.75
                       Mean reward: 372.46
               Mean episode length: 244.54
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 11.48s
                        Total time: 16757.96s
                               ETA: 1200244.5s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.286s, learning 0.255s)
               Value function loss: 1.6436
                    Surrogate loss: 0.0065
             Mean action noise std: 0.75
                       Mean reward: 372.05
               Mean episode length: 244.54
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 11.54s
                        Total time: 16769.50s
                               ETA: 1200187.3s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.603s, learning 0.178s)
               Value function loss: 1.7782
                    Surrogate loss: 0.0008
             Mean action noise std: 0.75
                       Mean reward: 365.02
               Mean episode length: 240.22
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 11.78s
                        Total time: 16781.28s
                               ETA: 1200147.4s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.260s, learning 0.172s)
               Value function loss: 1.7355
                    Surrogate loss: -0.0041
             Mean action noise std: 0.75
                       Mean reward: 361.64
               Mean episode length: 237.97
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 11.43s
                        Total time: 16792.71s
                               ETA: 1200082.5s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.610s, learning 0.166s)
               Value function loss: 3.9735
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 353.57
               Mean episode length: 232.85
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 11.78s
                        Total time: 16804.49s
                               ETA: 1200042.3s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.272s, learning 0.184s)
               Value function loss: 4.0149
                    Surrogate loss: -0.0034
             Mean action noise std: 0.75
                       Mean reward: 343.16
               Mean episode length: 226.47
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 11.46s
                        Total time: 16815.94s
                               ETA: 1199979.3s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.444s, learning 0.224s)
               Value function loss: 4.3939
                    Surrogate loss: -0.0032
             Mean action noise std: 0.75
                       Mean reward: 336.68
               Mean episode length: 222.17
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 11.67s
                        Total time: 16827.61s
                               ETA: 1199931.5s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.496s, learning 0.168s)
               Value function loss: 5.6314
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 339.49
               Mean episode length: 224.95
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 11.66s
                        Total time: 16839.27s
                               ETA: 1199883.4s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.240s, learning 0.199s)
               Value function loss: 3.7150
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 347.03
               Mean episode length: 229.75
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 11.44s
                        Total time: 16850.71s
                               ETA: 1199819.4s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.056s, learning 0.268s)
               Value function loss: 4.4815
                    Surrogate loss: -0.0058
             Mean action noise std: 0.75
                       Mean reward: 356.67
               Mean episode length: 236.35
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 11.32s
                        Total time: 16862.04s
                               ETA: 1199747.3s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.660s, learning 0.187s)
               Value function loss: 4.4288
                    Surrogate loss: -0.0108
             Mean action noise std: 0.75
                       Mean reward: 363.05
               Mean episode length: 240.15
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 11.85s
                        Total time: 16873.88s
                               ETA: 1199712.4s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.992s, learning 0.171s)
               Value function loss: 4.9488
                    Surrogate loss: -0.0122
             Mean action noise std: 0.75
                       Mean reward: 364.87
               Mean episode length: 241.83
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 11.16s
                        Total time: 16885.05s
                               ETA: 1199629.0s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.333s, learning 0.158s)
               Value function loss: 6.0929
                    Surrogate loss: -0.0091
             Mean action noise std: 0.75
                       Mean reward: 366.64
               Mean episode length: 242.86
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 11.49s
                        Total time: 16896.54s
                               ETA: 1199569.0s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.327s, learning 0.167s)
               Value function loss: 6.1508
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 370.05
               Mean episode length: 244.86
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 11.49s
                        Total time: 16908.03s
                               ETA: 1199509.3s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.790s, learning 0.168s)
               Value function loss: 4.6213
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 370.98
               Mean episode length: 245.12
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 11.96s
                        Total time: 16919.99s
                               ETA: 1199482.5s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.285s, learning 0.182s)
               Value function loss: 9.9026
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 375.15
               Mean episode length: 248.29
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 11.47s
                        Total time: 16931.46s
                               ETA: 1199421.0s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.701s, learning 0.186s)
               Value function loss: 10.8406
                    Surrogate loss: -0.0123
             Mean action noise std: 0.75
                       Mean reward: 376.83
               Mean episode length: 249.30
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 11.89s
                        Total time: 16943.34s
                               ETA: 1199389.2s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.322s, learning 0.167s)
               Value function loss: 20.3559
                    Surrogate loss: -0.0077
             Mean action noise std: 0.75
                       Mean reward: 379.17
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 11.49s
                        Total time: 16954.83s
                               ETA: 1199329.4s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.174s, learning 0.213s)
               Value function loss: 17.5980
                    Surrogate loss: -0.0115
             Mean action noise std: 0.75
                       Mean reward: 377.91
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 11.39s
                        Total time: 16966.22s
                               ETA: 1199262.4s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.671s, learning 0.161s)
               Value function loss: 25.0569
                    Surrogate loss: -0.0107
             Mean action noise std: 0.75
                       Mean reward: 377.46
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 11.83s
                        Total time: 16978.05s
                               ETA: 1199226.9s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.164s)
               Value function loss: 39.0145
                    Surrogate loss: -0.0040
             Mean action noise std: 0.75
                       Mean reward: 379.72
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 11.25s
                        Total time: 16989.30s
                               ETA: 1199150.3s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.274s, learning 0.165s)
               Value function loss: 46.1158
                    Surrogate loss: -0.0046
             Mean action noise std: 0.75
                       Mean reward: 379.35
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 11.44s
                        Total time: 17000.74s
                               ETA: 1199087.1s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.145s, learning 0.194s)
               Value function loss: 46.1753
                    Surrogate loss: -0.0103
             Mean action noise std: 0.75
                       Mean reward: 379.55
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 11.34s
                        Total time: 17012.08s
                               ETA: 1199017.0s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.432s, learning 0.169s)
               Value function loss: 19.7415
                    Surrogate loss: -0.0114
             Mean action noise std: 0.75
                       Mean reward: 379.27
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 11.60s
                        Total time: 17023.68s
                               ETA: 1198965.4s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.292s, learning 0.195s)
               Value function loss: 11.0660
                    Surrogate loss: -0.0168
             Mean action noise std: 0.75
                       Mean reward: 377.89
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 11.49s
                        Total time: 17035.16s
                               ETA: 1198905.9s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.299s, learning 0.203s)
               Value function loss: 9.1922
                    Surrogate loss: -0.0165
             Mean action noise std: 0.75
                       Mean reward: 378.71
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 11.50s
                        Total time: 17046.67s
                               ETA: 1198847.6s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.649s, learning 0.226s)
               Value function loss: 6.0289
                    Surrogate loss: -0.0054
             Mean action noise std: 0.75
                       Mean reward: 376.96
               Mean episode length: 248.70
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 11.87s
                        Total time: 17058.54s
                               ETA: 1198815.4s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.314s, learning 0.197s)
               Value function loss: 2.4743
                    Surrogate loss: 0.0089
             Mean action noise std: 0.75
                       Mean reward: 376.29
               Mean episode length: 248.70
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 11.51s
                        Total time: 17070.05s
                               ETA: 1198757.8s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.084s, learning 0.169s)
               Value function loss: 1.6385
                    Surrogate loss: -0.0081
             Mean action noise std: 0.75
                       Mean reward: 375.79
               Mean episode length: 248.70
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 11.25s
                        Total time: 17081.31s
                               ETA: 1198682.1s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.189s, learning 0.163s)
               Value function loss: 1.7803
                    Surrogate loss: 0.0040
             Mean action noise std: 0.75
                       Mean reward: 375.07
               Mean episode length: 248.70
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 11.35s
                        Total time: 17092.66s
                               ETA: 1198613.5s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.159s, learning 0.159s)
               Value function loss: 1.9428
                    Surrogate loss: -0.0080
             Mean action noise std: 0.75
                       Mean reward: 374.87
               Mean episode length: 248.70
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 11.32s
                        Total time: 17103.98s
                               ETA: 1198542.6s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.232s, learning 0.160s)
               Value function loss: 3.0449
                    Surrogate loss: 0.0131
             Mean action noise std: 0.75
                       Mean reward: 374.81
               Mean episode length: 248.70
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 11.39s
                        Total time: 17115.37s
                               ETA: 1198476.9s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.365s, learning 0.183s)
               Value function loss: 1.5472
                    Surrogate loss: 0.0053
             Mean action noise std: 0.75
                       Mean reward: 374.49
               Mean episode length: 248.70
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 11.55s
                        Total time: 17126.92s
                               ETA: 1198422.3s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.683s, learning 0.214s)
               Value function loss: 1.1064
                    Surrogate loss: 0.0230
             Mean action noise std: 0.75
                       Mean reward: 373.68
               Mean episode length: 248.70
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 11.90s
                        Total time: 17138.81s
                               ETA: 1198392.1s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.481s, learning 0.189s)
               Value function loss: 1.3522
                    Surrogate loss: 0.0066
             Mean action noise std: 0.75
                       Mean reward: 373.34
               Mean episode length: 248.70
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 11.67s
                        Total time: 17150.48s
                               ETA: 1198346.1s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.694s, learning 0.165s)
               Value function loss: 2.3308
                    Surrogate loss: -0.0030
             Mean action noise std: 0.75
                       Mean reward: 371.12
               Mean episode length: 247.60
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 11.86s
                        Total time: 17162.34s
                               ETA: 1198313.2s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.624s, learning 0.216s)
               Value function loss: 3.1833
                    Surrogate loss: -0.0099
             Mean action noise std: 0.75
                       Mean reward: 370.44
               Mean episode length: 247.34
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 11.84s
                        Total time: 17174.18s
                               ETA: 1198279.2s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.582s, learning 0.198s)
               Value function loss: 1.9492
                    Surrogate loss: 0.0227
             Mean action noise std: 0.75
                       Mean reward: 367.66
               Mean episode length: 246.11
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 11.78s
                        Total time: 17185.96s
                               ETA: 1198240.9s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.498s, learning 0.210s)
               Value function loss: 4.8014
                    Surrogate loss: -0.0056
             Mean action noise std: 0.75
                       Mean reward: 366.30
               Mean episode length: 244.99
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 11.71s
                        Total time: 17197.67s
                               ETA: 1198197.7s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.298s, learning 0.169s)
               Value function loss: 3.9889
                    Surrogate loss: -0.0086
             Mean action noise std: 0.75
                       Mean reward: 364.74
               Mean episode length: 242.77
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 11.47s
                        Total time: 17209.14s
                               ETA: 1198137.7s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.630s, learning 0.179s)
               Value function loss: 4.0118
                    Surrogate loss: -0.0060
             Mean action noise std: 0.75
                       Mean reward: 366.60
               Mean episode length: 243.29
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 11.81s
                        Total time: 17220.95s
                               ETA: 1198101.6s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.102s, learning 0.213s)
               Value function loss: 4.1988
                    Surrogate loss: -0.0067
             Mean action noise std: 0.75
                       Mean reward: 370.20
               Mean episode length: 245.63
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 11.32s
                        Total time: 17232.26s
                               ETA: 1198031.2s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.243s, learning 0.173s)
               Value function loss: 4.7782
                    Surrogate loss: -0.0079
             Mean action noise std: 0.75
                       Mean reward: 370.26
               Mean episode length: 245.49
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 11.42s
                        Total time: 17243.68s
                               ETA: 1197967.8s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.362s, learning 0.201s)
               Value function loss: 4.9967
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 367.98
               Mean episode length: 244.08
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 11.56s
                        Total time: 17255.24s
                               ETA: 1197914.8s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.568s, learning 0.174s)
               Value function loss: 5.9167
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 371.67
               Mean episode length: 246.88
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 11.74s
                        Total time: 17266.98s
                               ETA: 1197874.2s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.654s, learning 0.231s)
               Value function loss: 5.5011
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 372.92
               Mean episode length: 247.33
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 11.88s
                        Total time: 17278.87s
                               ETA: 1197843.6s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.628s, learning 0.183s)
               Value function loss: 8.3700
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 377.34
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 11.81s
                        Total time: 17290.68s
                               ETA: 1197807.9s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.476s, learning 0.184s)
               Value function loss: 8.8479
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 378.56
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 11.66s
                        Total time: 17302.34s
                               ETA: 1197761.7s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.406s, learning 0.193s)
               Value function loss: 18.6325
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 376.50
               Mean episode length: 248.41
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 11.60s
                        Total time: 17313.94s
                               ETA: 1197711.4s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.261s, learning 0.201s)
               Value function loss: 25.4205
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 376.91
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 11.46s
                        Total time: 17325.40s
                               ETA: 1197651.7s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.260s, learning 0.196s)
               Value function loss: 21.2130
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 377.68
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 11.46s
                        Total time: 17336.86s
                               ETA: 1197591.6s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.428s, learning 0.198s)
               Value function loss: 35.0527
                    Surrogate loss: 0.0042
             Mean action noise std: 0.74
                       Mean reward: 378.95
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 11.63s
                        Total time: 17348.48s
                               ETA: 1197543.4s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.384s, learning 0.198s)
               Value function loss: 52.7787
                    Surrogate loss: 0.0132
             Mean action noise std: 0.74
                       Mean reward: 380.16
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 11.58s
                        Total time: 17360.06s
                               ETA: 1197492.1s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.903s, learning 0.258s)
               Value function loss: 60.4429
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 381.80
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 12.16s
                        Total time: 17372.22s
                               ETA: 1197480.8s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.051s, learning 0.166s)
               Value function loss: 54.9437
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 382.82
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 11.22s
                        Total time: 17383.44s
                               ETA: 1197404.4s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.479s, learning 0.169s)
               Value function loss: 35.4848
                    Surrogate loss: -0.0186
             Mean action noise std: 0.74
                       Mean reward: 381.77
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 11.65s
                        Total time: 17395.09s
                               ETA: 1197357.9s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.173s, learning 0.175s)
               Value function loss: 23.3347
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 384.16
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 11.35s
                        Total time: 17406.44s
                               ETA: 1197290.8s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.836s, learning 0.166s)
               Value function loss: 11.7913
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 384.28
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 12.00s
                        Total time: 17418.44s
                               ETA: 1197268.7s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.319s, learning 0.162s)
               Value function loss: 4.3771
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 382.59
               Mean episode length: 249.07
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 11.48s
                        Total time: 17429.92s
                               ETA: 1197210.8s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.337s, learning 0.166s)
               Value function loss: 2.0554
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 382.43
               Mean episode length: 249.07
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 11.50s
                        Total time: 17441.42s
                               ETA: 1197154.5s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.617s, learning 0.225s)
               Value function loss: 2.2126
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 383.02
               Mean episode length: 249.07
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 11.84s
                        Total time: 17453.26s
                               ETA: 1197121.4s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.502s, learning 0.317s)
               Value function loss: 1.4311
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 383.00
               Mean episode length: 249.07
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 11.82s
                        Total time: 17465.08s
                               ETA: 1197086.9s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.862s, learning 0.249s)
               Value function loss: 1.3207
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 380.44
               Mean episode length: 247.56
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 12.11s
                        Total time: 17477.19s
                               ETA: 1197072.4s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.587s, learning 0.280s)
               Value function loss: 1.9421
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 379.33
               Mean episode length: 246.82
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 11.87s
                        Total time: 17489.06s
                               ETA: 1197041.2s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.281s, learning 0.162s)
               Value function loss: 1.1247
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 379.59
               Mean episode length: 246.82
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 11.44s
                        Total time: 17500.50s
                               ETA: 1196981.1s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.306s, learning 0.168s)
               Value function loss: 1.8580
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 379.49
               Mean episode length: 246.82
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 11.47s
                        Total time: 17511.98s
                               ETA: 1196923.0s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.944s, learning 0.167s)
               Value function loss: 4.1575
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 374.56
               Mean episode length: 243.52
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 12.11s
                        Total time: 17524.09s
                               ETA: 1196908.7s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.271s, learning 0.189s)
               Value function loss: 4.1194
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 371.12
               Mean episode length: 241.17
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 11.46s
                        Total time: 17535.55s
                               ETA: 1196849.8s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.044s, learning 0.252s)
               Value function loss: 3.2172
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 370.18
               Mean episode length: 240.49
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 12.30s
                        Total time: 17547.85s
                               ETA: 1196848.1s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.512s, learning 0.217s)
               Value function loss: 7.0977
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 370.35
               Mean episode length: 240.72
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 11.73s
                        Total time: 17559.57s
                               ETA: 1196807.6s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.160s, learning 0.171s)
               Value function loss: 6.1030
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 371.53
               Mean episode length: 241.46
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 11.33s
                        Total time: 17570.91s
                               ETA: 1196740.2s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.720s, learning 0.169s)
               Value function loss: 5.8343
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 371.60
               Mean episode length: 242.84
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 11.89s
                        Total time: 17582.79s
                               ETA: 1196710.8s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.981s, learning 0.161s)
               Value function loss: 5.8077
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 373.87
               Mean episode length: 244.16
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 11.14s
                        Total time: 17593.94s
                               ETA: 1196630.5s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.327s, learning 0.204s)
               Value function loss: 6.5840
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 368.10
               Mean episode length: 240.61
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 11.53s
                        Total time: 17605.47s
                               ETA: 1196576.9s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.780s, learning 0.261s)
               Value function loss: 6.8677
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 364.22
               Mean episode length: 238.12
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 12.04s
                        Total time: 17617.51s
                               ETA: 1196557.9s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.396s, learning 0.190s)
               Value function loss: 6.0613
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 366.21
               Mean episode length: 238.50
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 11.59s
                        Total time: 17629.09s
                               ETA: 1196508.0s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.555s, learning 0.174s)
               Value function loss: 5.6740
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 362.45
               Mean episode length: 236.97
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 11.73s
                        Total time: 17640.82s
                               ETA: 1196467.9s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.322s, learning 0.195s)
               Value function loss: 7.7334
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 369.16
               Mean episode length: 241.52
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 11.52s
                        Total time: 17652.34s
                               ETA: 1196413.4s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.567s, learning 0.259s)
               Value function loss: 11.5170
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 373.41
               Mean episode length: 246.17
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 11.83s
                        Total time: 17664.17s
                               ETA: 1196380.0s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.139s, learning 0.178s)
               Value function loss: 17.4423
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 375.69
               Mean episode length: 247.67
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 11.32s
                        Total time: 17675.48s
                               ETA: 1196312.1s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.553s, learning 0.238s)
               Value function loss: 23.4413
                    Surrogate loss: -0.0023
             Mean action noise std: 0.74
                       Mean reward: 375.33
               Mean episode length: 247.80
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 11.79s
                        Total time: 17687.27s
                               ETA: 1196276.3s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.313s, learning 0.168s)
               Value function loss: 20.5003
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 378.49
               Mean episode length: 249.84
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 11.48s
                        Total time: 17698.75s
                               ETA: 1196219.7s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.571s, learning 0.164s)
               Value function loss: 31.4271
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 378.86
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 11.74s
                        Total time: 17710.49s
                               ETA: 1196180.3s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.264s, learning 0.163s)
               Value function loss: 44.2796
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 374.40
               Mean episode length: 248.75
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 11.43s
                        Total time: 17721.92s
                               ETA: 1196120.1s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.193s, learning 0.162s)
               Value function loss: 51.1575
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 376.14
               Mean episode length: 249.98
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 11.36s
                        Total time: 17733.27s
                               ETA: 1196055.2s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.164s, learning 0.181s)
               Value function loss: 39.3217
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 377.29
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 11.34s
                        Total time: 17744.62s
                               ETA: 1195989.6s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.344s, learning 0.171s)
               Value function loss: 24.4530
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 376.29
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 11.51s
                        Total time: 17756.13s
                               ETA: 1195935.5s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.751s, learning 0.182s)
               Value function loss: 16.5664
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 376.11
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 11.93s
                        Total time: 17768.06s
                               ETA: 1195909.7s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.184s, learning 0.289s)
               Value function loss: 8.8776
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 376.16
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 11.47s
                        Total time: 17779.54s
                               ETA: 1195852.9s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.306s, learning 0.161s)
               Value function loss: 4.4521
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 377.73
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 11.47s
                        Total time: 17791.00s
                               ETA: 1195795.8s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.189s, learning 0.215s)
               Value function loss: 2.1699
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 376.43
               Mean episode length: 249.16
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 11.40s
                        Total time: 17802.41s
                               ETA: 1195734.5s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.300s, learning 0.175s)
               Value function loss: 1.6101
                    Surrogate loss: 0.0036
             Mean action noise std: 0.74
                       Mean reward: 375.31
               Mean episode length: 248.55
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 11.47s
                        Total time: 17813.88s
                               ETA: 1195678.0s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.292s, learning 0.157s)
               Value function loss: 1.5778
                    Surrogate loss: 0.0094
             Mean action noise std: 0.74
                       Mean reward: 374.85
               Mean episode length: 248.55
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 11.45s
                        Total time: 17825.33s
                               ETA: 1195619.9s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.384s, learning 0.170s)
               Value function loss: 0.9876
                    Surrogate loss: 0.0061
             Mean action noise std: 0.74
                       Mean reward: 374.72
               Mean episode length: 248.55
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 11.55s
                        Total time: 17836.89s
                               ETA: 1195568.8s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.481s, learning 0.267s)
               Value function loss: 1.7319
                    Surrogate loss: 0.0196
             Mean action noise std: 0.74
                       Mean reward: 373.60
               Mean episode length: 247.52
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 11.75s
                        Total time: 17848.63s
                               ETA: 1195530.8s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.309s, learning 0.206s)
               Value function loss: 1.7954
                    Surrogate loss: 0.0045
             Mean action noise std: 0.74
                       Mean reward: 374.46
               Mean episode length: 247.52
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 11.52s
                        Total time: 17860.15s
                               ETA: 1195477.3s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.336s, learning 0.234s)
               Value function loss: 1.4197
                    Surrogate loss: 0.0006
             Mean action noise std: 0.74
                       Mean reward: 374.18
               Mean episode length: 247.52
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 11.57s
                        Total time: 17871.72s
                               ETA: 1195427.5s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.210s, learning 0.187s)
               Value function loss: 3.1554
                    Surrogate loss: -0.0008
             Mean action noise std: 0.74
                       Mean reward: 375.22
               Mean episode length: 247.52
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 11.40s
                        Total time: 17883.12s
                               ETA: 1195366.2s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.348s, learning 0.219s)
               Value function loss: 5.9287
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 373.89
               Mean episode length: 246.02
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 11.57s
                        Total time: 17894.68s
                               ETA: 1195316.3s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.080s, learning 0.171s)
               Value function loss: 3.8763
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 373.50
               Mean episode length: 245.65
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 11.25s
                        Total time: 17905.93s
                               ETA: 1195245.3s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.161s)
               Value function loss: 6.9119
                    Surrogate loss: -0.0108
             Mean action noise std: 0.74
                       Mean reward: 375.21
               Mean episode length: 246.26
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 11.40s
                        Total time: 17917.34s
                               ETA: 1195184.6s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.439s, learning 0.220s)
               Value function loss: 5.2951
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 378.01
               Mean episode length: 247.29
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 11.66s
                        Total time: 17929.00s
                               ETA: 1195141.0s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.101s, learning 0.173s)
               Value function loss: 4.9323
                    Surrogate loss: -0.0078
             Mean action noise std: 0.74
                       Mean reward: 378.18
               Mean episode length: 247.29
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 11.27s
                        Total time: 17940.27s
                               ETA: 1195071.8s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.128s, learning 0.164s)
               Value function loss: 6.0459
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 380.72
               Mean episode length: 248.79
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 11.29s
                        Total time: 17951.56s
                               ETA: 1195003.9s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.156s, learning 0.238s)
               Value function loss: 6.4623
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 382.96
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 11.39s
                        Total time: 17962.96s
                               ETA: 1194942.8s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.087s, learning 0.192s)
               Value function loss: 6.8016
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 380.77
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 11.28s
                        Total time: 17974.23s
                               ETA: 1194874.2s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.385s, learning 0.163s)
               Value function loss: 6.4247
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 379.39
               Mean episode length: 249.24
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 11.55s
                        Total time: 17985.78s
                               ETA: 1194823.6s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.312s, learning 0.159s)
               Value function loss: 6.6917
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 379.21
               Mean episode length: 249.24
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 11.47s
                        Total time: 17997.25s
                               ETA: 1194767.8s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.166s)
               Value function loss: 6.6792
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 379.79
               Mean episode length: 249.01
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 11.38s
                        Total time: 18008.63s
                               ETA: 1194705.9s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.557s, learning 0.212s)
               Value function loss: 13.7786
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 380.31
               Mean episode length: 249.77
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 11.77s
                        Total time: 18020.40s
                               ETA: 1194670.0s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.115s, learning 0.162s)
               Value function loss: 14.4904
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 378.36
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 11.28s
                        Total time: 18031.68s
                               ETA: 1194601.6s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.778s, learning 0.168s)
               Value function loss: 25.3578
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 378.19
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 11.95s
                        Total time: 18043.62s
                               ETA: 1194577.5s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.159s)
               Value function loss: 21.4210
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 378.59
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 11.37s
                        Total time: 18054.99s
                               ETA: 1194515.4s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.051s, learning 0.190s)
               Value function loss: 29.1621
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 377.28
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 11.24s
                        Total time: 18066.23s
                               ETA: 1194444.8s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.526s, learning 0.175s)
               Value function loss: 41.2147
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 375.90
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 11.70s
                        Total time: 18077.94s
                               ETA: 1194404.7s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.379s, learning 0.162s)
               Value function loss: 41.8653
                    Surrogate loss: 0.0002
             Mean action noise std: 0.74
                       Mean reward: 374.99
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 11.54s
                        Total time: 18089.48s
                               ETA: 1194354.0s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.315s, learning 0.201s)
               Value function loss: 35.3592
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 375.90
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 11.52s
                        Total time: 18100.99s
                               ETA: 1194301.8s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.365s, learning 0.168s)
               Value function loss: 19.2458
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 374.44
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 11.53s
                        Total time: 18112.53s
                               ETA: 1194250.7s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.894s, learning 0.159s)
               Value function loss: 13.2343
                    Surrogate loss: 0.0003
             Mean action noise std: 0.74
                       Mean reward: 375.20
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 11.05s
                        Total time: 18123.58s
                               ETA: 1194168.0s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.250s, learning 0.184s)
               Value function loss: 8.5013
                    Surrogate loss: -0.0152
             Mean action noise std: 0.74
                       Mean reward: 371.70
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 11.43s
                        Total time: 18135.01s
                               ETA: 1194110.5s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.013s, learning 0.173s)
               Value function loss: 4.4703
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 370.34
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 11.19s
                        Total time: 18146.20s
                               ETA: 1194036.8s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.032s, learning 0.205s)
               Value function loss: 2.2337
                    Surrogate loss: 0.0009
             Mean action noise std: 0.74
                       Mean reward: 369.95
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 11.24s
                        Total time: 18157.43s
                               ETA: 1193966.4s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.053s, learning 0.200s)
               Value function loss: 1.8528
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: 370.45
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 11.25s
                        Total time: 18168.69s
                               ETA: 1193897.3s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.616s, learning 0.176s)
               Value function loss: 1.5895
                    Surrogate loss: -0.0007
             Mean action noise std: 0.74
                       Mean reward: 370.71
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 11.79s
                        Total time: 18180.48s
                               ETA: 1193863.6s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.335s, learning 0.264s)
               Value function loss: 1.2726
                    Surrogate loss: 0.0025
             Mean action noise std: 0.74
                       Mean reward: 370.38
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 11.60s
                        Total time: 18192.08s
                               ETA: 1193817.3s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.520s, learning 0.216s)
               Value function loss: 1.6926
                    Surrogate loss: 0.0094
             Mean action noise std: 0.74
                       Mean reward: 370.10
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 11.74s
                        Total time: 18203.81s
                               ETA: 1193780.0s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.056s, learning 0.164s)
               Value function loss: 1.2004
                    Surrogate loss: 0.0047
             Mean action noise std: 0.74
                       Mean reward: 369.96
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 11.22s
                        Total time: 18215.03s
                               ETA: 1193708.9s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.942s, learning 0.170s)
               Value function loss: 1.2838
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 369.51
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 11.11s
                        Total time: 18226.15s
                               ETA: 1193630.8s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.065s, learning 0.166s)
               Value function loss: 1.4026
                    Surrogate loss: -0.0017
             Mean action noise std: 0.74
                       Mean reward: 368.86
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 11.23s
                        Total time: 18237.38s
                               ETA: 1193560.6s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.335s, learning 0.230s)
               Value function loss: 2.7227
                    Surrogate loss: 0.0017
             Mean action noise std: 0.74
                       Mean reward: 368.13
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 11.57s
                        Total time: 18248.94s
                               ETA: 1193512.4s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.178s, learning 0.209s)
               Value function loss: 2.2942
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 366.26
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 11.39s
                        Total time: 18260.33s
                               ETA: 1193452.5s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.497s, learning 0.192s)
               Value function loss: 3.0507
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 364.72
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 11.69s
                        Total time: 18272.02s
                               ETA: 1193412.4s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.035s, learning 0.164s)
               Value function loss: 3.4657
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 362.38
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 11.20s
                        Total time: 18283.22s
                               ETA: 1193340.3s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.223s, learning 0.231s)
               Value function loss: 2.9523
                    Surrogate loss: -0.0030
             Mean action noise std: 0.74
                       Mean reward: 362.81
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 11.45s
                        Total time: 18294.67s
                               ETA: 1193285.0s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.006s, learning 0.160s)
               Value function loss: 3.4687
                    Surrogate loss: -0.0119
             Mean action noise std: 0.74
                       Mean reward: 364.61
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 11.17s
                        Total time: 18305.84s
                               ETA: 1193211.1s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.489s, learning 0.200s)
               Value function loss: 3.6763
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 367.15
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 11.69s
                        Total time: 18317.53s
                               ETA: 1193171.2s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.134s, learning 0.195s)
               Value function loss: 5.1613
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 369.41
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 11.33s
                        Total time: 18328.86s
                               ETA: 1193107.9s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.439s, learning 0.288s)
               Value function loss: 5.5759
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 371.27
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 11.73s
                        Total time: 18340.58s
                               ETA: 1193070.6s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.972s, learning 0.196s)
               Value function loss: 6.0781
                    Surrogate loss: -0.0118
             Mean action noise std: 0.74
                       Mean reward: 369.81
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 11.17s
                        Total time: 18351.75s
                               ETA: 1192997.0s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.571s, learning 0.234s)
               Value function loss: 4.3000
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 368.49
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 11.81s
                        Total time: 18363.56s
                               ETA: 1192964.9s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.424s, learning 0.199s)
               Value function loss: 7.2610
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 365.46
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 11.62s
                        Total time: 18375.18s
                               ETA: 1192920.9s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.268s, learning 0.157s)
               Value function loss: 10.3491
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 368.55
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 11.42s
                        Total time: 18386.60s
                               ETA: 1192864.1s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.430s, learning 0.162s)
               Value function loss: 18.6171
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 371.01
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 11.59s
                        Total time: 18398.19s
                               ETA: 1192818.3s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.402s, learning 0.289s)
               Value function loss: 17.9923
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 372.15
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 11.69s
                        Total time: 18409.89s
                               ETA: 1192778.9s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.978s, learning 0.162s)
               Value function loss: 21.4142
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 371.28
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 11.14s
                        Total time: 18421.03s
                               ETA: 1192703.9s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.605s, learning 0.226s)
               Value function loss: 33.6388
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 370.08
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 11.83s
                        Total time: 18432.86s
                               ETA: 1192673.7s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.353s, learning 0.168s)
               Value function loss: 51.2209
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 371.25
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 11.52s
                        Total time: 18444.38s
                               ETA: 1192623.4s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.382s, learning 0.158s)
               Value function loss: 50.5297
                    Surrogate loss: 0.0012
             Mean action noise std: 0.74
                       Mean reward: 373.08
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 11.54s
                        Total time: 18455.92s
                               ETA: 1192574.5s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.450s, learning 0.169s)
               Value function loss: 26.8108
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 368.82
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 11.62s
                        Total time: 18467.54s
                               ETA: 1192530.7s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.061s, learning 0.167s)
               Value function loss: 15.2799
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 372.02
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 11.23s
                        Total time: 18478.77s
                               ETA: 1192461.7s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.936s, learning 0.272s)
               Value function loss: 10.8775
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 370.60
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 11.21s
                        Total time: 18489.97s
                               ETA: 1192391.4s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.527s, learning 0.188s)
               Value function loss: 6.1322
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 370.66
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 11.72s
                        Total time: 18501.69s
                               ETA: 1192354.0s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.291s, learning 0.175s)
               Value function loss: 2.2784
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 369.64
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 11.47s
                        Total time: 18513.16s
                               ETA: 1192300.5s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.655s, learning 0.318s)
               Value function loss: 1.4613
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 369.60
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 11.97s
                        Total time: 18525.13s
                               ETA: 1192279.7s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.199s, learning 0.158s)
               Value function loss: 1.8566
                    Surrogate loss: -0.0069
             Mean action noise std: 0.74
                       Mean reward: 369.00
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 11.36s
                        Total time: 18536.49s
                               ETA: 1192219.3s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.324s, learning 0.157s)
               Value function loss: 1.2970
                    Surrogate loss: 0.0158
             Mean action noise std: 0.74
                       Mean reward: 369.08
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 11.48s
                        Total time: 18547.97s
                               ETA: 1192167.0s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.048s, learning 0.160s)
               Value function loss: 1.2179
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 369.08
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 11.21s
                        Total time: 18559.18s
                               ETA: 1192097.1s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.418s, learning 0.168s)
               Value function loss: 1.2608
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 368.88
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 11.59s
                        Total time: 18570.76s
                               ETA: 1192051.6s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.301s, learning 0.161s)
               Value function loss: 0.6589
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 366.19
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 11.46s
                        Total time: 18582.22s
                               ETA: 1191998.2s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.195s, learning 0.160s)
               Value function loss: 1.8623
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 366.85
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 11.35s
                        Total time: 18593.58s
                               ETA: 1191938.0s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.072s, learning 0.168s)
               Value function loss: 2.8558
                    Surrogate loss: -0.0124
             Mean action noise std: 0.74
                       Mean reward: 362.45
               Mean episode length: 249.97
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 11.24s
                        Total time: 18604.82s
                               ETA: 1191870.4s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.017s, learning 0.164s)
               Value function loss: 2.7251
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 360.42
               Mean episode length: 249.97
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 11.18s
                        Total time: 18616.00s
                               ETA: 1191799.1s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.370s, learning 0.170s)
               Value function loss: 2.3974
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 359.17
               Mean episode length: 249.97
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 11.54s
                        Total time: 18627.54s
                               ETA: 1191750.9s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.313s, learning 0.165s)
               Value function loss: 5.5071
                    Surrogate loss: -0.0045
             Mean action noise std: 0.74
                       Mean reward: 356.70
               Mean episode length: 249.97
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 11.48s
                        Total time: 18639.02s
                               ETA: 1191698.8s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.336s, learning 0.167s)
               Value function loss: 3.5418
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 352.94
               Mean episode length: 249.97
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 11.50s
                        Total time: 18650.52s
                               ETA: 1191648.3s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.688s, learning 0.174s)
               Value function loss: 4.3361
                    Surrogate loss: -0.0034
             Mean action noise std: 0.74
                       Mean reward: 352.56
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 11.86s
                        Total time: 18662.38s
                               ETA: 1191620.8s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.436s, learning 0.275s)
               Value function loss: 4.4139
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 348.15
               Mean episode length: 249.95
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 11.71s
                        Total time: 18674.09s
                               ETA: 1191583.8s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.935s, learning 0.200s)
               Value function loss: 4.1592
                    Surrogate loss: 0.0021
             Mean action noise std: 0.74
                       Mean reward: 343.53
               Mean episode length: 249.95
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 11.13s
                        Total time: 18685.23s
                               ETA: 1191510.0s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.311s, learning 0.183s)
               Value function loss: 4.6602
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: 342.85
               Mean episode length: 249.87
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 11.49s
                        Total time: 18696.72s
                               ETA: 1191459.2s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.625s, learning 0.276s)
               Value function loss: 5.0717
                    Surrogate loss: -0.0047
             Mean action noise std: 0.74
                       Mean reward: 340.98
               Mean episode length: 249.36
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 11.90s
                        Total time: 18708.62s
                               ETA: 1191434.3s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.072s, learning 0.201s)
               Value function loss: 5.3036
                    Surrogate loss: 0.0072
             Mean action noise std: 0.74
                       Mean reward: 340.07
               Mean episode length: 248.67
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 11.27s
                        Total time: 18719.90s
                               ETA: 1191369.5s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.563s, learning 0.198s)
               Value function loss: 5.9449
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 335.25
               Mean episode length: 248.16
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 11.76s
                        Total time: 18731.66s
                               ETA: 1191335.8s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.548s, learning 0.228s)
               Value function loss: 5.2227
                    Surrogate loss: 0.0013
             Mean action noise std: 0.74
                       Mean reward: 332.84
               Mean episode length: 249.18
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 11.78s
                        Total time: 18743.43s
                               ETA: 1191303.1s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.214s, learning 0.169s)
               Value function loss: 11.4704
                    Surrogate loss: 0.0057
             Mean action noise std: 0.74
                       Mean reward: 330.25
               Mean episode length: 249.10
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 11.38s
                        Total time: 18754.82s
                               ETA: 1191245.4s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.582s, learning 0.198s)
               Value function loss: 12.9651
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 340.10
               Mean episode length: 249.68
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 11.78s
                        Total time: 18766.60s
                               ETA: 1191213.0s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.397s, learning 0.161s)
               Value function loss: 11.9872
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 327.06
               Mean episode length: 248.98
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 11.56s
                        Total time: 18778.15s
                               ETA: 1191166.5s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.043s, learning 0.185s)
               Value function loss: 20.6611
                    Surrogate loss: -0.0076
             Mean action noise std: 0.74
                       Mean reward: 330.41
               Mean episode length: 249.57
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 11.23s
                        Total time: 18789.38s
                               ETA: 1191099.2s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.480s, learning 0.213s)
               Value function loss: 27.6511
                    Surrogate loss: -0.0019
             Mean action noise std: 0.74
                       Mean reward: 336.44
               Mean episode length: 249.96
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 11.69s
                        Total time: 18801.07s
                               ETA: 1191061.4s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.267s, learning 0.166s)
               Value function loss: 29.1678
                    Surrogate loss: -0.0036
             Mean action noise std: 0.74
                       Mean reward: 337.35
               Mean episode length: 249.98
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 11.43s
                        Total time: 18812.51s
                               ETA: 1191007.1s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.322s, learning 0.163s)
               Value function loss: 22.0515
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 338.77
               Mean episode length: 250.00
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 11.48s
                        Total time: 18823.99s
                               ETA: 1190956.2s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.140s, learning 0.160s)
               Value function loss: 13.2679
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 334.25
               Mean episode length: 249.85
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 11.30s
                        Total time: 18835.29s
                               ETA: 1190893.6s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.480s, learning 0.161s)
               Value function loss: 12.4164
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 332.37
               Mean episode length: 249.78
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 11.64s
                        Total time: 18846.93s
                               ETA: 1190852.6s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.147s, learning 0.166s)
               Value function loss: 5.3398
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 328.50
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 11.31s
                        Total time: 18858.24s
                               ETA: 1190791.0s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.304s, learning 0.230s)
               Value function loss: 2.0473
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 324.72
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 11.53s
                        Total time: 18869.78s
                               ETA: 1190743.4s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.711s, learning 0.208s)
               Value function loss: 1.3082
                    Surrogate loss: -0.0178
             Mean action noise std: 0.74
                       Mean reward: 324.47
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 11.92s
                        Total time: 18881.70s
                               ETA: 1190720.1s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.085s, learning 0.162s)
               Value function loss: 1.8739
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 326.16
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 11.25s
                        Total time: 18892.94s
                               ETA: 1190654.5s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.147s, learning 0.162s)
               Value function loss: 1.0226
                    Surrogate loss: -0.0194
             Mean action noise std: 0.74
                       Mean reward: 328.01
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 11.31s
                        Total time: 18904.25s
                               ETA: 1190592.9s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.162s, learning 0.197s)
               Value function loss: 0.8579
                    Surrogate loss: -0.0151
             Mean action noise std: 0.74
                       Mean reward: 328.66
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 11.36s
                        Total time: 18915.61s
                               ETA: 1190534.4s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.615s, learning 0.162s)
               Value function loss: 1.2650
                    Surrogate loss: -0.0153
             Mean action noise std: 0.74
                       Mean reward: 331.37
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 11.78s
                        Total time: 18927.39s
                               ETA: 1190502.4s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.473s, learning 0.165s)
               Value function loss: 0.9427
                    Surrogate loss: -0.0138
             Mean action noise std: 0.74
                       Mean reward: 330.92
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 11.64s
                        Total time: 18939.03s
                               ETA: 1190461.6s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.435s, learning 0.230s)
               Value function loss: 1.1860
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 332.19
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 11.66s
                        Total time: 18950.69s
                               ETA: 1190422.6s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.711s, learning 0.202s)
               Value function loss: 2.7827
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 335.09
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 11.91s
                        Total time: 18962.60s
                               ETA: 1190399.1s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.086s, learning 0.178s)
               Value function loss: 2.6163
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 337.81
               Mean episode length: 249.89
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 11.26s
                        Total time: 18973.87s
                               ETA: 1190335.0s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.326s, learning 0.158s)
               Value function loss: 2.3015
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 335.87
               Mean episode length: 249.63
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 11.48s
                        Total time: 18985.35s
                               ETA: 1190284.7s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.440s, learning 0.160s)
               Value function loss: 4.8977
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 333.31
               Mean episode length: 249.63
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 11.60s
                        Total time: 18996.95s
                               ETA: 1190241.8s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.117s, learning 0.287s)
               Value function loss: 3.5819
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 329.75
               Mean episode length: 249.63
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 11.40s
                        Total time: 19008.35s
                               ETA: 1190186.6s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.144s, learning 0.166s)
               Value function loss: 3.7124
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 332.88
               Mean episode length: 249.74
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 11.31s
                        Total time: 19019.66s
                               ETA: 1190125.5s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.548s, learning 0.186s)
               Value function loss: 3.5804
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 331.36
               Mean episode length: 249.74
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 11.73s
                        Total time: 19031.40s
                               ETA: 1190091.1s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.509s, learning 0.193s)
               Value function loss: 5.7850
                    Surrogate loss: 0.0000
             Mean action noise std: 0.74
                       Mean reward: 342.24
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 11.70s
                        Total time: 19043.10s
                               ETA: 1190054.7s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.399s, learning 0.188s)
               Value function loss: 6.7391
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 349.37
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 11.59s
                        Total time: 19054.69s
                               ETA: 1190011.1s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.564s, learning 0.174s)
               Value function loss: 7.0191
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 355.64
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 11.74s
                        Total time: 19066.42s
                               ETA: 1189977.0s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.343s, learning 0.212s)
               Value function loss: 6.5386
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 362.08
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 11.55s
                        Total time: 19077.98s
                               ETA: 1189931.6s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.435s, learning 0.172s)
               Value function loss: 8.4750
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 365.87
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 11.61s
                        Total time: 19089.59s
                               ETA: 1189889.4s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.142s, learning 0.188s)
               Value function loss: 12.8340
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 365.59
               Mean episode length: 249.92
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 11.33s
                        Total time: 19100.92s
                               ETA: 1189830.0s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.891s, learning 0.164s)
               Value function loss: 16.6341
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 366.21
               Mean episode length: 249.95
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 11.05s
                        Total time: 19111.97s
                               ETA: 1189753.5s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.658s, learning 0.184s)
               Value function loss: 23.3362
                    Surrogate loss: -0.0023
             Mean action noise std: 0.74
                       Mean reward: 368.67
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 11.84s
                        Total time: 19123.81s
                               ETA: 1189726.0s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.491s, learning 0.227s)
               Value function loss: 23.0060
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: 365.67
               Mean episode length: 249.72
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 11.72s
                        Total time: 19135.53s
                               ETA: 1189691.0s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.208s, learning 0.204s)
               Value function loss: 31.0953
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 368.52
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 11.41s
                        Total time: 19146.94s
                               ETA: 1189636.9s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.523s, learning 0.176s)
               Value function loss: 39.7887
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 368.01
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 11.70s
                        Total time: 19158.64s
                               ETA: 1189600.6s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.464s, learning 0.184s)
               Value function loss: 47.3260
                    Surrogate loss: 0.0041
             Mean action noise std: 0.74
                       Mean reward: 362.26
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 11.65s
                        Total time: 19170.29s
                               ETA: 1189561.3s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.240s, learning 0.160s)
               Value function loss: 32.5114
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 365.41
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 11.40s
                        Total time: 19181.69s
                               ETA: 1189506.6s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.438s, learning 0.165s)
               Value function loss: 18.4940
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 366.31
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 11.60s
                        Total time: 19193.29s
                               ETA: 1189464.5s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.198s, learning 0.185s)
               Value function loss: 13.6308
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 368.38
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 11.38s
                        Total time: 19204.68s
                               ETA: 1189408.9s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.331s, learning 0.160s)
               Value function loss: 7.4109
                    Surrogate loss: -0.0184
             Mean action noise std: 0.74
                       Mean reward: 369.83
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 11.49s
                        Total time: 19216.17s
                               ETA: 1189360.0s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.278s, learning 0.176s)
               Value function loss: 3.5004
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 368.22
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 11.45s
                        Total time: 19227.62s
                               ETA: 1189308.8s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.352s, learning 0.247s)
               Value function loss: 1.7868
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 368.82
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 11.60s
                        Total time: 19239.22s
                               ETA: 1189266.7s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.211s, learning 0.229s)
               Value function loss: 1.7017
                    Surrogate loss: -0.0141
             Mean action noise std: 0.74
                       Mean reward: 368.95
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 11.44s
                        Total time: 19250.66s
                               ETA: 1189214.8s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.398s, learning 0.171s)
               Value function loss: 1.2419
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 368.79
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 11.57s
                        Total time: 19262.23s
                               ETA: 1189170.9s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.174s, learning 0.217s)
               Value function loss: 0.8071
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 367.64
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 11.39s
                        Total time: 19273.62s
                               ETA: 1189116.0s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.172s, learning 0.195s)
               Value function loss: 1.2255
                    Surrogate loss: 0.0145
             Mean action noise std: 0.74
                       Mean reward: 367.27
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 11.37s
                        Total time: 19284.99s
                               ETA: 1189059.7s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.258s, learning 0.177s)
               Value function loss: 1.4682
                    Surrogate loss: -0.0017
             Mean action noise std: 0.74
                       Mean reward: 366.95
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 11.43s
                        Total time: 19296.42s
                               ETA: 1189007.7s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.470s, learning 0.203s)
               Value function loss: 1.2210
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 367.32
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 11.67s
                        Total time: 19308.10s
                               ETA: 1188970.3s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.369s, learning 0.179s)
               Value function loss: 1.9161
                    Surrogate loss: 0.0110
             Mean action noise std: 0.74
                       Mean reward: 367.83
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 11.55s
                        Total time: 19319.64s
                               ETA: 1188925.3s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.085s, learning 0.158s)
               Value function loss: 3.5008
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 369.88
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 11.24s
                        Total time: 19330.89s
                               ETA: 1188861.6s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.220s, learning 0.163s)
               Value function loss: 2.2374
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 369.68
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 11.38s
                        Total time: 19342.27s
                               ETA: 1188806.5s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.344s, learning 0.169s)
               Value function loss: 5.0190
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 368.95
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 11.51s
                        Total time: 19353.78s
                               ETA: 1188759.5s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.321s, learning 0.228s)
               Value function loss: 3.3918
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 371.50
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 11.55s
                        Total time: 19365.33s
                               ETA: 1188714.8s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.460s, learning 0.179s)
               Value function loss: 3.2101
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 371.05
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 11.64s
                        Total time: 19376.97s
                               ETA: 1188675.6s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1468 steps/s (collection: 11.000s, learning 0.158s)
               Value function loss: 3.3680
                    Surrogate loss: -0.0082
             Mean action noise std: 0.74
                       Mean reward: 370.54
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 11.16s
                        Total time: 19388.13s
                               ETA: 1188607.0s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.191s, learning 0.255s)
               Value function loss: 4.0556
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 372.69
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 11.45s
                        Total time: 19399.57s
                               ETA: 1188556.0s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.572s, learning 0.180s)
               Value function loss: 4.7737
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 374.29
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 11.75s
                        Total time: 19411.33s
                               ETA: 1188523.9s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.028s, learning 0.198s)
               Value function loss: 4.6266
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 377.14
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 11.23s
                        Total time: 19422.55s
                               ETA: 1188459.6s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.317s, learning 0.168s)
               Value function loss: 7.4165
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 378.05
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 11.48s
                        Total time: 19434.04s
                               ETA: 1188411.3s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.235s, learning 0.173s)
               Value function loss: 6.0914
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 379.45
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 11.41s
                        Total time: 19445.44s
                               ETA: 1188358.2s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.930s, learning 0.160s)
               Value function loss: 13.6076
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 381.75
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 11.09s
                        Total time: 19456.53s
                               ETA: 1188285.8s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.060s, learning 0.166s)
               Value function loss: 14.2225
                    Surrogate loss: -0.0062
             Mean action noise std: 0.74
                       Mean reward: 382.14
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 11.23s
                        Total time: 19467.76s
                               ETA: 1188221.8s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.424s, learning 0.259s)
               Value function loss: 23.6753
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 383.61
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 11.68s
                        Total time: 19479.44s
                               ETA: 1188185.7s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.255s, learning 0.202s)
               Value function loss: 22.6132
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 381.29
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 11.46s
                        Total time: 19490.90s
                               ETA: 1188135.8s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.363s, learning 0.192s)
               Value function loss: 28.7762
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 381.72
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 11.56s
                        Total time: 19502.46s
                               ETA: 1188092.0s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.551s, learning 0.182s)
               Value function loss: 43.9295
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 382.89
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 11.73s
                        Total time: 19514.19s
                               ETA: 1188059.1s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.129s, learning 0.193s)
               Value function loss: 56.6826
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 383.41
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 11.32s
                        Total time: 19525.51s
                               ETA: 1188001.1s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.189s, learning 0.160s)
               Value function loss: 58.3804
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 383.58
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 11.35s
                        Total time: 19536.86s
                               ETA: 1187944.9s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.043s, learning 0.160s)
               Value function loss: 31.0133
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 380.93
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 11.20s
                        Total time: 19548.06s
                               ETA: 1187879.8s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.233s, learning 0.185s)
               Value function loss: 21.1332
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 380.66
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 11.42s
                        Total time: 19559.48s
                               ETA: 1187828.0s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.405s, learning 0.250s)
               Value function loss: 13.6814
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 381.85
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 11.65s
                        Total time: 19571.14s
                               ETA: 1187790.4s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.355s, learning 0.173s)
               Value function loss: 6.4353
                    Surrogate loss: -0.0172
             Mean action noise std: 0.74
                       Mean reward: 381.47
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 11.53s
                        Total time: 19582.66s
                               ETA: 1187745.3s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.614s, learning 0.206s)
               Value function loss: 2.9064
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 379.83
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 11.82s
                        Total time: 19594.48s
                               ETA: 1187717.9s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.390s, learning 0.163s)
               Value function loss: 2.4321
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 379.47
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 11.55s
                        Total time: 19606.04s
                               ETA: 1187674.3s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.387s, learning 0.227s)
               Value function loss: 2.0852
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 380.78
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 11.61s
                        Total time: 19617.65s
                               ETA: 1187634.5s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.570s, learning 0.180s)
               Value function loss: 1.2078
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 380.10
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 11.75s
                        Total time: 19629.40s
                               ETA: 1187602.9s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.990s, learning 0.190s)
               Value function loss: 2.5211
                    Surrogate loss: -0.0049
             Mean action noise std: 0.74
                       Mean reward: 379.18
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 11.18s
                        Total time: 19640.58s
                               ETA: 1187536.9s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.902s, learning 0.171s)
               Value function loss: 1.3273
                    Surrogate loss: 0.0114
             Mean action noise std: 0.74
                       Mean reward: 379.40
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 11.07s
                        Total time: 19651.65s
                               ETA: 1187464.5s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.006s, learning 0.176s)
               Value function loss: 1.4605
                    Surrogate loss: 0.0032
             Mean action noise std: 0.74
                       Mean reward: 379.77
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 11.18s
                        Total time: 19662.84s
                               ETA: 1187398.7s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.288s, learning 0.175s)
               Value function loss: 2.2075
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 379.81
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 11.46s
                        Total time: 19674.30s
                               ETA: 1187350.0s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.464s, learning 0.192s)
               Value function loss: 4.9533
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 378.19
               Mean episode length: 248.42
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 11.66s
                        Total time: 19685.96s
                               ETA: 1187313.0s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.209s, learning 0.181s)
               Value function loss: 4.3993
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 379.11
               Mean episode length: 248.42
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 11.39s
                        Total time: 19697.35s
                               ETA: 1187259.9s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.408s, learning 0.299s)
               Value function loss: 5.4915
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 379.62
               Mean episode length: 248.42
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 11.71s
                        Total time: 19709.05s
                               ETA: 1187226.1s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.781s, learning 0.270s)
               Value function loss: 5.6883
                    Surrogate loss: 0.0047
             Mean action noise std: 0.74
                       Mean reward: 379.61
               Mean episode length: 247.40
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 12.05s
                        Total time: 19721.11s
                               ETA: 1187212.9s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.588s, learning 0.190s)
               Value function loss: 5.3872
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 379.11
               Mean episode length: 247.40
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 11.78s
                        Total time: 19732.88s
                               ETA: 1187183.4s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.185s, learning 0.175s)
               Value function loss: 5.3186
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 380.73
               Mean episode length: 248.98
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 11.36s
                        Total time: 19744.24s
                               ETA: 1187128.7s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.146s, learning 0.181s)
               Value function loss: 5.9352
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 381.06
               Mean episode length: 248.98
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 11.33s
                        Total time: 19755.57s
                               ETA: 1187072.1s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.305s, learning 0.195s)
               Value function loss: 7.9678
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 383.00
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 11.50s
                        Total time: 19767.07s
                               ETA: 1187025.9s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.366s, learning 0.167s)
               Value function loss: 8.3414
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 382.80
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 11.53s
                        Total time: 19778.60s
                               ETA: 1186981.8s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.496s, learning 0.173s)
               Value function loss: 9.0786
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 383.55
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 11.67s
                        Total time: 19790.27s
                               ETA: 1186945.8s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.552s, learning 0.164s)
               Value function loss: 6.0984
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 383.03
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 11.72s
                        Total time: 19801.99s
                               ETA: 1186912.7s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.068s, learning 0.164s)
               Value function loss: 12.7034
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 383.61
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 11.23s
                        Total time: 19813.22s
                               ETA: 1186850.6s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.173s, learning 0.166s)
               Value function loss: 15.2007
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 383.62
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 11.34s
                        Total time: 19824.56s
                               ETA: 1186795.0s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.422s, learning 0.159s)
               Value function loss: 24.1865
                    Surrogate loss: 0.0010
             Mean action noise std: 0.74
                       Mean reward: 384.45
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 11.58s
                        Total time: 19836.14s
                               ETA: 1186754.0s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.565s, learning 0.215s)
               Value function loss: 22.4731
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 384.67
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 11.78s
                        Total time: 19847.92s
                               ETA: 1186724.8s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.587s, learning 0.168s)
               Value function loss: 27.6873
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 384.96
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 11.76s
                        Total time: 19859.68s
                               ETA: 1186694.2s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.286s, learning 0.159s)
               Value function loss: 44.3999
                    Surrogate loss: -0.0038
             Mean action noise std: 0.74
                       Mean reward: 385.42
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 11.45s
                        Total time: 19871.12s
                               ETA: 1186645.1s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1499 steps/s (collection: 10.718s, learning 0.211s)
               Value function loss: 56.0450
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 385.76
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 10.93s
                        Total time: 19882.05s
                               ETA: 1186565.2s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.566s, learning 0.264s)
               Value function loss: 50.8720
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 385.46
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 11.83s
                        Total time: 19893.88s
                               ETA: 1186539.2s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.039s, learning 0.163s)
               Value function loss: 23.5312
                    Surrogate loss: -0.0140
             Mean action noise std: 0.74
                       Mean reward: 386.50
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 11.20s
                        Total time: 19905.09s
                               ETA: 1186475.8s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.609s, learning 0.191s)
               Value function loss: 15.8498
                    Surrogate loss: -0.0126
             Mean action noise std: 0.74
                       Mean reward: 387.33
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 11.80s
                        Total time: 19916.89s
                               ETA: 1186448.0s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.404s, learning 0.165s)
               Value function loss: 12.0572
                    Surrogate loss: -0.0147
             Mean action noise std: 0.74
                       Mean reward: 387.26
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 11.57s
                        Total time: 19928.45s
                               ETA: 1186406.5s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.276s, learning 0.167s)
               Value function loss: 5.8642
                    Surrogate loss: -0.0170
             Mean action noise std: 0.74
                       Mean reward: 388.21
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 11.44s
                        Total time: 19939.90s
                               ETA: 1186357.5s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.161s)
               Value function loss: 3.0635
                    Surrogate loss: -0.0137
             Mean action noise std: 0.74
                       Mean reward: 388.09
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 11.57s
                        Total time: 19951.47s
                               ETA: 1186316.3s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.159s, learning 0.169s)
               Value function loss: 2.9752
                    Surrogate loss: 0.0016
             Mean action noise std: 0.74
                       Mean reward: 388.13
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 11.33s
                        Total time: 19962.80s
                               ETA: 1186260.6s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.498s, learning 0.206s)
               Value function loss: 2.5982
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 388.21
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 11.70s
                        Total time: 19974.50s
                               ETA: 1186227.3s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.586s, learning 0.163s)
               Value function loss: 2.2607
                    Surrogate loss: 0.0374
             Mean action noise std: 0.74
                       Mean reward: 388.30
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 11.75s
                        Total time: 19986.25s
                               ETA: 1186196.7s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.167s)
               Value function loss: 1.7638
                    Surrogate loss: -0.0045
             Mean action noise std: 0.74
                       Mean reward: 388.06
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 11.38s
                        Total time: 19997.63s
                               ETA: 1186144.4s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.309s, learning 0.194s)
               Value function loss: 1.8137
                    Surrogate loss: 0.0115
             Mean action noise std: 0.74
                       Mean reward: 387.87
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 11.50s
                        Total time: 20009.14s
                               ETA: 1186099.2s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.058s, learning 0.169s)
               Value function loss: 1.0518
                    Surrogate loss: 0.0032
             Mean action noise std: 0.74
                       Mean reward: 387.84
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 11.23s
                        Total time: 20020.36s
                               ETA: 1186037.7s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.374s, learning 0.256s)
               Value function loss: 2.2231
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 387.96
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 11.63s
                        Total time: 20031.99s
                               ETA: 1186000.1s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.128s, learning 0.210s)
               Value function loss: 4.6791
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 388.22
               Mean episode length: 250.00
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 11.34s
                        Total time: 20043.33s
                               ETA: 1185945.3s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.055s, learning 0.183s)
               Value function loss: 3.7412
                    Surrogate loss: -0.0064
             Mean action noise std: 0.74
                       Mean reward: 388.38
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 11.24s
                        Total time: 20054.57s
                               ETA: 1185884.6s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.364s, learning 0.172s)
               Value function loss: 3.1551
                    Surrogate loss: -0.0072
             Mean action noise std: 0.74
                       Mean reward: 388.80
               Mean episode length: 250.00
                  Mean reward/step: 1.63
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 11.54s
                        Total time: 20066.10s
                               ETA: 1185841.7s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.637s, learning 0.193s)
               Value function loss: 6.4864
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 388.98
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 11.83s
                        Total time: 20077.93s
                               ETA: 1185816.1s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.925s, learning 0.182s)
               Value function loss: 5.6724
                    Surrogate loss: -0.0103
             Mean action noise std: 0.74
                       Mean reward: 388.71
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 11.11s
                        Total time: 20089.04s
                               ETA: 1185747.9s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.364s, learning 0.190s)
               Value function loss: 4.9761
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 388.67
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 11.55s
                        Total time: 20100.60s
                               ETA: 1185706.1s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.661s, learning 0.178s)
               Value function loss: 6.7457
                    Surrogate loss: -0.0013
             Mean action noise std: 0.74
                       Mean reward: 388.20
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 11.84s
                        Total time: 20112.44s
                               ETA: 1185681.1s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.701s, learning 0.167s)
               Value function loss: 5.6998
                    Surrogate loss: -0.0097
             Mean action noise std: 0.74
                       Mean reward: 388.05
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 11.87s
                        Total time: 20124.30s
                               ETA: 1185657.9s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.610s, learning 0.207s)
               Value function loss: 7.3253
                    Surrogate loss: -0.0122
             Mean action noise std: 0.74
                       Mean reward: 387.11
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 11.82s
                        Total time: 20136.12s
                               ETA: 1185631.7s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.727s, learning 0.157s)
               Value function loss: 6.9631
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 386.77
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 11.88s
                        Total time: 20148.01s
                               ETA: 1185609.5s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.062s, learning 0.186s)
               Value function loss: 6.3047
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 385.69
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 11.25s
                        Total time: 20159.25s
                               ETA: 1185549.8s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.677s, learning 0.301s)
               Value function loss: 8.7641
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 384.44
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 11.98s
                        Total time: 20171.23s
                               ETA: 1185533.1s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.165s, learning 0.188s)
               Value function loss: 9.7422
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 384.14
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 11.35s
                        Total time: 20182.58s
                               ETA: 1185479.7s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.501s, learning 0.170s)
               Value function loss: 15.3419
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 382.81
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 11.67s
                        Total time: 20194.26s
                               ETA: 1185445.0s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.159s, learning 0.158s)
               Value function loss: 19.8038
                    Surrogate loss: 0.0004
             Mean action noise std: 0.74
                       Mean reward: 382.88
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 11.32s
                        Total time: 20205.57s
                               ETA: 1185389.6s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.909s, learning 0.162s)
               Value function loss: 19.2615
                    Surrogate loss: -0.0121
             Mean action noise std: 0.74
                       Mean reward: 381.65
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 11.07s
                        Total time: 20216.64s
                               ETA: 1185319.8s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.213s, learning 0.163s)
               Value function loss: 28.2372
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 379.14
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 11.38s
                        Total time: 20228.02s
                               ETA: 1185267.9s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.531s, learning 0.193s)
               Value function loss: 39.9263
                    Surrogate loss: 0.0023
             Mean action noise std: 0.74
                       Mean reward: 379.27
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 11.72s
                        Total time: 20239.74s
                               ETA: 1185236.5s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.392s, learning 0.164s)
               Value function loss: 44.0832
                    Surrogate loss: 0.0042
             Mean action noise std: 0.74
                       Mean reward: 379.91
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 11.56s
                        Total time: 20251.30s
                               ETA: 1185195.3s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.155s, learning 0.202s)
               Value function loss: 30.9052
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 379.23
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 11.36s
                        Total time: 20262.66s
                               ETA: 1185142.4s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.533s, learning 0.164s)
               Value function loss: 15.9668
                    Surrogate loss: -0.0018
             Mean action noise std: 0.74
                       Mean reward: 377.15
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 11.70s
                        Total time: 20274.35s
                               ETA: 1185109.5s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.764s, learning 0.200s)
               Value function loss: 10.9357
                    Surrogate loss: -0.0150
             Mean action noise std: 0.74
                       Mean reward: 377.41
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 11.96s
                        Total time: 20286.32s
                               ETA: 1185092.3s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.049s, learning 0.231s)
               Value function loss: 6.0831
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 375.26
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 11.28s
                        Total time: 20297.60s
                               ETA: 1185035.0s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.422s, learning 0.180s)
               Value function loss: 2.3955
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 372.61
               Mean episode length: 248.38
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 11.60s
                        Total time: 20309.20s
                               ETA: 1184996.6s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.652s, learning 0.169s)
               Value function loss: 1.2050
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 372.91
               Mean episode length: 248.38
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 11.82s
                        Total time: 20321.02s
                               ETA: 1184971.0s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.040s, learning 0.170s)
               Value function loss: 1.5713
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 373.48
               Mean episode length: 248.38
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 11.21s
                        Total time: 20332.23s
                               ETA: 1184909.9s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.211s, learning 0.197s)
               Value function loss: 1.1637
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 374.03
               Mean episode length: 248.38
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 11.41s
                        Total time: 20343.64s
                               ETA: 1184860.3s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.373s, learning 0.159s)
               Value function loss: 0.8983
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 374.05
               Mean episode length: 248.38
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 11.53s
                        Total time: 20355.17s
                               ETA: 1184817.9s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.383s, learning 0.227s)
               Value function loss: 1.4427
                    Surrogate loss: -0.0109
             Mean action noise std: 0.74
                       Mean reward: 373.12
               Mean episode length: 248.38
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 11.61s
                        Total time: 20366.78s
                               ETA: 1184780.2s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.578s, learning 0.187s)
               Value function loss: 0.9778
                    Surrogate loss: 0.0062
             Mean action noise std: 0.74
                       Mean reward: 373.33
               Mean episode length: 248.38
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 11.77s
                        Total time: 20378.55s
                               ETA: 1184751.5s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.162s)
               Value function loss: 1.5817
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 373.37
               Mean episode length: 248.38
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 11.55s
                        Total time: 20390.09s
                               ETA: 1184710.2s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.200s, learning 0.237s)
               Value function loss: 3.0840
                    Surrogate loss: -0.0085
             Mean action noise std: 0.74
                       Mean reward: 372.83
               Mean episode length: 248.38
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 11.44s
                        Total time: 20401.53s
                               ETA: 1184662.6s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.245s, learning 0.166s)
               Value function loss: 3.2747
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 375.36
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 11.41s
                        Total time: 20412.94s
                               ETA: 1184613.4s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.378s, learning 0.271s)
               Value function loss: 2.7424
                    Surrogate loss: -0.0105
             Mean action noise std: 0.74
                       Mean reward: 375.21
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 11.65s
                        Total time: 20424.59s
                               ETA: 1184578.1s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.636s, learning 0.222s)
               Value function loss: 6.7092
                    Surrogate loss: -0.0054
             Mean action noise std: 0.74
                       Mean reward: 376.78
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 11.86s
                        Total time: 20436.45s
                               ETA: 1184554.9s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.467s, learning 0.165s)
               Value function loss: 4.5874
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 378.33
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 11.63s
                        Total time: 20448.08s
                               ETA: 1184518.6s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.517s, learning 0.351s)
               Value function loss: 4.6891
                    Surrogate loss: -0.0051
             Mean action noise std: 0.74
                       Mean reward: 380.30
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 11.87s
                        Total time: 20459.95s
                               ETA: 1184496.1s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.167s)
               Value function loss: 4.5158
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 381.37
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 11.39s
                        Total time: 20471.34s
                               ETA: 1184445.9s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.355s, learning 0.183s)
               Value function loss: 5.9627
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 381.52
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 11.54s
                        Total time: 20482.88s
                               ETA: 1184404.3s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.142s, learning 0.176s)
               Value function loss: 6.8337
                    Surrogate loss: -0.0133
             Mean action noise std: 0.74
                       Mean reward: 382.42
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 11.32s
                        Total time: 20494.20s
                               ETA: 1184350.0s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.429s, learning 0.198s)
               Value function loss: 6.3331
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 382.73
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 11.63s
                        Total time: 20505.82s
                               ETA: 1184313.7s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.206s, learning 0.265s)
               Value function loss: 6.3045
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 382.76
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 11.47s
                        Total time: 20517.29s
                               ETA: 1184268.3s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.677s, learning 0.169s)
               Value function loss: 6.7865
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 382.53
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 11.85s
                        Total time: 20529.14s
                               ETA: 1184244.6s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.847s, learning 0.264s)
               Value function loss: 11.0474
                    Surrogate loss: -0.0135
             Mean action noise std: 0.74
                       Mean reward: 382.70
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 12.11s
                        Total time: 20541.25s
                               ETA: 1184236.2s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.140s, learning 0.197s)
               Value function loss: 15.5076
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 383.26
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 11.34s
                        Total time: 20552.59s
                               ETA: 1184183.2s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.486s, learning 0.172s)
               Value function loss: 22.6048
                    Surrogate loss: -0.0158
             Mean action noise std: 0.74
                       Mean reward: 383.20
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 11.66s
                        Total time: 20564.24s
                               ETA: 1184148.7s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.168s, learning 0.190s)
               Value function loss: 22.3678
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 382.89
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 11.36s
                        Total time: 20575.60s
                               ETA: 1184097.0s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.282s, learning 0.190s)
               Value function loss: 29.4685
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 382.28
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 11.47s
                        Total time: 20587.07s
                               ETA: 1184051.9s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.375s, learning 0.223s)
               Value function loss: 37.1328
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 383.40
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 11.60s
                        Total time: 20598.67s
                               ETA: 1184014.1s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.759s, learning 0.180s)
               Value function loss: 48.9839
                    Surrogate loss: -0.0081
             Mean action noise std: 0.74
                       Mean reward: 384.10
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 11.94s
                        Total time: 20610.61s
                               ETA: 1183995.9s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.200s, learning 0.160s)
               Value function loss: 43.3899
                    Surrogate loss: 0.0375
             Mean action noise std: 0.74
                       Mean reward: 383.35
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 11.36s
                        Total time: 20621.97s
                               ETA: 1183944.5s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.300s, learning 0.193s)
               Value function loss: 24.1220
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 384.23
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 11.49s
                        Total time: 20633.46s
                               ETA: 1183900.7s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.211s, learning 0.187s)
               Value function loss: 18.5969
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 383.27
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 11.40s
                        Total time: 20644.86s
                               ETA: 1183851.6s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.174s)
               Value function loss: 11.3175
                    Surrogate loss: -0.0116
             Mean action noise std: 0.74
                       Mean reward: 382.49
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 11.50s
                        Total time: 20656.36s
                               ETA: 1183808.3s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1518 steps/s (collection: 10.611s, learning 0.177s)
               Value function loss: 4.4606
                    Surrogate loss: -0.0083
             Mean action noise std: 0.74
                       Mean reward: 383.35
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 10.79s
                        Total time: 20667.15s
                               ETA: 1183724.3s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.245s, learning 0.187s)
               Value function loss: 1.7270
                    Surrogate loss: -0.0134
             Mean action noise std: 0.74
                       Mean reward: 383.44
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 11.43s
                        Total time: 20678.58s
                               ETA: 1183677.2s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.175s, learning 0.208s)
               Value function loss: 1.6604
                    Surrogate loss: -0.0048
             Mean action noise std: 0.74
                       Mean reward: 383.44
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 11.38s
                        Total time: 20689.96s
                               ETA: 1183627.4s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.627s, learning 0.252s)
               Value function loss: 1.0143
                    Surrogate loss: -0.0131
             Mean action noise std: 0.74
                       Mean reward: 383.17
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 11.88s
                        Total time: 20701.84s
                               ETA: 1183605.9s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.344s, learning 0.205s)
               Value function loss: 0.9974
                    Surrogate loss: 0.0008
             Mean action noise std: 0.74
                       Mean reward: 383.23
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 11.55s
                        Total time: 20713.39s
                               ETA: 1183565.7s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.294s, learning 0.249s)
               Value function loss: 1.4019
                    Surrogate loss: -0.0095
             Mean action noise std: 0.74
                       Mean reward: 382.92
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 11.54s
                        Total time: 20724.94s
                               ETA: 1183525.1s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.521s, learning 0.213s)
               Value function loss: 1.5150
                    Surrogate loss: -0.0114
             Mean action noise std: 0.74
                       Mean reward: 381.78
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 11.73s
                        Total time: 20736.67s
                               ETA: 1183495.5s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.153s, learning 0.162s)
               Value function loss: 1.1434
                    Surrogate loss: -0.0071
             Mean action noise std: 0.74
                       Mean reward: 381.61
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 11.32s
                        Total time: 20747.99s
                               ETA: 1183442.0s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.426s, learning 0.255s)
               Value function loss: 2.3792
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 380.65
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 11.68s
                        Total time: 20759.67s
                               ETA: 1183409.4s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.434s, learning 0.187s)
               Value function loss: 5.2999
                    Surrogate loss: -0.0063
             Mean action noise std: 0.74
                       Mean reward: 380.44
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 11.62s
                        Total time: 20771.29s
                               ETA: 1183373.4s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.597s, learning 0.227s)
               Value function loss: 3.4995
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 378.92
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 11.82s
                        Total time: 20783.11s
                               ETA: 1183349.0s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.423s, learning 0.185s)
               Value function loss: 6.1132
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 377.20
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 11.61s
                        Total time: 20794.72s
                               ETA: 1183312.4s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.182s, learning 0.227s)
               Value function loss: 3.9930
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 377.55
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 11.41s
                        Total time: 20806.13s
                               ETA: 1183264.4s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.175s, learning 0.161s)
               Value function loss: 3.6523
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 376.83
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 11.34s
                        Total time: 20817.47s
                               ETA: 1183212.3s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.163s)
               Value function loss: 3.9225
                    Surrogate loss: -0.0009
             Mean action noise std: 0.74
                       Mean reward: 375.72
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 11.40s
                        Total time: 20828.87s
                               ETA: 1183163.9s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.356s, learning 0.210s)
               Value function loss: 5.2912
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 375.98
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 11.57s
                        Total time: 20840.43s
                               ETA: 1183124.9s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.161s)
               Value function loss: 5.5021
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 375.78
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 11.33s
                        Total time: 20851.77s
                               ETA: 1183072.9s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.332s, learning 0.170s)
               Value function loss: 6.1088
                    Surrogate loss: -0.0033
             Mean action noise std: 0.74
                       Mean reward: 374.67
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 11.50s
                        Total time: 20863.27s
                               ETA: 1183030.4s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.340s, learning 0.161s)
               Value function loss: 6.0626
                    Surrogate loss: -0.0059
             Mean action noise std: 0.74
                       Mean reward: 372.54
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 11.50s
                        Total time: 20874.77s
                               ETA: 1182987.8s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.379s, learning 0.159s)
               Value function loss: 5.9494
                    Surrogate loss: -0.0065
             Mean action noise std: 0.74
                       Mean reward: 370.22
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 11.54s
                        Total time: 20886.31s
                               ETA: 1182947.4s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.179s, learning 0.166s)
               Value function loss: 12.1910
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: 366.57
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 11.35s
                        Total time: 20897.65s
                               ETA: 1182896.2s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.237s, learning 0.167s)
               Value function loss: 13.6491
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 367.34
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 11.40s
                        Total time: 20909.06s
                               ETA: 1182848.3s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.832s, learning 0.164s)
               Value function loss: 21.9369
                    Surrogate loss: -0.0112
             Mean action noise std: 0.74
                       Mean reward: 363.16
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 11.00s
                        Total time: 20920.05s
                               ETA: 1182777.3s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.328s, learning 0.167s)
               Value function loss: 21.1317
                    Surrogate loss: 0.0048
             Mean action noise std: 0.74
                       Mean reward: 361.61
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 11.50s
                        Total time: 20931.55s
                               ETA: 1182734.7s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.361s, learning 0.160s)
               Value function loss: 27.2314
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 359.13
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 11.52s
                        Total time: 20943.07s
                               ETA: 1182693.5s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.330s, learning 0.253s)
               Value function loss: 42.7371
                    Surrogate loss: -0.0011
             Mean action noise std: 0.74
                       Mean reward: 358.98
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 11.58s
                        Total time: 20954.65s
                               ETA: 1182655.9s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.433s, learning 0.159s)
               Value function loss: 52.8651
                    Surrogate loss: 0.0012
             Mean action noise std: 0.74
                       Mean reward: 358.57
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 11.59s
                        Total time: 20966.24s
                               ETA: 1182618.9s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.209s, learning 0.167s)
               Value function loss: 45.2949
                    Surrogate loss: 0.0005
             Mean action noise std: 0.74
                       Mean reward: 358.19
               Mean episode length: 250.00
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 11.38s
                        Total time: 20977.62s
                               ETA: 1182569.7s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.336s, learning 0.214s)
               Value function loss: 27.1825
                    Surrogate loss: -0.0077
             Mean action noise std: 0.74
                       Mean reward: 352.48
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 11.55s
                        Total time: 20989.17s
                               ETA: 1182530.3s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.250s, learning 0.170s)
               Value function loss: 21.4053
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: 351.34
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 11.42s
                        Total time: 21000.59s
                               ETA: 1182483.7s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.642s, learning 0.193s)
               Value function loss: 14.0798
                    Surrogate loss: -0.0067
             Mean action noise std: 0.74
                       Mean reward: 350.33
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 11.84s
                        Total time: 21012.43s
                               ETA: 1182460.4s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.413s, learning 0.168s)
               Value function loss: 7.8983
                    Surrogate loss: -0.0143
             Mean action noise std: 0.74
                       Mean reward: 350.98
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 11.58s
                        Total time: 21024.01s
                               ETA: 1182422.9s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.137s, learning 0.162s)
               Value function loss: 2.5381
                    Surrogate loss: -0.0160
             Mean action noise std: 0.74
                       Mean reward: 349.41
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 11.30s
                        Total time: 21035.30s
                               ETA: 1182369.4s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.203s, learning 0.163s)
               Value function loss: 1.9762
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 351.21
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 11.37s
                        Total time: 21046.67s
                               ETA: 1182319.9s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.147s, learning 0.157s)
               Value function loss: 2.2223
                    Surrogate loss: -0.0087
             Mean action noise std: 0.74
                       Mean reward: 351.14
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 11.30s
                        Total time: 21057.98s
                               ETA: 1182266.9s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.055s, learning 0.262s)
               Value function loss: 1.0478
                    Surrogate loss: -0.0169
             Mean action noise std: 0.74
                       Mean reward: 350.61
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 11.32s
                        Total time: 21069.29s
                               ETA: 1182214.7s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.097s, learning 0.161s)
               Value function loss: 2.1858
                    Surrogate loss: -0.0004
             Mean action noise std: 0.74
                       Mean reward: 349.58
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 11.26s
                        Total time: 21080.55s
                               ETA: 1182159.2s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.339s, learning 0.168s)
               Value function loss: 1.7574
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 349.76
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 11.51s
                        Total time: 21092.06s
                               ETA: 1182117.8s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.704s, learning 0.206s)
               Value function loss: 1.6488
                    Surrogate loss: 0.0021
             Mean action noise std: 0.74
                       Mean reward: 349.80
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 11.91s
                        Total time: 21103.97s
                               ETA: 1182098.9s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.097s, learning 0.162s)
               Value function loss: 2.6456
                    Surrogate loss: 0.0017
             Mean action noise std: 0.74
                       Mean reward: 349.81
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 11.26s
                        Total time: 21115.23s
                               ETA: 1182043.6s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.474s, learning 0.193s)
               Value function loss: 5.6859
                    Surrogate loss: -0.0042
             Mean action noise std: 0.74
                       Mean reward: 346.78
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 11.67s
                        Total time: 21126.89s
                               ETA: 1182011.2s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.638s, learning 0.157s)
               Value function loss: 3.9600
                    Surrogate loss: -0.0031
             Mean action noise std: 0.74
                       Mean reward: 345.59
               Mean episode length: 248.85
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 11.80s
                        Total time: 21138.69s
                               ETA: 1181985.9s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.479s, learning 0.190s)
               Value function loss: 4.4422
                    Surrogate loss: -0.0028
             Mean action noise std: 0.74
                       Mean reward: 343.90
               Mean episode length: 248.85
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 11.67s
                        Total time: 21150.36s
                               ETA: 1181953.7s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.241s, learning 0.156s)
               Value function loss: 5.8742
                    Surrogate loss: 0.0008
             Mean action noise std: 0.74
                       Mean reward: 344.09
               Mean episode length: 248.85
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 11.40s
                        Total time: 21161.75s
                               ETA: 1181906.2s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1510 steps/s (collection: 10.661s, learning 0.186s)
               Value function loss: 4.9131
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 343.44
               Mean episode length: 248.85
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 10.85s
                        Total time: 21172.60s
                               ETA: 1181828.1s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.256s, learning 0.212s)
               Value function loss: 4.8586
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 346.03
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 11.47s
                        Total time: 21184.07s
                               ETA: 1181784.8s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.413s, learning 0.163s)
               Value function loss: 4.9507
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 344.92
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 11.58s
                        Total time: 21195.64s
                               ETA: 1181747.4s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.247s, learning 0.165s)
               Value function loss: 7.0636
                    Surrogate loss: -0.0006
             Mean action noise std: 0.74
                       Mean reward: 342.89
               Mean episode length: 249.19
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 11.41s
                        Total time: 21207.06s
                               ETA: 1181701.0s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.253s, learning 0.374s)
               Value function loss: 7.5391
                    Surrogate loss: -0.0050
             Mean action noise std: 0.74
                       Mean reward: 341.84
               Mean episode length: 249.19
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 11.63s
                        Total time: 21218.68s
                               ETA: 1181666.5s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.042s, learning 0.194s)
               Value function loss: 7.2923
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 341.12
               Mean episode length: 249.19
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 11.24s
                        Total time: 21229.92s
                               ETA: 1181610.4s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.375s, learning 0.168s)
               Value function loss: 4.9878
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 341.82
               Mean episode length: 249.19
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 11.54s
                        Total time: 21241.46s
                               ETA: 1181571.4s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.830s, learning 0.186s)
               Value function loss: 9.6760
                    Surrogate loss: 0.0015
             Mean action noise std: 0.74
                       Mean reward: 343.55
               Mean episode length: 249.31
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 12.02s
                        Total time: 21253.48s
                               ETA: 1181558.7s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.399s, learning 0.168s)
               Value function loss: 11.5840
                    Surrogate loss: -0.0055
             Mean action noise std: 0.74
                       Mean reward: 342.98
               Mean episode length: 249.31
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 11.57s
                        Total time: 21265.05s
                               ETA: 1181521.0s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.211s, learning 0.181s)
               Value function loss: 18.1655
                    Surrogate loss: 0.0001
             Mean action noise std: 0.74
                       Mean reward: 343.77
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 11.39s
                        Total time: 21276.44s
                               ETA: 1181473.7s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.134s, learning 0.174s)
               Value function loss: 16.7532
                    Surrogate loss: -0.0022
             Mean action noise std: 0.74
                       Mean reward: 347.67
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 11.31s
                        Total time: 21287.75s
                               ETA: 1181421.8s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.371s, learning 0.206s)
               Value function loss: 22.1633
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 350.66
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 11.58s
                        Total time: 21299.32s
                               ETA: 1181384.8s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.350s, learning 0.194s)
               Value function loss: 33.9957
                    Surrogate loss: 0.0032
             Mean action noise std: 0.74
                       Mean reward: 350.69
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 11.54s
                        Total time: 21310.87s
                               ETA: 1181346.0s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.758s, learning 0.282s)
               Value function loss: 45.3438
                    Surrogate loss: 0.0167
             Mean action noise std: 0.74
                       Mean reward: 352.41
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 12.04s
                        Total time: 21322.91s
                               ETA: 1181334.8s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.886s, learning 0.191s)
               Value function loss: 44.3121
                    Surrogate loss: 0.0056
             Mean action noise std: 0.74
                       Mean reward: 357.10
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 11.08s
                        Total time: 21333.98s
                               ETA: 1181270.2s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.166s)
               Value function loss: 27.2238
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 360.98
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 11.42s
                        Total time: 21345.40s
                               ETA: 1181224.4s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.148s, learning 0.168s)
               Value function loss: 17.3915
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 359.47
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 11.32s
                        Total time: 21356.72s
                               ETA: 1181173.1s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.094s, learning 0.200s)
               Value function loss: 12.2810
                    Surrogate loss: -0.0123
             Mean action noise std: 0.74
                       Mean reward: 362.59
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 11.29s
                        Total time: 21368.01s
                               ETA: 1181120.7s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.484s, learning 0.188s)
               Value function loss: 7.2965
                    Surrogate loss: -0.0128
             Mean action noise std: 0.74
                       Mean reward: 364.26
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 11.67s
                        Total time: 21379.68s
                               ETA: 1181089.2s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.161s, learning 0.186s)
               Value function loss: 2.9736
                    Surrogate loss: -0.0084
             Mean action noise std: 0.74
                       Mean reward: 365.10
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 11.35s
                        Total time: 21391.03s
                               ETA: 1181039.7s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.369s, learning 0.199s)
               Value function loss: 2.4754
                    Surrogate loss: 0.0493
             Mean action noise std: 0.74
                       Mean reward: 365.56
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 11.57s
                        Total time: 21402.60s
                               ETA: 1181002.5s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.669s, learning 0.231s)
               Value function loss: 2.4724
                    Surrogate loss: 0.0016
             Mean action noise std: 0.74
                       Mean reward: 366.39
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 11.90s
                        Total time: 21414.50s
                               ETA: 1180983.7s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.516s, learning 0.168s)
               Value function loss: 1.6811
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 366.53
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 11.68s
                        Total time: 21426.18s
                               ETA: 1180952.9s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.290s, learning 0.155s)
               Value function loss: 1.6862
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 367.28
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 11.44s
                        Total time: 21437.63s
                               ETA: 1180909.0s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.228s, learning 0.161s)
               Value function loss: 1.9714
                    Surrogate loss: 0.0225
             Mean action noise std: 0.74
                       Mean reward: 367.72
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 11.39s
                        Total time: 21449.02s
                               ETA: 1180862.1s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.352s, learning 0.166s)
               Value function loss: 0.8796
                    Surrogate loss: -0.0020
             Mean action noise std: 0.74
                       Mean reward: 368.39
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 11.52s
                        Total time: 21460.53s
                               ETA: 1180822.3s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.266s, learning 0.158s)
               Value function loss: 2.2899
                    Surrogate loss: -0.0021
             Mean action noise std: 0.74
                       Mean reward: 368.63
               Mean episode length: 249.41
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 11.42s
                        Total time: 21471.96s
                               ETA: 1180777.3s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.992s, learning 0.195s)
               Value function loss: 4.5813
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 371.73
               Mean episode length: 249.41
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 11.19s
                        Total time: 21483.14s
                               ETA: 1180719.3s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.378s, learning 0.246s)
               Value function loss: 4.1309
                    Surrogate loss: -0.0061
             Mean action noise std: 0.74
                       Mean reward: 374.46
               Mean episode length: 249.41
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 11.62s
                        Total time: 21494.77s
                               ETA: 1180685.5s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.290s, learning 0.161s)
               Value function loss: 3.2139
                    Surrogate loss: -0.0060
             Mean action noise std: 0.74
                       Mean reward: 376.28
               Mean episode length: 249.41
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 11.45s
                        Total time: 21506.22s
                               ETA: 1180642.1s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.380s, learning 0.200s)
               Value function loss: 6.6952
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 377.07
               Mean episode length: 249.41
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 11.58s
                        Total time: 21517.80s
                               ETA: 1180605.8s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.314s, learning 0.174s)
               Value function loss: 5.7959
                    Surrogate loss: -0.0094
             Mean action noise std: 0.74
                       Mean reward: 377.04
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 11.49s
                        Total time: 21529.29s
                               ETA: 1180564.6s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.498s, learning 0.311s)
               Value function loss: 4.8033
                    Surrogate loss: 0.0026
             Mean action noise std: 0.74
                       Mean reward: 376.29
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 11.81s
                        Total time: 21541.09s
                               ETA: 1180540.9s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.675s, learning 0.163s)
               Value function loss: 6.0182
                    Surrogate loss: -0.0088
             Mean action noise std: 0.74
                       Mean reward: 375.04
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 11.84s
                        Total time: 21552.93s
                               ETA: 1180518.9s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.407s, learning 0.190s)
               Value function loss: 6.3100
                    Surrogate loss: -0.0066
             Mean action noise std: 0.74
                       Mean reward: 374.77
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 11.60s
                        Total time: 21564.53s
                               ETA: 1180483.7s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.445s, learning 0.232s)
               Value function loss: 7.6213
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 375.12
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 11.68s
                        Total time: 21576.21s
                               ETA: 1180452.9s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.395s, learning 0.170s)
               Value function loss: 6.8487
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 375.53
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 11.56s
                        Total time: 21587.77s
                               ETA: 1180416.0s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.079s, learning 0.199s)
               Value function loss: 7.1026
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 376.25
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 11.28s
                        Total time: 21599.05s
                               ETA: 1180363.4s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.778s, learning 0.206s)
               Value function loss: 9.9982
                    Surrogate loss: -0.0110
             Mean action noise std: 0.74
                       Mean reward: 376.69
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 10.98s
                        Total time: 21610.03s
                               ETA: 1180294.9s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.316s, learning 0.164s)
               Value function loss: 10.7239
                    Surrogate loss: -0.0091
             Mean action noise std: 0.74
                       Mean reward: 376.37
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 11.48s
                        Total time: 21621.51s
                               ETA: 1180253.4s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.162s, learning 0.226s)
               Value function loss: 19.7367
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 375.22
               Mean episode length: 249.30
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 11.39s
                        Total time: 21632.90s
                               ETA: 1180207.0s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.737s, learning 0.189s)
               Value function loss: 23.7550
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 377.35
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 11.93s
                        Total time: 21644.83s
                               ETA: 1180189.9s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.181s, learning 0.251s)
               Value function loss: 23.9966
                    Surrogate loss: -0.0080
             Mean action noise std: 0.74
                       Mean reward: 375.50
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 11.43s
                        Total time: 21656.26s
                               ETA: 1180145.9s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.399s, learning 0.163s)
               Value function loss: 40.8122
                    Surrogate loss: -0.0102
             Mean action noise std: 0.74
                       Mean reward: 376.65
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 11.56s
                        Total time: 21667.82s
                               ETA: 1180109.0s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.126s, learning 0.195s)
               Value function loss: 57.1327
                    Surrogate loss: 0.0104
             Mean action noise std: 0.74
                       Mean reward: 377.54
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 11.32s
                        Total time: 21679.14s
                               ETA: 1180059.1s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.261s, learning 0.176s)
               Value function loss: 58.8764
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 377.05
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 11.44s
                        Total time: 21690.58s
                               ETA: 1180015.5s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.671s, learning 0.161s)
               Value function loss: 44.9142
                    Surrogate loss: -0.0139
             Mean action noise std: 0.74
                       Mean reward: 378.74
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 11.83s
                        Total time: 21702.41s
                               ETA: 1179993.5s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.127s, learning 0.165s)
               Value function loss: 26.0769
                    Surrogate loss: 0.0053
             Mean action noise std: 0.74
                       Mean reward: 377.31
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 11.29s
                        Total time: 21713.70s
                               ETA: 1179942.1s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.527s, learning 0.249s)
               Value function loss: 20.1352
                    Surrogate loss: 0.0011
             Mean action noise std: 0.74
                       Mean reward: 378.92
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 11.78s
                        Total time: 21725.48s
                               ETA: 1179917.0s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.454s, learning 0.165s)
               Value function loss: 9.8321
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 379.43
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 11.62s
                        Total time: 21737.10s
                               ETA: 1179883.4s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.222s, learning 0.174s)
               Value function loss: 3.4751
                    Surrogate loss: -0.0132
             Mean action noise std: 0.74
                       Mean reward: 380.41
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 11.40s
                        Total time: 21748.49s
                               ETA: 1179837.7s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.556s, learning 0.226s)
               Value function loss: 1.9932
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 380.28
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 11.78s
                        Total time: 21760.27s
                               ETA: 1179813.0s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.451s, learning 0.226s)
               Value function loss: 2.0723
                    Surrogate loss: -0.0100
             Mean action noise std: 0.74
                       Mean reward: 380.75
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 11.68s
                        Total time: 21771.95s
                               ETA: 1179782.6s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.254s, learning 0.166s)
               Value function loss: 1.1355
                    Surrogate loss: -0.0129
             Mean action noise std: 0.74
                       Mean reward: 380.92
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 11.42s
                        Total time: 21783.37s
                               ETA: 1179738.4s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.236s, learning 0.217s)
               Value function loss: 1.2056
                    Surrogate loss: -0.0101
             Mean action noise std: 0.74
                       Mean reward: 380.98
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 11.45s
                        Total time: 21794.82s
                               ETA: 1179695.9s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.984s, learning 0.194s)
               Value function loss: 1.5528
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 381.12
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 11.18s
                        Total time: 21806.00s
                               ETA: 1179638.7s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1497 steps/s (collection: 10.774s, learning 0.166s)
               Value function loss: 1.2822
                    Surrogate loss: -0.0086
             Mean action noise std: 0.74
                       Mean reward: 381.21
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 10.94s
                        Total time: 21816.94s
                               ETA: 1179568.6s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.216s, learning 0.161s)
               Value function loss: 1.6625
                    Surrogate loss: -0.0052
             Mean action noise std: 0.74
                       Mean reward: 381.37
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 11.38s
                        Total time: 21828.32s
                               ETA: 1179522.2s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.288s, learning 0.158s)
               Value function loss: 3.4659
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 382.14
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 11.45s
                        Total time: 21839.77s
                               ETA: 1179479.5s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.374s, learning 0.197s)
               Value function loss: 4.1928
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 382.35
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 11.57s
                        Total time: 21851.34s
                               ETA: 1179443.6s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.435s, learning 0.177s)
               Value function loss: 3.2602
                    Surrogate loss: -0.0073
             Mean action noise std: 0.74
                       Mean reward: 382.26
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 11.61s
                        Total time: 21862.95s
                               ETA: 1179410.0s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.192s, learning 0.160s)
               Value function loss: 6.8203
                    Surrogate loss: -0.0057
             Mean action noise std: 0.74
                       Mean reward: 382.13
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 11.35s
                        Total time: 21874.30s
                               ETA: 1179362.4s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.251s, learning 0.226s)
               Value function loss: 5.6973
                    Surrogate loss: -0.0115
             Mean action noise std: 0.74
                       Mean reward: 380.94
               Mean episode length: 249.54
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 11.48s
                        Total time: 21885.78s
                               ETA: 1179321.5s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.813s, learning 0.167s)
               Value function loss: 4.8736
                    Surrogate loss: -0.0040
             Mean action noise std: 0.74
                       Mean reward: 380.59
               Mean episode length: 249.54
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 11.98s
                        Total time: 21897.76s
                               ETA: 1179307.8s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.299s, learning 0.161s)
               Value function loss: 5.0208
                    Surrogate loss: -0.0125
             Mean action noise std: 0.74
                       Mean reward: 379.98
               Mean episode length: 249.54
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 11.46s
                        Total time: 21909.22s
                               ETA: 1179266.1s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.323s, learning 0.167s)
               Value function loss: 6.7203
                    Surrogate loss: -0.0039
             Mean action noise std: 0.74
                       Mean reward: 378.96
               Mean episode length: 249.54
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 11.49s
                        Total time: 21920.71s
                               ETA: 1179226.0s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.300s, learning 0.156s)
               Value function loss: 6.7673
                    Surrogate loss: -0.0113
             Mean action noise std: 0.74
                       Mean reward: 378.16
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 11.46s
                        Total time: 21932.16s
                               ETA: 1179184.1s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.161s)
               Value function loss: 6.2482
                    Surrogate loss: -0.0096
             Mean action noise std: 0.74
                       Mean reward: 377.63
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 11.37s
                        Total time: 21943.53s
                               ETA: 1179137.4s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.300s, learning 0.164s)
               Value function loss: 6.1639
                    Surrogate loss: -0.0111
             Mean action noise std: 0.74
                       Mean reward: 377.32
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 11.46s
                        Total time: 21954.99s
                               ETA: 1179096.0s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.397s, learning 0.164s)
               Value function loss: 8.1193
                    Surrogate loss: 0.0072
             Mean action noise std: 0.74
                       Mean reward: 376.52
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 11.56s
                        Total time: 21966.55s
                               ETA: 1179059.9s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.428s, learning 0.184s)
               Value function loss: 11.5500
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 375.90
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 11.61s
                        Total time: 21978.17s
                               ETA: 1179026.5s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.527s, learning 0.188s)
               Value function loss: 15.9689
                    Surrogate loss: -0.0104
             Mean action noise std: 0.74
                       Mean reward: 375.13
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 11.72s
                        Total time: 21989.88s
                               ETA: 1178998.7s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.904s, learning 0.163s)
               Value function loss: 21.9542
                    Surrogate loss: 0.0013
             Mean action noise std: 0.74
                       Mean reward: 373.66
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 12.07s
                        Total time: 22001.95s
                               ETA: 1178989.8s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.330s, learning 0.164s)
               Value function loss: 21.1446
                    Surrogate loss: 0.0000
             Mean action noise std: 0.74
                       Mean reward: 375.27
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 11.49s
                        Total time: 22013.44s
                               ETA: 1178950.1s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.983s, learning 0.228s)
               Value function loss: 28.4027
                    Surrogate loss: -0.0099
             Mean action noise std: 0.74
                       Mean reward: 374.89
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 11.21s
                        Total time: 22024.65s
                               ETA: 1178895.4s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.780s, learning 0.204s)
               Value function loss: 44.3418
                    Surrogate loss: -0.0106
             Mean action noise std: 0.74
                       Mean reward: 374.75
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 11.98s
                        Total time: 22036.64s
                               ETA: 1178882.0s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.079s, learning 0.229s)
               Value function loss: 53.0103
                    Surrogate loss: -0.0015
             Mean action noise std: 0.74
                       Mean reward: 376.09
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 11.31s
                        Total time: 22047.95s
                               ETA: 1178832.6s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.062s, learning 0.159s)
               Value function loss: 35.9351
                    Surrogate loss: 0.0056
             Mean action noise std: 0.74
                       Mean reward: 375.72
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 11.22s
                        Total time: 22059.17s
                               ETA: 1178778.5s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.836s, learning 0.163s)
               Value function loss: 22.7118
                    Surrogate loss: -0.0009
             Mean action noise std: 0.74
                       Mean reward: 376.88
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 11.00s
                        Total time: 22070.17s
                               ETA: 1178712.6s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.042s, learning 0.186s)
               Value function loss: 16.1529
                    Surrogate loss: -0.0046
             Mean action noise std: 0.74
                       Mean reward: 375.38
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 11.23s
                        Total time: 22081.39s
                               ETA: 1178659.0s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.888s, learning 0.163s)
               Value function loss: 7.4464
                    Surrogate loss: -0.0174
             Mean action noise std: 0.74
                       Mean reward: 374.78
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 11.05s
                        Total time: 22092.44s
                               ETA: 1178595.9s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.346s, learning 0.187s)
               Value function loss: 3.8335
                    Surrogate loss: -0.0089
             Mean action noise std: 0.74
                       Mean reward: 374.58
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 11.53s
                        Total time: 22103.98s
                               ETA: 1178558.7s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.097s, learning 0.180s)
               Value function loss: 1.7806
                    Surrogate loss: -0.0090
             Mean action noise std: 0.74
                       Mean reward: 374.11
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 11.28s
                        Total time: 22115.25s
                               ETA: 1178507.7s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.683s, learning 0.159s)
               Value function loss: 1.9856
                    Surrogate loss: 0.0068
             Mean action noise std: 0.74
                       Mean reward: 374.53
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 11.84s
                        Total time: 22127.10s
                               ETA: 1178487.0s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.207s, learning 0.187s)
               Value function loss: 1.2489
                    Surrogate loss: -0.0029
             Mean action noise std: 0.74
                       Mean reward: 374.97
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 11.39s
                        Total time: 22138.49s
                               ETA: 1178442.4s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.409s, learning 0.166s)
               Value function loss: 1.1068
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 374.88
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 11.57s
                        Total time: 22150.07s
                               ETA: 1178407.5s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.120s, learning 0.172s)
               Value function loss: 1.4388
                    Surrogate loss: 0.0071
             Mean action noise std: 0.74
                       Mean reward: 374.77
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 11.29s
                        Total time: 22161.36s
                               ETA: 1178357.6s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.662s, learning 0.201s)
               Value function loss: 1.5799
                    Surrogate loss: -0.0010
             Mean action noise std: 0.74
                       Mean reward: 374.61
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 11.86s
                        Total time: 22173.22s
                               ETA: 1178338.1s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.146s, learning 0.169s)
               Value function loss: 1.3040
                    Surrogate loss: -0.0058
             Mean action noise std: 0.74
                       Mean reward: 374.57
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 11.31s
                        Total time: 22184.54s
                               ETA: 1178289.4s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1469 steps/s (collection: 10.987s, learning 0.162s)
               Value function loss: 3.0110
                    Surrogate loss: 0.0016
             Mean action noise std: 0.74
                       Mean reward: 375.16
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 11.15s
                        Total time: 22195.68s
                               ETA: 1178231.9s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.440s, learning 0.199s)
               Value function loss: 5.7714
                    Surrogate loss: -0.0092
             Mean action noise std: 0.74
                       Mean reward: 375.20
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 11.64s
                        Total time: 22207.32s
                               ETA: 1178200.6s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.171s, learning 0.198s)
               Value function loss: 3.6508
                    Surrogate loss: -0.0032
             Mean action noise std: 0.74
                       Mean reward: 375.31
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 11.37s
                        Total time: 22218.69s
                               ETA: 1178154.9s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.649s, learning 0.257s)
               Value function loss: 6.0798
                    Surrogate loss: -0.0075
             Mean action noise std: 0.74
                       Mean reward: 374.65
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 11.91s
                        Total time: 22230.60s
                               ETA: 1178137.8s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.937s, learning 0.198s)
               Value function loss: 4.4016
                    Surrogate loss: -0.0107
             Mean action noise std: 0.74
                       Mean reward: 374.39
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 11.14s
                        Total time: 22241.74s
                               ETA: 1178079.8s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.313s, learning 0.216s)
               Value function loss: 3.8358
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 374.11
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 11.53s
                        Total time: 22253.26s
                               ETA: 1178042.7s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.465s, learning 0.162s)
               Value function loss: 4.6270
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 374.42
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 11.63s
                        Total time: 22264.89s
                               ETA: 1178010.8s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.317s, learning 0.179s)
               Value function loss: 4.5511
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 374.70
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 11.50s
                        Total time: 22276.39s
                               ETA: 1177972.0s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.365s, learning 0.167s)
               Value function loss: 6.0153
                    Surrogate loss: -0.0079
             Mean action noise std: 0.74
                       Mean reward: 374.83
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 11.53s
                        Total time: 22287.92s
                               ETA: 1177935.1s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.091s, learning 0.198s)
               Value function loss: 6.0192
                    Surrogate loss: -0.0026
             Mean action noise std: 0.74
                       Mean reward: 374.78
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 11.29s
                        Total time: 22299.21s
                               ETA: 1177885.4s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.067s, learning 0.175s)
               Value function loss: 6.1301
                    Surrogate loss: -0.0053
             Mean action noise std: 0.74
                       Mean reward: 374.24
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 11.24s
                        Total time: 22310.45s
                               ETA: 1177833.3s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.188s, learning 0.187s)
               Value function loss: 6.5585
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 374.37
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 11.38s
                        Total time: 22321.83s
                               ETA: 1177788.3s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.208s, learning 0.158s)
               Value function loss: 11.1738
                    Surrogate loss: 0.0036
             Mean action noise std: 0.74
                       Mean reward: 371.12
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 11.37s
                        Total time: 22333.19s
                               ETA: 1177742.8s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.241s, learning 0.193s)
               Value function loss: 13.7197
                    Surrogate loss: -0.0070
             Mean action noise std: 0.74
                       Mean reward: 370.08
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 11.43s
                        Total time: 22344.62s
                               ETA: 1177700.9s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.961s, learning 0.190s)
               Value function loss: 20.2725
                    Surrogate loss: -0.0074
             Mean action noise std: 0.74
                       Mean reward: 368.32
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 12.15s
                        Total time: 22356.78s
                               ETA: 1177696.9s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 890 steps/s (collection: 18.194s, learning 0.198s)
               Value function loss: 18.4509
                    Surrogate loss: -0.0068
             Mean action noise std: 0.74
                       Mean reward: 370.09
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 18.39s
                        Total time: 22375.17s
                               ETA: 1178021.3s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 726 steps/s (collection: 22.381s, learning 0.177s)
               Value function loss: 23.4035
                    Surrogate loss: -0.0037
             Mean action noise std: 0.74
                       Mean reward: 370.17
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 22.56s
                        Total time: 22397.73s
                               ETA: 1178564.7s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 723 steps/s (collection: 22.472s, learning 0.167s)
               Value function loss: 33.0132
                    Surrogate loss: -0.0025
             Mean action noise std: 0.74
                       Mean reward: 367.81
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 22.64s
                        Total time: 22420.36s
                               ETA: 1179111.7s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 739 steps/s (collection: 21.945s, learning 0.198s)
               Value function loss: 39.0718
                    Surrogate loss: 0.0078
             Mean action noise std: 0.74
                       Mean reward: 365.70
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 22.14s
                        Total time: 22442.51s
                               ETA: 1179632.0s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 735 steps/s (collection: 22.116s, learning 0.164s)
               Value function loss: 34.2935
                    Surrogate loss: 0.0034
             Mean action noise std: 0.74
                       Mean reward: 365.08
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 22.28s
                        Total time: 22464.79s
                               ETA: 1180159.0s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.435s, learning 0.181s)
               Value function loss: 21.0463
                    Surrogate loss: -0.0093
             Mean action noise std: 0.74
                       Mean reward: 364.55
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 21.62s
                        Total time: 22486.40s
                               ETA: 1180650.4s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 727 steps/s (collection: 22.329s, learning 0.203s)
               Value function loss: 13.3863
                    Surrogate loss: -0.0098
             Mean action noise std: 0.74
                       Mean reward: 363.36
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 22.53s
                        Total time: 22508.93s
                               ETA: 1181189.5s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 740 steps/s (collection: 21.914s, learning 0.200s)
               Value function loss: 8.4486
                    Surrogate loss: -0.0149
             Mean action noise std: 0.74
                       Mean reward: 359.86
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 22.11s
                        Total time: 22531.05s
                               ETA: 1181706.0s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 722 steps/s (collection: 22.460s, learning 0.223s)
               Value function loss: 4.9092
                    Surrogate loss: -0.0156
             Mean action noise std: 0.74
                       Mean reward: 359.37
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 22.68s
                        Total time: 22553.73s
                               ETA: 1182251.7s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 748 steps/s (collection: 21.722s, learning 0.174s)
               Value function loss: 2.6770
                    Surrogate loss: 0.0030
             Mean action noise std: 0.74
                       Mean reward: 359.63
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 21.90s
                        Total time: 22575.63s
                               ETA: 1182755.7s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.607s, learning 0.165s)
               Value function loss: 1.9593
                    Surrogate loss: -0.0044
             Mean action noise std: 0.74
                       Mean reward: 359.18
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 21.77s
                        Total time: 22597.40s
                               ETA: 1183252.5s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.763s, learning 0.160s)
               Value function loss: 1.8924
                    Surrogate loss: -0.0035
             Mean action noise std: 0.74
                       Mean reward: 359.33
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 21.92s
                        Total time: 22619.32s
                               ETA: 1183756.7s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 749 steps/s (collection: 21.703s, learning 0.156s)
               Value function loss: 1.1392
                    Surrogate loss: 0.0219
             Mean action noise std: 0.74
                       Mean reward: 359.35
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 21.86s
                        Total time: 22641.18s
                               ETA: 1184257.0s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 733 steps/s (collection: 22.153s, learning 0.171s)
               Value function loss: 1.7885
                    Surrogate loss: -0.0027
             Mean action noise std: 0.74
                       Mean reward: 359.16
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 22.32s
                        Total time: 22663.51s
                               ETA: 1184781.0s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 738 steps/s (collection: 21.983s, learning 0.194s)
               Value function loss: 1.3427
                    Surrogate loss: -0.0056
             Mean action noise std: 0.74
                       Mean reward: 359.54
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 22.18s
                        Total time: 22685.68s
                               ETA: 1185296.8s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 755 steps/s (collection: 21.533s, learning 0.162s)
               Value function loss: 1.3483
                    Surrogate loss: -0.0020
             Mean action noise std: 0.73
                       Mean reward: 359.08
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 21.70s
                        Total time: 22707.38s
                               ETA: 1185786.8s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 734 steps/s (collection: 22.072s, learning 0.234s)
               Value function loss: 2.1199
                    Surrogate loss: 0.0027
             Mean action noise std: 0.73
                       Mean reward: 359.47
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 22.31s
                        Total time: 22729.68s
                               ETA: 1186308.2s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 727 steps/s (collection: 22.346s, learning 0.169s)
               Value function loss: 4.2464
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 358.57
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 22.52s
                        Total time: 22752.20s
                               ETA: 1186839.9s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 764 steps/s (collection: 21.192s, learning 0.237s)
               Value function loss: 3.4397
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 358.86
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 21.43s
                        Total time: 22773.63s
                               ETA: 1187314.4s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 724 steps/s (collection: 22.442s, learning 0.173s)
               Value function loss: 3.6544
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 358.92
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 22.61s
                        Total time: 22796.24s
                               ETA: 1187850.2s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.822s, learning 0.165s)
               Value function loss: 4.7887
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 359.10
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 21.99s
                        Total time: 22818.23s
                               ETA: 1188352.6s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 740 steps/s (collection: 21.930s, learning 0.195s)
               Value function loss: 3.7747
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 358.84
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 22.12s
                        Total time: 22840.36s
                               ETA: 1188861.7s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 731 steps/s (collection: 22.192s, learning 0.207s)
               Value function loss: 3.9907
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 359.34
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 22.40s
                        Total time: 22862.76s
                               ETA: 1189384.5s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 731 steps/s (collection: 22.217s, learning 0.183s)
               Value function loss: 3.8404
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 359.24
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 22.40s
                        Total time: 22885.16s
                               ETA: 1189906.8s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 714 steps/s (collection: 22.710s, learning 0.216s)
               Value function loss: 5.8431
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 360.15
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 22.93s
                        Total time: 22908.08s
                               ETA: 1190455.9s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 757 steps/s (collection: 21.431s, learning 0.198s)
               Value function loss: 5.8264
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 361.54
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 21.63s
                        Total time: 22929.71s
                               ETA: 1190936.9s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 732 steps/s (collection: 22.207s, learning 0.169s)
               Value function loss: 6.1992
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 363.33
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 22.38s
                        Total time: 22952.09s
                               ETA: 1191456.2s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 727 steps/s (collection: 22.376s, learning 0.160s)
               Value function loss: 4.5538
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 364.36
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 22.54s
                        Total time: 22974.62s
                               ETA: 1191983.2s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 732 steps/s (collection: 22.198s, learning 0.171s)
               Value function loss: 9.4707
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 365.89
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 22.37s
                        Total time: 22996.99s
                               ETA: 1192501.1s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 735 steps/s (collection: 22.041s, learning 0.227s)
               Value function loss: 10.8997
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 367.19
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 22.27s
                        Total time: 23019.26s
                               ETA: 1193013.0s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.822s, learning 0.165s)
               Value function loss: 17.4848
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: 367.10
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 21.99s
                        Total time: 23041.25s
                               ETA: 1193509.9s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.747s, learning 0.198s)
               Value function loss: 15.6918
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 367.20
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 21.95s
                        Total time: 23063.19s
                               ETA: 1194004.0s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 739 steps/s (collection: 21.946s, learning 0.203s)
               Value function loss: 21.2226
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 368.53
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 22.15s
                        Total time: 23085.34s
                               ETA: 1194508.2s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 729 steps/s (collection: 22.246s, learning 0.203s)
               Value function loss: 32.1011
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 370.37
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 22.45s
                        Total time: 23107.79s
                               ETA: 1195027.3s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.758s, learning 0.168s)
               Value function loss: 43.0895
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 370.08
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 21.93s
                        Total time: 23129.72s
                               ETA: 1195518.8s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.794s, learning 0.164s)
               Value function loss: 51.1776
                    Surrogate loss: -0.0002
             Mean action noise std: 0.73
                       Mean reward: 371.94
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 21.96s
                        Total time: 23151.68s
                               ETA: 1196011.5s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.622s, learning 0.159s)
               Value function loss: 31.4225
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 371.82
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 21.78s
                        Total time: 23173.46s
                               ETA: 1196494.4s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 744 steps/s (collection: 21.809s, learning 0.194s)
               Value function loss: 19.7194
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 371.91
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 22.00s
                        Total time: 23195.46s
                               ETA: 1196988.3s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.882s, learning 0.159s)
               Value function loss: 13.0093
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 373.39
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 12.04s
                        Total time: 23207.50s
                               ETA: 1196967.8s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.705s, learning 0.181s)
               Value function loss: 7.7418
                    Surrogate loss: 0.0120
             Mean action noise std: 0.73
                       Mean reward: 372.98
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 11.89s
                        Total time: 23219.39s
                               ETA: 1196939.3s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.010s, learning 0.195s)
               Value function loss: 2.4881
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 372.69
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 11.21s
                        Total time: 23230.59s
                               ETA: 1196875.8s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.503s, learning 0.164s)
               Value function loss: 1.5760
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 372.52
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 11.67s
                        Total time: 23242.26s
                               ETA: 1196836.1s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.876s, learning 0.208s)
               Value function loss: 1.8846
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 373.22
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 11.08s
                        Total time: 23253.34s
                               ETA: 1196766.4s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.352s, learning 0.231s)
               Value function loss: 0.8714
                    Surrogate loss: 0.0101
             Mean action noise std: 0.73
                       Mean reward: 373.38
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 11.58s
                        Total time: 23264.93s
                               ETA: 1196722.4s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.118s, learning 0.179s)
               Value function loss: 1.3616
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 373.75
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 11.30s
                        Total time: 23276.22s
                               ETA: 1196663.8s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.317s, learning 0.207s)
               Value function loss: 1.2975
                    Surrogate loss: -0.0000
             Mean action noise std: 0.73
                       Mean reward: 374.04
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 11.52s
                        Total time: 23287.75s
                               ETA: 1196616.9s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.490s, learning 0.165s)
               Value function loss: 0.9570
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 374.20
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 11.65s
                        Total time: 23299.40s
                               ETA: 1196576.8s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.112s, learning 0.232s)
               Value function loss: 1.8914
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 375.07
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 11.34s
                        Total time: 23310.75s
                               ETA: 1196520.7s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.263s, learning 0.176s)
               Value function loss: 3.7564
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 376.30
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 11.44s
                        Total time: 23322.19s
                               ETA: 1196469.6s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.261s, learning 0.190s)
               Value function loss: 3.8104
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 377.00
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 11.45s
                        Total time: 23333.64s
                               ETA: 1196419.1s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.569s, learning 0.177s)
               Value function loss: 2.9440
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 378.49
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 11.75s
                        Total time: 23345.38s
                               ETA: 1196383.8s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.321s, learning 0.167s)
               Value function loss: 7.1167
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 380.52
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 11.49s
                        Total time: 23356.87s
                               ETA: 1196335.2s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.483s, learning 0.202s)
               Value function loss: 5.2021
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 381.07
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 11.69s
                        Total time: 23368.56s
                               ETA: 1196296.8s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.473s, learning 0.212s)
               Value function loss: 5.3292
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 381.40
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 11.68s
                        Total time: 23380.24s
                               ETA: 1196258.4s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.501s, learning 0.168s)
               Value function loss: 6.1585
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 381.58
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 11.67s
                        Total time: 23391.91s
                               ETA: 1196219.3s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.333s, learning 0.264s)
               Value function loss: 5.8639
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 381.08
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 11.60s
                        Total time: 23403.51s
                               ETA: 1196176.5s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.266s, learning 0.239s)
               Value function loss: 7.7379
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 381.38
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 11.51s
                        Total time: 23415.01s
                               ETA: 1196129.0s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.184s, learning 0.188s)
               Value function loss: 6.4817
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 381.26
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 11.37s
                        Total time: 23426.38s
                               ETA: 1196074.8s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.383s, learning 0.185s)
               Value function loss: 7.2456
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 381.35
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 11.57s
                        Total time: 23437.95s
                               ETA: 1196030.5s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.927s, learning 0.213s)
               Value function loss: 10.4314
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 380.48
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 11.14s
                        Total time: 23449.09s
                               ETA: 1195964.5s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.466s, learning 0.160s)
               Value function loss: 10.5550
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 380.62
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 11.63s
                        Total time: 23460.72s
                               ETA: 1195923.4s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.167s, learning 0.191s)
               Value function loss: 20.1572
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 382.48
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 11.36s
                        Total time: 23472.07s
                               ETA: 1195868.6s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.539s, learning 0.199s)
               Value function loss: 26.6952
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 381.63
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 11.74s
                        Total time: 23483.81s
                               ETA: 1195833.2s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.715s, learning 0.213s)
               Value function loss: 23.9427
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 381.02
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 11.93s
                        Total time: 23495.74s
                               ETA: 1195807.6s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.627s, learning 0.235s)
               Value function loss: 37.4341
                    Surrogate loss: 0.0029
             Mean action noise std: 0.73
                       Mean reward: 381.99
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 11.86s
                        Total time: 23507.60s
                               ETA: 1195778.5s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1484 steps/s (collection: 10.839s, learning 0.199s)
               Value function loss: 48.7097
                    Surrogate loss: 0.0200
             Mean action noise std: 0.73
                       Mean reward: 380.69
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 11.04s
                        Total time: 23518.64s
                               ETA: 1195707.6s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.050s, learning 0.162s)
               Value function loss: 54.3164
                    Surrogate loss: 0.0167
             Mean action noise std: 0.73
                       Mean reward: 382.19
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 11.21s
                        Total time: 23529.85s
                               ETA: 1195645.6s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.217s, learning 0.161s)
               Value function loss: 40.1109
                    Surrogate loss: -0.0014
             Mean action noise std: 0.73
                       Mean reward: 380.72
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 11.38s
                        Total time: 23541.23s
                               ETA: 1195592.1s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.320s, learning 0.161s)
               Value function loss: 25.8617
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 380.27
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 11.48s
                        Total time: 23552.71s
                               ETA: 1195543.9s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.973s, learning 0.161s)
               Value function loss: 19.9878
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 380.17
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 11.13s
                        Total time: 23563.84s
                               ETA: 1195478.1s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.161s)
               Value function loss: 10.3542
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 380.78
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 11.54s
                        Total time: 23575.38s
                               ETA: 1195432.9s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.113s, learning 0.167s)
               Value function loss: 3.6363
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 380.32
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 11.28s
                        Total time: 23586.66s
                               ETA: 1195374.6s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.358s, learning 0.220s)
               Value function loss: 2.3793
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 379.99
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 11.58s
                        Total time: 23598.24s
                               ETA: 1195331.4s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.108s, learning 0.191s)
               Value function loss: 2.5297
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: 379.69
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 11.30s
                        Total time: 23609.54s
                               ETA: 1195274.2s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.392s, learning 0.201s)
               Value function loss: 2.0412
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 379.13
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 11.59s
                        Total time: 23621.13s
                               ETA: 1195231.8s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.209s, learning 0.233s)
               Value function loss: 2.1454
                    Surrogate loss: 0.0081
             Mean action noise std: 0.73
                       Mean reward: 378.98
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 11.44s
                        Total time: 23632.58s
                               ETA: 1195181.8s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.304s, learning 0.165s)
               Value function loss: 1.8150
                    Surrogate loss: 0.0021
             Mean action noise std: 0.73
                       Mean reward: 378.46
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 11.47s
                        Total time: 23644.04s
                               ETA: 1195133.3s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.333s, learning 0.216s)
               Value function loss: 1.5322
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 377.87
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 11.55s
                        Total time: 23655.59s
                               ETA: 1195088.8s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.277s, learning 0.194s)
               Value function loss: 1.8732
                    Surrogate loss: -0.0000
             Mean action noise std: 0.73
                       Mean reward: 377.37
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 11.47s
                        Total time: 23667.06s
                               ETA: 1195040.5s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.561s, learning 0.319s)
               Value function loss: 4.0487
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 376.24
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 11.88s
                        Total time: 23678.94s
                               ETA: 1195012.8s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.094s, learning 0.155s)
               Value function loss: 5.0068
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 375.31
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 11.25s
                        Total time: 23690.19s
                               ETA: 1194953.3s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.982s, learning 0.193s)
               Value function loss: 3.9490
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 372.99
               Mean episode length: 248.72
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 11.17s
                        Total time: 23701.37s
                               ETA: 1194890.1s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.360s, learning 0.299s)
               Value function loss: 7.4450
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 372.25
               Mean episode length: 248.72
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 11.66s
                        Total time: 23713.03s
                               ETA: 1194851.4s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.272s, learning 0.177s)
               Value function loss: 5.8562
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 372.48
               Mean episode length: 248.72
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 11.45s
                        Total time: 23724.48s
                               ETA: 1194802.2s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.078s, learning 0.213s)
               Value function loss: 5.7630
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 371.85
               Mean episode length: 248.72
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 11.29s
                        Total time: 23735.77s
                               ETA: 1194745.0s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.454s, learning 0.203s)
               Value function loss: 5.5779
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 371.52
               Mean episode length: 248.72
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 11.66s
                        Total time: 23747.43s
                               ETA: 1194706.3s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1495 steps/s (collection: 10.790s, learning 0.169s)
               Value function loss: 7.5578
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 372.77
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 10.96s
                        Total time: 23758.38s
                               ETA: 1194632.5s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.335s, learning 0.166s)
               Value function loss: 8.5295
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 370.26
               Mean episode length: 248.26
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 11.50s
                        Total time: 23769.89s
                               ETA: 1194586.0s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.293s, learning 0.229s)
               Value function loss: 7.4664
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 370.77
               Mean episode length: 247.85
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 11.52s
                        Total time: 23781.41s
                               ETA: 1194540.6s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.238s, learning 0.188s)
               Value function loss: 7.3946
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 371.76
               Mean episode length: 247.85
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 11.43s
                        Total time: 23792.83s
                               ETA: 1194490.4s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.400s, learning 0.166s)
               Value function loss: 10.2497
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 372.80
               Mean episode length: 247.83
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 11.57s
                        Total time: 23804.40s
                               ETA: 1194447.2s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.166s)
               Value function loss: 14.9681
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 375.59
               Mean episode length: 249.14
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 11.73s
                        Total time: 23816.13s
                               ETA: 1194412.4s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.138s, learning 0.199s)
               Value function loss: 20.4250
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 377.64
               Mean episode length: 249.90
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 11.34s
                        Total time: 23827.47s
                               ETA: 1194357.8s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.016s, learning 0.182s)
               Value function loss: 27.2162
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 378.44
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 11.20s
                        Total time: 23838.66s
                               ETA: 1194296.3s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.119s, learning 0.199s)
               Value function loss: 23.9243
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 378.53
               Mean episode length: 249.68
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 11.32s
                        Total time: 23849.98s
                               ETA: 1194240.9s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.231s, learning 0.244s)
               Value function loss: 31.1520
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 379.02
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 11.47s
                        Total time: 23861.46s
                               ETA: 1194193.4s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.433s, learning 0.194s)
               Value function loss: 41.7261
                    Surrogate loss: 0.0166
             Mean action noise std: 0.73
                       Mean reward: 379.00
               Mean episode length: 249.72
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 11.63s
                        Total time: 23873.08s
                               ETA: 1194153.6s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.833s, learning 0.181s)
               Value function loss: 43.5069
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 379.32
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 11.01s
                        Total time: 23884.10s
                               ETA: 1194083.1s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.084s, learning 0.179s)
               Value function loss: 28.6895
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 378.02
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 11.26s
                        Total time: 23895.36s
                               ETA: 1194025.1s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.080s, learning 0.252s)
               Value function loss: 16.4747
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 377.05
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 11.33s
                        Total time: 23906.69s
                               ETA: 1193970.6s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.530s, learning 0.163s)
               Value function loss: 10.8344
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 372.91
               Mean episode length: 248.74
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 11.69s
                        Total time: 23918.39s
                               ETA: 1193934.2s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.681s, learning 0.188s)
               Value function loss: 6.4808
                    Surrogate loss: -0.0180
             Mean action noise std: 0.73
                       Mean reward: 373.24
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 11.87s
                        Total time: 23930.25s
                               ETA: 1193906.5s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.397s, learning 0.200s)
               Value function loss: 2.4891
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 371.91
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 11.60s
                        Total time: 23941.85s
                               ETA: 1193865.4s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.622s, learning 0.197s)
               Value function loss: 1.1882
                    Surrogate loss: 0.0195
             Mean action noise std: 0.73
                       Mean reward: 370.94
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 11.82s
                        Total time: 23953.67s
                               ETA: 1193835.3s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.582s, learning 0.196s)
               Value function loss: 1.2191
                    Surrogate loss: 0.0083
             Mean action noise std: 0.73
                       Mean reward: 370.21
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 11.78s
                        Total time: 23965.45s
                               ETA: 1193803.2s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.118s, learning 0.179s)
               Value function loss: 0.8109
                    Surrogate loss: 0.0090
             Mean action noise std: 0.73
                       Mean reward: 369.45
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 11.30s
                        Total time: 23976.75s
                               ETA: 1193747.2s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.145s, learning 0.217s)
               Value function loss: 0.7007
                    Surrogate loss: 0.0092
             Mean action noise std: 0.73
                       Mean reward: 368.48
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 11.36s
                        Total time: 23988.11s
                               ETA: 1193694.5s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.666s, learning 0.245s)
               Value function loss: 1.0552
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 366.61
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 11.91s
                        Total time: 24000.02s
                               ETA: 1193669.1s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.384s, learning 0.185s)
               Value function loss: 1.0213
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: 364.24
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 11.57s
                        Total time: 24011.59s
                               ETA: 1193626.8s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.150s, learning 0.193s)
               Value function loss: 0.9015
                    Surrogate loss: 0.0093
             Mean action noise std: 0.73
                       Mean reward: 363.76
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 11.34s
                        Total time: 24022.93s
                               ETA: 1193573.2s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.245s, learning 0.219s)
               Value function loss: 1.6213
                    Surrogate loss: 0.0018
             Mean action noise std: 0.73
                       Mean reward: 360.08
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 11.46s
                        Total time: 24034.39s
                               ETA: 1193525.7s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.885s, learning 0.203s)
               Value function loss: 2.7011
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 353.41
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 12.09s
                        Total time: 24046.48s
                               ETA: 1193509.1s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.007s, learning 0.167s)
               Value function loss: 1.8338
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 348.75
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 11.17s
                        Total time: 24057.66s
                               ETA: 1193447.3s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.219s, learning 0.177s)
               Value function loss: 2.9115
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 343.84
               Mean episode length: 249.80
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 11.40s
                        Total time: 24069.05s
                               ETA: 1193396.5s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.245s, learning 0.196s)
               Value function loss: 1.9551
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 341.67
               Mean episode length: 249.80
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 11.44s
                        Total time: 24080.49s
                               ETA: 1193348.0s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.343s, learning 0.203s)
               Value function loss: 2.4474
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 339.53
               Mean episode length: 249.03
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 11.55s
                        Total time: 24092.04s
                               ETA: 1193304.7s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.390s, learning 0.209s)
               Value function loss: 2.2847
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 340.15
               Mean episode length: 249.03
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 11.60s
                        Total time: 24103.64s
                               ETA: 1193264.1s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.190s, learning 0.229s)
               Value function loss: 2.5737
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 341.14
               Mean episode length: 249.23
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 11.42s
                        Total time: 24115.06s
                               ETA: 1193214.5s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.618s, learning 0.179s)
               Value function loss: 4.0744
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 346.38
               Mean episode length: 249.23
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 11.80s
                        Total time: 24126.86s
                               ETA: 1193183.8s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.438s, learning 0.172s)
               Value function loss: 3.8977
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 350.57
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 11.61s
                        Total time: 24138.46s
                               ETA: 1193143.7s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1492 steps/s (collection: 10.804s, learning 0.170s)
               Value function loss: 4.6718
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 353.56
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 10.97s
                        Total time: 24149.44s
                               ETA: 1193072.4s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.248s, learning 0.171s)
               Value function loss: 5.6708
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 355.38
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 11.42s
                        Total time: 24160.86s
                               ETA: 1193023.0s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.575s, learning 0.181s)
               Value function loss: 10.6370
                    Surrogate loss: -0.0005
             Mean action noise std: 0.73
                       Mean reward: 364.23
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 11.76s
                        Total time: 24172.61s
                               ETA: 1192990.3s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.908s, learning 0.171s)
               Value function loss: 14.3693
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 370.67
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 12.08s
                        Total time: 24184.69s
                               ETA: 1192973.6s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.453s, learning 0.217s)
               Value function loss: 21.3954
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 373.69
               Mean episode length: 249.52
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 11.67s
                        Total time: 24196.36s
                               ETA: 1192936.7s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.553s, learning 0.195s)
               Value function loss: 21.4487
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 376.22
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 11.75s
                        Total time: 24208.11s
                               ETA: 1192903.7s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.402s, learning 0.240s)
               Value function loss: 28.3139
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 378.04
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 11.64s
                        Total time: 24219.75s
                               ETA: 1192865.4s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.197s, learning 0.170s)
               Value function loss: 44.3097
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: 382.01
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 11.37s
                        Total time: 24231.12s
                               ETA: 1192813.7s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.522s, learning 0.207s)
               Value function loss: 69.7741
                    Surrogate loss: 0.0062
             Mean action noise std: 0.73
                       Mean reward: 383.80
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 11.73s
                        Total time: 24242.85s
                               ETA: 1192779.8s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.663s, learning 0.197s)
               Value function loss: 57.2106
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 386.42
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 11.86s
                        Total time: 24254.71s
                               ETA: 1192752.4s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.196s, learning 0.194s)
               Value function loss: 34.6367
                    Surrogate loss: 0.0096
             Mean action noise std: 0.73
                       Mean reward: 386.80
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 11.39s
                        Total time: 24266.10s
                               ETA: 1192701.9s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.516s, learning 0.191s)
               Value function loss: 28.2312
                    Surrogate loss: 0.0033
             Mean action noise std: 0.73
                       Mean reward: 387.76
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 11.71s
                        Total time: 24277.81s
                               ETA: 1192667.0s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.358s, learning 0.199s)
               Value function loss: 12.7399
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 387.74
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 11.56s
                        Total time: 24289.36s
                               ETA: 1192624.8s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.320s, learning 0.231s)
               Value function loss: 5.5103
                    Surrogate loss: -0.0177
             Mean action noise std: 0.73
                       Mean reward: 387.94
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 11.55s
                        Total time: 24300.91s
                               ETA: 1192582.3s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.109s, learning 0.166s)
               Value function loss: 2.3051
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 387.59
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 11.27s
                        Total time: 24312.19s
                               ETA: 1192526.3s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.669s, learning 0.156s)
               Value function loss: 2.1195
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 387.00
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 11.83s
                        Total time: 24324.01s
                               ETA: 1192497.3s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.162s)
               Value function loss: 2.0559
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 386.81
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 11.40s
                        Total time: 24335.42s
                               ETA: 1192447.6s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.128s, learning 0.200s)
               Value function loss: 1.1173
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 386.64
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 11.33s
                        Total time: 24346.74s
                               ETA: 1192394.3s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.792s, learning 0.162s)
               Value function loss: 1.9259
                    Surrogate loss: 0.0076
             Mean action noise std: 0.73
                       Mean reward: 386.12
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 11.95s
                        Total time: 24358.70s
                               ETA: 1192371.6s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.224s, learning 0.165s)
               Value function loss: 1.4604
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 385.53
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 11.39s
                        Total time: 24370.09s
                               ETA: 1192321.4s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.007s, learning 0.216s)
               Value function loss: 1.2451
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 385.33
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 11.22s
                        Total time: 24381.31s
                               ETA: 1192263.0s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.405s, learning 0.193s)
               Value function loss: 2.3219
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 384.91
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 11.60s
                        Total time: 24392.91s
                               ETA: 1192223.1s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.463s, learning 0.179s)
               Value function loss: 4.8422
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 382.51
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 11.64s
                        Total time: 24404.55s
                               ETA: 1192185.3s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.538s, learning 0.185s)
               Value function loss: 4.3583
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 379.73
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 11.72s
                        Total time: 24416.27s
                               ETA: 1192151.6s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.890s, learning 0.191s)
               Value function loss: 5.5287
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 377.10
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 12.08s
                        Total time: 24428.35s
                               ETA: 1192135.3s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.428s, learning 0.213s)
               Value function loss: 5.8550
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 374.64
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 11.64s
                        Total time: 24439.99s
                               ETA: 1192097.5s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.778s, learning 0.168s)
               Value function loss: 5.0522
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 373.60
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 11.95s
                        Total time: 24451.94s
                               ETA: 1192074.6s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.203s, learning 0.188s)
               Value function loss: 4.9279
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 372.66
               Mean episode length: 249.75
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 11.39s
                        Total time: 24463.33s
                               ETA: 1192024.7s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.110s, learning 0.230s)
               Value function loss: 4.9056
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 371.00
               Mean episode length: 249.75
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 11.34s
                        Total time: 24474.67s
                               ETA: 1191972.4s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.370s, learning 0.184s)
               Value function loss: 8.3432
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 369.34
               Mean episode length: 249.75
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 11.55s
                        Total time: 24486.22s
                               ETA: 1191930.5s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.200s, learning 0.241s)
               Value function loss: 7.9736
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 367.23
               Mean episode length: 249.75
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 11.44s
                        Total time: 24497.66s
                               ETA: 1191883.1s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.206s, learning 0.193s)
               Value function loss: 8.4748
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 366.03
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 11.40s
                        Total time: 24509.06s
                               ETA: 1191833.7s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.286s, learning 0.192s)
               Value function loss: 5.9562
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 363.95
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 11.48s
                        Total time: 24520.54s
                               ETA: 1191788.3s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.432s, learning 0.160s)
               Value function loss: 12.3866
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 363.77
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 11.59s
                        Total time: 24532.13s
                               ETA: 1191748.4s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.903s, learning 0.161s)
               Value function loss: 15.7516
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 363.70
               Mean episode length: 249.83
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 12.06s
                        Total time: 24544.20s
                               ETA: 1191731.4s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.444s, learning 0.214s)
               Value function loss: 23.6864
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 365.34
               Mean episode length: 249.83
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 11.66s
                        Total time: 24555.85s
                               ETA: 1191694.8s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.671s, learning 0.159s)
               Value function loss: 23.5100
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 370.45
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 11.83s
                        Total time: 24567.68s
                               ETA: 1191666.5s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.263s, learning 0.164s)
               Value function loss: 29.8045
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 370.01
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 11.43s
                        Total time: 24579.11s
                               ETA: 1191618.7s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.755s, learning 0.165s)
               Value function loss: 46.7077
                    Surrogate loss: 0.0059
             Mean action noise std: 0.73
                       Mean reward: 372.46
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 11.92s
                        Total time: 24591.03s
                               ETA: 1191594.9s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.126s, learning 0.155s)
               Value function loss: 75.9491
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 373.59
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 11.28s
                        Total time: 24602.31s
                               ETA: 1191540.1s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.296s, learning 0.230s)
               Value function loss: 49.2453
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 374.58
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 11.53s
                        Total time: 24613.84s
                               ETA: 1191497.1s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.376s, learning 0.192s)
               Value function loss: 30.0389
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 378.32
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 11.57s
                        Total time: 24625.41s
                               ETA: 1191456.3s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.692s, learning 0.159s)
               Value function loss: 17.8871
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 379.21
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 11.85s
                        Total time: 24637.26s
                               ETA: 1191429.1s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.570s, learning 0.201s)
               Value function loss: 13.8064
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 380.80
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 11.77s
                        Total time: 24649.03s
                               ETA: 1191398.1s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.448s, learning 0.211s)
               Value function loss: 6.7514
                    Surrogate loss: -0.0186
             Mean action noise std: 0.73
                       Mean reward: 377.67
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 11.66s
                        Total time: 24660.69s
                               ETA: 1191361.7s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.479s, learning 0.212s)
               Value function loss: 2.9952
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 376.84
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 11.69s
                        Total time: 24672.38s
                               ETA: 1191326.9s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.729s, learning 0.278s)
               Value function loss: 1.7372
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 377.54
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 12.01s
                        Total time: 24684.39s
                               ETA: 1191307.4s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.363s, learning 0.285s)
               Value function loss: 2.0690
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 377.61
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 11.65s
                        Total time: 24696.03s
                               ETA: 1191270.5s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.849s, learning 0.257s)
               Value function loss: 1.1918
                    Surrogate loss: -0.0176
             Mean action noise std: 0.73
                       Mean reward: 377.43
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 12.11s
                        Total time: 24708.14s
                               ETA: 1191255.8s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.310s, learning 0.269s)
               Value function loss: 1.4609
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 376.62
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 11.58s
                        Total time: 24719.72s
                               ETA: 1191215.6s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.294s, learning 0.239s)
               Value function loss: 1.8364
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 375.69
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 11.53s
                        Total time: 24731.25s
                               ETA: 1191173.2s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.729s, learning 0.246s)
               Value function loss: 1.5336
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 375.03
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 11.98s
                        Total time: 24743.23s
                               ETA: 1191152.2s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.294s, learning 0.165s)
               Value function loss: 2.6352
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 373.78
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 11.46s
                        Total time: 24754.68s
                               ETA: 1191106.4s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.173s, learning 0.191s)
               Value function loss: 4.2809
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 370.86
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 11.36s
                        Total time: 24766.05s
                               ETA: 1191056.0s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.935s, learning 0.200s)
               Value function loss: 3.3947
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 364.75
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 12.13s
                        Total time: 24778.18s
                               ETA: 1191042.7s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.450s, learning 0.202s)
               Value function loss: 3.3989
                    Surrogate loss: 0.0016
             Mean action noise std: 0.73
                       Mean reward: 360.65
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 11.65s
                        Total time: 24789.83s
                               ETA: 1191006.3s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.424s, learning 0.217s)
               Value function loss: 4.8973
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 350.82
               Mean episode length: 250.00
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 11.64s
                        Total time: 24801.48s
                               ETA: 1190969.3s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.471s, learning 0.159s)
               Value function loss: 3.6287
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 345.53
               Mean episode length: 250.00
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 11.63s
                        Total time: 24813.11s
                               ETA: 1190931.8s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.346s, learning 0.226s)
               Value function loss: 3.3433
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 341.75
               Mean episode length: 250.00
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 11.57s
                        Total time: 24824.68s
                               ETA: 1190891.6s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.479s, learning 0.279s)
               Value function loss: 3.9501
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 335.20
               Mean episode length: 250.00
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 11.76s
                        Total time: 24836.44s
                               ETA: 1190860.3s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.162s)
               Value function loss: 3.9393
                    Surrogate loss: 0.0097
             Mean action noise std: 0.73
                       Mean reward: 327.21
               Mean episode length: 249.71
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 11.68s
                        Total time: 24848.11s
                               ETA: 1190825.1s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.349s, learning 0.162s)
               Value function loss: 4.2341
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: 316.19
               Mean episode length: 249.39
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 11.51s
                        Total time: 24859.62s
                               ETA: 1190782.0s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.552s, learning 0.159s)
               Value function loss: 3.8881
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 313.28
               Mean episode length: 249.03
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 11.71s
                        Total time: 24871.33s
                               ETA: 1190748.5s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.442s, learning 0.160s)
               Value function loss: 3.9763
                    Surrogate loss: 0.0055
             Mean action noise std: 0.73
                       Mean reward: 305.53
               Mean episode length: 248.52
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 11.60s
                        Total time: 24882.94s
                               ETA: 1190709.8s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.426s, learning 0.170s)
               Value function loss: 4.8776
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 298.88
               Mean episode length: 246.80
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 11.60s
                        Total time: 24894.53s
                               ETA: 1190670.9s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.447s, learning 0.161s)
               Value function loss: 4.8329
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 295.42
               Mean episode length: 246.30
                  Mean reward/step: 0.87
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 11.61s
                        Total time: 24906.14s
                               ETA: 1190632.6s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.412s, learning 0.230s)
               Value function loss: 7.1137
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 291.37
               Mean episode length: 247.76
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 11.64s
                        Total time: 24917.78s
                               ETA: 1190595.9s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.171s, learning 0.158s)
               Value function loss: 8.0234
                    Surrogate loss: 0.0111
             Mean action noise std: 0.73
                       Mean reward: 281.23
               Mean episode length: 247.34
                  Mean reward/step: 0.85
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 11.33s
                        Total time: 24929.11s
                               ETA: 1190544.3s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.581s, learning 0.165s)
               Value function loss: 7.8249
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 271.32
               Mean episode length: 244.89
                  Mean reward/step: 0.86
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 11.75s
                        Total time: 24940.86s
                               ETA: 1190512.7s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.320s, learning 0.158s)
               Value function loss: 10.7110
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 279.18
               Mean episode length: 249.30
                  Mean reward/step: 0.89
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 11.48s
                        Total time: 24952.34s
                               ETA: 1190468.3s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.255s, learning 0.157s)
               Value function loss: 15.8681
                    Surrogate loss: -0.0132
             Mean action noise std: 0.73
                       Mean reward: 281.04
               Mean episode length: 248.49
                  Mean reward/step: 0.90
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 11.41s
                        Total time: 24963.75s
                               ETA: 1190420.7s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.747s, learning 0.170s)
               Value function loss: 16.3674
                    Surrogate loss: 0.0187
             Mean action noise std: 0.73
                       Mean reward: 273.20
               Mean episode length: 249.27
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 11.92s
                        Total time: 24975.67s
                               ETA: 1190397.3s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.372s, learning 0.161s)
               Value function loss: 10.6226
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 271.30
               Mean episode length: 249.42
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 11.53s
                        Total time: 24987.20s
                               ETA: 1190355.6s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.714s, learning 0.199s)
               Value function loss: 7.6890
                    Surrogate loss: -0.0207
             Mean action noise std: 0.73
                       Mean reward: 275.95
               Mean episode length: 248.46
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 11.91s
                        Total time: 24999.11s
                               ETA: 1190332.0s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.702s, learning 0.174s)
               Value function loss: 5.1069
                    Surrogate loss: -0.0199
             Mean action noise std: 0.73
                       Mean reward: 269.90
               Mean episode length: 249.69
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 11.88s
                        Total time: 25010.99s
                               ETA: 1190306.7s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.137s, learning 0.174s)
               Value function loss: 2.8243
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 272.08
               Mean episode length: 249.11
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 11.31s
                        Total time: 25022.30s
                               ETA: 1190254.5s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.372s, learning 0.158s)
               Value function loss: 1.2627
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 271.01
               Mean episode length: 248.99
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 11.53s
                        Total time: 25033.83s
                               ETA: 1190212.7s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.143s, learning 0.168s)
               Value function loss: 0.7059
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 271.52
               Mean episode length: 248.75
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 11.31s
                        Total time: 25045.14s
                               ETA: 1190160.6s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.044s, learning 0.169s)
               Value function loss: 0.8171
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 274.97
               Mean episode length: 249.06
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 11.21s
                        Total time: 25056.35s
                               ETA: 1190103.8s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.287s, learning 0.163s)
               Value function loss: 1.0402
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 274.42
               Mean episode length: 248.35
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 11.45s
                        Total time: 25067.80s
                               ETA: 1190058.4s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.442s, learning 0.163s)
               Value function loss: 1.0334
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 274.03
               Mean episode length: 247.46
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 11.60s
                        Total time: 25079.41s
                               ETA: 1190020.3s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1480 steps/s (collection: 10.880s, learning 0.190s)
               Value function loss: 1.1503
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 274.17
               Mean episode length: 247.18
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 11.07s
                        Total time: 25090.48s
                               ETA: 1189956.9s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.343s, learning 0.158s)
               Value function loss: 0.9560
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 275.29
               Mean episode length: 246.59
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 11.50s
                        Total time: 25101.98s
                               ETA: 1189914.0s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.300s, learning 0.161s)
               Value function loss: 2.1228
                    Surrogate loss: 0.0059
             Mean action noise std: 0.73
                       Mean reward: 277.55
               Mean episode length: 245.75
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 11.46s
                        Total time: 25113.44s
                               ETA: 1189869.2s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.078s, learning 0.198s)
               Value function loss: 3.4095
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 284.02
               Mean episode length: 245.44
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 11.28s
                        Total time: 25124.72s
                               ETA: 1189815.6s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1337 steps/s (collection: 11.989s, learning 0.257s)
               Value function loss: 3.7644
                    Surrogate loss: 0.0054
             Mean action noise std: 0.73
                       Mean reward: 294.47
               Mean episode length: 245.28
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 12.25s
                        Total time: 25136.96s
                               ETA: 1189808.1s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.628s, learning 0.161s)
               Value function loss: 3.3742
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 302.18
               Mean episode length: 245.19
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 11.79s
                        Total time: 25148.75s
                               ETA: 1189778.9s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.300s, learning 0.166s)
               Value function loss: 6.5531
                    Surrogate loss: 0.0060
             Mean action noise std: 0.73
                       Mean reward: 313.84
               Mean episode length: 245.76
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 11.47s
                        Total time: 25160.22s
                               ETA: 1189734.4s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.229s, learning 0.174s)
               Value function loss: 5.4283
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 320.12
               Mean episode length: 245.62
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 11.40s
                        Total time: 25171.62s
                               ETA: 1189687.1s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.561s, learning 0.188s)
               Value function loss: 5.9118
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: 322.22
               Mean episode length: 244.52
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 11.75s
                        Total time: 25183.37s
                               ETA: 1189656.1s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.363s, learning 0.183s)
               Value function loss: 8.7573
                    Surrogate loss: 0.0449
             Mean action noise std: 0.73
                       Mean reward: 314.32
               Mean episode length: 239.24
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 11.55s
                        Total time: 25194.92s
                               ETA: 1189615.5s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.978s, learning 0.160s)
               Value function loss: 10.6662
                    Surrogate loss: -0.0001
             Mean action noise std: 0.73
                       Mean reward: 304.60
               Mean episode length: 232.55
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 11.14s
                        Total time: 25206.05s
                               ETA: 1189555.7s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.277s, learning 0.214s)
               Value function loss: 12.0518
                    Surrogate loss: 0.0244
             Mean action noise std: 0.73
                       Mean reward: 296.89
               Mean episode length: 226.69
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 11.49s
                        Total time: 25217.55s
                               ETA: 1189512.6s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.666s, learning 0.168s)
               Value function loss: 13.4676
                    Surrogate loss: 0.0090
             Mean action noise std: 0.73
                       Mean reward: 287.00
               Mean episode length: 221.69
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 11.83s
                        Total time: 25229.38s
                               ETA: 1189485.7s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.172s, learning 0.165s)
               Value function loss: 15.8418
                    Surrogate loss: 0.0127
             Mean action noise std: 0.73
                       Mean reward: 273.54
               Mean episode length: 215.32
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 11.34s
                        Total time: 25240.72s
                               ETA: 1189435.3s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1490 steps/s (collection: 10.835s, learning 0.158s)
               Value function loss: 17.4213
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: 293.62
               Mean episode length: 220.93
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 10.99s
                        Total time: 25251.71s
                               ETA: 1189368.8s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.153s, learning 0.242s)
               Value function loss: 20.0763
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: 288.93
               Mean episode length: 224.42
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 11.39s
                        Total time: 25263.10s
                               ETA: 1189321.3s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.163s, learning 0.190s)
               Value function loss: 37.7583
                    Surrogate loss: 0.0101
             Mean action noise std: 0.73
                       Mean reward: 302.74
               Mean episode length: 229.67
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 11.35s
                        Total time: 25274.46s
                               ETA: 1189271.9s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.176s, learning 0.191s)
               Value function loss: 26.6500
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 300.12
               Mean episode length: 229.46
                  Mean reward/step: 0.93
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 11.37s
                        Total time: 25285.82s
                               ETA: 1189223.1s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.368s, learning 0.169s)
               Value function loss: 25.2408
                    Surrogate loss: 0.0012
             Mean action noise std: 0.73
                       Mean reward: 305.31
               Mean episode length: 234.89
                  Mean reward/step: 0.91
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 11.54s
                        Total time: 25297.36s
                               ETA: 1189182.4s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.554s, learning 0.197s)
               Value function loss: 18.8492
                    Surrogate loss: 0.0207
             Mean action noise std: 0.73
                       Mean reward: 309.07
               Mean episode length: 237.37
                  Mean reward/step: 0.92
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 11.75s
                        Total time: 25309.11s
                               ETA: 1189151.8s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.581s, learning 0.199s)
               Value function loss: 16.2117
                    Surrogate loss: 0.0042
             Mean action noise std: 0.73
                       Mean reward: 317.09
               Mean episode length: 245.54
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 11.78s
                        Total time: 25320.89s
                               ETA: 1189122.5s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.249s, learning 0.164s)
               Value function loss: 14.2882
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 313.06
               Mean episode length: 244.39
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 11.41s
                        Total time: 25332.30s
                               ETA: 1189076.0s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.278s, learning 0.163s)
               Value function loss: 9.4256
                    Surrogate loss: 0.0347
             Mean action noise std: 0.73
                       Mean reward: 312.45
               Mean episode length: 246.33
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 11.44s
                        Total time: 25343.75s
                               ETA: 1189031.0s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.400s, learning 0.163s)
               Value function loss: 6.8608
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 310.77
               Mean episode length: 245.51
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 11.56s
                        Total time: 25355.31s
                               ETA: 1188991.6s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.563s, learning 0.158s)
               Value function loss: 4.3414
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 293.68
               Mean episode length: 239.81
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 11.72s
                        Total time: 25367.03s
                               ETA: 1188959.6s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.001s, learning 0.162s)
               Value function loss: 3.1208
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 290.29
               Mean episode length: 238.54
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 11.16s
                        Total time: 25378.19s
                               ETA: 1188901.6s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.020s, learning 0.178s)
               Value function loss: 2.3563
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 283.51
               Mean episode length: 235.69
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 11.20s
                        Total time: 25389.39s
                               ETA: 1188845.2s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.461s, learning 0.170s)
               Value function loss: 1.7187
                    Surrogate loss: -0.0165
             Mean action noise std: 0.73
                       Mean reward: 269.43
               Mean episode length: 228.77
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 11.63s
                        Total time: 25401.02s
                               ETA: 1188809.1s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.204s, learning 0.158s)
               Value function loss: 1.7969
                    Surrogate loss: -0.0139
             Mean action noise std: 0.73
                       Mean reward: 260.16
               Mean episode length: 224.01
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 11.36s
                        Total time: 25412.38s
                               ETA: 1188760.5s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.038s, learning 0.230s)
               Value function loss: 1.9772
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 245.44
               Mean episode length: 216.22
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 11.27s
                        Total time: 25423.65s
                               ETA: 1188707.5s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.109s, learning 0.207s)
               Value function loss: 1.6945
                    Surrogate loss: -0.0157
             Mean action noise std: 0.73
                       Mean reward: 227.63
               Mean episode length: 207.62
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 11.32s
                        Total time: 25434.97s
                               ETA: 1188656.8s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.162s, learning 0.165s)
               Value function loss: 2.1818
                    Surrogate loss: 0.0022
             Mean action noise std: 0.73
                       Mean reward: 206.65
               Mean episode length: 196.50
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 11.33s
                        Total time: 25446.30s
                               ETA: 1188606.7s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.888s, learning 0.196s)
               Value function loss: 1.9535
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 197.58
               Mean episode length: 191.75
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 11.08s
                        Total time: 25457.38s
                               ETA: 1188545.2s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.297s, learning 0.286s)
               Value function loss: 2.2705
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 193.31
               Mean episode length: 189.78
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 11.58s
                        Total time: 25468.96s
                               ETA: 1188507.1s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.244s, learning 0.169s)
               Value function loss: 3.1480
                    Surrogate loss: 0.0028
             Mean action noise std: 0.73
                       Mean reward: 192.63
               Mean episode length: 190.35
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 11.41s
                        Total time: 25480.38s
                               ETA: 1188461.1s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.339s, learning 0.231s)
               Value function loss: 5.2923
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 207.53
               Mean episode length: 199.99
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 11.57s
                        Total time: 25491.95s
                               ETA: 1188422.4s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.878s, learning 0.168s)
               Value function loss: 7.1627
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 209.36
               Mean episode length: 201.74
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 12.05s
                        Total time: 25503.99s
                               ETA: 1188405.9s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.495s, learning 0.217s)
               Value function loss: 10.2948
                    Surrogate loss: 0.0473
             Mean action noise std: 0.73
                       Mean reward: 208.20
               Mean episode length: 201.29
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 11.71s
                        Total time: 25515.71s
                               ETA: 1188373.9s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.465s, learning 0.166s)
               Value function loss: 4.8356
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 204.24
               Mean episode length: 199.95
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 11.63s
                        Total time: 25527.34s
                               ETA: 1188338.2s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.518s, learning 0.218s)
               Value function loss: 4.5173
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 207.07
               Mean episode length: 202.32
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 11.74s
                        Total time: 25539.07s
                               ETA: 1188307.3s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.368s, learning 0.164s)
               Value function loss: 4.5627
                    Surrogate loss: 0.0328
             Mean action noise std: 0.73
                       Mean reward: 214.02
               Mean episode length: 205.23
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 11.53s
                        Total time: 25550.60s
                               ETA: 1188267.0s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.057s, learning 0.159s)
               Value function loss: 3.1938
                    Surrogate loss: 0.0010
             Mean action noise std: 0.73
                       Mean reward: 218.47
               Mean episode length: 208.85
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 11.22s
                        Total time: 25561.82s
                               ETA: 1188212.0s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.451s, learning 0.155s)
               Value function loss: 4.2252
                    Surrogate loss: 0.0104
             Mean action noise std: 0.73
                       Mean reward: 227.54
               Mean episode length: 213.54
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 11.61s
                        Total time: 25573.43s
                               ETA: 1188175.1s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.592s, learning 0.198s)
               Value function loss: 4.4955
                    Surrogate loss: -0.0004
             Mean action noise std: 0.73
                       Mean reward: 229.35
               Mean episode length: 213.90
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 11.79s
                        Total time: 25585.22s
                               ETA: 1188146.8s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.226s, learning 0.157s)
               Value function loss: 4.8991
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 234.44
               Mean episode length: 218.67
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 11.38s
                        Total time: 25596.60s
                               ETA: 1188099.7s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.177s, learning 0.164s)
               Value function loss: 5.8366
                    Surrogate loss: 0.0118
             Mean action noise std: 0.73
                       Mean reward: 242.13
               Mean episode length: 222.97
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 11.34s
                        Total time: 25607.94s
                               ETA: 1188050.6s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.165s, learning 0.227s)
               Value function loss: 5.4795
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 245.34
               Mean episode length: 225.51
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 11.39s
                        Total time: 25619.33s
                               ETA: 1188004.0s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.441s, learning 0.162s)
               Value function loss: 6.4301
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 259.27
               Mean episode length: 233.46
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 11.60s
                        Total time: 25630.94s
                               ETA: 1187967.1s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.533s, learning 0.165s)
               Value function loss: 8.7040
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 277.02
               Mean episode length: 241.18
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 11.70s
                        Total time: 25642.63s
                               ETA: 1187934.7s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.542s, learning 0.163s)
               Value function loss: 7.9924
                    Surrogate loss: -0.0149
             Mean action noise std: 0.73
                       Mean reward: 278.60
               Mean episode length: 242.43
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 11.71s
                        Total time: 25654.34s
                               ETA: 1187902.6s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.485s, learning 0.188s)
               Value function loss: 8.8142
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: 281.64
               Mean episode length: 244.13
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 11.67s
                        Total time: 25666.01s
                               ETA: 1187869.1s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.562s, learning 0.182s)
               Value function loss: 11.5092
                    Surrogate loss: 0.0068
             Mean action noise std: 0.73
                       Mean reward: 292.93
               Mean episode length: 246.23
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 11.74s
                        Total time: 25677.75s
                               ETA: 1187838.8s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.125s, learning 0.216s)
               Value function loss: 11.8936
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 302.08
               Mean episode length: 248.49
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 11.34s
                        Total time: 25689.10s
                               ETA: 1187790.0s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.538s, learning 0.237s)
               Value function loss: 7.4031
                    Surrogate loss: -0.0164
             Mean action noise std: 0.73
                       Mean reward: 300.01
               Mean episode length: 248.83
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 11.78s
                        Total time: 25700.87s
                               ETA: 1187761.2s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.271s, learning 0.161s)
               Value function loss: 6.4423
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 297.49
               Mean episode length: 249.25
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 11.43s
                        Total time: 25712.30s
                               ETA: 1187716.6s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.481s, learning 0.248s)
               Value function loss: 4.5846
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 302.56
               Mean episode length: 249.36
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 11.73s
                        Total time: 25724.03s
                               ETA: 1187685.8s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.117s, learning 0.181s)
               Value function loss: 3.7280
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 303.04
               Mean episode length: 249.70
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 11.30s
                        Total time: 25735.33s
                               ETA: 1187635.1s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.691s, learning 0.178s)
               Value function loss: 2.5325
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 303.20
               Mean episode length: 249.62
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 11.87s
                        Total time: 25747.20s
                               ETA: 1187610.8s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.286s, learning 0.174s)
               Value function loss: 1.9912
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 307.92
               Mean episode length: 249.67
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 11.46s
                        Total time: 25758.66s
                               ETA: 1187567.5s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.327s, learning 0.190s)
               Value function loss: 1.6257
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 310.71
               Mean episode length: 249.78
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 11.52s
                        Total time: 25770.17s
                               ETA: 1187527.0s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.697s, learning 0.184s)
               Value function loss: 2.4224
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 314.14
               Mean episode length: 249.90
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 11.88s
                        Total time: 25782.06s
                               ETA: 1187503.3s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.101s, learning 0.193s)
               Value function loss: 2.0153
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 318.49
               Mean episode length: 249.90
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 11.29s
                        Total time: 25793.35s
                               ETA: 1187452.5s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.385s, learning 0.234s)
               Value function loss: 3.0145
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 327.19
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 11.62s
                        Total time: 25804.97s
                               ETA: 1187416.8s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.749s, learning 0.231s)
               Value function loss: 2.9634
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 329.30
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 11.98s
                        Total time: 25816.95s
                               ETA: 1187397.7s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.408s, learning 0.178s)
               Value function loss: 4.5680
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: 331.91
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 11.59s
                        Total time: 25828.54s
                               ETA: 1187360.5s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.355s, learning 0.187s)
               Value function loss: 5.4538
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 338.61
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 11.54s
                        Total time: 25840.08s
                               ETA: 1187321.2s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.339s, learning 0.194s)
               Value function loss: 6.3121
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 340.56
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 11.53s
                        Total time: 25851.61s
                               ETA: 1187281.6s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.275s, learning 0.186s)
               Value function loss: 5.4363
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 343.44
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 11.46s
                        Total time: 25863.07s
                               ETA: 1187238.8s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.179s, learning 0.163s)
               Value function loss: 9.2916
                    Surrogate loss: 0.0100
             Mean action noise std: 0.73
                       Mean reward: 346.87
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 11.34s
                        Total time: 25874.41s
                               ETA: 1187190.4s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.594s, learning 0.203s)
               Value function loss: 9.1328
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 346.92
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 11.80s
                        Total time: 25886.21s
                               ETA: 1187163.0s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.297s, learning 0.159s)
               Value function loss: 8.3602
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 346.68
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 11.46s
                        Total time: 25897.67s
                               ETA: 1187120.0s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.358s, learning 0.161s)
               Value function loss: 8.6700
                    Surrogate loss: 0.0043
             Mean action noise std: 0.73
                       Mean reward: 345.12
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 11.52s
                        Total time: 25909.19s
                               ETA: 1187079.9s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.378s, learning 0.180s)
               Value function loss: 6.1904
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 345.29
               Mean episode length: 250.00
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 11.56s
                        Total time: 25920.74s
                               ETA: 1187041.5s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.666s, learning 0.167s)
               Value function loss: 8.9801
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 341.21
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 11.83s
                        Total time: 25932.58s
                               ETA: 1187015.8s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.352s, learning 0.200s)
               Value function loss: 10.9357
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 342.48
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 11.55s
                        Total time: 25944.13s
                               ETA: 1186977.3s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.571s, learning 0.266s)
               Value function loss: 10.7564
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 339.01
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 11.84s
                        Total time: 25955.97s
                               ETA: 1186951.8s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.353s, learning 0.169s)
               Value function loss: 10.2555
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 338.08
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 11.52s
                        Total time: 25967.49s
                               ETA: 1186912.0s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.425s, learning 0.257s)
               Value function loss: 9.8540
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 337.53
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 11.68s
                        Total time: 25979.17s
                               ETA: 1186879.5s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.415s, learning 0.166s)
               Value function loss: 10.3078
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 331.60
               Mean episode length: 250.00
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 11.58s
                        Total time: 25990.75s
                               ETA: 1186842.4s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.344s, learning 0.196s)
               Value function loss: 11.8816
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 336.59
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 11.54s
                        Total time: 26002.29s
                               ETA: 1186803.4s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.650s, learning 0.168s)
               Value function loss: 10.5311
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 333.64
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 11.82s
                        Total time: 26014.11s
                               ETA: 1186777.1s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.653s, learning 0.189s)
               Value function loss: 10.6332
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 325.25
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 11.84s
                        Total time: 26025.95s
                               ETA: 1186751.9s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.499s, learning 0.198s)
               Value function loss: 11.2328
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 334.30
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 11.70s
                        Total time: 26037.65s
                               ETA: 1186720.1s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.568s, learning 0.191s)
               Value function loss: 9.8470
                    Surrogate loss: -0.0174
             Mean action noise std: 0.73
                       Mean reward: 323.97
               Mean episode length: 250.00
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 11.76s
                        Total time: 26049.41s
                               ETA: 1186691.3s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.934s, learning 0.279s)
               Value function loss: 7.9046
                    Surrogate loss: -0.0173
             Mean action noise std: 0.73
                       Mean reward: 316.09
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 12.21s
                        Total time: 26061.62s
                               ETA: 1186683.0s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.211s, learning 0.204s)
               Value function loss: 6.5596
                    Surrogate loss: -0.0163
             Mean action noise std: 0.73
                       Mean reward: 310.27
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 11.42s
                        Total time: 26073.04s
                               ETA: 1186638.5s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.571s, learning 0.258s)
               Value function loss: 5.1608
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 320.22
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 11.83s
                        Total time: 26084.87s
                               ETA: 1186612.8s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.488s, learning 0.190s)
               Value function loss: 3.5999
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 316.61
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 11.68s
                        Total time: 26096.54s
                               ETA: 1186580.2s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.819s, learning 0.161s)
               Value function loss: 2.4304
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 307.84
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 11.98s
                        Total time: 26108.52s
                               ETA: 1186561.4s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.656s, learning 0.185s)
               Value function loss: 2.1883
                    Surrogate loss: 0.0298
             Mean action noise std: 0.73
                       Mean reward: 314.33
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 11.84s
                        Total time: 26120.36s
                               ETA: 1186536.3s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.296s, learning 0.169s)
               Value function loss: 1.4189
                    Surrogate loss: 0.0217
             Mean action noise std: 0.73
                       Mean reward: 314.02
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 11.47s
                        Total time: 26131.83s
                               ETA: 1186494.2s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.895s, learning 0.210s)
               Value function loss: 2.7123
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 313.61
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 12.10s
                        Total time: 26143.93s
                               ETA: 1186481.0s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.170s, learning 0.157s)
               Value function loss: 1.5071
                    Surrogate loss: 0.0015
             Mean action noise std: 0.73
                       Mean reward: 308.75
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 11.33s
                        Total time: 26155.26s
                               ETA: 1186432.7s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.360s, learning 0.161s)
               Value function loss: 1.9705
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 308.64
               Mean episode length: 249.75
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 11.52s
                        Total time: 26166.78s
                               ETA: 1186393.1s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.138s, learning 0.210s)
               Value function loss: 3.1678
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 314.85
               Mean episode length: 249.75
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 11.35s
                        Total time: 26178.13s
                               ETA: 1186345.8s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.616s, learning 0.161s)
               Value function loss: 2.5128
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 305.62
               Mean episode length: 249.75
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 11.78s
                        Total time: 26189.91s
                               ETA: 1186317.9s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.046s, learning 0.163s)
               Value function loss: 3.5654
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 307.71
               Mean episode length: 249.75
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 11.21s
                        Total time: 26201.12s
                               ETA: 1186264.3s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.212s, learning 0.204s)
               Value function loss: 4.7426
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 311.12
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 11.42s
                        Total time: 26212.53s
                               ETA: 1186220.1s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.496s, learning 0.158s)
               Value function loss: 3.9930
                    Surrogate loss: 0.0547
             Mean action noise std: 0.73
                       Mean reward: 313.76
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 11.65s
                        Total time: 26224.19s
                               ETA: 1186186.7s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.385s, learning 0.196s)
               Value function loss: 5.6097
                    Surrogate loss: 0.0040
             Mean action noise std: 0.73
                       Mean reward: 322.54
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 11.58s
                        Total time: 26235.77s
                               ETA: 1186150.1s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.681s, learning 0.163s)
               Value function loss: 5.4109
                    Surrogate loss: 0.0121
             Mean action noise std: 0.73
                       Mean reward: 324.51
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 11.84s
                        Total time: 26247.61s
                               ETA: 1186125.3s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.654s, learning 0.171s)
               Value function loss: 4.7594
                    Surrogate loss: 0.0014
             Mean action noise std: 0.73
                       Mean reward: 319.23
               Mean episode length: 250.00
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 11.82s
                        Total time: 26259.44s
                               ETA: 1186099.7s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.604s, learning 0.160s)
               Value function loss: 6.6962
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 321.63
               Mean episode length: 250.00
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 11.76s
                        Total time: 26271.20s
                               ETA: 1186071.3s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.395s, learning 0.246s)
               Value function loss: 7.9389
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 324.91
               Mean episode length: 250.00
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 11.64s
                        Total time: 26282.84s
                               ETA: 1186037.4s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.616s, learning 0.173s)
               Value function loss: 7.3047
                    Surrogate loss: 0.0670
             Mean action noise std: 0.73
                       Mean reward: 327.55
               Mean episode length: 250.00
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 11.79s
                        Total time: 26294.63s
                               ETA: 1186010.2s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.307s, learning 0.229s)
               Value function loss: 5.9261
                    Surrogate loss: -0.0167
             Mean action noise std: 0.73
                       Mean reward: 324.35
               Mean episode length: 250.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 11.54s
                        Total time: 26306.17s
                               ETA: 1185971.7s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.209s, learning 0.239s)
               Value function loss: 6.8345
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 326.86
               Mean episode length: 250.00
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 11.45s
                        Total time: 26317.61s
                               ETA: 1185929.2s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.258s, learning 0.161s)
               Value function loss: 6.4926
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 319.43
               Mean episode length: 249.87
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 11.42s
                        Total time: 26329.03s
                               ETA: 1185885.4s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.341s, learning 0.192s)
               Value function loss: 7.8893
                    Surrogate loss: 0.0029
             Mean action noise std: 0.73
                       Mean reward: 321.59
               Mean episode length: 249.87
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 11.53s
                        Total time: 26340.57s
                               ETA: 1185846.8s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.282s, learning 0.211s)
               Value function loss: 7.2431
                    Surrogate loss: 0.0005
             Mean action noise std: 0.73
                       Mean reward: 319.48
               Mean episode length: 250.00
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 11.49s
                        Total time: 26352.06s
                               ETA: 1185806.3s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.915s, learning 0.167s)
               Value function loss: 9.5537
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 323.54
               Mean episode length: 249.51
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 12.08s
                        Total time: 26364.14s
                               ETA: 1185792.4s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.300s, learning 0.183s)
               Value function loss: 8.0960
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 316.32
               Mean episode length: 249.04
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 11.48s
                        Total time: 26375.62s
                               ETA: 1185751.6s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.282s, learning 0.189s)
               Value function loss: 9.0549
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 317.35
               Mean episode length: 248.99
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 11.47s
                        Total time: 26387.10s
                               ETA: 1185710.3s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.316s, learning 0.165s)
               Value function loss: 10.4787
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 313.15
               Mean episode length: 249.20
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 11.48s
                        Total time: 26398.58s
                               ETA: 1185669.4s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.575s, learning 0.174s)
               Value function loss: 10.9555
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 313.89
               Mean episode length: 247.82
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 11.75s
                        Total time: 26410.32s
                               ETA: 1185640.6s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.111s, learning 0.185s)
               Value function loss: 8.7422
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 311.65
               Mean episode length: 249.41
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 11.30s
                        Total time: 26421.62s
                               ETA: 1185591.5s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.973s, learning 0.157s)
               Value function loss: 8.0721
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 309.08
               Mean episode length: 248.08
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 11.13s
                        Total time: 26432.75s
                               ETA: 1185535.0s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.951s, learning 0.159s)
               Value function loss: 4.0484
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 311.31
               Mean episode length: 249.35
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 11.11s
                        Total time: 26443.86s
                               ETA: 1185477.5s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.206s, learning 0.171s)
               Value function loss: 3.9646
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 310.71
               Mean episode length: 248.54
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 11.38s
                        Total time: 26455.24s
                               ETA: 1185432.1s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.293s, learning 0.184s)
               Value function loss: 2.7610
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 315.47
               Mean episode length: 248.82
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 11.48s
                        Total time: 26466.71s
                               ETA: 1185391.3s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1498 steps/s (collection: 10.749s, learning 0.184s)
               Value function loss: 2.0985
                    Surrogate loss: 0.0155
             Mean action noise std: 0.73
                       Mean reward: 315.63
               Mean episode length: 248.25
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 10.93s
                        Total time: 26477.65s
                               ETA: 1185326.1s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.094s, learning 0.165s)
               Value function loss: 2.1394
                    Surrogate loss: 0.0025
             Mean action noise std: 0.73
                       Mean reward: 311.48
               Mean episode length: 246.93
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 11.26s
                        Total time: 26488.91s
                               ETA: 1185275.5s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.347s, learning 0.176s)
               Value function loss: 2.8312
                    Surrogate loss: 0.0080
             Mean action noise std: 0.73
                       Mean reward: 302.07
               Mean episode length: 243.89
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 11.52s
                        Total time: 26500.43s
                               ETA: 1185236.8s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.231s, learning 0.258s)
               Value function loss: 2.4825
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 289.73
               Mean episode length: 240.98
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 11.49s
                        Total time: 26511.92s
                               ETA: 1185196.6s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.398s, learning 0.156s)
               Value function loss: 2.7546
                    Surrogate loss: 0.0185
             Mean action noise std: 0.73
                       Mean reward: 280.04
               Mean episode length: 238.04
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 11.55s
                        Total time: 26523.47s
                               ETA: 1185159.3s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.938s, learning 0.166s)
               Value function loss: 3.2456
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 276.36
               Mean episode length: 236.76
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 11.10s
                        Total time: 26534.58s
                               ETA: 1185102.0s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.751s, learning 0.250s)
               Value function loss: 3.0363
                    Surrogate loss: 0.0137
             Mean action noise std: 0.73
                       Mean reward: 270.37
               Mean episode length: 233.56
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 12.00s
                        Total time: 26546.58s
                               ETA: 1185084.7s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.301s, learning 0.164s)
               Value function loss: 4.2668
                    Surrogate loss: 0.0013
             Mean action noise std: 0.73
                       Mean reward: 272.67
               Mean episode length: 233.39
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 11.47s
                        Total time: 26558.04s
                               ETA: 1185043.5s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.258s, learning 0.176s)
               Value function loss: 4.3540
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 269.87
               Mean episode length: 232.47
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 11.43s
                        Total time: 26569.48s
                               ETA: 1185001.0s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.329s, learning 0.169s)
               Value function loss: 5.2218
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 265.81
               Mean episode length: 229.51
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 11.50s
                        Total time: 26580.97s
                               ETA: 1184961.4s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.014s, learning 0.171s)
               Value function loss: 7.0762
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 260.93
               Mean episode length: 227.02
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 11.19s
                        Total time: 26592.16s
                               ETA: 1184907.8s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.298s, learning 0.162s)
               Value function loss: 5.8573
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 262.97
               Mean episode length: 226.75
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 11.46s
                        Total time: 26603.62s
                               ETA: 1184866.5s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.165s)
               Value function loss: 4.4637
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 260.99
               Mean episode length: 226.43
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 11.34s
                        Total time: 26614.96s
                               ETA: 1184820.0s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.488s, learning 0.182s)
               Value function loss: 4.8203
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 254.26
               Mean episode length: 223.69
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 11.67s
                        Total time: 26626.63s
                               ETA: 1184788.1s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.173s, learning 0.164s)
               Value function loss: 5.4926
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: 254.70
               Mean episode length: 224.25
                  Mean reward/step: 0.95
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 11.34s
                        Total time: 26637.97s
                               ETA: 1184741.4s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.450s, learning 0.188s)
               Value function loss: 5.0100
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 242.72
               Mean episode length: 218.43
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 11.64s
                        Total time: 26649.60s
                               ETA: 1184708.2s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.324s, learning 0.163s)
               Value function loss: 6.0195
                    Surrogate loss: 0.0026
             Mean action noise std: 0.73
                       Mean reward: 249.45
               Mean episode length: 219.88
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 11.49s
                        Total time: 26661.09s
                               ETA: 1184668.2s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.246s, learning 0.234s)
               Value function loss: 6.3894
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 258.64
               Mean episode length: 225.31
                  Mean reward/step: 0.94
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 11.48s
                        Total time: 26672.57s
                               ETA: 1184628.0s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.458s, learning 0.162s)
               Value function loss: 6.1485
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 239.15
               Mean episode length: 216.08
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 11.62s
                        Total time: 26684.19s
                               ETA: 1184594.0s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.527s, learning 0.227s)
               Value function loss: 5.5654
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 240.43
               Mean episode length: 216.37
                  Mean reward/step: 1.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 11.75s
                        Total time: 26695.95s
                               ETA: 1184565.9s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.166s)
               Value function loss: 5.3442
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 245.00
               Mean episode length: 219.79
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 11.45s
                        Total time: 26707.39s
                               ETA: 1184524.4s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.380s, learning 0.193s)
               Value function loss: 6.0261
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 252.12
               Mean episode length: 221.69
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 11.57s
                        Total time: 26718.97s
                               ETA: 1184488.3s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.213s, learning 0.174s)
               Value function loss: 5.2619
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 258.00
               Mean episode length: 224.55
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 11.39s
                        Total time: 26730.35s
                               ETA: 1184444.1s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.445s, learning 0.161s)
               Value function loss: 5.3365
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 261.33
               Mean episode length: 227.27
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 11.61s
                        Total time: 26741.96s
                               ETA: 1184409.5s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.260s, learning 0.179s)
               Value function loss: 5.1457
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 262.40
               Mean episode length: 229.87
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 11.44s
                        Total time: 26753.40s
                               ETA: 1184367.6s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.548s, learning 0.157s)
               Value function loss: 4.5533
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 270.18
               Mean episode length: 232.26
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 11.71s
                        Total time: 26765.10s
                               ETA: 1184337.6s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.325s, learning 0.182s)
               Value function loss: 3.0771
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 270.51
               Mean episode length: 233.31
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 11.51s
                        Total time: 26776.61s
                               ETA: 1184298.8s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.371s, learning 0.161s)
               Value function loss: 2.8656
                    Surrogate loss: 0.0043
             Mean action noise std: 0.73
                       Mean reward: 265.43
               Mean episode length: 230.72
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 11.53s
                        Total time: 26788.14s
                               ETA: 1184261.1s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.081s, learning 0.160s)
               Value function loss: 2.7294
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 260.37
               Mean episode length: 227.20
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 11.24s
                        Total time: 26799.38s
                               ETA: 1184210.5s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.351s, learning 0.216s)
               Value function loss: 2.8970
                    Surrogate loss: 0.0035
             Mean action noise std: 0.73
                       Mean reward: 252.86
               Mean episode length: 221.70
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 11.57s
                        Total time: 26810.95s
                               ETA: 1184174.4s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.912s, learning 0.200s)
               Value function loss: 2.8831
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 245.15
               Mean episode length: 216.39
                  Mean reward/step: 1.24
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 11.11s
                        Total time: 26822.06s
                               ETA: 1184118.3s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.577s, learning 0.191s)
               Value function loss: 3.1351
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 234.11
               Mean episode length: 208.79
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 11.77s
                        Total time: 26833.83s
                               ETA: 1184091.1s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.736s, learning 0.226s)
               Value function loss: 3.2956
                    Surrogate loss: 0.0241
             Mean action noise std: 0.73
                       Mean reward: 224.08
               Mean episode length: 201.87
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 11.96s
                        Total time: 26845.79s
                               ETA: 1184072.5s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.329s, learning 0.305s)
               Value function loss: 3.3019
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 222.39
               Mean episode length: 201.03
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 11.63s
                        Total time: 26857.42s
                               ETA: 1184039.4s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.386s, learning 0.159s)
               Value function loss: 3.7998
                    Surrogate loss: 0.0091
             Mean action noise std: 0.73
                       Mean reward: 225.87
               Mean episode length: 203.95
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 11.55s
                        Total time: 26868.97s
                               ETA: 1184002.5s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1478 steps/s (collection: 10.911s, learning 0.169s)
               Value function loss: 4.4481
                    Surrogate loss: 0.0034
             Mean action noise std: 0.73
                       Mean reward: 227.85
               Mean episode length: 206.13
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 11.08s
                        Total time: 26880.05s
                               ETA: 1183945.1s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.949s, learning 0.189s)
               Value function loss: 4.3917
                    Surrogate loss: 0.0035
             Mean action noise std: 0.73
                       Mean reward: 225.50
               Mean episode length: 204.66
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 12.14s
                        Total time: 26892.19s
                               ETA: 1183934.3s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.952s, learning 0.208s)
               Value function loss: 4.7109
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 228.57
               Mean episode length: 207.68
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 11.16s
                        Total time: 26903.35s
                               ETA: 1183880.5s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.215s, learning 0.200s)
               Value function loss: 3.8171
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: 231.48
               Mean episode length: 211.60
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 11.41s
                        Total time: 26914.76s
                               ETA: 1183837.9s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.195s, learning 0.170s)
               Value function loss: 4.1507
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 227.36
               Mean episode length: 209.54
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 11.37s
                        Total time: 26926.13s
                               ETA: 1183793.2s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.126s, learning 0.190s)
               Value function loss: 4.8769
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 225.04
               Mean episode length: 209.19
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 11.32s
                        Total time: 26937.44s
                               ETA: 1183746.3s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.168s)
               Value function loss: 5.1987
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 231.05
               Mean episode length: 214.54
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 11.40s
                        Total time: 26948.85s
                               ETA: 1183703.2s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.325s, learning 0.314s)
               Value function loss: 5.5524
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 230.46
               Mean episode length: 214.82
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 11.64s
                        Total time: 26960.48s
                               ETA: 1183670.6s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.117s, learning 0.194s)
               Value function loss: 7.7911
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 231.59
               Mean episode length: 215.65
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 11.31s
                        Total time: 26971.80s
                               ETA: 1183623.6s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.306s, learning 0.180s)
               Value function loss: 5.8629
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 226.58
               Mean episode length: 214.76
                  Mean reward/step: 0.96
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 11.49s
                        Total time: 26983.28s
                               ETA: 1183584.3s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.380s, learning 0.226s)
               Value function loss: 5.1729
                    Surrogate loss: -0.0169
             Mean action noise std: 0.73
                       Mean reward: 234.61
               Mean episode length: 221.06
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 11.61s
                        Total time: 26994.89s
                               ETA: 1183550.3s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.583s, learning 0.182s)
               Value function loss: 4.5432
                    Surrogate loss: 0.0016
             Mean action noise std: 0.73
                       Mean reward: 240.55
               Mean episode length: 227.55
                  Mean reward/step: 0.97
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 11.76s
                        Total time: 27006.65s
                               ETA: 1183523.3s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.182s, learning 0.164s)
               Value function loss: 5.5173
                    Surrogate loss: 0.0028
             Mean action noise std: 0.73
                       Mean reward: 238.63
               Mean episode length: 226.44
                  Mean reward/step: 0.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 11.35s
                        Total time: 27018.00s
                               ETA: 1183477.9s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.315s, learning 0.214s)
               Value function loss: 5.0398
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 245.19
               Mean episode length: 232.10
                  Mean reward/step: 0.99
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 11.53s
                        Total time: 27029.53s
                               ETA: 1183440.6s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.311s, learning 0.202s)
               Value function loss: 4.9365
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 243.92
               Mean episode length: 231.16
                  Mean reward/step: 1.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 11.51s
                        Total time: 27041.04s
                               ETA: 1183402.6s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.380s, learning 0.176s)
               Value function loss: 5.1226
                    Surrogate loss: -0.0156
             Mean action noise std: 0.73
                       Mean reward: 246.33
               Mean episode length: 233.47
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 11.56s
                        Total time: 27052.60s
                               ETA: 1183366.5s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.488s, learning 0.205s)
               Value function loss: 4.6076
                    Surrogate loss: -0.0210
             Mean action noise std: 0.73
                       Mean reward: 248.57
               Mean episode length: 235.61
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 11.69s
                        Total time: 27064.29s
                               ETA: 1183336.4s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.717s, learning 0.282s)
               Value function loss: 4.2798
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 250.76
               Mean episode length: 238.44
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 12.00s
                        Total time: 27076.29s
                               ETA: 1183319.7s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1508 steps/s (collection: 10.669s, learning 0.196s)
               Value function loss: 4.0181
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 251.99
               Mean episode length: 239.27
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 10.86s
                        Total time: 27087.15s
                               ETA: 1183253.4s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.331s, learning 0.170s)
               Value function loss: 3.4709
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 254.42
               Mean episode length: 239.82
                  Mean reward/step: 1.15
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 11.50s
                        Total time: 27098.65s
                               ETA: 1183215.1s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.434s, learning 0.160s)
               Value function loss: 3.4937
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 257.91
               Mean episode length: 242.81
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 11.59s
                        Total time: 27110.25s
                               ETA: 1183180.8s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.165s)
               Value function loss: 3.6976
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 254.21
               Mean episode length: 240.81
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 11.52s
                        Total time: 27121.76s
                               ETA: 1183143.1s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.131s, learning 0.159s)
               Value function loss: 2.7888
                    Surrogate loss: -0.0196
             Mean action noise std: 0.73
                       Mean reward: 255.76
               Mean episode length: 241.34
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 11.29s
                        Total time: 27133.05s
                               ETA: 1183095.6s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.507s, learning 0.280s)
               Value function loss: 2.6981
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 254.17
               Mean episode length: 240.00
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 11.79s
                        Total time: 27144.84s
                               ETA: 1183069.7s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.267s, learning 0.171s)
               Value function loss: 2.9464
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 253.83
               Mean episode length: 239.08
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 11.44s
                        Total time: 27156.28s
                               ETA: 1183028.7s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1465 steps/s (collection: 10.989s, learning 0.193s)
               Value function loss: 2.2111
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 252.42
               Mean episode length: 237.46
                  Mean reward/step: 1.18
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 11.18s
                        Total time: 27167.46s
                               ETA: 1182976.6s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.990s, learning 0.168s)
               Value function loss: 2.5105
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 251.30
               Mean episode length: 236.36
                  Mean reward/step: 1.16
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 11.16s
                        Total time: 27178.62s
                               ETA: 1182923.4s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.320s, learning 0.180s)
               Value function loss: 2.7986
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 250.00
               Mean episode length: 234.73
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 11.50s
                        Total time: 27190.12s
                               ETA: 1182885.2s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.436s, learning 0.202s)
               Value function loss: 2.3515
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 251.06
               Mean episode length: 235.43
                  Mean reward/step: 1.13
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 11.64s
                        Total time: 27201.76s
                               ETA: 1182853.0s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.594s, learning 0.168s)
               Value function loss: 3.0860
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 251.16
               Mean episode length: 235.53
                  Mean reward/step: 1.11
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 11.76s
                        Total time: 27213.52s
                               ETA: 1182826.2s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.199s, learning 0.166s)
               Value function loss: 3.3394
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 251.71
               Mean episode length: 236.51
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 11.36s
                        Total time: 27224.89s
                               ETA: 1182782.1s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.367s, learning 0.170s)
               Value function loss: 3.4311
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 253.34
               Mean episode length: 238.61
                  Mean reward/step: 1.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 11.54s
                        Total time: 27236.42s
                               ETA: 1182745.6s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.324s, learning 0.198s)
               Value function loss: 3.5466
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 251.97
               Mean episode length: 236.63
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 11.52s
                        Total time: 27247.94s
                               ETA: 1182708.4s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.940s, learning 0.177s)
               Value function loss: 3.8906
                    Surrogate loss: -0.0168
             Mean action noise std: 0.73
                       Mean reward: 253.26
               Mean episode length: 237.58
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 11.12s
                        Total time: 27259.06s
                               ETA: 1182653.6s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.858s, learning 0.200s)
               Value function loss: 4.0562
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 260.00
               Mean episode length: 241.61
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 11.06s
                        Total time: 27270.12s
                               ETA: 1182596.4s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.188s, learning 0.183s)
               Value function loss: 3.8276
                    Surrogate loss: 0.0020
             Mean action noise std: 0.73
                       Mean reward: 263.52
               Mean episode length: 243.80
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 11.37s
                        Total time: 27281.49s
                               ETA: 1182552.8s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.809s, learning 0.299s)
               Value function loss: 4.4575
                    Surrogate loss: -0.0145
             Mean action noise std: 0.73
                       Mean reward: 260.99
               Mean episode length: 242.24
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 11.11s
                        Total time: 27292.60s
                               ETA: 1182497.8s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.158s, learning 0.163s)
               Value function loss: 5.1534
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 265.09
               Mean episode length: 245.20
                  Mean reward/step: 1.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 11.32s
                        Total time: 27303.92s
                               ETA: 1182452.0s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.628s, learning 0.171s)
               Value function loss: 4.9109
                    Surrogate loss: -0.0215
             Mean action noise std: 0.73
                       Mean reward: 262.71
               Mean episode length: 243.38
                  Mean reward/step: 1.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 11.80s
                        Total time: 27315.72s
                               ETA: 1182427.0s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.142s, learning 0.263s)
               Value function loss: 5.1373
                    Surrogate loss: -0.0154
             Mean action noise std: 0.73
                       Mean reward: 261.28
               Mean episode length: 244.18
                  Mean reward/step: 1.04
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 11.41s
                        Total time: 27327.12s
                               ETA: 1182385.0s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.927s, learning 0.193s)
               Value function loss: 4.2997
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 263.39
               Mean episode length: 244.74
                  Mean reward/step: 1.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 11.12s
                        Total time: 27338.24s
                               ETA: 1182330.6s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.355s, learning 0.200s)
               Value function loss: 5.0440
                    Surrogate loss: 0.0097
             Mean action noise std: 0.73
                       Mean reward: 267.38
               Mean episode length: 247.35
                  Mean reward/step: 1.06
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 11.56s
                        Total time: 27349.80s
                               ETA: 1182295.1s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.686s, learning 0.183s)
               Value function loss: 4.2535
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 269.81
               Mean episode length: 247.54
                  Mean reward/step: 1.08
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 11.87s
                        Total time: 27361.67s
                               ETA: 1182273.2s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.105s, learning 0.192s)
               Value function loss: 5.3886
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 274.65
               Mean episode length: 248.69
                  Mean reward/step: 1.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 11.30s
                        Total time: 27372.96s
                               ETA: 1182226.6s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.115s, learning 0.158s)
               Value function loss: 5.4222
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 275.23
               Mean episode length: 248.65
                  Mean reward/step: 1.10
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 12.27s
                        Total time: 27385.24s
                               ETA: 1182222.1s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.364s, learning 0.165s)
               Value function loss: 5.3419
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 278.68
               Mean episode length: 249.03
                  Mean reward/step: 1.12
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 11.53s
                        Total time: 27396.77s
                               ETA: 1182185.5s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.161s)
               Value function loss: 5.5870
                    Surrogate loss: -0.0181
             Mean action noise std: 0.73
                       Mean reward: 282.46
               Mean episode length: 249.58
                  Mean reward/step: 1.14
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 11.55s
                        Total time: 27408.31s
                               ETA: 1182149.9s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.354s, learning 0.231s)
               Value function loss: 5.5396
                    Surrogate loss: -0.0179
             Mean action noise std: 0.73
                       Mean reward: 278.97
               Mean episode length: 248.90
                  Mean reward/step: 1.17
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 11.59s
                        Total time: 27419.90s
                               ETA: 1182115.8s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.410s, learning 0.222s)
               Value function loss: 4.7702
                    Surrogate loss: -0.0191
             Mean action noise std: 0.73
                       Mean reward: 283.21
               Mean episode length: 249.09
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 11.63s
                        Total time: 27431.53s
                               ETA: 1182083.8s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.239s, learning 0.157s)
               Value function loss: 4.7551
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 283.67
               Mean episode length: 249.29
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 11.40s
                        Total time: 27442.93s
                               ETA: 1182041.5s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.251s, learning 0.183s)
               Value function loss: 4.4885
                    Surrogate loss: -0.0159
             Mean action noise std: 0.73
                       Mean reward: 285.12
               Mean episode length: 249.32
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 11.43s
                        Total time: 27454.36s
                               ETA: 1182001.0s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.480s, learning 0.203s)
               Value function loss: 4.4817
                    Surrogate loss: -0.0162
             Mean action noise std: 0.73
                       Mean reward: 287.17
               Mean episode length: 249.21
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 11.68s
                        Total time: 27466.05s
                               ETA: 1181971.2s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.424s, learning 0.174s)
               Value function loss: 4.3098
                    Surrogate loss: -0.0153
             Mean action noise std: 0.73
                       Mean reward: 292.25
               Mean episode length: 249.51
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 11.60s
                        Total time: 27477.64s
                               ETA: 1181937.8s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.474s, learning 0.195s)
               Value function loss: 4.3108
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 294.24
               Mean episode length: 249.71
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 11.67s
                        Total time: 27489.31s
                               ETA: 1181907.4s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.272s, learning 0.164s)
               Value function loss: 2.5355
                    Surrogate loss: -0.0183
             Mean action noise std: 0.73
                       Mean reward: 294.71
               Mean episode length: 249.73
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 11.44s
                        Total time: 27500.75s
                               ETA: 1181867.0s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.271s, learning 0.163s)
               Value function loss: 3.4906
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 294.91
               Mean episode length: 249.97
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 11.43s
                        Total time: 27512.18s
                               ETA: 1181826.6s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.425s, learning 0.209s)
               Value function loss: 2.7494
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 295.72
               Mean episode length: 249.85
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 11.63s
                        Total time: 27523.82s
                               ETA: 1181794.8s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.679s, learning 0.158s)
               Value function loss: 3.7365
                    Surrogate loss: -0.0144
             Mean action noise std: 0.73
                       Mean reward: 298.80
               Mean episode length: 249.88
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 11.84s
                        Total time: 27535.65s
                               ETA: 1181771.6s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.683s, learning 0.179s)
               Value function loss: 4.1875
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 302.23
               Mean episode length: 249.88
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 11.86s
                        Total time: 27547.51s
                               ETA: 1181749.6s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.255s, learning 0.162s)
               Value function loss: 3.5777
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 304.14
               Mean episode length: 249.84
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 11.42s
                        Total time: 27558.93s
                               ETA: 1181708.6s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1461 steps/s (collection: 11.049s, learning 0.164s)
               Value function loss: 4.6285
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 305.47
               Mean episode length: 249.60
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 11.21s
                        Total time: 27570.14s
                               ETA: 1181658.8s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.258s, learning 0.189s)
               Value function loss: 5.6082
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 309.03
               Mean episode length: 249.76
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 11.45s
                        Total time: 27581.59s
                               ETA: 1181619.1s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.440s, learning 0.162s)
               Value function loss: 6.1010
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 310.96
               Mean episode length: 249.79
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 11.60s
                        Total time: 27593.19s
                               ETA: 1181586.0s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.294s, learning 0.186s)
               Value function loss: 6.4553
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 315.64
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 11.48s
                        Total time: 27604.67s
                               ETA: 1181547.7s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.330s, learning 0.188s)
               Value function loss: 7.3620
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 319.32
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 11.52s
                        Total time: 27616.19s
                               ETA: 1181511.1s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.478s, learning 0.169s)
               Value function loss: 6.9726
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 321.66
               Mean episode length: 250.00
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 11.65s
                        Total time: 27627.84s
                               ETA: 1181480.0s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.387s, learning 0.160s)
               Value function loss: 7.2636
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 321.12
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 11.55s
                        Total time: 27639.39s
                               ETA: 1181444.7s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.270s, learning 0.162s)
               Value function loss: 10.6465
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 323.62
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 11.43s
                        Total time: 27650.82s
                               ETA: 1181404.4s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.186s, learning 0.226s)
               Value function loss: 12.1267
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 327.06
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 11.41s
                        Total time: 27662.23s
                               ETA: 1181363.4s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.161s)
               Value function loss: 10.8082
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 330.37
               Mean episode length: 249.53
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 11.12s
                        Total time: 27673.35s
                               ETA: 1181310.1s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.408s, learning 0.203s)
               Value function loss: 12.2507
                    Surrogate loss: -0.0161
             Mean action noise std: 0.73
                       Mean reward: 337.21
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 11.61s
                        Total time: 27684.96s
                               ETA: 1181277.5s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.406s, learning 0.174s)
               Value function loss: 9.6192
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 336.75
               Mean episode length: 249.51
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 11.58s
                        Total time: 27696.54s
                               ETA: 1181243.7s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.349s, learning 0.165s)
               Value function loss: 12.0060
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 341.28
               Mean episode length: 249.51
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 11.51s
                        Total time: 27708.06s
                               ETA: 1181207.1s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.332s, learning 0.163s)
               Value function loss: 9.9456
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 341.67
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 11.50s
                        Total time: 27719.55s
                               ETA: 1181169.7s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.508s, learning 0.284s)
               Value function loss: 13.2747
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 347.44
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 11.79s
                        Total time: 27731.35s
                               ETA: 1181145.0s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.186s, learning 0.165s)
               Value function loss: 15.9935
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 350.89
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 11.35s
                        Total time: 27742.70s
                               ETA: 1181101.5s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.685s, learning 0.202s)
               Value function loss: 12.6773
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 352.85
               Mean episode length: 249.79
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 11.89s
                        Total time: 27754.58s
                               ETA: 1181080.8s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.049s, learning 0.188s)
               Value function loss: 13.9897
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 355.46
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 11.24s
                        Total time: 27765.82s
                               ETA: 1181032.5s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.342s, learning 0.235s)
               Value function loss: 13.5547
                    Surrogate loss: -0.0146
             Mean action noise std: 0.73
                       Mean reward: 357.62
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 11.58s
                        Total time: 27777.40s
                               ETA: 1180998.7s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.894s, learning 0.179s)
               Value function loss: 11.3622
                    Surrogate loss: -0.0143
             Mean action noise std: 0.73
                       Mean reward: 357.34
               Mean episode length: 249.97
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 11.07s
                        Total time: 27788.47s
                               ETA: 1180943.5s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.491s, learning 0.179s)
               Value function loss: 11.8771
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 358.55
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 11.67s
                        Total time: 27800.14s
                               ETA: 1180913.7s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.267s, learning 0.189s)
               Value function loss: 8.7187
                    Surrogate loss: -0.0151
             Mean action noise std: 0.73
                       Mean reward: 360.63
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 11.46s
                        Total time: 27811.60s
                               ETA: 1180874.8s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.511s, learning 0.202s)
               Value function loss: 8.3253
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 360.98
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 11.71s
                        Total time: 27823.31s
                               ETA: 1180846.8s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.053s, learning 0.179s)
               Value function loss: 7.0154
                    Surrogate loss: -0.0172
             Mean action noise std: 0.73
                       Mean reward: 361.65
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 11.23s
                        Total time: 27834.54s
                               ETA: 1180798.5s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1464 steps/s (collection: 11.016s, learning 0.174s)
               Value function loss: 7.8951
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 360.99
               Mean episode length: 249.81
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 11.19s
                        Total time: 27845.73s
                               ETA: 1180748.4s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.463s, learning 0.203s)
               Value function loss: 4.0650
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 361.06
               Mean episode length: 249.81
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 11.67s
                        Total time: 27857.40s
                               ETA: 1180718.5s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.179s, learning 0.162s)
               Value function loss: 4.7201
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 361.25
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 11.34s
                        Total time: 27868.74s
                               ETA: 1180674.9s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.512s, learning 0.286s)
               Value function loss: 4.1469
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 361.10
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 11.80s
                        Total time: 27880.54s
                               ETA: 1180650.7s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.551s, learning 0.189s)
               Value function loss: 4.8046
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 360.61
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 11.74s
                        Total time: 27892.28s
                               ETA: 1180624.0s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.460s, learning 0.231s)
               Value function loss: 6.2083
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 358.58
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 11.69s
                        Total time: 27903.97s
                               ETA: 1180595.2s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.905s, learning 0.207s)
               Value function loss: 4.8940
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 362.74
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 12.11s
                        Total time: 27916.08s
                               ETA: 1180584.3s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.109s, learning 0.161s)
               Value function loss: 4.5868
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 362.08
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 11.27s
                        Total time: 27927.35s
                               ETA: 1180537.8s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.316s, learning 0.165s)
               Value function loss: 7.2432
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 362.02
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 11.48s
                        Total time: 27938.83s
                               ETA: 1180500.2s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.575s, learning 0.175s)
               Value function loss: 6.7430
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 361.62
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 11.75s
                        Total time: 27950.58s
                               ETA: 1180474.0s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.331s, learning 0.244s)
               Value function loss: 9.7398
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 364.02
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 11.57s
                        Total time: 27962.16s
                               ETA: 1180440.4s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.558s, learning 0.190s)
               Value function loss: 7.3680
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 362.63
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 11.75s
                        Total time: 27973.90s
                               ETA: 1180414.1s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.212s, learning 0.171s)
               Value function loss: 9.0043
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 362.41
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 11.38s
                        Total time: 27985.29s
                               ETA: 1180372.5s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.155s, learning 0.159s)
               Value function loss: 8.2763
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 365.09
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 11.31s
                        Total time: 27996.60s
                               ETA: 1180328.0s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.227s, learning 0.166s)
               Value function loss: 12.3304
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 368.39
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 11.39s
                        Total time: 28007.99s
                               ETA: 1180286.8s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.894s, learning 0.197s)
               Value function loss: 14.7501
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 366.35
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 11.09s
                        Total time: 28019.08s
                               ETA: 1180233.0s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.302s, learning 0.242s)
               Value function loss: 13.6639
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 368.89
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 11.54s
                        Total time: 28030.63s
                               ETA: 1180198.2s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.652s, learning 0.194s)
               Value function loss: 12.5224
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 367.62
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 11.85s
                        Total time: 28042.47s
                               ETA: 1180176.2s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.181s, learning 0.158s)
               Value function loss: 13.6172
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 366.47
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 11.34s
                        Total time: 28053.81s
                               ETA: 1180132.9s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.408s, learning 0.232s)
               Value function loss: 13.9592
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 368.40
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 11.64s
                        Total time: 28065.45s
                               ETA: 1180102.2s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.389s, learning 0.167s)
               Value function loss: 13.2023
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 369.31
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.56s
                        Total time: 28077.01s
                               ETA: 1180068.0s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.250s, learning 0.171s)
               Value function loss: 16.2295
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 370.32
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 11.42s
                        Total time: 28088.43s
                               ETA: 1180028.2s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.247s, learning 0.158s)
               Value function loss: 18.3796
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 369.95
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 11.41s
                        Total time: 28099.84s
                               ETA: 1179987.8s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.069s, learning 0.164s)
               Value function loss: 15.3838
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 369.64
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 11.23s
                        Total time: 28111.07s
                               ETA: 1179940.1s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.019s, learning 0.164s)
               Value function loss: 18.9503
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 369.98
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 11.18s
                        Total time: 28122.25s
                               ETA: 1179890.4s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.216s, learning 0.167s)
               Value function loss: 19.9294
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 370.30
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 11.38s
                        Total time: 28133.64s
                               ETA: 1179849.1s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.952s, learning 0.172s)
               Value function loss: 14.6248
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 370.11
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 11.12s
                        Total time: 28144.76s
                               ETA: 1179796.9s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.398s, learning 0.206s)
               Value function loss: 15.1715
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 371.07
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 11.60s
                        Total time: 28156.36s
                               ETA: 1179764.9s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.153s, learning 0.197s)
               Value function loss: 13.5417
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 369.25
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 11.35s
                        Total time: 28167.71s
                               ETA: 1179722.3s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.309s, learning 0.210s)
               Value function loss: 11.2369
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 368.24
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 11.52s
                        Total time: 28179.23s
                               ETA: 1179686.8s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.262s, learning 0.163s)
               Value function loss: 9.3751
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 367.84
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 11.43s
                        Total time: 28190.66s
                               ETA: 1179647.4s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.192s, learning 0.261s)
               Value function loss: 10.8058
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 368.31
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 11.45s
                        Total time: 28202.11s
                               ETA: 1179609.2s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.547s, learning 0.201s)
               Value function loss: 7.3305
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 368.48
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 11.75s
                        Total time: 28213.86s
                               ETA: 1179583.3s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.020s, learning 0.178s)
               Value function loss: 5.9804
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 368.43
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 11.20s
                        Total time: 28225.06s
                               ETA: 1179534.5s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.142s, learning 0.155s)
               Value function loss: 7.1558
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 367.72
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 11.30s
                        Total time: 28236.36s
                               ETA: 1179489.8s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.784s, learning 0.164s)
               Value function loss: 7.6185
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 365.72
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 11.95s
                        Total time: 28248.30s
                               ETA: 1179472.3s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.143s, learning 0.188s)
               Value function loss: 7.7582
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 364.62
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 11.33s
                        Total time: 28259.63s
                               ETA: 1179429.1s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.611s, learning 0.166s)
               Value function loss: 6.4847
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 363.89
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 11.78s
                        Total time: 28271.41s
                               ETA: 1179404.5s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.284s, learning 0.167s)
               Value function loss: 6.6043
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 362.33
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 11.45s
                        Total time: 28282.86s
                               ETA: 1179366.4s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.199s, learning 0.227s)
               Value function loss: 10.4610
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: 360.83
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 11.43s
                        Total time: 28294.29s
                               ETA: 1179327.2s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.580s, learning 0.231s)
               Value function loss: 7.7891
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 358.54
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 11.81s
                        Total time: 28306.10s
                               ETA: 1179304.1s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.156s, learning 0.199s)
               Value function loss: 8.6941
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 357.53
               Mean episode length: 250.00
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 11.35s
                        Total time: 28317.45s
                               ETA: 1179262.0s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.049s, learning 0.183s)
               Value function loss: 7.7639
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 354.65
               Mean episode length: 250.00
                  Mean reward/step: 1.35
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 11.23s
                        Total time: 28328.69s
                               ETA: 1179214.8s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.383s, learning 0.183s)
               Value function loss: 10.7277
                    Surrogate loss: 0.0256
             Mean action noise std: 0.73
                       Mean reward: 353.77
               Mean episode length: 250.00
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 11.57s
                        Total time: 28340.25s
                               ETA: 1179181.5s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.223s, learning 0.230s)
               Value function loss: 7.6479
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 352.47
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 11.45s
                        Total time: 28351.71s
                               ETA: 1179143.6s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.369s, learning 0.205s)
               Value function loss: 10.2772
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 350.40
               Mean episode length: 250.00
                  Mean reward/step: 1.31
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 11.57s
                        Total time: 28363.28s
                               ETA: 1179110.6s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.414s, learning 0.212s)
               Value function loss: 13.7554
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 350.09
               Mean episode length: 250.00
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 11.63s
                        Total time: 28374.90s
                               ETA: 1179079.9s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.173s)
               Value function loss: 12.2853
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 345.29
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 11.50s
                        Total time: 28386.40s
                               ETA: 1179044.0s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.231s, learning 0.300s)
               Value function loss: 13.4028
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 346.05
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 11.53s
                        Total time: 28397.94s
                               ETA: 1179009.3s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.148s, learning 0.169s)
               Value function loss: 13.2789
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 347.20
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 11.32s
                        Total time: 28409.25s
                               ETA: 1178965.9s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1507 steps/s (collection: 10.687s, learning 0.179s)
               Value function loss: 12.5747
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 342.58
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 10.87s
                        Total time: 28420.12s
                               ETA: 1178903.7s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.364s, learning 0.211s)
               Value function loss: 12.3515
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 340.18
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 11.58s
                        Total time: 28431.69s
                               ETA: 1178871.0s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.231s, learning 0.199s)
               Value function loss: 10.7729
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 333.06
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 11.43s
                        Total time: 28443.12s
                               ETA: 1178832.3s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.180s, learning 0.201s)
               Value function loss: 15.2238
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 336.88
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 11.38s
                        Total time: 28454.51s
                               ETA: 1178791.6s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.007s, learning 0.197s)
               Value function loss: 11.5115
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 332.05
               Mean episode length: 249.96
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 11.20s
                        Total time: 28465.71s
                               ETA: 1178743.6s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.755s, learning 0.251s)
               Value function loss: 10.2308
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 327.69
               Mean episode length: 250.00
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 12.01s
                        Total time: 28477.72s
                               ETA: 1178728.8s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.344s, learning 0.195s)
               Value function loss: 12.6026
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 327.83
               Mean episode length: 249.46
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 11.54s
                        Total time: 28489.25s
                               ETA: 1178694.6s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.568s, learning 0.177s)
               Value function loss: 10.6311
                    Surrogate loss: 0.0121
             Mean action noise std: 0.73
                       Mean reward: 328.45
               Mean episode length: 250.00
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 11.74s
                        Total time: 28501.00s
                               ETA: 1178669.0s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.759s, learning 0.224s)
               Value function loss: 8.2498
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 325.30
               Mean episode length: 250.00
                  Mean reward/step: 1.30
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 11.98s
                        Total time: 28512.98s
                               ETA: 1178653.3s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.390s, learning 0.212s)
               Value function loss: 7.2974
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 322.27
               Mean episode length: 250.00
                  Mean reward/step: 1.32
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 11.60s
                        Total time: 28524.59s
                               ETA: 1178621.9s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.120s, learning 0.232s)
               Value function loss: 7.8715
                    Surrogate loss: 0.0016
             Mean action noise std: 0.73
                       Mean reward: 322.38
               Mean episode length: 250.00
                  Mean reward/step: 1.34
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 11.35s
                        Total time: 28535.94s
                               ETA: 1178580.1s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.219s, learning 0.176s)
               Value function loss: 5.8877
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 318.43
               Mean episode length: 249.75
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 11.39s
                        Total time: 28547.33s
                               ETA: 1178540.1s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.138s, learning 0.176s)
               Value function loss: 6.5748
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 320.41
               Mean episode length: 249.77
                  Mean reward/step: 1.37
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 11.31s
                        Total time: 28558.65s
                               ETA: 1178496.8s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.218s, learning 0.172s)
               Value function loss: 5.1822
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 322.72
               Mean episode length: 249.77
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 11.39s
                        Total time: 28570.04s
                               ETA: 1178456.7s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.449s, learning 0.158s)
               Value function loss: 3.5912
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 322.67
               Mean episode length: 249.77
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 11.61s
                        Total time: 28581.64s
                               ETA: 1178425.5s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.174s, learning 0.178s)
               Value function loss: 4.8993
                    Surrogate loss: -0.0141
             Mean action noise std: 0.73
                       Mean reward: 321.49
               Mean episode length: 249.57
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 11.35s
                        Total time: 28593.00s
                               ETA: 1178383.8s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.166s, learning 0.166s)
               Value function loss: 4.4640
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 321.67
               Mean episode length: 249.48
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 11.33s
                        Total time: 28604.33s
                               ETA: 1178341.4s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.567s, learning 0.164s)
               Value function loss: 5.0861
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 322.11
               Mean episode length: 249.48
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 11.73s
                        Total time: 28616.06s
                               ETA: 1178315.4s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.343s, learning 0.157s)
               Value function loss: 5.9611
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 326.82
               Mean episode length: 249.91
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 11.50s
                        Total time: 28627.56s
                               ETA: 1178279.9s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.236s, learning 0.189s)
               Value function loss: 6.0890
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 330.03
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 11.43s
                        Total time: 28638.98s
                               ETA: 1178241.4s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.773s, learning 0.154s)
               Value function loss: 8.4260
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 331.65
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 11.93s
                        Total time: 28650.91s
                               ETA: 1178223.5s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.092s, learning 0.236s)
               Value function loss: 9.4441
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 334.39
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 11.33s
                        Total time: 28662.24s
                               ETA: 1178181.0s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.878s, learning 0.203s)
               Value function loss: 9.1834
                    Surrogate loss: 0.0072
             Mean action noise std: 0.73
                       Mean reward: 338.56
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 12.08s
                        Total time: 28674.32s
                               ETA: 1178169.5s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.393s, learning 0.200s)
               Value function loss: 8.2795
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 340.67
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 11.59s
                        Total time: 28685.91s
                               ETA: 1178137.9s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.105s, learning 0.162s)
               Value function loss: 12.1887
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 343.13
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 11.27s
                        Total time: 28697.18s
                               ETA: 1178092.9s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.094s, learning 0.176s)
               Value function loss: 9.8317
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 344.39
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 11.27s
                        Total time: 28708.45s
                               ETA: 1178048.1s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.306s, learning 0.180s)
               Value function loss: 13.1547
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: 347.47
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 11.49s
                        Total time: 28719.94s
                               ETA: 1178012.2s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.432s, learning 0.168s)
               Value function loss: 16.9653
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 354.04
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 11.60s
                        Total time: 28731.54s
                               ETA: 1177981.0s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.300s, learning 0.161s)
               Value function loss: 19.2134
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 357.50
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 11.46s
                        Total time: 28743.00s
                               ETA: 1177944.1s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.215s, learning 0.158s)
               Value function loss: 17.2901
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 359.81
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 11.37s
                        Total time: 28754.37s
                               ETA: 1177903.6s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.620s, learning 0.217s)
               Value function loss: 18.6797
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 363.81
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 11.84s
                        Total time: 28766.21s
                               ETA: 1177882.1s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.346s, learning 0.197s)
               Value function loss: 13.8174
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 360.33
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 11.54s
                        Total time: 28777.75s
                               ETA: 1177848.6s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.472s, learning 0.207s)
               Value function loss: 17.7818
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 364.91
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 11.68s
                        Total time: 28789.43s
                               ETA: 1177820.7s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.546s, learning 0.189s)
               Value function loss: 15.4085
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 368.14
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 11.73s
                        Total time: 28801.16s
                               ETA: 1177795.1s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.187s, learning 0.162s)
               Value function loss: 20.5808
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 368.11
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 11.35s
                        Total time: 28812.51s
                               ETA: 1177753.7s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.879s, learning 0.174s)
               Value function loss: 18.5463
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 370.73
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 12.05s
                        Total time: 28824.57s
                               ETA: 1177741.1s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.550s, learning 0.157s)
               Value function loss: 19.7018
                    Surrogate loss: 0.0006
             Mean action noise std: 0.73
                       Mean reward: 372.88
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 11.71s
                        Total time: 28836.27s
                               ETA: 1177714.4s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.054s, learning 0.162s)
               Value function loss: 19.9324
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 373.11
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 11.22s
                        Total time: 28847.49s
                               ETA: 1177667.7s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.380s, learning 0.168s)
               Value function loss: 17.1254
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 372.89
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 11.55s
                        Total time: 28859.04s
                               ETA: 1177634.5s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.417s, learning 0.193s)
               Value function loss: 12.5448
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 374.50
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 11.61s
                        Total time: 28870.65s
                               ETA: 1177603.9s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.546s, learning 0.239s)
               Value function loss: 13.7158
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 374.42
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 11.78s
                        Total time: 28882.43s
                               ETA: 1177580.4s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.129s, learning 0.206s)
               Value function loss: 10.4000
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 374.03
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 11.34s
                        Total time: 28893.77s
                               ETA: 1177538.6s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.336s, learning 0.163s)
               Value function loss: 10.2000
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 374.83
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 11.50s
                        Total time: 28905.27s
                               ETA: 1177503.5s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.644s, learning 0.169s)
               Value function loss: 8.5564
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 372.01
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 11.81s
                        Total time: 28917.08s
                               ETA: 1177481.2s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.326s, learning 0.180s)
               Value function loss: 6.9294
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 370.81
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 11.51s
                        Total time: 28928.59s
                               ETA: 1177446.5s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.386s, learning 0.175s)
               Value function loss: 3.6742
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 371.29
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 11.56s
                        Total time: 28940.15s
                               ETA: 1177414.0s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.965s, learning 0.202s)
               Value function loss: 6.5391
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 370.10
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 12.17s
                        Total time: 28952.31s
                               ETA: 1177406.1s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.219s, learning 0.172s)
               Value function loss: 5.1054
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 369.11
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 11.39s
                        Total time: 28963.70s
                               ETA: 1177366.7s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.418s, learning 0.198s)
               Value function loss: 5.1923
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 368.50
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 11.62s
                        Total time: 28975.32s
                               ETA: 1177336.5s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.911s, learning 0.210s)
               Value function loss: 6.6641
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 365.63
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 12.12s
                        Total time: 28987.44s
                               ETA: 1177326.8s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.790s, learning 0.205s)
               Value function loss: 4.9703
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 364.42
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 12.00s
                        Total time: 28999.44s
                               ETA: 1177312.0s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.224s, learning 0.157s)
               Value function loss: 5.9611
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 362.87
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 11.38s
                        Total time: 29010.82s
                               ETA: 1177272.2s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.330s, learning 0.271s)
               Value function loss: 6.8443
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 362.17
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 11.60s
                        Total time: 29022.42s
                               ETA: 1177241.4s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.210s, learning 0.203s)
               Value function loss: 6.1418
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 361.66
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 11.41s
                        Total time: 29033.83s
                               ETA: 1177203.0s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.100s, learning 0.159s)
               Value function loss: 7.0283
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 360.49
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 11.26s
                        Total time: 29045.09s
                               ETA: 1177158.4s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.430s, learning 0.175s)
               Value function loss: 7.7829
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 361.32
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 11.60s
                        Total time: 29056.69s
                               ETA: 1177127.8s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.316s, learning 0.172s)
               Value function loss: 7.8250
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 363.80
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 11.49s
                        Total time: 29068.18s
                               ETA: 1177092.5s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.766s, learning 0.188s)
               Value function loss: 7.7485
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 363.29
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 11.95s
                        Total time: 29080.14s
                               ETA: 1177076.1s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.100s, learning 0.191s)
               Value function loss: 10.0298
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 364.07
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 12.29s
                        Total time: 29092.43s
                               ETA: 1177073.4s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.383s, learning 0.163s)
               Value function loss: 12.9873
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 366.12
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 11.55s
                        Total time: 29103.97s
                               ETA: 1177040.4s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.962s, learning 0.165s)
               Value function loss: 9.7952
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 367.63
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 12.13s
                        Total time: 29116.10s
                               ETA: 1177031.1s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.704s, learning 0.158s)
               Value function loss: 12.3832
                    Surrogate loss: -0.0018
             Mean action noise std: 0.73
                       Mean reward: 368.13
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 11.86s
                        Total time: 29127.96s
                               ETA: 1177010.9s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.119s, learning 0.162s)
               Value function loss: 11.3332
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 368.47
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 11.28s
                        Total time: 29139.24s
                               ETA: 1176967.4s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.676s, learning 0.259s)
               Value function loss: 13.2604
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 370.54
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 11.93s
                        Total time: 29151.18s
                               ETA: 1176950.2s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.324s, learning 0.162s)
               Value function loss: 12.1964
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 370.48
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 11.49s
                        Total time: 29162.66s
                               ETA: 1176914.9s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.331s, learning 0.202s)
               Value function loss: 15.2738
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 371.71
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 11.53s
                        Total time: 29174.20s
                               ETA: 1176881.6s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.249s, learning 0.161s)
               Value function loss: 18.9082
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 372.92
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 11.41s
                        Total time: 29185.61s
                               ETA: 1176843.3s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.162s, learning 0.178s)
               Value function loss: 13.3848
                    Surrogate loss: -0.0127
             Mean action noise std: 0.73
                       Mean reward: 371.55
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 11.34s
                        Total time: 29196.95s
                               ETA: 1176802.2s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.224s, learning 0.158s)
               Value function loss: 17.2041
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 374.24
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 11.38s
                        Total time: 29208.33s
                               ETA: 1176762.8s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.077s, learning 0.215s)
               Value function loss: 18.0652
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 373.84
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 11.29s
                        Total time: 29219.62s
                               ETA: 1176719.9s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.629s, learning 0.212s)
               Value function loss: 12.3192
                    Surrogate loss: -0.0150
             Mean action noise std: 0.73
                       Mean reward: 374.32
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 11.84s
                        Total time: 29231.46s
                               ETA: 1176699.1s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.256s, learning 0.163s)
               Value function loss: 15.4770
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 375.93
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 11.42s
                        Total time: 29242.88s
                               ETA: 1176661.2s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.711s, learning 0.165s)
               Value function loss: 12.5725
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 376.00
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 11.88s
                        Total time: 29254.76s
                               ETA: 1176641.8s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.139s, learning 0.165s)
               Value function loss: 12.1180
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 377.14
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 11.30s
                        Total time: 29266.06s
                               ETA: 1176599.4s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.340s, learning 0.166s)
               Value function loss: 9.3219
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 377.49
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 11.51s
                        Total time: 29277.57s
                               ETA: 1176565.1s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.174s, learning 0.159s)
               Value function loss: 10.4667
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 377.41
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 11.33s
                        Total time: 29288.90s
                               ETA: 1176523.9s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.214s, learning 0.167s)
               Value function loss: 5.1868
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 377.18
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 11.38s
                        Total time: 29300.28s
                               ETA: 1176484.7s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.224s, learning 0.163s)
               Value function loss: 6.8436
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 377.49
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 11.39s
                        Total time: 29311.67s
                               ETA: 1176445.7s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.535s, learning 0.163s)
               Value function loss: 6.4571
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 377.20
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 11.70s
                        Total time: 29323.37s
                               ETA: 1176419.3s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.829s, learning 0.227s)
               Value function loss: 7.7128
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 376.16
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 12.06s
                        Total time: 29335.42s
                               ETA: 1176407.2s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.874s, learning 0.195s)
               Value function loss: 7.7499
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 375.93
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 12.07s
                        Total time: 29347.49s
                               ETA: 1176395.6s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.698s, learning 0.164s)
               Value function loss: 6.4989
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 375.71
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 11.86s
                        Total time: 29359.35s
                               ETA: 1176375.7s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.380s, learning 0.275s)
               Value function loss: 6.8877
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 375.92
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 11.66s
                        Total time: 29371.01s
                               ETA: 1176347.6s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.417s, learning 0.185s)
               Value function loss: 8.7298
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 376.62
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 11.60s
                        Total time: 29382.61s
                               ETA: 1176317.3s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.604s, learning 0.158s)
               Value function loss: 8.4348
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 376.99
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 11.76s
                        Total time: 29394.38s
                               ETA: 1176293.5s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.367s, learning 0.315s)
               Value function loss: 11.9514
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 374.79
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 11.68s
                        Total time: 29406.06s
                               ETA: 1176266.4s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.656s, learning 0.231s)
               Value function loss: 9.3440
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 374.38
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 11.89s
                        Total time: 29417.94s
                               ETA: 1176247.5s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.166s, learning 0.168s)
               Value function loss: 11.2400
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 374.67
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 11.33s
                        Total time: 29429.28s
                               ETA: 1176206.6s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.186s, learning 0.211s)
               Value function loss: 9.9070
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 374.95
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 11.40s
                        Total time: 29440.68s
                               ETA: 1176168.2s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.211s, learning 0.162s)
               Value function loss: 13.0741
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 374.83
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 11.37s
                        Total time: 29452.05s
                               ETA: 1176128.9s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.212s, learning 0.167s)
               Value function loss: 14.7376
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 373.61
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 11.38s
                        Total time: 29463.43s
                               ETA: 1176089.8s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.912s, learning 0.198s)
               Value function loss: 15.9231
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 375.24
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 11.11s
                        Total time: 29474.54s
                               ETA: 1176040.0s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.351s, learning 0.170s)
               Value function loss: 15.1714
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 376.80
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 11.52s
                        Total time: 29486.06s
                               ETA: 1176006.7s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.647s, learning 0.184s)
               Value function loss: 14.3709
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 375.78
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 11.83s
                        Total time: 29497.89s
                               ETA: 1175985.7s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.332s, learning 0.162s)
               Value function loss: 14.3499
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 373.65
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 11.49s
                        Total time: 29509.38s
                               ETA: 1175951.3s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.110s, learning 0.160s)
               Value function loss: 14.0507
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 374.25
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 11.27s
                        Total time: 29520.65s
                               ETA: 1175908.0s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.953s, learning 0.167s)
               Value function loss: 15.6152
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 375.30
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 11.12s
                        Total time: 29531.77s
                               ETA: 1175858.8s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.330s, learning 0.175s)
               Value function loss: 19.5364
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 376.03
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 11.50s
                        Total time: 29543.28s
                               ETA: 1175824.9s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.449s, learning 0.203s)
               Value function loss: 16.0145
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 375.90
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 11.65s
                        Total time: 29554.93s
                               ETA: 1175796.8s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.473s, learning 0.168s)
               Value function loss: 17.6843
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 375.85
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 11.64s
                        Total time: 29566.57s
                               ETA: 1175768.4s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.507s, learning 0.193s)
               Value function loss: 19.8436
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 376.09
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 11.70s
                        Total time: 29578.27s
                               ETA: 1175742.3s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.289s, learning 0.240s)
               Value function loss: 13.9833
                    Surrogate loss: -0.0021
             Mean action noise std: 0.73
                       Mean reward: 376.91
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 11.53s
                        Total time: 29589.80s
                               ETA: 1175709.4s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.217s, learning 0.203s)
               Value function loss: 15.8540
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 376.65
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 11.42s
                        Total time: 29601.22s
                               ETA: 1175672.2s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.063s, learning 0.240s)
               Value function loss: 12.8916
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 375.95
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 11.30s
                        Total time: 29612.52s
                               ETA: 1175630.4s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.306s, learning 0.166s)
               Value function loss: 11.2233
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 377.45
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 11.47s
                        Total time: 29623.99s
                               ETA: 1175595.3s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.238s, learning 0.212s)
               Value function loss: 10.7465
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 378.62
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 11.45s
                        Total time: 29635.44s
                               ETA: 1175559.4s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.537s, learning 0.230s)
               Value function loss: 11.3740
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 378.33
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 11.77s
                        Total time: 29647.21s
                               ETA: 1175536.0s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.509s, learning 0.158s)
               Value function loss: 7.1383
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 376.43
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 11.67s
                        Total time: 29658.88s
                               ETA: 1175508.7s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.487s, learning 0.158s)
               Value function loss: 6.8119
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 376.11
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 11.64s
                        Total time: 29670.52s
                               ETA: 1175480.6s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.705s, learning 0.190s)
               Value function loss: 7.0157
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 376.94
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 11.90s
                        Total time: 29682.42s
                               ETA: 1175462.3s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.821s, learning 0.187s)
               Value function loss: 7.0537
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 377.63
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 12.01s
                        Total time: 29694.43s
                               ETA: 1175448.6s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.067s, learning 0.193s)
               Value function loss: 7.6674
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 377.60
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 11.26s
                        Total time: 29705.69s
                               ETA: 1175405.2s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.133s, learning 0.187s)
               Value function loss: 6.6226
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 377.93
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 11.32s
                        Total time: 29717.01s
                               ETA: 1175364.2s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.497s, learning 0.188s)
               Value function loss: 7.7532
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 378.52
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 11.69s
                        Total time: 29728.69s
                               ETA: 1175337.7s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.492s, learning 0.163s)
               Value function loss: 11.7533
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 377.65
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 11.65s
                        Total time: 29740.35s
                               ETA: 1175310.1s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.185s, learning 0.182s)
               Value function loss: 8.8710
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 377.05
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 11.37s
                        Total time: 29751.71s
                               ETA: 1175271.0s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.378s, learning 0.274s)
               Value function loss: 9.7290
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 377.39
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 11.65s
                        Total time: 29763.37s
                               ETA: 1175243.2s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.439s, learning 0.250s)
               Value function loss: 10.9377
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 377.89
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 11.69s
                        Total time: 29775.05s
                               ETA: 1175216.9s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.287s, learning 0.167s)
               Value function loss: 12.4874
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 377.27
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 11.45s
                        Total time: 29786.51s
                               ETA: 1175181.4s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.423s, learning 0.170s)
               Value function loss: 10.4775
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 377.91
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 11.59s
                        Total time: 29798.10s
                               ETA: 1175151.3s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.428s, learning 0.203s)
               Value function loss: 13.3362
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 378.48
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 11.63s
                        Total time: 29809.73s
                               ETA: 1175122.7s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.404s, learning 0.167s)
               Value function loss: 17.6750
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 378.44
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 11.57s
                        Total time: 29821.30s
                               ETA: 1175091.9s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.111s, learning 0.170s)
               Value function loss: 15.7944
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 378.99
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 12.28s
                        Total time: 29833.58s
                               ETA: 1175088.9s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.164s)
               Value function loss: 15.9197
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 379.45
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 11.55s
                        Total time: 29845.13s
                               ETA: 1175057.3s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.370s, learning 0.219s)
               Value function loss: 16.1758
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 379.06
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 11.59s
                        Total time: 29856.72s
                               ETA: 1175027.1s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.677s, learning 0.171s)
               Value function loss: 16.8256
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 379.96
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 11.85s
                        Total time: 29868.57s
                               ETA: 1175007.2s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.223s, learning 0.164s)
               Value function loss: 15.3973
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 379.25
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 11.39s
                        Total time: 29879.96s
                               ETA: 1174969.1s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.463s, learning 0.160s)
               Value function loss: 13.8885
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 379.90
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 11.62s
                        Total time: 29891.58s
                               ETA: 1174940.3s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.242s, learning 0.179s)
               Value function loss: 20.0935
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 379.46
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 11.42s
                        Total time: 29903.00s
                               ETA: 1174903.6s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.464s, learning 0.183s)
               Value function loss: 17.7313
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 380.78
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 11.65s
                        Total time: 29914.65s
                               ETA: 1174875.8s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.125s, learning 0.165s)
               Value function loss: 18.9371
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 379.69
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 11.29s
                        Total time: 29925.94s
                               ETA: 1174834.0s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.142s, learning 0.178s)
               Value function loss: 20.9067
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 380.19
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 11.32s
                        Total time: 29937.26s
                               ETA: 1174793.4s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.387s, learning 0.164s)
               Value function loss: 16.7489
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 379.77
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 11.55s
                        Total time: 29948.81s
                               ETA: 1174761.9s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.574s, learning 0.164s)
               Value function loss: 12.8506
                    Surrogate loss: -0.0142
             Mean action noise std: 0.73
                       Mean reward: 378.97
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 11.74s
                        Total time: 29960.55s
                               ETA: 1174737.7s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.347s, learning 0.170s)
               Value function loss: 12.9527
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 379.33
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 11.52s
                        Total time: 29972.06s
                               ETA: 1174704.9s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.451s, learning 0.163s)
               Value function loss: 12.6413
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 377.67
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 11.61s
                        Total time: 29983.68s
                               ETA: 1174675.9s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.842s, learning 0.217s)
               Value function loss: 10.1462
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 378.76
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 12.06s
                        Total time: 29995.73s
                               ETA: 1174664.3s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.977s, learning 0.170s)
               Value function loss: 12.5373
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 378.84
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 12.15s
                        Total time: 30007.88s
                               ETA: 1174656.2s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.328s, learning 0.205s)
               Value function loss: 7.6383
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 377.95
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 11.53s
                        Total time: 30019.41s
                               ETA: 1174624.1s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.634s, learning 0.164s)
               Value function loss: 5.7388
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 376.91
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 11.80s
                        Total time: 30031.21s
                               ETA: 1174602.3s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.134s, learning 0.173s)
               Value function loss: 7.6435
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 375.68
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 11.31s
                        Total time: 30042.52s
                               ETA: 1174561.4s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1501 steps/s (collection: 10.758s, learning 0.157s)
               Value function loss: 5.6073
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 375.79
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 10.91s
                        Total time: 30053.43s
                               ETA: 1174505.1s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.472s, learning 0.194s)
               Value function loss: 5.6385
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 375.91
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 11.67s
                        Total time: 30065.10s
                               ETA: 1174478.3s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.277s, learning 0.171s)
               Value function loss: 7.7057
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 376.15
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 11.45s
                        Total time: 30076.55s
                               ETA: 1174442.9s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.605s, learning 0.209s)
               Value function loss: 7.1897
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 374.67
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 11.81s
                        Total time: 30088.36s
                               ETA: 1174421.8s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1504 steps/s (collection: 10.730s, learning 0.157s)
               Value function loss: 10.4935
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 374.04
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 10.89s
                        Total time: 30099.25s
                               ETA: 1174364.6s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1470 steps/s (collection: 10.979s, learning 0.160s)
               Value function loss: 10.6064
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 373.13
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 11.14s
                        Total time: 30110.39s
                               ETA: 1174317.2s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.072s, learning 0.186s)
               Value function loss: 10.3967
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 372.25
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 11.26s
                        Total time: 30121.65s
                               ETA: 1174274.5s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.276s, learning 0.167s)
               Value function loss: 9.4369
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 371.05
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 11.44s
                        Total time: 30133.09s
                               ETA: 1174239.1s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.269s, learning 0.164s)
               Value function loss: 13.2057
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 369.87
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 11.43s
                        Total time: 30144.52s
                               ETA: 1174203.3s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1481 steps/s (collection: 10.903s, learning 0.160s)
               Value function loss: 9.6633
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 370.21
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 11.06s
                        Total time: 30155.59s
                               ETA: 1174153.0s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.433s, learning 0.161s)
               Value function loss: 12.5348
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 369.84
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 11.59s
                        Total time: 30167.18s
                               ETA: 1174123.5s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.233s, learning 0.187s)
               Value function loss: 16.3918
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 368.21
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 11.42s
                        Total time: 30178.60s
                               ETA: 1174087.3s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.440s, learning 0.195s)
               Value function loss: 17.9999
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 368.03
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 11.63s
                        Total time: 30190.24s
                               ETA: 1174059.4s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.872s, learning 0.205s)
               Value function loss: 15.3218
                    Surrogate loss: 0.0001
             Mean action noise std: 0.73
                       Mean reward: 366.24
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 11.08s
                        Total time: 30201.31s
                               ETA: 1174009.8s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.079s, learning 0.160s)
               Value function loss: 16.6907
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 365.88
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 11.24s
                        Total time: 30212.55s
                               ETA: 1173966.6s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.265s, learning 0.161s)
               Value function loss: 12.7574
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 363.61
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 11.43s
                        Total time: 30223.98s
                               ETA: 1173930.7s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.168s)
               Value function loss: 15.4627
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 362.61
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 11.58s
                        Total time: 30235.56s
                               ETA: 1173900.6s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.913s, learning 0.188s)
               Value function loss: 13.7370
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 363.99
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 12.10s
                        Total time: 30247.66s
                               ETA: 1173890.9s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.869s, learning 0.198s)
               Value function loss: 17.3882
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 359.27
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 12.07s
                        Total time: 30259.73s
                               ETA: 1173879.9s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.366s, learning 0.163s)
               Value function loss: 17.2529
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 361.30
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 11.53s
                        Total time: 30271.25s
                               ETA: 1173848.0s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.681s, learning 0.189s)
               Value function loss: 17.0725
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 362.58
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 11.87s
                        Total time: 30283.13s
                               ETA: 1173829.3s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.504s, learning 0.188s)
               Value function loss: 15.6897
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 360.31
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 11.69s
                        Total time: 30294.82s
                               ETA: 1173803.8s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.486s, learning 0.172s)
               Value function loss: 15.7587
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 361.04
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 11.66s
                        Total time: 30306.47s
                               ETA: 1173776.8s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.404s, learning 0.156s)
               Value function loss: 12.9955
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 362.58
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 11.56s
                        Total time: 30318.03s
                               ETA: 1173746.2s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.539s, learning 0.164s)
               Value function loss: 12.6444
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 360.48
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 11.70s
                        Total time: 30329.74s
                               ETA: 1173721.1s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.287s, learning 0.195s)
               Value function loss: 11.4173
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 360.07
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 11.48s
                        Total time: 30341.22s
                               ETA: 1173687.5s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.790s, learning 0.162s)
               Value function loss: 10.8103
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 358.89
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 11.95s
                        Total time: 30353.17s
                               ETA: 1173672.0s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.282s, learning 0.186s)
               Value function loss: 8.6737
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 359.43
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 11.47s
                        Total time: 30364.64s
                               ETA: 1173637.9s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.863s, learning 0.198s)
               Value function loss: 8.5279
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 359.21
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 12.06s
                        Total time: 30376.70s
                               ETA: 1173626.7s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.045s, learning 0.172s)
               Value function loss: 3.8898
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 358.88
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 12.22s
                        Total time: 30388.92s
                               ETA: 1173621.5s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.260s, learning 0.164s)
               Value function loss: 8.4771
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 358.52
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 11.42s
                        Total time: 30400.34s
                               ETA: 1173585.6s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.163s, learning 0.226s)
               Value function loss: 6.7198
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 358.70
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 11.39s
                        Total time: 30411.73s
                               ETA: 1173548.5s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.247s, learning 0.206s)
               Value function loss: 7.3266
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 357.42
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 11.45s
                        Total time: 30423.19s
                               ETA: 1173513.9s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.237s, learning 0.166s)
               Value function loss: 8.3114
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 358.32
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 11.40s
                        Total time: 30434.59s
                               ETA: 1173477.3s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.808s, learning 0.209s)
               Value function loss: 6.7270
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 359.92
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 12.02s
                        Total time: 30446.61s
                               ETA: 1173464.4s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.322s, learning 0.164s)
               Value function loss: 8.5908
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 358.23
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 11.49s
                        Total time: 30458.09s
                               ETA: 1173431.1s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.643s, learning 0.231s)
               Value function loss: 10.8777
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 357.98
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 11.87s
                        Total time: 30469.97s
                               ETA: 1173412.7s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.580s, learning 0.230s)
               Value function loss: 8.6827
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 359.69
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 11.81s
                        Total time: 30481.78s
                               ETA: 1173391.8s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.456s, learning 0.174s)
               Value function loss: 12.0683
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 358.11
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 11.63s
                        Total time: 30493.41s
                               ETA: 1173364.1s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.726s, learning 0.160s)
               Value function loss: 12.3074
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 357.36
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 11.89s
                        Total time: 30505.29s
                               ETA: 1173346.2s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.353s, learning 0.241s)
               Value function loss: 10.8310
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 356.92
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 11.59s
                        Total time: 30516.89s
                               ETA: 1173317.1s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.336s, learning 0.164s)
               Value function loss: 13.1156
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 355.59
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 11.50s
                        Total time: 30528.39s
                               ETA: 1173284.4s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.666s, learning 0.171s)
               Value function loss: 16.6507
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 354.98
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 11.84s
                        Total time: 30540.22s
                               ETA: 1173264.6s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.654s, learning 0.286s)
               Value function loss: 18.7608
                    Surrogate loss: 0.0037
             Mean action noise std: 0.73
                       Mean reward: 352.33
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 11.94s
                        Total time: 30552.16s
                               ETA: 1173248.8s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.241s, learning 0.257s)
               Value function loss: 14.3554
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 352.46
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 11.50s
                        Total time: 30563.66s
                               ETA: 1173216.1s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.343s, learning 0.188s)
               Value function loss: 16.7862
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 354.05
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 11.53s
                        Total time: 30575.19s
                               ETA: 1173184.6s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.692s, learning 0.375s)
               Value function loss: 13.9572
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 352.73
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 12.07s
                        Total time: 30587.26s
                               ETA: 1173173.7s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.262s, learning 0.168s)
               Value function loss: 15.7600
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 353.13
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 11.43s
                        Total time: 30598.69s
                               ETA: 1173138.3s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.521s, learning 0.256s)
               Value function loss: 14.0337
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 351.89
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 11.78s
                        Total time: 30610.47s
                               ETA: 1173116.3s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.341s, learning 0.338s)
               Value function loss: 17.8246
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 353.41
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 11.68s
                        Total time: 30622.14s
                               ETA: 1173090.6s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.237s, learning 0.301s)
               Value function loss: 20.1633
                    Surrogate loss: 0.0037
             Mean action noise std: 0.73
                       Mean reward: 353.58
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 12.54s
                        Total time: 30634.68s
                               ETA: 1173097.7s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.161s)
               Value function loss: 16.8074
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 353.59
               Mean episode length: 250.00
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 11.45s
                        Total time: 30646.13s
                               ETA: 1173063.1s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.654s, learning 0.165s)
               Value function loss: 18.1996
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: 353.69
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 11.82s
                        Total time: 30657.95s
                               ETA: 1173042.7s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.906s, learning 0.167s)
               Value function loss: 18.7063
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 354.09
               Mean episode length: 250.00
                  Mean reward/step: 1.39
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 11.07s
                        Total time: 30669.02s
                               ETA: 1172993.8s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.271s, learning 0.175s)
               Value function loss: 13.8232
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 353.10
               Mean episode length: 250.00
                  Mean reward/step: 1.40
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 11.45s
                        Total time: 30680.47s
                               ETA: 1172959.2s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.540s, learning 0.206s)
               Value function loss: 15.3058
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 354.58
               Mean episode length: 250.00
                  Mean reward/step: 1.41
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 11.75s
                        Total time: 30692.21s
                               ETA: 1172936.0s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.476s, learning 0.172s)
               Value function loss: 12.2846
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 355.39
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 11.65s
                        Total time: 30703.86s
                               ETA: 1172909.2s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.624s, learning 0.184s)
               Value function loss: 11.8679
                    Surrogate loss: -0.0128
             Mean action noise std: 0.73
                       Mean reward: 353.34
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 11.81s
                        Total time: 30715.67s
                               ETA: 1172888.5s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.357s, learning 0.265s)
               Value function loss: 9.0018
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 354.96
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 11.62s
                        Total time: 30727.29s
                               ETA: 1172860.6s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.570s, learning 0.266s)
               Value function loss: 10.7099
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 354.67
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 11.84s
                        Total time: 30739.13s
                               ETA: 1172841.0s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.579s, learning 0.171s)
               Value function loss: 6.2073
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 355.49
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 11.75s
                        Total time: 30750.88s
                               ETA: 1172818.0s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.734s, learning 0.212s)
               Value function loss: 7.5634
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 356.60
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 11.95s
                        Total time: 30762.82s
                               ETA: 1172802.6s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.399s, learning 0.214s)
               Value function loss: 6.7034
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 357.87
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 11.61s
                        Total time: 30774.44s
                               ETA: 1172774.5s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.836s, learning 0.278s)
               Value function loss: 8.1556
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 358.77
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 12.11s
                        Total time: 30786.55s
                               ETA: 1172765.4s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.611s, learning 0.213s)
               Value function loss: 9.1197
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 359.35
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 11.82s
                        Total time: 30798.38s
                               ETA: 1172745.3s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.553s, learning 0.160s)
               Value function loss: 7.0903
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 361.27
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 11.71s
                        Total time: 30810.09s
                               ETA: 1172721.0s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.926s, learning 0.172s)
               Value function loss: 6.9072
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 363.49
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 12.10s
                        Total time: 30822.19s
                               ETA: 1172711.4s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.490s, learning 0.207s)
               Value function loss: 9.5560
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 365.58
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 11.70s
                        Total time: 30833.88s
                               ETA: 1172686.5s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.030s, learning 0.211s)
               Value function loss: 8.4074
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 366.09
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 12.24s
                        Total time: 30846.12s
                               ETA: 1172682.3s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.098s, learning 0.214s)
               Value function loss: 13.1985
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 367.29
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 11.31s
                        Total time: 30857.44s
                               ETA: 1172642.7s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.401s, learning 0.169s)
               Value function loss: 10.1854
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 368.79
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 11.57s
                        Total time: 30869.01s
                               ETA: 1172613.0s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.103s, learning 0.160s)
               Value function loss: 12.0431
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 369.40
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 11.26s
                        Total time: 30880.27s
                               ETA: 1172571.7s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.367s, learning 0.182s)
               Value function loss: 10.3583
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 370.33
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 11.55s
                        Total time: 30891.82s
                               ETA: 1172541.3s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.404s, learning 0.196s)
               Value function loss: 13.1238
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 371.16
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 11.60s
                        Total time: 30903.42s
                               ETA: 1172512.8s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.305s, learning 0.264s)
               Value function loss: 15.6554
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 372.93
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 11.57s
                        Total time: 30914.99s
                               ETA: 1172483.1s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.569s, learning 0.212s)
               Value function loss: 15.3812
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 372.81
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 11.78s
                        Total time: 30926.77s
                               ETA: 1172461.5s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.522s, learning 0.162s)
               Value function loss: 15.5173
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 373.75
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 11.68s
                        Total time: 30938.45s
                               ETA: 1172436.3s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.891s, learning 0.164s)
               Value function loss: 14.2672
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 375.01
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 12.05s
                        Total time: 30950.51s
                               ETA: 1172425.0s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.311s, learning 0.188s)
               Value function loss: 14.7695
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 375.90
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 11.50s
                        Total time: 30962.01s
                               ETA: 1172392.7s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.514s, learning 0.198s)
               Value function loss: 14.9894
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 377.04
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 11.71s
                        Total time: 30973.72s
                               ETA: 1172368.5s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.537s, learning 0.157s)
               Value function loss: 16.2278
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 377.86
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 11.69s
                        Total time: 30985.41s
                               ETA: 1172343.6s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.578s, learning 0.164s)
               Value function loss: 20.4371
                    Surrogate loss: -0.0130
             Mean action noise std: 0.73
                       Mean reward: 378.04
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 11.74s
                        Total time: 30997.15s
                               ETA: 1172320.6s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.215s, learning 0.213s)
               Value function loss: 17.1877
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 379.00
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 11.43s
                        Total time: 31008.58s
                               ETA: 1172285.6s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.351s, learning 0.225s)
               Value function loss: 19.5369
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 378.91
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 11.58s
                        Total time: 31020.16s
                               ETA: 1172256.4s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.804s, learning 0.205s)
               Value function loss: 21.8600
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 380.66
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 12.01s
                        Total time: 31032.17s
                               ETA: 1172243.4s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.425s, learning 0.160s)
               Value function loss: 14.3300
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 380.98
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 11.58s
                        Total time: 31043.75s
                               ETA: 1172214.5s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.396s, learning 0.295s)
               Value function loss: 17.1571
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 381.62
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 11.69s
                        Total time: 31055.44s
                               ETA: 1172189.5s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.589s, learning 0.155s)
               Value function loss: 13.7403
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 380.42
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 11.74s
                        Total time: 31067.19s
                               ETA: 1172166.6s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.273s, learning 0.180s)
               Value function loss: 12.5515
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 380.66
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 11.45s
                        Total time: 31078.64s
                               ETA: 1172132.8s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.322s, learning 0.161s)
               Value function loss: 9.8407
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 382.05
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 11.48s
                        Total time: 31090.12s
                               ETA: 1172100.1s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.447s, learning 0.174s)
               Value function loss: 12.0013
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 382.34
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 11.62s
                        Total time: 31101.74s
                               ETA: 1172072.5s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.104s, learning 0.225s)
               Value function loss: 6.8578
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 382.82
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 11.33s
                        Total time: 31113.07s
                               ETA: 1172034.0s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.237s, learning 0.169s)
               Value function loss: 7.1243
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 382.67
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 11.41s
                        Total time: 31124.48s
                               ETA: 1171998.4s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.237s, learning 0.162s)
               Value function loss: 7.0359
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 383.46
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 11.40s
                        Total time: 31135.88s
                               ETA: 1171962.6s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.782s, learning 0.189s)
               Value function loss: 7.8612
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 382.50
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 11.97s
                        Total time: 31147.85s
                               ETA: 1171948.3s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.107s, learning 0.187s)
               Value function loss: 8.3117
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 382.63
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 11.29s
                        Total time: 31159.14s
                               ETA: 1171908.6s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.376s, learning 0.160s)
               Value function loss: 6.6759
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 382.19
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 11.54s
                        Total time: 31170.68s
                               ETA: 1171878.0s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.307s, learning 0.200s)
               Value function loss: 6.9361
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 382.32
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 11.51s
                        Total time: 31182.19s
                               ETA: 1171846.3s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.162s)
               Value function loss: 10.8929
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 382.94
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 11.57s
                        Total time: 31193.75s
                               ETA: 1171816.9s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.583s, learning 0.161s)
               Value function loss: 9.0347
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 383.48
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 11.74s
                        Total time: 31205.50s
                               ETA: 1171794.1s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.348s, learning 0.160s)
               Value function loss: 9.5260
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 383.82
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 11.51s
                        Total time: 31217.01s
                               ETA: 1171762.5s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.728s, learning 0.192s)
               Value function loss: 10.1106
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 383.11
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 11.92s
                        Total time: 31228.93s
                               ETA: 1171746.3s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.370s, learning 0.184s)
               Value function loss: 13.5864
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 381.50
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 11.55s
                        Total time: 31240.48s
                               ETA: 1171716.5s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.907s, learning 0.159s)
               Value function loss: 10.3472
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 381.14
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 12.07s
                        Total time: 31252.55s
                               ETA: 1171705.8s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.163s)
               Value function loss: 13.1691
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 381.98
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 11.33s
                        Total time: 31263.87s
                               ETA: 1171667.5s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.142s, learning 0.174s)
               Value function loss: 18.1967
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 383.00
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 11.32s
                        Total time: 31275.19s
                               ETA: 1171628.8s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.953s, learning 0.161s)
               Value function loss: 15.9709
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 381.54
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 11.11s
                        Total time: 31286.30s
                               ETA: 1171582.5s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.157s, learning 0.159s)
               Value function loss: 16.3388
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 382.02
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 11.32s
                        Total time: 31297.62s
                               ETA: 1171543.8s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.249s, learning 0.161s)
               Value function loss: 16.3743
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 381.30
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 11.41s
                        Total time: 31309.03s
                               ETA: 1171508.6s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.387s, learning 0.257s)
               Value function loss: 17.3193
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 380.80
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 11.64s
                        Total time: 31320.67s
                               ETA: 1171482.2s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.010s, learning 0.233s)
               Value function loss: 15.4922
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 380.01
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 12.24s
                        Total time: 31332.92s
                               ETA: 1171478.2s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.358s, learning 0.258s)
               Value function loss: 13.8819
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 380.89
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 11.62s
                        Total time: 31344.53s
                               ETA: 1171450.8s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.845s, learning 0.161s)
               Value function loss: 20.5099
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 380.00
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 12.01s
                        Total time: 31356.54s
                               ETA: 1171437.9s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.068s, learning 0.161s)
               Value function loss: 18.3446
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 378.39
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 11.23s
                        Total time: 31367.77s
                               ETA: 1171396.1s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.248s, learning 0.251s)
               Value function loss: 18.2424
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 378.48
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 12.50s
                        Total time: 31380.27s
                               ETA: 1171401.6s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.675s, learning 0.160s)
               Value function loss: 19.9925
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 375.76
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 11.83s
                        Total time: 31392.10s
                               ETA: 1171382.4s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.414s, learning 0.200s)
               Value function loss: 17.4299
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 375.03
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 11.61s
                        Total time: 31403.72s
                               ETA: 1171355.0s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.657s, learning 0.155s)
               Value function loss: 13.4063
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 373.07
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 11.81s
                        Total time: 31415.53s
                               ETA: 1171334.9s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.468s, learning 0.190s)
               Value function loss: 12.9798
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 371.86
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 11.66s
                        Total time: 31427.18s
                               ETA: 1171309.1s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.459s, learning 0.193s)
               Value function loss: 12.0041
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 372.21
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 11.65s
                        Total time: 31438.84s
                               ETA: 1171283.1s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.429s, learning 0.178s)
               Value function loss: 8.9270
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 372.29
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 11.61s
                        Total time: 31450.44s
                               ETA: 1171255.4s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.164s)
               Value function loss: 11.3511
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 369.57
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 11.64s
                        Total time: 31462.08s
                               ETA: 1171228.9s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.422s, learning 0.164s)
               Value function loss: 6.9035
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 368.84
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 11.59s
                        Total time: 31473.67s
                               ETA: 1171200.5s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.375s, learning 0.201s)
               Value function loss: 4.3909
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 368.63
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 11.58s
                        Total time: 31485.24s
                               ETA: 1171171.7s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.722s, learning 0.195s)
               Value function loss: 5.4683
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 368.10
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 11.92s
                        Total time: 31497.16s
                               ETA: 1171155.6s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.164s)
               Value function loss: 4.6088
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 366.81
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 11.52s
                        Total time: 31508.68s
                               ETA: 1171124.7s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.342s, learning 0.195s)
               Value function loss: 4.8802
                    Surrogate loss: -0.0134
             Mean action noise std: 0.73
                       Mean reward: 364.99
               Mean episode length: 248.85
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 11.54s
                        Total time: 31520.22s
                               ETA: 1171094.5s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.294s, learning 0.167s)
               Value function loss: 6.2948
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 361.17
               Mean episode length: 247.80
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 11.46s
                        Total time: 31531.68s
                               ETA: 1171061.5s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.380s, learning 0.203s)
               Value function loss: 5.8244
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 359.55
               Mean episode length: 247.80
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 11.58s
                        Total time: 31543.26s
                               ETA: 1171033.0s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.585s, learning 0.165s)
               Value function loss: 6.9774
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 359.38
               Mean episode length: 248.95
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 11.75s
                        Total time: 31555.01s
                               ETA: 1171010.7s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.328s, learning 0.195s)
               Value function loss: 8.1841
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 360.90
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 11.52s
                        Total time: 31566.53s
                               ETA: 1170980.0s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.766s, learning 0.159s)
               Value function loss: 6.5874
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 361.02
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 11.93s
                        Total time: 31578.46s
                               ETA: 1170964.3s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.632s, learning 0.163s)
               Value function loss: 6.3820
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 359.63
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 11.80s
                        Total time: 31590.25s
                               ETA: 1170943.8s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.274s, learning 0.162s)
               Value function loss: 9.8241
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 356.08
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 11.44s
                        Total time: 31601.69s
                               ETA: 1170909.9s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.429s, learning 0.185s)
               Value function loss: 7.0143
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 354.07
               Mean episode length: 249.31
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 11.61s
                        Total time: 31613.30s
                               ETA: 1170882.7s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.202s, learning 0.165s)
               Value function loss: 8.1858
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 354.60
               Mean episode length: 249.31
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 11.37s
                        Total time: 31624.67s
                               ETA: 1170846.3s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.321s, learning 0.206s)
               Value function loss: 13.0292
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 355.03
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 11.53s
                        Total time: 31636.20s
                               ETA: 1170815.8s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.315s, learning 0.186s)
               Value function loss: 12.3005
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 352.39
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 11.50s
                        Total time: 31647.70s
                               ETA: 1170784.4s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.256s, learning 0.161s)
               Value function loss: 11.7347
                    Surrogate loss: -0.0135
             Mean action noise std: 0.73
                       Mean reward: 352.98
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 11.42s
                        Total time: 31659.11s
                               ETA: 1170749.9s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.720s, learning 0.198s)
               Value function loss: 12.8455
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 354.68
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 11.92s
                        Total time: 31671.03s
                               ETA: 1170734.0s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.253s, learning 0.171s)
               Value function loss: 9.6623
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 356.36
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 11.42s
                        Total time: 31682.45s
                               ETA: 1170699.8s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.291s, learning 0.191s)
               Value function loss: 12.1183
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 358.69
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 11.48s
                        Total time: 31693.94s
                               ETA: 1170667.7s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.611s, learning 0.170s)
               Value function loss: 10.2832
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 357.53
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 11.78s
                        Total time: 31705.72s
                               ETA: 1170646.8s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.491s, learning 0.164s)
               Value function loss: 14.3370
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 357.69
               Mean episode length: 249.41
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 11.65s
                        Total time: 31717.37s
                               ETA: 1170621.1s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.606s, learning 0.159s)
               Value function loss: 14.3798
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 359.09
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 11.77s
                        Total time: 31729.14s
                               ETA: 1170599.6s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.240s, learning 0.171s)
               Value function loss: 14.4114
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 359.14
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 11.41s
                        Total time: 31740.55s
                               ETA: 1170565.0s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.461s, learning 0.198s)
               Value function loss: 14.5382
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 361.16
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 11.66s
                        Total time: 31752.21s
                               ETA: 1170539.6s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.080s, learning 0.267s)
               Value function loss: 13.1075
                    Surrogate loss: -0.0148
             Mean action noise std: 0.73
                       Mean reward: 365.06
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 12.35s
                        Total time: 31764.56s
                               ETA: 1170539.5s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.160s)
               Value function loss: 10.6628
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 366.84
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 11.40s
                        Total time: 31775.95s
                               ETA: 1170504.4s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.659s, learning 0.158s)
               Value function loss: 10.6976
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 369.12
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 11.82s
                        Total time: 31787.77s
                               ETA: 1170484.8s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.323s, learning 0.160s)
               Value function loss: 10.8670
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 370.27
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 11.48s
                        Total time: 31799.25s
                               ETA: 1170452.9s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.638s, learning 0.187s)
               Value function loss: 8.9924
                    Surrogate loss: -0.0029
             Mean action noise std: 0.73
                       Mean reward: 371.80
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 11.82s
                        Total time: 31811.08s
                               ETA: 1170433.6s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.676s, learning 0.233s)
               Value function loss: 7.7936
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 371.37
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 11.91s
                        Total time: 31822.99s
                               ETA: 1170417.4s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.122s, learning 0.232s)
               Value function loss: 7.2254
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 371.24
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 11.35s
                        Total time: 31834.34s
                               ETA: 1170380.8s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.611s, learning 0.260s)
               Value function loss: 3.1441
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 372.46
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 11.87s
                        Total time: 31846.21s
                               ETA: 1170363.2s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.185s, learning 0.159s)
               Value function loss: 6.9704
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 372.42
               Mean episode length: 249.94
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 11.34s
                        Total time: 31857.55s
                               ETA: 1170326.3s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.647s, learning 0.197s)
               Value function loss: 5.9089
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 371.56
               Mean episode length: 249.94
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 11.84s
                        Total time: 31869.40s
                               ETA: 1170307.8s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.664s, learning 0.190s)
               Value function loss: 7.8355
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 371.36
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 11.85s
                        Total time: 31881.25s
                               ETA: 1170289.6s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.705s, learning 0.160s)
               Value function loss: 7.5537
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 372.15
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 11.86s
                        Total time: 31893.12s
                               ETA: 1170271.8s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.717s, learning 0.200s)
               Value function loss: 6.0038
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 371.23
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 11.92s
                        Total time: 31905.03s
                               ETA: 1170255.9s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.963s, learning 0.165s)
               Value function loss: 7.3932
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 368.45
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 11.13s
                        Total time: 31916.16s
                               ETA: 1170211.2s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.397s, learning 0.329s)
               Value function loss: 9.3025
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 369.35
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 11.73s
                        Total time: 31927.89s
                               ETA: 1170188.3s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.771s, learning 0.181s)
               Value function loss: 8.0903
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 369.66
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 11.95s
                        Total time: 31939.84s
                               ETA: 1170173.8s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.032s, learning 0.165s)
               Value function loss: 10.0602
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 370.56
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 11.20s
                        Total time: 31951.04s
                               ETA: 1170131.6s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.411s, learning 0.280s)
               Value function loss: 11.4628
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 370.38
               Mean episode length: 249.98
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 11.69s
                        Total time: 31962.73s
                               ETA: 1170107.5s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.355s, learning 0.185s)
               Value function loss: 10.2723
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 370.74
               Mean episode length: 249.98
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 11.54s
                        Total time: 31974.27s
                               ETA: 1170077.8s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1489 steps/s (collection: 10.828s, learning 0.172s)
               Value function loss: 9.2691
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 368.18
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 11.00s
                        Total time: 31985.27s
                               ETA: 1170028.5s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.217s, learning 0.172s)
               Value function loss: 14.2381
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 368.12
               Mean episode length: 249.71
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 11.39s
                        Total time: 31996.65s
                               ETA: 1169993.4s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.146s, learning 0.197s)
               Value function loss: 15.5332
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 369.56
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 11.34s
                        Total time: 32008.00s
                               ETA: 1169956.6s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.592s, learning 0.177s)
               Value function loss: 12.5829
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 369.32
               Mean episode length: 249.80
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 11.77s
                        Total time: 32019.77s
                               ETA: 1169935.5s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.395s, learning 0.165s)
               Value function loss: 14.6015
                    Surrogate loss: -0.0138
             Mean action noise std: 0.73
                       Mean reward: 369.47
               Mean episode length: 249.82
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 11.56s
                        Total time: 32031.33s
                               ETA: 1169906.7s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.337s, learning 0.165s)
               Value function loss: 12.5449
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 370.71
               Mean episode length: 249.82
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 11.50s
                        Total time: 32042.83s
                               ETA: 1169875.8s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.321s, learning 0.325s)
               Value function loss: 12.7378
                    Surrogate loss: -0.0137
             Mean action noise std: 0.73
                       Mean reward: 370.61
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 11.65s
                        Total time: 32054.48s
                               ETA: 1169850.2s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.479s, learning 0.170s)
               Value function loss: 11.0962
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 371.62
               Mean episode length: 249.64
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 11.65s
                        Total time: 32066.13s
                               ETA: 1169824.6s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.829s, learning 0.189s)
               Value function loss: 13.6459
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 371.35
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 11.02s
                        Total time: 32077.14s
                               ETA: 1169776.1s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.527s, learning 0.217s)
               Value function loss: 14.7948
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 373.12
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 11.74s
                        Total time: 32088.89s
                               ETA: 1169754.1s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.684s, learning 0.163s)
               Value function loss: 11.8827
                    Surrogate loss: -0.0166
             Mean action noise std: 0.73
                       Mean reward: 374.81
               Mean episode length: 249.86
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 11.85s
                        Total time: 32100.73s
                               ETA: 1169735.8s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.263s, learning 0.161s)
               Value function loss: 13.9306
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 372.89
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 11.42s
                        Total time: 32112.16s
                               ETA: 1169702.2s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.327s, learning 0.181s)
               Value function loss: 12.7085
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 374.22
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 11.51s
                        Total time: 32123.67s
                               ETA: 1169671.6s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.505s, learning 0.167s)
               Value function loss: 10.2893
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 374.15
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 11.67s
                        Total time: 32135.34s
                               ETA: 1169647.0s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.585s, learning 0.216s)
               Value function loss: 10.1363
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 375.94
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 11.80s
                        Total time: 32147.14s
                               ETA: 1169627.1s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.593s, learning 0.164s)
               Value function loss: 8.9054
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 374.61
               Mean episode length: 249.50
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 11.76s
                        Total time: 32158.90s
                               ETA: 1169605.7s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.254s, learning 0.200s)
               Value function loss: 8.2441
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 375.51
               Mean episode length: 249.50
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 11.45s
                        Total time: 32170.35s
                               ETA: 1169573.2s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.337s, learning 0.229s)
               Value function loss: 5.6129
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 376.61
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 11.57s
                        Total time: 32181.92s
                               ETA: 1169544.8s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.475s, learning 0.205s)
               Value function loss: 6.7443
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 375.75
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 11.68s
                        Total time: 32193.60s
                               ETA: 1169520.5s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.706s, learning 0.161s)
               Value function loss: 4.1626
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 376.29
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 11.87s
                        Total time: 32205.47s
                               ETA: 1169503.0s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.619s, learning 0.164s)
               Value function loss: 4.7558
                    Surrogate loss: -0.0133
             Mean action noise std: 0.73
                       Mean reward: 378.91
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 11.78s
                        Total time: 32217.25s
                               ETA: 1169482.5s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.288s, learning 0.203s)
               Value function loss: 4.3388
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 379.62
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 11.49s
                        Total time: 32228.74s
                               ETA: 1169451.4s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.343s, learning 0.157s)
               Value function loss: 5.0410
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 380.72
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 11.50s
                        Total time: 32240.24s
                               ETA: 1169420.7s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.264s, learning 0.173s)
               Value function loss: 5.5793
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 380.86
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 11.44s
                        Total time: 32251.68s
                               ETA: 1169387.6s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.586s, learning 0.158s)
               Value function loss: 4.9680
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 379.73
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 11.74s
                        Total time: 32263.42s
                               ETA: 1169365.8s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.117s, learning 0.214s)
               Value function loss: 4.3945
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 379.09
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 11.33s
                        Total time: 32274.75s
                               ETA: 1169328.9s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.478s, learning 0.208s)
               Value function loss: 7.0518
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 380.91
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 11.69s
                        Total time: 32286.44s
                               ETA: 1169305.0s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.400s, learning 0.185s)
               Value function loss: 6.8442
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 380.32
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 11.58s
                        Total time: 32298.02s
                               ETA: 1169277.3s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1461 steps/s (collection: 10.987s, learning 0.222s)
               Value function loss: 9.8571
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 380.73
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 11.21s
                        Total time: 32309.23s
                               ETA: 1169236.1s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.895s, learning 0.199s)
               Value function loss: 7.5132
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 381.44
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 12.09s
                        Total time: 32321.33s
                               ETA: 1169227.0s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.596s, learning 0.167s)
               Value function loss: 9.1494
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 381.45
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 11.76s
                        Total time: 32333.09s
                               ETA: 1169205.8s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.095s, learning 0.168s)
               Value function loss: 8.1487
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 380.45
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 11.26s
                        Total time: 32344.35s
                               ETA: 1169166.6s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.131s, learning 0.218s)
               Value function loss: 10.7371
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 381.08
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 12.35s
                        Total time: 32356.70s
                               ETA: 1169166.6s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.847s, learning 0.177s)
               Value function loss: 12.8253
                    Surrogate loss: -0.0008
             Mean action noise std: 0.73
                       Mean reward: 381.79
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 12.02s
                        Total time: 32368.72s
                               ETA: 1169154.9s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.294s, learning 0.189s)
               Value function loss: 11.4451
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 382.28
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 11.48s
                        Total time: 32380.21s
                               ETA: 1169123.7s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.300s, learning 0.165s)
               Value function loss: 13.9229
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 384.27
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 11.47s
                        Total time: 32391.67s
                               ETA: 1169091.8s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.548s, learning 0.297s)
               Value function loss: 11.1155
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 384.30
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 11.84s
                        Total time: 32403.52s
                               ETA: 1169073.7s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.336s, learning 0.159s)
               Value function loss: 12.5810
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 383.87
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 11.50s
                        Total time: 32415.01s
                               ETA: 1169043.0s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.223s, learning 0.160s)
               Value function loss: 12.4996
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 384.41
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 11.38s
                        Total time: 32426.40s
                               ETA: 1169008.2s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.428s, learning 0.159s)
               Value function loss: 14.1221
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 384.85
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 11.59s
                        Total time: 32437.98s
                               ETA: 1168980.8s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.142s, learning 0.157s)
               Value function loss: 19.8711
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 386.41
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 11.30s
                        Total time: 32449.28s
                               ETA: 1168943.0s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.824s, learning 0.161s)
               Value function loss: 15.1906
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 386.68
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 11.98s
                        Total time: 32461.27s
                               ETA: 1168929.9s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.781s, learning 0.165s)
               Value function loss: 18.4536
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 389.51
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 11.95s
                        Total time: 32473.21s
                               ETA: 1168915.5s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.680s, learning 0.208s)
               Value function loss: 19.9488
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 389.10
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 11.89s
                        Total time: 32485.10s
                               ETA: 1168898.9s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.844s, learning 0.167s)
               Value function loss: 12.6994
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 390.71
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 12.01s
                        Total time: 32497.11s
                               ETA: 1168886.8s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.513s, learning 0.167s)
               Value function loss: 12.5951
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 392.20
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 11.68s
                        Total time: 32508.79s
                               ETA: 1168862.8s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.139s, learning 0.173s)
               Value function loss: 12.6292
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 392.39
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 11.31s
                        Total time: 32520.10s
                               ETA: 1168825.5s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.083s, learning 0.207s)
               Value function loss: 9.6745
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 393.12
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 11.29s
                        Total time: 32531.39s
                               ETA: 1168787.5s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.825s, learning 0.196s)
               Value function loss: 8.6496
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 393.39
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 12.02s
                        Total time: 32543.41s
                               ETA: 1168775.8s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.368s, learning 0.191s)
               Value function loss: 9.9522
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 394.09
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 11.56s
                        Total time: 32554.97s
                               ETA: 1168747.4s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.149s, learning 0.241s)
               Value function loss: 6.0326
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 394.90
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 11.39s
                        Total time: 32566.36s
                               ETA: 1168713.1s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.522s, learning 0.206s)
               Value function loss: 6.9227
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 395.16
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 11.73s
                        Total time: 32578.09s
                               ETA: 1168690.8s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.392s, learning 0.278s)
               Value function loss: 7.0737
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 395.09
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 11.67s
                        Total time: 32589.76s
                               ETA: 1168666.5s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.649s, learning 0.260s)
               Value function loss: 8.2087
                    Surrogate loss: -0.0124
             Mean action noise std: 0.73
                       Mean reward: 395.46
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 11.91s
                        Total time: 32601.67s
                               ETA: 1168650.8s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.593s, learning 0.161s)
               Value function loss: 8.6603
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 396.03
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 11.75s
                        Total time: 32613.42s
                               ETA: 1168629.6s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.538s, learning 0.163s)
               Value function loss: 6.7456
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 395.70
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 11.70s
                        Total time: 32625.12s
                               ETA: 1168606.4s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.236s, learning 0.161s)
               Value function loss: 7.1049
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 395.78
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 11.40s
                        Total time: 32636.52s
                               ETA: 1168572.3s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.527s, learning 0.158s)
               Value function loss: 11.1808
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 395.78
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 11.69s
                        Total time: 32648.20s
                               ETA: 1168548.6s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.807s, learning 0.162s)
               Value function loss: 9.1622
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 395.29
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 11.97s
                        Total time: 32660.17s
                               ETA: 1168535.1s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.825s, learning 0.227s)
               Value function loss: 10.5781
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 395.21
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 12.05s
                        Total time: 32672.22s
                               ETA: 1168524.5s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.252s, learning 0.161s)
               Value function loss: 11.0715
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 396.16
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 11.41s
                        Total time: 32683.64s
                               ETA: 1168491.1s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.148s, learning 0.160s)
               Value function loss: 13.4576
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 394.45
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 11.31s
                        Total time: 32694.95s
                               ETA: 1168453.9s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.722s, learning 0.209s)
               Value function loss: 11.0699
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 393.62
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 11.93s
                        Total time: 32706.88s
                               ETA: 1168439.1s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.445s, learning 0.167s)
               Value function loss: 13.4716
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 394.99
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 11.61s
                        Total time: 32718.49s
                               ETA: 1168412.8s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.602s, learning 0.212s)
               Value function loss: 18.0027
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: 393.53
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 11.81s
                        Total time: 32730.30s
                               ETA: 1168393.7s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.654s, learning 0.158s)
               Value function loss: 17.4456
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 393.25
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 11.81s
                        Total time: 32742.11s
                               ETA: 1168374.6s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.971s, learning 0.162s)
               Value function loss: 17.2708
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 393.04
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 12.13s
                        Total time: 32754.25s
                               ETA: 1168366.9s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1466 steps/s (collection: 11.012s, learning 0.157s)
               Value function loss: 17.3669
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 392.76
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 11.17s
                        Total time: 32765.42s
                               ETA: 1168324.9s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.181s, learning 0.209s)
               Value function loss: 17.1027
                    Surrogate loss: -0.0136
             Mean action noise std: 0.73
                       Mean reward: 391.59
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 11.39s
                        Total time: 32776.81s
                               ETA: 1168290.7s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.409s, learning 0.163s)
               Value function loss: 17.4326
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 390.96
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 11.57s
                        Total time: 32788.38s
                               ETA: 1168263.1s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.238s, learning 0.169s)
               Value function loss: 15.5753
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 391.13
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 11.41s
                        Total time: 32799.78s
                               ETA: 1168229.6s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.760s, learning 0.170s)
               Value function loss: 24.1368
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 389.18
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 11.93s
                        Total time: 32811.71s
                               ETA: 1168214.7s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.638s, learning 0.200s)
               Value function loss: 18.0080
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 390.27
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 11.84s
                        Total time: 32823.55s
                               ETA: 1168196.6s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.387s, learning 0.185s)
               Value function loss: 19.1538
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 389.61
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 11.57s
                        Total time: 32835.12s
                               ETA: 1168169.0s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.666s, learning 0.163s)
               Value function loss: 20.3048
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 386.52
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 11.83s
                        Total time: 32846.95s
                               ETA: 1168150.6s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.199s, learning 0.200s)
               Value function loss: 17.1630
                    Surrogate loss: -0.0122
             Mean action noise std: 0.73
                       Mean reward: 388.85
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 11.40s
                        Total time: 32858.35s
                               ETA: 1168116.9s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.440s, learning 0.181s)
               Value function loss: 13.7732
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 387.08
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 11.62s
                        Total time: 32869.97s
                               ETA: 1168091.0s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.437s, learning 0.164s)
               Value function loss: 14.0728
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 387.06
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 11.60s
                        Total time: 32881.57s
                               ETA: 1168064.5s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.449s, learning 0.163s)
               Value function loss: 14.0479
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 385.51
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 11.61s
                        Total time: 32893.19s
                               ETA: 1168038.4s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.379s, learning 0.185s)
               Value function loss: 11.3504
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 384.51
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 11.56s
                        Total time: 32904.75s
                               ETA: 1168010.6s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.383s, learning 0.227s)
               Value function loss: 12.7039
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 385.20
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 11.61s
                        Total time: 32916.36s
                               ETA: 1167984.4s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.006s, learning 0.179s)
               Value function loss: 8.4889
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 385.61
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 12.19s
                        Total time: 32928.55s
                               ETA: 1167978.7s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.536s, learning 0.229s)
               Value function loss: 5.7247
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 385.48
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 11.77s
                        Total time: 32940.31s
                               ETA: 1167958.1s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.689s, learning 0.165s)
               Value function loss: 7.5577
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 385.26
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 11.85s
                        Total time: 32952.17s
                               ETA: 1167940.6s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.330s, learning 0.164s)
               Value function loss: 6.3653
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 384.08
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 11.49s
                        Total time: 32963.66s
                               ETA: 1167910.3s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.308s, learning 0.195s)
               Value function loss: 6.6807
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 383.29
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 11.50s
                        Total time: 32975.16s
                               ETA: 1167880.4s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.597s, learning 0.169s)
               Value function loss: 8.5073
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 383.09
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 11.77s
                        Total time: 32986.93s
                               ETA: 1167859.8s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.222s, learning 0.200s)
               Value function loss: 7.3064
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 383.27
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 11.42s
                        Total time: 32998.35s
                               ETA: 1167827.0s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.201s, learning 0.190s)
               Value function loss: 9.3004
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 381.70
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 11.39s
                        Total time: 33009.74s
                               ETA: 1167793.2s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.195s, learning 0.179s)
               Value function loss: 10.5443
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 381.47
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 11.37s
                        Total time: 33021.12s
                               ETA: 1167758.7s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.368s, learning 0.188s)
               Value function loss: 9.8236
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 380.47
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 11.56s
                        Total time: 33032.67s
                               ETA: 1167730.8s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.415s, learning 0.180s)
               Value function loss: 8.6186
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 379.46
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 11.59s
                        Total time: 33044.27s
                               ETA: 1167704.2s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.883s, learning 0.166s)
               Value function loss: 12.4312
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 380.39
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 12.05s
                        Total time: 33056.31s
                               ETA: 1167693.6s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.683s, learning 0.181s)
               Value function loss: 8.7903
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 379.52
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 11.86s
                        Total time: 33068.18s
                               ETA: 1167676.5s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.151s, learning 0.213s)
               Value function loss: 11.8505
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 378.20
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 11.36s
                        Total time: 33079.54s
                               ETA: 1167641.8s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.369s, learning 0.171s)
               Value function loss: 15.4432
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 376.50
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 11.54s
                        Total time: 33091.08s
                               ETA: 1167613.3s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.869s, learning 0.194s)
               Value function loss: 16.0946
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 377.24
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 12.06s
                        Total time: 33103.15s
                               ETA: 1167603.3s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.480s, learning 0.188s)
               Value function loss: 12.6719
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 377.33
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 11.67s
                        Total time: 33114.81s
                               ETA: 1167579.4s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.349s, learning 0.187s)
               Value function loss: 13.0392
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 375.14
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 11.54s
                        Total time: 33126.35s
                               ETA: 1167550.8s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.669s, learning 0.164s)
               Value function loss: 9.6665
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 374.84
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 11.83s
                        Total time: 33138.18s
                               ETA: 1167532.6s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.599s, learning 0.184s)
               Value function loss: 12.8428
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 375.51
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 11.78s
                        Total time: 33149.97s
                               ETA: 1167512.7s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.462s, learning 0.181s)
               Value function loss: 10.7481
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 375.10
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 11.64s
                        Total time: 33161.61s
                               ETA: 1167487.9s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.043s, learning 0.187s)
               Value function loss: 15.6494
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 374.33
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 11.23s
                        Total time: 33172.84s
                               ETA: 1167448.6s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.247s, learning 0.221s)
               Value function loss: 14.7548
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 373.27
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 11.47s
                        Total time: 33184.31s
                               ETA: 1167417.6s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.734s, learning 0.160s)
               Value function loss: 15.3363
                    Surrogate loss: -0.0000
             Mean action noise std: 0.73
                       Mean reward: 372.41
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 11.89s
                        Total time: 33196.20s
                               ETA: 1167401.7s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.622s, learning 0.169s)
               Value function loss: 15.3509
                    Surrogate loss: 0.0014
             Mean action noise std: 0.73
                       Mean reward: 372.52
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 11.79s
                        Total time: 33207.99s
                               ETA: 1167382.1s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.202s, learning 0.159s)
               Value function loss: 14.5115
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 371.40
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 11.36s
                        Total time: 33219.35s
                               ETA: 1167347.5s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.187s, learning 0.239s)
               Value function loss: 10.2064
                    Surrogate loss: 0.0060
             Mean action noise std: 0.73
                       Mean reward: 371.33
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 11.43s
                        Total time: 33230.78s
                               ETA: 1167315.1s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.380s, learning 0.163s)
               Value function loss: 12.2938
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 372.02
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 11.54s
                        Total time: 33242.32s
                               ETA: 1167286.8s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.283s, learning 0.166s)
               Value function loss: 10.9117
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 371.12
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 11.45s
                        Total time: 33253.77s
                               ETA: 1167255.3s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.241s, learning 0.164s)
               Value function loss: 10.0192
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 372.61
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 11.41s
                        Total time: 33265.17s
                               ETA: 1167222.3s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.395s, learning 0.172s)
               Value function loss: 8.6637
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 371.33
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 11.57s
                        Total time: 33276.74s
                               ETA: 1167194.9s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.490s, learning 0.172s)
               Value function loss: 7.9512
                    Surrogate loss: -0.0056
             Mean action noise std: 0.73
                       Mean reward: 370.64
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 11.66s
                        Total time: 33288.40s
                               ETA: 1167170.9s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.214s, learning 0.162s)
               Value function loss: 3.3828
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 371.17
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 11.38s
                        Total time: 33299.78s
                               ETA: 1167136.9s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.277s, learning 0.168s)
               Value function loss: 7.1088
                    Surrogate loss: -0.0027
             Mean action noise std: 0.73
                       Mean reward: 371.92
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 11.44s
                        Total time: 33311.23s
                               ETA: 1167105.3s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.487s, learning 0.161s)
               Value function loss: 6.0925
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 371.38
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 11.65s
                        Total time: 33322.87s
                               ETA: 1167080.8s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.457s, learning 0.228s)
               Value function loss: 7.2580
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 369.75
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 11.69s
                        Total time: 33334.56s
                               ETA: 1167057.6s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.577s, learning 0.196s)
               Value function loss: 8.2934
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 369.27
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 11.77s
                        Total time: 33346.33s
                               ETA: 1167037.5s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.529s, learning 0.171s)
               Value function loss: 5.8128
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 368.08
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 11.70s
                        Total time: 33358.03s
                               ETA: 1167014.9s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.293s, learning 0.159s)
               Value function loss: 7.6623
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 365.98
               Mean episode length: 249.11
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 11.45s
                        Total time: 33369.48s
                               ETA: 1166983.6s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.447s, learning 0.185s)
               Value function loss: 8.9633
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 365.76
               Mean episode length: 249.11
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 11.63s
                        Total time: 33381.11s
                               ETA: 1166958.6s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.460s, learning 0.174s)
               Value function loss: 7.0358
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 366.16
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 11.63s
                        Total time: 33392.75s
                               ETA: 1166933.7s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.631s, learning 0.162s)
               Value function loss: 8.6885
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 365.82
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 11.79s
                        Total time: 33404.54s
                               ETA: 1166914.4s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.400s, learning 0.166s)
               Value function loss: 9.4842
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 366.20
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 11.57s
                        Total time: 33416.11s
                               ETA: 1166887.1s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.539s, learning 0.170s)
               Value function loss: 8.3531
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 365.94
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 11.71s
                        Total time: 33427.81s
                               ETA: 1166864.8s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.164s)
               Value function loss: 8.3228
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 365.10
               Mean episode length: 249.24
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 11.31s
                        Total time: 33439.12s
                               ETA: 1166828.4s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.664s, learning 0.168s)
               Value function loss: 10.7303
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 365.64
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 11.83s
                        Total time: 33450.95s
                               ETA: 1166810.5s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.470s, learning 0.216s)
               Value function loss: 11.4011
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 365.32
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 11.69s
                        Total time: 33462.64s
                               ETA: 1166787.5s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.459s, learning 0.162s)
               Value function loss: 8.9614
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 363.92
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 11.62s
                        Total time: 33474.26s
                               ETA: 1166762.1s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.478s, learning 0.279s)
               Value function loss: 10.3169
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 364.12
               Mean episode length: 250.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 11.76s
                        Total time: 33486.02s
                               ETA: 1166741.6s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.306s, learning 0.165s)
               Value function loss: 7.7987
                    Surrogate loss: -0.0147
             Mean action noise std: 0.73
                       Mean reward: 363.90
               Mean episode length: 250.00
                  Mean reward/step: 1.43
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 11.47s
                        Total time: 33497.49s
                               ETA: 1166711.1s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.391s, learning 0.161s)
               Value function loss: 9.7848
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 363.50
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 11.55s
                        Total time: 33509.04s
                               ETA: 1166683.4s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.193s, learning 0.184s)
               Value function loss: 8.1725
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 362.36
               Mean episode length: 250.00
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 11.38s
                        Total time: 33520.42s
                               ETA: 1166649.7s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.646s, learning 0.185s)
               Value function loss: 10.4535
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 363.23
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 11.83s
                        Total time: 33532.25s
                               ETA: 1166631.8s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.286s, learning 0.176s)
               Value function loss: 11.7922
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 361.27
               Mean episode length: 249.32
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 11.46s
                        Total time: 33543.71s
                               ETA: 1166601.0s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.400s, learning 0.162s)
               Value function loss: 9.9426
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 362.20
               Mean episode length: 249.32
                  Mean reward/step: 1.44
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 11.56s
                        Total time: 33555.27s
                               ETA: 1166573.7s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 936 steps/s (collection: 17.190s, learning 0.297s)
               Value function loss: 11.5884
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 363.67
               Mean episode length: 250.00
                  Mean reward/step: 1.45
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 17.49s
                        Total time: 33572.76s
                               ETA: 1166752.4s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 720 steps/s (collection: 22.533s, learning 0.196s)
               Value function loss: 11.6952
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 362.56
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 22.73s
                        Total time: 33595.49s
                               ETA: 1167113.0s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 729 steps/s (collection: 22.289s, learning 0.162s)
               Value function loss: 8.4803
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 365.52
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 22.45s
                        Total time: 33617.94s
                               ETA: 1167463.7s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 722 steps/s (collection: 22.516s, learning 0.174s)
               Value function loss: 8.8904
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 363.10
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 22.69s
                        Total time: 33640.63s
                               ETA: 1167822.4s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 727 steps/s (collection: 22.335s, learning 0.190s)
               Value function loss: 7.8089
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 365.56
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 22.53s
                        Total time: 33663.15s
                               ETA: 1168175.2s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 738 steps/s (collection: 22.007s, learning 0.177s)
               Value function loss: 6.9493
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 367.87
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 22.18s
                        Total time: 33685.34s
                               ETA: 1168515.8s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 735 steps/s (collection: 22.120s, learning 0.161s)
               Value function loss: 4.7151
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 368.05
               Mean episode length: 249.51
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 22.28s
                        Total time: 33707.62s
                               ETA: 1168859.5s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 721 steps/s (collection: 22.493s, learning 0.230s)
               Value function loss: 6.2991
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 368.84
               Mean episode length: 249.51
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 22.72s
                        Total time: 33730.34s
                               ETA: 1169218.3s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 721 steps/s (collection: 22.525s, learning 0.170s)
               Value function loss: 3.8531
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 369.97
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 22.70s
                        Total time: 33753.04s
                               ETA: 1169575.9s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.757s, learning 0.164s)
               Value function loss: 6.0747
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 372.84
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 21.92s
                        Total time: 33774.96s
                               ETA: 1169906.3s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 749 steps/s (collection: 21.685s, learning 0.171s)
               Value function loss: 3.7179
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 374.84
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 21.86s
                        Total time: 33796.82s
                               ETA: 1170234.3s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 745 steps/s (collection: 21.820s, learning 0.166s)
               Value function loss: 5.5120
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 378.39
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 21.99s
                        Total time: 33818.80s
                               ETA: 1170566.5s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.821s, learning 0.188s)
               Value function loss: 5.6960
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 379.68
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 23.01s
                        Total time: 33841.81s
                               ETA: 1170933.8s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 713 steps/s (collection: 22.763s, learning 0.212s)
               Value function loss: 5.4718
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 381.80
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 22.97s
                        Total time: 33864.78s
                               ETA: 1171299.7s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.496s, learning 0.174s)
               Value function loss: 5.2523
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 383.18
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 21.67s
                        Total time: 33886.45s
                               ETA: 1171620.2s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 715 steps/s (collection: 22.612s, learning 0.278s)
               Value function loss: 7.3761
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 384.15
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 22.89s
                        Total time: 33909.34s
                               ETA: 1171982.7s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 731 steps/s (collection: 22.235s, learning 0.164s)
               Value function loss: 7.4201
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 384.77
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 22.40s
                        Total time: 33931.74s
                               ETA: 1172327.9s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 747 steps/s (collection: 21.766s, learning 0.158s)
               Value function loss: 11.1555
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 386.24
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 21.92s
                        Total time: 33953.67s
                               ETA: 1172656.4s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.722s, learning 0.223s)
               Value function loss: 9.4262
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 386.41
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 21.94s
                        Total time: 33975.61s
                               ETA: 1172985.4s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 733 steps/s (collection: 22.166s, learning 0.185s)
               Value function loss: 11.0146
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 387.26
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 22.35s
                        Total time: 33997.96s
                               ETA: 1173328.1s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 725 steps/s (collection: 22.390s, learning 0.182s)
               Value function loss: 10.8769
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 387.70
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 22.57s
                        Total time: 34020.53s
                               ETA: 1173678.2s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 728 steps/s (collection: 22.318s, learning 0.162s)
               Value function loss: 12.6648
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 388.80
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 22.48s
                        Total time: 34043.01s
                               ETA: 1174024.9s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 728 steps/s (collection: 22.332s, learning 0.165s)
               Value function loss: 14.6398
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 388.57
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 22.50s
                        Total time: 34065.51s
                               ETA: 1174371.9s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 738 steps/s (collection: 22.010s, learning 0.169s)
               Value function loss: 13.9060
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 389.49
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 22.18s
                        Total time: 34087.69s
                               ETA: 1174707.7s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 739 steps/s (collection: 21.974s, learning 0.174s)
               Value function loss: 14.7467
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 388.94
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 22.15s
                        Total time: 34109.84s
                               ETA: 1175042.2s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 732 steps/s (collection: 22.186s, learning 0.189s)
               Value function loss: 13.0266
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 388.22
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 22.37s
                        Total time: 34132.21s
                               ETA: 1175384.2s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 750 steps/s (collection: 21.662s, learning 0.172s)
               Value function loss: 14.0077
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 387.91
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 21.83s
                        Total time: 34154.05s
                               ETA: 1175707.3s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 723 steps/s (collection: 22.455s, learning 0.198s)
               Value function loss: 14.5498
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 388.71
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 22.65s
                        Total time: 34176.70s
                               ETA: 1176058.4s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 756 steps/s (collection: 21.461s, learning 0.204s)
               Value function loss: 16.2586
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 389.62
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 21.66s
                        Total time: 34198.36s
                               ETA: 1176375.3s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 722 steps/s (collection: 22.498s, learning 0.165s)
               Value function loss: 19.8949
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 390.17
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 22.66s
                        Total time: 34221.03s
                               ETA: 1176726.2s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 725 steps/s (collection: 22.404s, learning 0.194s)
               Value function loss: 16.3445
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 390.58
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 22.60s
                        Total time: 34243.62s
                               ETA: 1177074.6s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 721 steps/s (collection: 22.434s, learning 0.262s)
               Value function loss: 17.3106
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 389.19
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 22.70s
                        Total time: 34266.32s
                               ETA: 1177426.1s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 734 steps/s (collection: 22.156s, learning 0.165s)
               Value function loss: 20.4895
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 391.71
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 22.32s
                        Total time: 34288.64s
                               ETA: 1177764.5s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 729 steps/s (collection: 22.300s, learning 0.160s)
               Value function loss: 14.9193
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 389.60
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 22.46s
                        Total time: 34311.10s
                               ETA: 1178107.4s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 731 steps/s (collection: 22.147s, learning 0.260s)
               Value function loss: 16.4064
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 390.61
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 22.41s
                        Total time: 34333.51s
                               ETA: 1178448.3s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 746 steps/s (collection: 21.802s, learning 0.156s)
               Value function loss: 12.0510
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 389.58
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 21.96s
                        Total time: 34355.47s
                               ETA: 1178773.5s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 715 steps/s (collection: 22.678s, learning 0.209s)
               Value function loss: 10.8638
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 389.76
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 22.89s
                        Total time: 34378.35s
                               ETA: 1179130.2s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 726 steps/s (collection: 22.378s, learning 0.168s)
               Value function loss: 9.0006
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 389.53
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 22.55s
                        Total time: 34400.90s
                               ETA: 1179475.1s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1302 steps/s (collection: 12.384s, learning 0.193s)
               Value function loss: 9.9350
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 389.56
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 12.58s
                        Total time: 34413.48s
                               ETA: 1179477.9s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.147s, learning 0.162s)
               Value function loss: 6.8075
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 389.35
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 11.31s
                        Total time: 34424.79s
                               ETA: 1179437.4s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.622s, learning 0.171s)
               Value function loss: 6.3315
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 388.46
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 11.79s
                        Total time: 34436.58s
                               ETA: 1179413.4s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.169s, learning 0.162s)
               Value function loss: 6.2874
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 388.63
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 11.33s
                        Total time: 34447.91s
                               ETA: 1179373.7s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1475 steps/s (collection: 10.948s, learning 0.158s)
               Value function loss: 6.6646
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 388.22
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 11.11s
                        Total time: 34459.02s
                               ETA: 1179326.2s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.501s, learning 0.188s)
               Value function loss: 7.5577
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 388.20
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 11.69s
                        Total time: 34470.71s
                               ETA: 1179298.7s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.630s, learning 0.200s)
               Value function loss: 6.3978
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 388.00
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 11.83s
                        Total time: 34482.54s
                               ETA: 1179276.0s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.521s, learning 0.197s)
               Value function loss: 7.0385
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 388.12
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 11.72s
                        Total time: 34494.25s
                               ETA: 1179249.5s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.337s, learning 0.174s)
               Value function loss: 10.1609
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 388.65
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 11.51s
                        Total time: 34505.76s
                               ETA: 1179216.0s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.180s, learning 0.156s)
               Value function loss: 8.7715
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 387.68
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 11.34s
                        Total time: 34517.10s
                               ETA: 1179176.5s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.088s, learning 0.189s)
               Value function loss: 9.3319
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 387.30
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 11.28s
                        Total time: 34528.38s
                               ETA: 1179135.0s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.660s, learning 0.170s)
               Value function loss: 8.7860
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 387.02
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 11.83s
                        Total time: 34540.21s
                               ETA: 1179112.4s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.252s, learning 0.162s)
               Value function loss: 12.4576
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 386.62
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 11.41s
                        Total time: 34551.62s
                               ETA: 1179075.6s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.263s, learning 0.246s)
               Value function loss: 10.7834
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 387.72
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 11.51s
                        Total time: 34563.13s
                               ETA: 1179042.1s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.344s, learning 0.202s)
               Value function loss: 12.2261
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 387.63
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 11.55s
                        Total time: 34574.68s
                               ETA: 1179009.8s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.361s, learning 0.183s)
               Value function loss: 17.0438
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 386.78
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 11.54s
                        Total time: 34586.22s
                               ETA: 1178977.5s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.061s, learning 0.237s)
               Value function loss: 15.4870
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 387.05
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 11.30s
                        Total time: 34597.52s
                               ETA: 1178936.8s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.438s, learning 0.210s)
               Value function loss: 15.6841
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 386.86
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 11.65s
                        Total time: 34609.17s
                               ETA: 1178908.1s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.301s, learning 0.165s)
               Value function loss: 15.7770
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 386.89
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 11.47s
                        Total time: 34620.63s
                               ETA: 1178873.2s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.454s, learning 0.188s)
               Value function loss: 15.1241
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 386.42
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 11.64s
                        Total time: 34632.27s
                               ETA: 1178844.3s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.765s, learning 0.220s)
               Value function loss: 15.3372
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 388.84
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 11.98s
                        Total time: 34644.26s
                               ETA: 1178827.0s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.621s, learning 0.220s)
               Value function loss: 13.6176
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 388.66
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 11.84s
                        Total time: 34656.10s
                               ETA: 1178804.9s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.391s, learning 0.162s)
               Value function loss: 21.0158
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 388.67
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 11.55s
                        Total time: 34667.65s
                               ETA: 1178773.0s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.024s, learning 0.226s)
               Value function loss: 18.5061
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 388.67
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 11.25s
                        Total time: 34678.90s
                               ETA: 1178730.8s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.374s, learning 0.191s)
               Value function loss: 19.1097
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 389.86
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 11.57s
                        Total time: 34690.47s
                               ETA: 1178699.4s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.542s, learning 0.166s)
               Value function loss: 18.7558
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 389.39
               Mean episode length: 249.49
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 11.71s
                        Total time: 34702.18s
                               ETA: 1178672.8s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.206s, learning 0.160s)
               Value function loss: 15.2064
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 390.83
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 11.37s
                        Total time: 34713.54s
                               ETA: 1178634.6s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.247s, learning 0.162s)
               Value function loss: 12.2362
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 391.51
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 11.41s
                        Total time: 34724.95s
                               ETA: 1178597.9s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.403s, learning 0.208s)
               Value function loss: 12.6266
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 390.99
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 11.61s
                        Total time: 34736.56s
                               ETA: 1178568.0s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.510s, learning 0.164s)
               Value function loss: 11.0255
                    Surrogate loss: -0.0106
             Mean action noise std: 0.73
                       Mean reward: 389.66
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 11.67s
                        Total time: 34748.24s
                               ETA: 1178540.3s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.695s, learning 0.167s)
               Value function loss: 10.0269
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 390.38
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 11.86s
                        Total time: 34760.10s
                               ETA: 1178519.0s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.085s, learning 0.172s)
               Value function loss: 9.8666
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 390.18
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 11.26s
                        Total time: 34771.36s
                               ETA: 1178477.2s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.390s, learning 0.215s)
               Value function loss: 6.7544
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 389.40
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 11.60s
                        Total time: 34782.96s
                               ETA: 1178447.2s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.761s, learning 0.192s)
               Value function loss: 5.9469
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 389.87
               Mean episode length: 249.32
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 11.95s
                        Total time: 34794.91s
                               ETA: 1178429.0s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.696s, learning 0.226s)
               Value function loss: 6.7581
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 390.37
               Mean episode length: 249.32
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 11.92s
                        Total time: 34806.84s
                               ETA: 1178409.7s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.553s, learning 0.158s)
               Value function loss: 5.5759
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 390.43
               Mean episode length: 249.32
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 11.71s
                        Total time: 34818.55s
                               ETA: 1178383.4s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.416s, learning 0.164s)
               Value function loss: 6.2279
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 391.19
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 11.58s
                        Total time: 34830.13s
                               ETA: 1178352.6s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.351s, learning 0.180s)
               Value function loss: 7.4691
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 390.39
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 11.53s
                        Total time: 34841.66s
                               ETA: 1178320.1s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.182s, learning 0.220s)
               Value function loss: 6.8963
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 390.64
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 11.40s
                        Total time: 34853.06s
                               ETA: 1178283.4s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.959s, learning 0.175s)
               Value function loss: 9.4225
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 391.00
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 11.13s
                        Total time: 34864.19s
                               ETA: 1178237.5s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.309s, learning 0.160s)
               Value function loss: 9.8338
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 391.88
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 11.47s
                        Total time: 34875.66s
                               ETA: 1178203.1s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.407s, learning 0.229s)
               Value function loss: 10.2106
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 392.05
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 11.64s
                        Total time: 34887.30s
                               ETA: 1178174.2s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.338s, learning 0.236s)
               Value function loss: 9.1562
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 393.26
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 11.57s
                        Total time: 34898.87s
                               ETA: 1178143.3s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.428s, learning 0.199s)
               Value function loss: 12.9299
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 392.96
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 11.63s
                        Total time: 34910.50s
                               ETA: 1178114.2s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1477 steps/s (collection: 10.926s, learning 0.164s)
               Value function loss: 10.6540
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 392.49
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 11.09s
                        Total time: 34921.59s
                               ETA: 1178066.9s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.195s, learning 0.258s)
               Value function loss: 13.0692
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 391.99
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 11.45s
                        Total time: 34933.04s
                               ETA: 1178032.0s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.436s, learning 0.158s)
               Value function loss: 17.4505
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 392.18
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 11.59s
                        Total time: 34944.64s
                               ETA: 1178001.8s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.371s, learning 0.174s)
               Value function loss: 19.7762
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 390.52
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 11.54s
                        Total time: 34956.18s
                               ETA: 1177969.9s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.394s, learning 0.172s)
               Value function loss: 16.1212
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 390.97
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 11.57s
                        Total time: 34967.75s
                               ETA: 1177938.8s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.577s, learning 0.252s)
               Value function loss: 17.8126
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 391.31
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 11.83s
                        Total time: 34979.58s
                               ETA: 1177916.6s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.799s, learning 0.194s)
               Value function loss: 12.8267
                    Surrogate loss: -0.0119
             Mean action noise std: 0.73
                       Mean reward: 391.81
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 11.99s
                        Total time: 34991.57s
                               ETA: 1177899.9s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.285s, learning 0.170s)
               Value function loss: 17.9447
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 391.42
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 11.45s
                        Total time: 35003.02s
                               ETA: 1177865.1s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.230s, learning 0.189s)
               Value function loss: 14.6836
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 391.61
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 11.42s
                        Total time: 35014.44s
                               ETA: 1177829.1s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.434s, learning 0.158s)
               Value function loss: 21.8745
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 393.28
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 11.59s
                        Total time: 35026.04s
                               ETA: 1177799.0s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.931s, learning 0.185s)
               Value function loss: 19.4354
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 391.34
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 11.12s
                        Total time: 35037.15s
                               ETA: 1177752.8s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.758s, learning 0.160s)
               Value function loss: 22.0150
                    Surrogate loss: -0.0068
             Mean action noise std: 0.73
                       Mean reward: 394.71
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 11.92s
                        Total time: 35049.07s
                               ETA: 1177733.7s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.789s, learning 0.160s)
               Value function loss: 21.4395
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 394.47
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 11.95s
                        Total time: 35061.02s
                               ETA: 1177715.5s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.220s, learning 0.158s)
               Value function loss: 18.9912
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 393.78
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 11.38s
                        Total time: 35072.40s
                               ETA: 1177678.2s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.254s, learning 0.208s)
               Value function loss: 16.1982
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 394.24
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 11.46s
                        Total time: 35083.86s
                               ETA: 1177643.8s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.084s, learning 0.172s)
               Value function loss: 17.7613
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 393.50
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 11.26s
                        Total time: 35095.12s
                               ETA: 1177602.4s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.086s, learning 0.160s)
               Value function loss: 15.3912
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 394.84
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 11.25s
                        Total time: 35106.36s
                               ETA: 1177560.7s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.081s, learning 0.172s)
               Value function loss: 14.0054
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 395.23
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 11.25s
                        Total time: 35117.61s
                               ETA: 1177519.3s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.604s, learning 0.177s)
               Value function loss: 12.2069
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 395.78
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 11.78s
                        Total time: 35129.39s
                               ETA: 1177495.6s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.054s, learning 0.265s)
               Value function loss: 10.4632
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 396.64
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 11.32s
                        Total time: 35140.71s
                               ETA: 1177456.4s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.611s, learning 0.185s)
               Value function loss: 5.4396
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 395.94
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 11.80s
                        Total time: 35152.51s
                               ETA: 1177433.3s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.390s, learning 0.186s)
               Value function loss: 10.1086
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 396.11
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 11.58s
                        Total time: 35164.09s
                               ETA: 1177402.7s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.254s, learning 0.216s)
               Value function loss: 7.6161
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 395.86
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 11.47s
                        Total time: 35175.56s
                               ETA: 1177368.7s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.298s, learning 0.212s)
               Value function loss: 9.4334
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 396.89
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 11.51s
                        Total time: 35187.07s
                               ETA: 1177336.0s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.590s, learning 0.170s)
               Value function loss: 10.9441
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 396.97
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 11.76s
                        Total time: 35198.83s
                               ETA: 1177311.6s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.186s, learning 0.174s)
               Value function loss: 8.3297
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 397.15
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 11.36s
                        Total time: 35210.19s
                               ETA: 1177273.9s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.484s, learning 0.163s)
               Value function loss: 10.7650
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 397.05
               Mean episode length: 250.00
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 11.65s
                        Total time: 35221.83s
                               ETA: 1177245.8s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.501s, learning 0.190s)
               Value function loss: 12.3923
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 396.43
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 11.69s
                        Total time: 35233.52s
                               ETA: 1177219.2s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.568s, learning 0.159s)
               Value function loss: 10.5050
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 396.25
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 11.73s
                        Total time: 35245.25s
                               ETA: 1177193.8s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.142s, learning 0.190s)
               Value function loss: 12.8019
                    Surrogate loss: -0.0019
             Mean action noise std: 0.73
                       Mean reward: 396.31
               Mean episode length: 250.00
                  Mean reward/step: 1.61
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 11.33s
                        Total time: 35256.58s
                               ETA: 1177155.3s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.225s, learning 0.188s)
               Value function loss: 14.2169
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 396.87
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 11.41s
                        Total time: 35268.00s
                               ETA: 1177119.4s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.535s, learning 0.224s)
               Value function loss: 12.8979
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 396.29
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 11.76s
                        Total time: 35279.75s
                               ETA: 1177095.1s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.269s, learning 0.157s)
               Value function loss: 14.3962
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 395.42
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 11.43s
                        Total time: 35291.18s
                               ETA: 1177059.7s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.136s, learning 0.165s)
               Value function loss: 19.1633
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 395.71
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 11.30s
                        Total time: 35302.48s
                               ETA: 1177020.1s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.097s, learning 0.327s)
               Value function loss: 21.0700
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 396.46
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 11.42s
                        Total time: 35313.91s
                               ETA: 1176984.7s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.828s, learning 0.161s)
               Value function loss: 16.4167
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 396.32
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 11.99s
                        Total time: 35325.89s
                               ETA: 1176968.1s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.037s, learning 0.168s)
               Value function loss: 18.5776
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 395.59
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 11.21s
                        Total time: 35337.10s
                               ETA: 1176925.4s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.492s, learning 0.225s)
               Value function loss: 15.7538
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 395.23
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 11.72s
                        Total time: 35348.82s
                               ETA: 1176899.8s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.423s, learning 0.236s)
               Value function loss: 17.9670
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 395.27
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 11.66s
                        Total time: 35360.48s
                               ETA: 1176872.3s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.661s, learning 0.160s)
               Value function loss: 15.9075
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 396.22
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 11.82s
                        Total time: 35372.30s
                               ETA: 1176850.1s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.134s, learning 0.162s)
               Value function loss: 21.3792
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 396.56
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 11.30s
                        Total time: 35383.59s
                               ETA: 1176810.6s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.342s, learning 0.156s)
               Value function loss: 24.2304
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 396.13
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 11.50s
                        Total time: 35395.09s
                               ETA: 1176777.7s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.368s, learning 0.160s)
               Value function loss: 20.8343
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 395.66
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 11.53s
                        Total time: 35406.62s
                               ETA: 1176745.9s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.761s, learning 0.161s)
               Value function loss: 23.1874
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 394.91
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 11.92s
                        Total time: 35418.54s
                               ETA: 1176727.1s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.492s, learning 0.168s)
               Value function loss: 23.9530
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 395.20
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 11.66s
                        Total time: 35430.20s
                               ETA: 1176699.7s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.272s, learning 0.199s)
               Value function loss: 16.3269
                    Surrogate loss: -0.0123
             Mean action noise std: 0.73
                       Mean reward: 393.79
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 11.47s
                        Total time: 35441.67s
                               ETA: 1176665.9s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.011s, learning 0.230s)
               Value function loss: 17.1701
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 393.30
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 11.24s
                        Total time: 35452.91s
                               ETA: 1176624.6s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.617s, learning 0.155s)
               Value function loss: 14.2521
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 393.47
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 11.77s
                        Total time: 35464.69s
                               ETA: 1176601.0s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.094s, learning 0.160s)
               Value function loss: 14.6185
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 393.70
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 11.25s
                        Total time: 35475.94s
                               ETA: 1176560.1s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.393s, learning 0.184s)
               Value function loss: 10.9198
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 394.08
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 11.58s
                        Total time: 35487.52s
                               ETA: 1176529.9s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.526s, learning 0.219s)
               Value function loss: 12.1040
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 392.38
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 11.75s
                        Total time: 35499.26s
                               ETA: 1176505.4s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.413s, learning 0.161s)
               Value function loss: 6.4099
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 391.48
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 11.57s
                        Total time: 35510.84s
                               ETA: 1176475.2s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1458 steps/s (collection: 11.078s, learning 0.159s)
               Value function loss: 9.2498
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 390.40
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 11.24s
                        Total time: 35522.07s
                               ETA: 1176433.9s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.171s, learning 0.170s)
               Value function loss: 7.9945
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 390.90
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 11.34s
                        Total time: 35533.41s
                               ETA: 1176395.9s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.122s, learning 0.164s)
               Value function loss: 9.7207
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 391.49
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 11.29s
                        Total time: 35544.70s
                               ETA: 1176356.2s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.445s, learning 0.223s)
               Value function loss: 10.1525
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 391.60
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 11.67s
                        Total time: 35556.37s
                               ETA: 1176329.2s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.201s, learning 0.167s)
               Value function loss: 7.4480
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 391.98
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 11.37s
                        Total time: 35567.74s
                               ETA: 1176292.3s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.926s, learning 0.211s)
               Value function loss: 8.3027
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 391.46
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 11.14s
                        Total time: 35578.87s
                               ETA: 1176247.7s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.324s, learning 0.171s)
               Value function loss: 11.5829
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 391.07
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 11.49s
                        Total time: 35590.37s
                               ETA: 1176215.0s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.148s, learning 0.162s)
               Value function loss: 10.8559
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 391.26
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 11.31s
                        Total time: 35601.68s
                               ETA: 1176176.2s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.250s, learning 0.257s)
               Value function loss: 15.9958
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 391.63
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 11.51s
                        Total time: 35613.18s
                               ETA: 1176143.9s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.158s)
               Value function loss: 12.8705
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 391.18
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 11.67s
                        Total time: 35624.86s
                               ETA: 1176117.1s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.331s, learning 0.203s)
               Value function loss: 14.4935
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 389.87
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 11.53s
                        Total time: 35636.39s
                               ETA: 1176085.7s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.679s, learning 0.234s)
               Value function loss: 13.7347
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 390.32
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 11.91s
                        Total time: 35648.30s
                               ETA: 1176066.9s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.244s, learning 0.182s)
               Value function loss: 17.6801
                    Surrogate loss: -0.0044
             Mean action noise std: 0.73
                       Mean reward: 390.90
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 11.43s
                        Total time: 35659.73s
                               ETA: 1176031.9s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.442s, learning 0.195s)
               Value function loss: 20.3503
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 390.62
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 11.64s
                        Total time: 35671.37s
                               ETA: 1176004.0s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.587s, learning 0.193s)
               Value function loss: 18.9082
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 389.58
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 11.78s
                        Total time: 35683.15s
                               ETA: 1175980.8s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.301s, learning 0.200s)
               Value function loss: 18.5124
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 389.12
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 11.50s
                        Total time: 35694.65s
                               ETA: 1175948.4s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.588s, learning 0.176s)
               Value function loss: 16.9327
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 389.53
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 11.76s
                        Total time: 35706.41s
                               ETA: 1175924.7s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.066s, learning 0.163s)
               Value function loss: 17.6394
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 389.93
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 11.23s
                        Total time: 35717.64s
                               ETA: 1175883.4s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.151s, learning 0.158s)
               Value function loss: 18.4289
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 389.47
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 11.31s
                        Total time: 35728.95s
                               ETA: 1175844.7s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1465 steps/s (collection: 11.018s, learning 0.158s)
               Value function loss: 19.7401
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 389.76
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 11.18s
                        Total time: 35740.13s
                               ETA: 1175801.7s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.661s, learning 0.306s)
               Value function loss: 24.6238
                    Surrogate loss: -0.0000
             Mean action noise std: 0.73
                       Mean reward: 389.39
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 11.97s
                        Total time: 35752.09s
                               ETA: 1175784.7s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.583s, learning 0.182s)
               Value function loss: 18.8275
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 388.77
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 11.77s
                        Total time: 35763.86s
                               ETA: 1175761.1s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.082s, learning 0.189s)
               Value function loss: 20.5980
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 388.49
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 11.27s
                        Total time: 35775.13s
                               ETA: 1175721.2s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.356s, learning 0.194s)
               Value function loss: 22.7119
                    Surrogate loss: 0.0005
             Mean action noise std: 0.73
                       Mean reward: 389.15
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 11.55s
                        Total time: 35786.68s
                               ETA: 1175690.6s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.031s, learning 0.194s)
               Value function loss: 14.6328
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 388.16
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 11.22s
                        Total time: 35797.91s
                               ETA: 1175649.2s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.098s, learning 0.172s)
               Value function loss: 16.4496
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 388.62
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 11.27s
                        Total time: 35809.18s
                               ETA: 1175609.4s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.413s, learning 0.224s)
               Value function loss: 13.6866
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 388.35
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 11.64s
                        Total time: 35820.81s
                               ETA: 1175581.6s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.238s, learning 0.167s)
               Value function loss: 11.4732
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 388.12
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 11.41s
                        Total time: 35832.22s
                               ETA: 1175546.3s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.579s, learning 0.205s)
               Value function loss: 10.4188
                    Surrogate loss: -0.0126
             Mean action noise std: 0.73
                       Mean reward: 387.70
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 11.78s
                        Total time: 35844.00s
                               ETA: 1175523.4s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.724s, learning 0.159s)
               Value function loss: 9.8652
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 387.00
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 11.88s
                        Total time: 35855.88s
                               ETA: 1175503.7s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1474 steps/s (collection: 10.950s, learning 0.161s)
               Value function loss: 6.5242
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 386.77
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 11.11s
                        Total time: 35867.00s
                               ETA: 1175458.7s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.537s, learning 0.286s)
               Value function loss: 6.4011
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 386.75
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 11.82s
                        Total time: 35878.82s
                               ETA: 1175437.1s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.755s, learning 0.212s)
               Value function loss: 6.1154
                    Surrogate loss: -0.0129
             Mean action noise std: 0.73
                       Mean reward: 386.20
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 11.97s
                        Total time: 35890.79s
                               ETA: 1175420.2s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.449s, learning 0.173s)
               Value function loss: 6.7495
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 385.77
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 11.62s
                        Total time: 35902.41s
                               ETA: 1175392.1s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.596s, learning 0.272s)
               Value function loss: 6.6404
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 385.25
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 11.87s
                        Total time: 35914.28s
                               ETA: 1175371.9s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.343s, learning 0.281s)
               Value function loss: 5.9945
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 384.81
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 11.62s
                        Total time: 35925.90s
                               ETA: 1175343.8s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.556s, learning 0.277s)
               Value function loss: 6.2953
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 383.95
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 11.83s
                        Total time: 35937.73s
                               ETA: 1175322.6s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.464s, learning 0.264s)
               Value function loss: 9.1032
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 383.43
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 11.73s
                        Total time: 35949.46s
                               ETA: 1175297.9s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.577s, learning 0.179s)
               Value function loss: 7.7629
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 384.70
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 11.76s
                        Total time: 35961.22s
                               ETA: 1175274.1s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.059s, learning 0.259s)
               Value function loss: 7.9095
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 384.62
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 11.32s
                        Total time: 35972.54s
                               ETA: 1175236.1s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.641s, learning 0.327s)
               Value function loss: 7.5641
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 383.53
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 11.97s
                        Total time: 35984.50s
                               ETA: 1175219.2s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.100s, learning 0.220s)
               Value function loss: 11.0251
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 382.64
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 11.32s
                        Total time: 35995.82s
                               ETA: 1175181.3s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.243s, learning 0.230s)
               Value function loss: 9.6935
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 382.58
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 11.47s
                        Total time: 36007.30s
                               ETA: 1175148.3s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.161s)
               Value function loss: 11.4284
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 381.51
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 11.45s
                        Total time: 36018.75s
                               ETA: 1175114.6s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.965s, learning 0.160s)
               Value function loss: 16.2967
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 381.68
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 11.12s
                        Total time: 36029.87s
                               ETA: 1175070.3s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.164s)
               Value function loss: 14.6186
                    Surrogate loss: -0.0009
             Mean action noise std: 0.73
                       Mean reward: 381.43
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 11.53s
                        Total time: 36041.40s
                               ETA: 1175039.1s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.265s, learning 0.200s)
               Value function loss: 15.2820
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 380.84
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 11.46s
                        Total time: 36052.86s
                               ETA: 1175005.9s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.673s, learning 0.190s)
               Value function loss: 14.7192
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 382.57
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 11.86s
                        Total time: 36064.72s
                               ETA: 1174985.8s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.440s, learning 0.169s)
               Value function loss: 15.7612
                    Surrogate loss: -0.0043
             Mean action noise std: 0.73
                       Mean reward: 383.19
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 11.61s
                        Total time: 36076.33s
                               ETA: 1174957.3s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.349s, learning 0.212s)
               Value function loss: 14.8184
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 381.92
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 11.56s
                        Total time: 36087.89s
                               ETA: 1174927.3s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.132s, learning 0.159s)
               Value function loss: 15.2953
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 382.19
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 11.29s
                        Total time: 36099.18s
                               ETA: 1174888.6s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.172s, learning 0.201s)
               Value function loss: 22.2184
                    Surrogate loss: 0.0062
             Mean action noise std: 0.73
                       Mean reward: 384.15
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 11.37s
                        Total time: 36110.56s
                               ETA: 1174852.5s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1467 steps/s (collection: 11.012s, learning 0.155s)
               Value function loss: 17.9151
                    Surrogate loss: 0.0004
             Mean action noise std: 0.73
                       Mean reward: 383.86
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 11.17s
                        Total time: 36121.72s
                               ETA: 1174809.7s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.524s, learning 0.175s)
               Value function loss: 19.9750
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 383.57
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 11.70s
                        Total time: 36133.42s
                               ETA: 1174784.3s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.434s, learning 0.261s)
               Value function loss: 20.6488
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 384.36
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 11.70s
                        Total time: 36145.12s
                               ETA: 1174758.7s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.450s, learning 0.250s)
               Value function loss: 16.4876
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 384.14
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 11.70s
                        Total time: 36156.82s
                               ETA: 1174733.3s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.248s, learning 0.166s)
               Value function loss: 15.3595
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 384.46
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 11.41s
                        Total time: 36168.23s
                               ETA: 1174698.7s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.341s, learning 0.206s)
               Value function loss: 15.4639
                    Surrogate loss: 0.0023
             Mean action noise std: 0.73
                       Mean reward: 384.80
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 11.55s
                        Total time: 36179.78s
                               ETA: 1174668.3s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.521s, learning 0.186s)
               Value function loss: 14.8266
                    Surrogate loss: -0.0017
             Mean action noise std: 0.73
                       Mean reward: 385.03
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 11.71s
                        Total time: 36191.49s
                               ETA: 1174643.2s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.244s, learning 0.161s)
               Value function loss: 12.2707
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 384.55
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 11.41s
                        Total time: 36202.89s
                               ETA: 1174608.3s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.532s, learning 0.252s)
               Value function loss: 14.9504
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 385.48
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 11.78s
                        Total time: 36214.68s
                               ETA: 1174585.6s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.354s, learning 0.304s)
               Value function loss: 10.0698
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 385.10
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 11.66s
                        Total time: 36226.33s
                               ETA: 1174558.9s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.780s, learning 0.204s)
               Value function loss: 7.5577
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 384.55
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 11.98s
                        Total time: 36238.32s
                               ETA: 1174542.8s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.565s, learning 0.163s)
               Value function loss: 8.9700
                    Surrogate loss: -0.0033
             Mean action noise std: 0.73
                       Mean reward: 384.77
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 11.73s
                        Total time: 36250.04s
                               ETA: 1174518.4s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.914s, learning 0.191s)
               Value function loss: 7.1118
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 384.84
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 12.11s
                        Total time: 36262.15s
                               ETA: 1174506.2s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.684s, learning 0.225s)
               Value function loss: 7.9385
                    Surrogate loss: -0.0028
             Mean action noise std: 0.73
                       Mean reward: 384.86
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 11.91s
                        Total time: 36274.06s
                               ETA: 1174487.7s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.464s, learning 0.219s)
               Value function loss: 10.1431
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 383.89
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 11.68s
                        Total time: 36285.74s
                               ETA: 1174461.8s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.551s, learning 0.185s)
               Value function loss: 9.2204
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 383.86
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 11.74s
                        Total time: 36297.48s
                               ETA: 1174437.7s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.590s, learning 0.162s)
               Value function loss: 11.9159
                    Surrogate loss: 0.0009
             Mean action noise std: 0.73
                       Mean reward: 383.50
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 11.75s
                        Total time: 36309.23s
                               ETA: 1174414.1s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.272s, learning 0.163s)
               Value function loss: 12.2868
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 383.96
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 11.43s
                        Total time: 36320.66s
                               ETA: 1174380.3s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.156s, learning 0.174s)
               Value function loss: 11.5496
                    Surrogate loss: -0.0013
             Mean action noise std: 0.73
                       Mean reward: 384.73
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 11.33s
                        Total time: 36331.99s
                               ETA: 1174343.0s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.336s, learning 0.161s)
               Value function loss: 10.6203
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 383.95
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 11.50s
                        Total time: 36343.49s
                               ETA: 1174311.2s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.143s, learning 0.161s)
               Value function loss: 13.4978
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 382.40
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 11.30s
                        Total time: 36354.80s
                               ETA: 1174273.2s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.245s, learning 0.186s)
               Value function loss: 10.3955
                    Surrogate loss: -0.0012
             Mean action noise std: 0.73
                       Mean reward: 381.83
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 11.43s
                        Total time: 36366.23s
                               ETA: 1174239.3s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.822s, learning 0.165s)
               Value function loss: 12.8652
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 380.96
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 11.99s
                        Total time: 36378.21s
                               ETA: 1174223.3s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.730s, learning 0.162s)
               Value function loss: 16.5154
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 379.73
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 11.89s
                        Total time: 36390.10s
                               ETA: 1174204.3s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.135s, learning 0.170s)
               Value function loss: 17.5582
                    Surrogate loss: -0.0046
             Mean action noise std: 0.73
                       Mean reward: 377.76
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 11.31s
                        Total time: 36401.41s
                               ETA: 1174166.4s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.271s, learning 0.162s)
               Value function loss: 14.8450
                    Surrogate loss: -0.0077
             Mean action noise std: 0.73
                       Mean reward: 378.23
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 11.43s
                        Total time: 36412.84s
                               ETA: 1174132.6s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.678s, learning 0.174s)
               Value function loss: 17.5607
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 380.05
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 11.85s
                        Total time: 36424.69s
                               ETA: 1174112.3s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.396s, learning 0.168s)
               Value function loss: 12.1108
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 378.83
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 11.56s
                        Total time: 36436.26s
                               ETA: 1174082.8s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.319s, learning 0.160s)
               Value function loss: 15.8542
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 378.85
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 11.48s
                        Total time: 36447.74s
                               ETA: 1174050.5s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.542s, learning 0.187s)
               Value function loss: 13.0324
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 378.00
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 11.73s
                        Total time: 36459.47s
                               ETA: 1174026.3s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.204s, learning 0.162s)
               Value function loss: 18.0456
                    Surrogate loss: -0.0015
             Mean action noise std: 0.73
                       Mean reward: 376.49
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 11.37s
                        Total time: 36470.83s
                               ETA: 1173990.5s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.142s, learning 0.211s)
               Value function loss: 17.3752
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 377.76
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 11.35s
                        Total time: 36482.19s
                               ETA: 1173954.2s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.247s, learning 0.165s)
               Value function loss: 15.6405
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 375.67
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 11.41s
                        Total time: 36493.60s
                               ETA: 1173919.8s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.057s, learning 0.290s)
               Value function loss: 15.9909
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 376.79
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 11.35s
                        Total time: 36504.95s
                               ETA: 1173883.3s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.639s, learning 0.200s)
               Value function loss: 14.6357
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 376.13
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 11.84s
                        Total time: 36516.78s
                               ETA: 1173862.7s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.589s, learning 0.163s)
               Value function loss: 12.3814
                    Surrogate loss: -0.0131
             Mean action noise std: 0.73
                       Mean reward: 377.04
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 11.75s
                        Total time: 36528.54s
                               ETA: 1173839.3s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.442s, learning 0.216s)
               Value function loss: 14.6398
                    Surrogate loss: -0.0032
             Mean action noise std: 0.73
                       Mean reward: 378.09
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 11.66s
                        Total time: 36540.19s
                               ETA: 1173812.9s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.288s, learning 0.167s)
               Value function loss: 11.3407
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 378.26
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 12.46s
                        Total time: 36552.65s
                               ETA: 1173812.1s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.192s, learning 0.228s)
               Value function loss: 10.9106
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 378.24
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 11.42s
                        Total time: 36564.07s
                               ETA: 1173778.0s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.418s, learning 0.171s)
               Value function loss: 10.4129
                    Surrogate loss: -0.0006
             Mean action noise std: 0.73
                       Mean reward: 378.28
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 11.59s
                        Total time: 36575.66s
                               ETA: 1173749.4s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.378s, learning 0.188s)
               Value function loss: 8.2446
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 378.97
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 11.57s
                        Total time: 36587.22s
                               ETA: 1173720.0s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.430s, learning 0.192s)
               Value function loss: 4.5188
                    Surrogate loss: -0.0100
             Mean action noise std: 0.73
                       Mean reward: 379.91
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 11.62s
                        Total time: 36598.84s
                               ETA: 1173692.5s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.611s, learning 0.160s)
               Value function loss: 8.6470
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 380.75
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 11.77s
                        Total time: 36610.62s
                               ETA: 1173669.8s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.459s, learning 0.167s)
               Value function loss: 7.0286
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 381.41
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 11.63s
                        Total time: 36622.24s
                               ETA: 1173642.4s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.792s, learning 0.164s)
               Value function loss: 8.3043
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 380.64
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 11.96s
                        Total time: 36634.20s
                               ETA: 1173625.6s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.391s, learning 0.199s)
               Value function loss: 9.9866
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 380.89
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 11.59s
                        Total time: 36645.79s
                               ETA: 1173597.1s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.557s, learning 0.171s)
               Value function loss: 7.5279
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 381.76
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 11.73s
                        Total time: 36657.52s
                               ETA: 1173573.0s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.440s, learning 0.164s)
               Value function loss: 9.8491
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 381.54
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 11.60s
                        Total time: 36669.12s
                               ETA: 1173545.0s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.405s, learning 0.180s)
               Value function loss: 10.6378
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 380.61
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 11.58s
                        Total time: 36680.71s
                               ETA: 1173516.3s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.404s, learning 0.157s)
               Value function loss: 8.6630
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 379.78
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 11.56s
                        Total time: 36692.27s
                               ETA: 1173486.9s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.088s, learning 0.231s)
               Value function loss: 12.0742
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 379.56
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 11.32s
                        Total time: 36703.58s
                               ETA: 1173449.8s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1479 steps/s (collection: 10.884s, learning 0.186s)
               Value function loss: 13.4933
                    Surrogate loss: -0.0071
             Mean action noise std: 0.73
                       Mean reward: 380.92
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 11.07s
                        Total time: 36714.66s
                               ETA: 1173404.7s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.714s, learning 0.161s)
               Value function loss: 12.0667
                    Surrogate loss: -0.0025
             Mean action noise std: 0.73
                       Mean reward: 380.62
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 11.87s
                        Total time: 36726.53s
                               ETA: 1173385.4s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.498s, learning 0.165s)
               Value function loss: 12.8696
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 379.70
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 11.66s
                        Total time: 36738.19s
                               ETA: 1173359.3s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.689s, learning 0.200s)
               Value function loss: 18.6431
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 380.40
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 11.89s
                        Total time: 36750.08s
                               ETA: 1173340.4s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1445 steps/s (collection: 11.170s, learning 0.162s)
               Value function loss: 18.7388
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 381.27
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 11.33s
                        Total time: 36761.41s
                               ETA: 1173303.8s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.244s, learning 0.158s)
               Value function loss: 15.8624
                    Surrogate loss: -0.0116
             Mean action noise std: 0.73
                       Mean reward: 381.70
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 11.40s
                        Total time: 36772.81s
                               ETA: 1173269.4s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.262s, learning 0.161s)
               Value function loss: 18.2288
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 378.96
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 11.42s
                        Total time: 36784.24s
                               ETA: 1173235.7s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.856s, learning 0.163s)
               Value function loss: 14.4009
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 380.91
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 12.02s
                        Total time: 36796.26s
                               ETA: 1173221.0s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.494s, learning 0.206s)
               Value function loss: 17.0932
                    Surrogate loss: 0.0019
             Mean action noise std: 0.73
                       Mean reward: 381.53
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 11.70s
                        Total time: 36807.96s
                               ETA: 1173196.2s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.389s, learning 0.188s)
               Value function loss: 15.5059
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 382.26
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 11.58s
                        Total time: 36819.53s
                               ETA: 1173167.4s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.766s, learning 0.170s)
               Value function loss: 20.2338
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 382.05
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 11.94s
                        Total time: 36831.47s
                               ETA: 1173150.1s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.254s, learning 0.276s)
               Value function loss: 23.4259
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 382.26
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 11.53s
                        Total time: 36843.00s
                               ETA: 1173119.9s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.450s, learning 0.178s)
               Value function loss: 18.4499
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 382.67
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 11.63s
                        Total time: 36854.63s
                               ETA: 1173092.7s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.371s, learning 0.260s)
               Value function loss: 21.1666
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 382.38
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 11.63s
                        Total time: 36866.26s
                               ETA: 1173065.7s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.143s, learning 0.214s)
               Value function loss: 21.0499
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 382.03
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 11.36s
                        Total time: 36877.62s
                               ETA: 1173030.0s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.488s, learning 0.159s)
               Value function loss: 16.0954
                    Surrogate loss: -0.0086
             Mean action noise std: 0.73
                       Mean reward: 381.22
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 11.65s
                        Total time: 36889.26s
                               ETA: 1173003.6s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.276s, learning 0.165s)
               Value function loss: 16.5847
                    Surrogate loss: 0.0142
             Mean action noise std: 0.73
                       Mean reward: 382.24
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 11.44s
                        Total time: 36900.70s
                               ETA: 1172970.5s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.414s, learning 0.270s)
               Value function loss: 13.1552
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 382.76
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 11.68s
                        Total time: 36912.39s
                               ETA: 1172945.3s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.375s, learning 0.163s)
               Value function loss: 12.5861
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 382.92
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 11.54s
                        Total time: 36923.93s
                               ETA: 1172915.4s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.310s, learning 0.164s)
               Value function loss: 9.6105
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 383.38
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 11.47s
                        Total time: 36935.40s
                               ETA: 1172883.4s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.442s, learning 0.223s)
               Value function loss: 10.7918
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 384.08
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 11.66s
                        Total time: 36947.06s
                               ETA: 1172857.6s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.090s, learning 0.193s)
               Value function loss: 6.3111
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 384.59
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 11.28s
                        Total time: 36958.35s
                               ETA: 1172819.6s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1491 steps/s (collection: 10.820s, learning 0.166s)
               Value function loss: 8.5690
                    Surrogate loss: -0.0110
             Mean action noise std: 0.73
                       Mean reward: 384.56
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 10.99s
                        Total time: 36969.33s
                               ETA: 1172772.3s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.440s, learning 0.229s)
               Value function loss: 6.2967
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 385.05
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 11.67s
                        Total time: 36981.00s
                               ETA: 1172746.6s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.279s, learning 0.166s)
               Value function loss: 8.4257
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 385.26
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 11.45s
                        Total time: 36992.45s
                               ETA: 1172713.9s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.039s, learning 0.165s)
               Value function loss: 8.7882
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 385.91
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 11.20s
                        Total time: 37003.65s
                               ETA: 1172673.5s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.523s, learning 0.165s)
               Value function loss: 6.9176
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 385.61
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 11.69s
                        Total time: 37015.34s
                               ETA: 1172648.4s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.298s, learning 0.167s)
               Value function loss: 7.1831
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 385.69
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 11.47s
                        Total time: 37026.81s
                               ETA: 1172616.3s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.250s, learning 0.165s)
               Value function loss: 9.9019
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 386.43
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 11.41s
                        Total time: 37038.22s
                               ETA: 1172582.6s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.564s, learning 0.197s)
               Value function loss: 8.4610
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 386.71
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 11.76s
                        Total time: 37049.98s
                               ETA: 1172559.9s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.477s, learning 0.233s)
               Value function loss: 12.6152
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 385.85
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 11.71s
                        Total time: 37061.69s
                               ETA: 1172535.6s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.434s, learning 0.158s)
               Value function loss: 10.6573
                    Surrogate loss: -0.0011
             Mean action noise std: 0.73
                       Mean reward: 385.86
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 11.59s
                        Total time: 37073.28s
                               ETA: 1172507.6s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.222s, learning 0.161s)
               Value function loss: 11.8354
                    Surrogate loss: -0.0036
             Mean action noise std: 0.73
                       Mean reward: 386.54
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 11.38s
                        Total time: 37084.67s
                               ETA: 1172473.0s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.616s, learning 0.161s)
               Value function loss: 10.7263
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 386.09
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 11.78s
                        Total time: 37096.44s
                               ETA: 1172450.8s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.110s, learning 0.206s)
               Value function loss: 13.4945
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 385.90
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 11.32s
                        Total time: 37107.76s
                               ETA: 1172414.1s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.295s, learning 0.207s)
               Value function loss: 16.1658
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 386.14
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 11.50s
                        Total time: 37119.26s
                               ETA: 1172383.3s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.315s, learning 0.249s)
               Value function loss: 15.4939
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 386.44
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 11.56s
                        Total time: 37130.82s
                               ETA: 1172354.4s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.208s, learning 0.162s)
               Value function loss: 14.3779
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 386.82
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 11.37s
                        Total time: 37142.19s
                               ETA: 1172319.4s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.277s, learning 0.169s)
               Value function loss: 13.0199
                    Surrogate loss: -0.0117
             Mean action noise std: 0.73
                       Mean reward: 385.79
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 11.45s
                        Total time: 37153.64s
                               ETA: 1172286.9s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.154s, learning 0.174s)
               Value function loss: 14.1893
                    Surrogate loss: -0.0089
             Mean action noise std: 0.73
                       Mean reward: 385.95
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 11.33s
                        Total time: 37164.97s
                               ETA: 1172250.6s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.306s, learning 0.167s)
               Value function loss: 13.8055
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 385.32
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 11.47s
                        Total time: 37176.44s
                               ETA: 1172218.9s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.663s, learning 0.231s)
               Value function loss: 15.1899
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 385.75
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 11.89s
                        Total time: 37188.34s
                               ETA: 1172200.5s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.714s, learning 0.160s)
               Value function loss: 18.9400
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 384.98
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 11.87s
                        Total time: 37200.21s
                               ETA: 1172181.5s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.427s, learning 0.165s)
               Value function loss: 14.4990
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 384.81
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 11.59s
                        Total time: 37211.80s
                               ETA: 1172153.6s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.091s, learning 0.162s)
               Value function loss: 15.7951
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 385.34
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 11.25s
                        Total time: 37223.05s
                               ETA: 1172115.0s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.436s, learning 0.225s)
               Value function loss: 17.0196
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 384.56
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 11.66s
                        Total time: 37234.71s
                               ETA: 1172089.3s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.239s, learning 0.168s)
               Value function loss: 12.1967
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 384.81
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 11.41s
                        Total time: 37246.12s
                               ETA: 1172055.6s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.074s, learning 0.164s)
               Value function loss: 12.9377
                    Surrogate loss: -0.0108
             Mean action noise std: 0.73
                       Mean reward: 384.12
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 11.24s
                        Total time: 37257.36s
                               ETA: 1172016.6s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.042s, learning 0.158s)
               Value function loss: 11.3887
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 384.04
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 11.20s
                        Total time: 37268.56s
                               ETA: 1171976.5s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.957s, learning 0.161s)
               Value function loss: 8.9898
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 384.29
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 11.12s
                        Total time: 37279.68s
                               ETA: 1171933.8s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1487 steps/s (collection: 10.846s, learning 0.169s)
               Value function loss: 8.1255
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 381.74
               Mean episode length: 248.57
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 11.01s
                        Total time: 37290.69s
                               ETA: 1171887.8s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.907s, learning 0.188s)
               Value function loss: 7.8175
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 381.33
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 12.09s
                        Total time: 37302.79s
                               ETA: 1171875.8s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.960s, learning 0.161s)
               Value function loss: 5.6492
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 382.40
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 11.12s
                        Total time: 37313.91s
                               ETA: 1171833.2s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.256s, learning 0.159s)
               Value function loss: 5.6659
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 382.78
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 11.41s
                        Total time: 37325.32s
                               ETA: 1171799.9s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.533s, learning 0.181s)
               Value function loss: 5.7241
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 378.94
               Mean episode length: 247.37
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 11.71s
                        Total time: 37337.04s
                               ETA: 1171776.0s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1450 steps/s (collection: 11.101s, learning 0.193s)
               Value function loss: 6.0673
                    Surrogate loss: -0.0022
             Mean action noise std: 0.73
                       Mean reward: 378.33
               Mean episode length: 247.37
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 11.29s
                        Total time: 37348.33s
                               ETA: 1171738.9s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.490s, learning 0.260s)
               Value function loss: 6.1855
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 379.00
               Mean episode length: 247.37
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 11.75s
                        Total time: 37360.08s
                               ETA: 1171716.1s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.233s, learning 0.166s)
               Value function loss: 5.5613
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 383.13
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 11.40s
                        Total time: 37371.48s
                               ETA: 1171682.3s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.480s, learning 0.197s)
               Value function loss: 6.0809
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 382.82
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 11.68s
                        Total time: 37383.16s
                               ETA: 1171657.3s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.084s, learning 0.172s)
               Value function loss: 9.3516
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 381.97
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 11.26s
                        Total time: 37394.41s
                               ETA: 1171619.0s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.493s, learning 0.232s)
               Value function loss: 8.5702
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 382.04
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 11.73s
                        Total time: 37406.14s
                               ETA: 1171595.5s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.268s, learning 0.167s)
               Value function loss: 8.7518
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 382.35
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 11.43s
                        Total time: 37417.57s
                               ETA: 1171562.9s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.707s, learning 0.193s)
               Value function loss: 8.7179
                    Surrogate loss: -0.0097
             Mean action noise std: 0.73
                       Mean reward: 382.33
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 11.90s
                        Total time: 37429.47s
                               ETA: 1171544.9s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.517s, learning 0.218s)
               Value function loss: 11.4817
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 380.77
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 11.74s
                        Total time: 37441.21s
                               ETA: 1171521.8s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.628s, learning 0.182s)
               Value function loss: 9.7703
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 381.81
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 11.81s
                        Total time: 37453.02s
                               ETA: 1171500.9s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.421s, learning 0.199s)
               Value function loss: 11.4210
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 381.95
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 11.62s
                        Total time: 37464.64s
                               ETA: 1171474.1s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.238s, learning 0.225s)
               Value function loss: 16.5835
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 382.64
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 11.46s
                        Total time: 37476.10s
                               ETA: 1171442.5s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.252s, learning 0.173s)
               Value function loss: 14.2210
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 381.56
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 11.43s
                        Total time: 37487.53s
                               ETA: 1171409.7s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.575s, learning 0.166s)
               Value function loss: 14.9892
                    Surrogate loss: -0.0120
             Mean action noise std: 0.73
                       Mean reward: 380.89
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 11.74s
                        Total time: 37499.27s
                               ETA: 1171386.7s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.184s, learning 0.184s)
               Value function loss: 15.0326
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 382.48
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 11.37s
                        Total time: 37510.64s
                               ETA: 1171352.1s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1443 steps/s (collection: 11.176s, learning 0.171s)
               Value function loss: 14.6948
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 382.57
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 11.35s
                        Total time: 37521.98s
                               ETA: 1171316.9s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.550s, learning 0.178s)
               Value function loss: 14.4360
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 382.73
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 11.73s
                        Total time: 37533.71s
                               ETA: 1171293.6s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.260s, learning 0.158s)
               Value function loss: 12.6171
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 383.16
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 11.42s
                        Total time: 37545.13s
                               ETA: 1171260.6s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.123s, learning 0.184s)
               Value function loss: 18.5207
                    Surrogate loss: 0.0005
             Mean action noise std: 0.73
                       Mean reward: 383.03
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 11.31s
                        Total time: 37556.44s
                               ETA: 1171224.1s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1441 steps/s (collection: 11.202s, learning 0.162s)
               Value function loss: 17.1307
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 381.50
               Mean episode length: 248.93
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 11.36s
                        Total time: 37567.80s
                               ETA: 1171189.5s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.751s, learning 0.197s)
               Value function loss: 17.1612
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 382.59
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 11.95s
                        Total time: 37579.75s
                               ETA: 1171173.1s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.446s, learning 0.209s)
               Value function loss: 19.3337
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 383.55
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 11.66s
                        Total time: 37591.41s
                               ETA: 1171147.5s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.287s, learning 0.159s)
               Value function loss: 14.7935
                    Surrogate loss: -0.0121
             Mean action noise std: 0.73
                       Mean reward: 383.79
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 11.45s
                        Total time: 37602.85s
                               ETA: 1171115.5s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.432s, learning 0.193s)
               Value function loss: 12.6453
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 384.05
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 11.63s
                        Total time: 37614.48s
                               ETA: 1171089.0s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.466s, learning 0.197s)
               Value function loss: 12.7360
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 384.69
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 11.66s
                        Total time: 37626.14s
                               ETA: 1171063.7s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.757s, learning 0.173s)
               Value function loss: 12.5600
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 384.51
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 11.93s
                        Total time: 37638.07s
                               ETA: 1171046.8s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.665s, learning 0.194s)
               Value function loss: 9.7385
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 383.06
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 11.86s
                        Total time: 37649.93s
                               ETA: 1171027.6s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.321s, learning 0.191s)
               Value function loss: 10.6770
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 384.08
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 11.51s
                        Total time: 37661.44s
                               ETA: 1170997.7s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.436s, learning 0.233s)
               Value function loss: 7.1810
                    Surrogate loss: -0.0010
             Mean action noise std: 0.73
                       Mean reward: 383.64
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 11.67s
                        Total time: 37673.11s
                               ETA: 1170972.6s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.614s, learning 0.160s)
               Value function loss: 4.9603
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 383.79
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 11.77s
                        Total time: 37684.88s
                               ETA: 1170950.8s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.567s, learning 0.291s)
               Value function loss: 6.4058
                    Surrogate loss: -0.0047
             Mean action noise std: 0.73
                       Mean reward: 384.32
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 11.86s
                        Total time: 37696.74s
                               ETA: 1170931.6s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.586s, learning 0.252s)
               Value function loss: 6.0524
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 382.74
               Mean episode length: 248.86
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 11.84s
                        Total time: 37708.58s
                               ETA: 1170911.8s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.550s, learning 0.184s)
               Value function loss: 5.9942
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 378.44
               Mean episode length: 247.56
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 11.73s
                        Total time: 37720.31s
                               ETA: 1170888.8s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.771s, learning 0.189s)
               Value function loss: 7.5782
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 377.76
               Mean episode length: 247.56
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 11.96s
                        Total time: 37732.27s
                               ETA: 1170872.8s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.144s, learning 0.181s)
               Value function loss: 6.9451
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 381.54
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 11.33s
                        Total time: 37743.60s
                               ETA: 1170837.1s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.522s, learning 0.185s)
               Value function loss: 9.0158
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 382.23
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 11.71s
                        Total time: 37755.31s
                               ETA: 1170813.3s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.230s, learning 0.165s)
               Value function loss: 9.7654
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 380.19
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 11.40s
                        Total time: 37766.70s
                               ETA: 1170779.8s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.639s, learning 0.174s)
               Value function loss: 9.3424
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 380.42
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 11.81s
                        Total time: 37778.52s
                               ETA: 1170759.3s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.960s, learning 0.229s)
               Value function loss: 8.7631
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 380.35
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 11.19s
                        Total time: 37789.70s
                               ETA: 1170719.5s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.321s, learning 0.265s)
               Value function loss: 12.0205
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 378.35
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 11.59s
                        Total time: 37801.29s
                               ETA: 1170691.9s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.625s, learning 0.221s)
               Value function loss: 9.6476
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 377.45
               Mean episode length: 249.21
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 11.85s
                        Total time: 37813.14s
                               ETA: 1170672.5s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.218s, learning 0.230s)
               Value function loss: 11.4898
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 376.60
               Mean episode length: 249.21
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 11.45s
                        Total time: 37824.58s
                               ETA: 1170640.7s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.844s, learning 0.236s)
               Value function loss: 14.4106
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 377.63
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 12.08s
                        Total time: 37836.66s
                               ETA: 1170628.5s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.282s, learning 0.164s)
               Value function loss: 16.7064
                    Surrogate loss: -0.0024
             Mean action noise std: 0.73
                       Mean reward: 376.81
               Mean episode length: 249.45
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 11.45s
                        Total time: 37848.11s
                               ETA: 1170596.6s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.451s, learning 0.180s)
               Value function loss: 14.1976
                    Surrogate loss: -0.0112
             Mean action noise std: 0.73
                       Mean reward: 374.86
               Mean episode length: 249.11
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 11.63s
                        Total time: 37859.74s
                               ETA: 1170570.5s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.278s, learning 0.161s)
               Value function loss: 14.8970
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 374.24
               Mean episode length: 249.11
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 11.44s
                        Total time: 37871.18s
                               ETA: 1170538.5s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.559s, learning 0.260s)
               Value function loss: 11.0473
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 375.48
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 11.82s
                        Total time: 37883.00s
                               ETA: 1170518.2s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.306s, learning 0.191s)
               Value function loss: 15.6142
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 374.04
               Mean episode length: 248.51
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 11.50s
                        Total time: 37894.49s
                               ETA: 1170487.9s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1455 steps/s (collection: 11.072s, learning 0.185s)
               Value function loss: 11.6021
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 373.33
               Mean episode length: 249.40
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 11.26s
                        Total time: 37905.75s
                               ETA: 1170450.3s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.674s, learning 0.229s)
               Value function loss: 17.7406
                    Surrogate loss: 0.0005
             Mean action noise std: 0.73
                       Mean reward: 373.40
               Mean episode length: 249.11
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 11.90s
                        Total time: 37917.65s
                               ETA: 1170432.7s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.910s, learning 0.169s)
               Value function loss: 15.7585
                    Surrogate loss: -0.0030
             Mean action noise std: 0.73
                       Mean reward: 371.86
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 12.08s
                        Total time: 37929.73s
                               ETA: 1170420.5s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.542s, learning 0.188s)
               Value function loss: 16.1554
                    Surrogate loss: -0.0093
             Mean action noise std: 0.73
                       Mean reward: 373.87
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 11.73s
                        Total time: 37941.46s
                               ETA: 1170397.5s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.685s, learning 0.166s)
               Value function loss: 15.6540
                    Surrogate loss: -0.0069
             Mean action noise std: 0.73
                       Mean reward: 374.04
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 11.85s
                        Total time: 37953.31s
                               ETA: 1170378.2s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.142s, learning 0.165s)
               Value function loss: 14.7961
                    Surrogate loss: -0.0082
             Mean action noise std: 0.73
                       Mean reward: 371.98
               Mean episode length: 248.78
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 11.31s
                        Total time: 37964.62s
                               ETA: 1170342.2s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1440 steps/s (collection: 11.206s, learning 0.165s)
               Value function loss: 12.2720
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 375.57
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 11.37s
                        Total time: 37975.99s
                               ETA: 1170308.2s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.446s, learning 0.181s)
               Value function loss: 13.2014
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 374.06
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 11.63s
                        Total time: 37987.62s
                               ETA: 1170282.1s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.153s, learning 0.174s)
               Value function loss: 11.2827
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 372.77
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 11.33s
                        Total time: 37998.95s
                               ETA: 1170246.7s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.614s, learning 0.181s)
               Value function loss: 11.2454
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 376.05
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 11.79s
                        Total time: 38010.74s
                               ETA: 1170225.8s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.773s, learning 0.203s)
               Value function loss: 9.4003
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 376.76
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 11.98s
                        Total time: 38022.72s
                               ETA: 1170210.5s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.293s, learning 0.163s)
               Value function loss: 9.0736
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 376.80
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 11.46s
                        Total time: 38034.17s
                               ETA: 1170179.1s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.432s, learning 0.162s)
               Value function loss: 4.9987
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 377.58
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 11.59s
                        Total time: 38045.77s
                               ETA: 1170152.0s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.577s, learning 0.234s)
               Value function loss: 9.4803
                    Surrogate loss: -0.0074
             Mean action noise std: 0.73
                       Mean reward: 375.25
               Mean episode length: 248.68
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 11.81s
                        Total time: 38057.58s
                               ETA: 1170131.6s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1463 steps/s (collection: 11.028s, learning 0.168s)
               Value function loss: 6.4909
                    Surrogate loss: -0.0114
             Mean action noise std: 0.73
                       Mean reward: 374.60
               Mean episode length: 248.68
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 11.20s
                        Total time: 38068.78s
                               ETA: 1170092.3s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.135s, learning 0.179s)
               Value function loss: 8.4118
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 373.84
               Mean episode length: 248.68
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 11.31s
                        Total time: 38080.09s
                               ETA: 1170056.7s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.694s, learning 0.255s)
               Value function loss: 9.5367
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 376.61
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 11.95s
                        Total time: 38092.04s
                               ETA: 1170040.5s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.468s, learning 0.196s)
               Value function loss: 6.5719
                    Surrogate loss: -0.0079
             Mean action noise std: 0.73
                       Mean reward: 377.65
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 11.66s
                        Total time: 38103.70s
                               ETA: 1170015.7s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1468 steps/s (collection: 10.994s, learning 0.161s)
               Value function loss: 9.0092
                    Surrogate loss: -0.0073
             Mean action noise std: 0.73
                       Mean reward: 377.56
               Mean episode length: 249.56
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 11.16s
                        Total time: 38114.86s
                               ETA: 1169975.2s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.434s, learning 0.176s)
               Value function loss: 10.2377
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 376.50
               Mean episode length: 249.56
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 11.61s
                        Total time: 38126.47s
                               ETA: 1169948.6s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.540s, learning 0.187s)
               Value function loss: 8.4142
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 374.89
               Mean episode length: 248.93
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 11.73s
                        Total time: 38138.20s
                               ETA: 1169925.7s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.456s, learning 0.161s)
               Value function loss: 11.7981
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 375.67
               Mean episode length: 248.93
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 11.62s
                        Total time: 38149.81s
                               ETA: 1169899.4s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.352s, learning 0.204s)
               Value function loss: 12.7392
                    Surrogate loss: -0.0037
             Mean action noise std: 0.73
                       Mean reward: 377.91
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 11.56s
                        Total time: 38161.37s
                               ETA: 1169871.2s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.165s)
               Value function loss: 11.6451
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 377.27
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 11.57s
                        Total time: 38172.94s
                               ETA: 1169843.5s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.519s, learning 0.165s)
               Value function loss: 12.1829
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 377.25
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 11.68s
                        Total time: 38184.62s
                               ETA: 1169819.3s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.277s, learning 0.307s)
               Value function loss: 16.7067
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 377.12
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 11.58s
                        Total time: 38196.21s
                               ETA: 1169792.0s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.398s, learning 0.177s)
               Value function loss: 18.8731
                    Surrogate loss: -0.0105
             Mean action noise std: 0.73
                       Mean reward: 377.19
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 11.58s
                        Total time: 38207.78s
                               ETA: 1169764.5s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1466 steps/s (collection: 10.980s, learning 0.191s)
               Value function loss: 14.4860
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 377.40
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 11.17s
                        Total time: 38218.95s
                               ETA: 1169724.7s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.736s, learning 0.185s)
               Value function loss: 16.5825
                    Surrogate loss: -0.0061
             Mean action noise std: 0.73
                       Mean reward: 377.12
               Mean episode length: 249.95
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 11.92s
                        Total time: 38230.87s
                               ETA: 1169707.7s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1447 steps/s (collection: 11.145s, learning 0.176s)
               Value function loss: 12.5864
                    Surrogate loss: -0.0125
             Mean action noise std: 0.73
                       Mean reward: 376.16
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 11.32s
                        Total time: 38242.19s
                               ETA: 1169672.4s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.446s, learning 0.198s)
               Value function loss: 14.5328
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 376.92
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 11.64s
                        Total time: 38253.84s
                               ETA: 1169647.1s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.714s, learning 0.218s)
               Value function loss: 12.2711
                    Surrogate loss: -0.0103
             Mean action noise std: 0.73
                       Mean reward: 376.72
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 11.93s
                        Total time: 38265.77s
                               ETA: 1169630.5s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1494 steps/s (collection: 10.786s, learning 0.178s)
               Value function loss: 16.0707
                    Surrogate loss: -0.0099
             Mean action noise std: 0.73
                       Mean reward: 375.64
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 10.96s
                        Total time: 38276.74s
                               ETA: 1169584.4s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.058s, learning 0.193s)
               Value function loss: 18.4547
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 376.73
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 11.25s
                        Total time: 38287.99s
                               ETA: 1169547.0s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.359s, learning 0.179s)
               Value function loss: 15.0884
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 376.23
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 11.54s
                        Total time: 38299.53s
                               ETA: 1169518.5s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.624s, learning 0.363s)
               Value function loss: 15.9363
                    Surrogate loss: -0.0038
             Mean action noise std: 0.73
                       Mean reward: 375.46
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 11.99s
                        Total time: 38311.51s
                               ETA: 1169503.6s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.542s, learning 0.177s)
               Value function loss: 15.8821
                    Surrogate loss: -0.0109
             Mean action noise std: 0.73
                       Mean reward: 373.74
               Mean episode length: 250.00
                  Mean reward/step: 1.46
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 11.72s
                        Total time: 38323.23s
                               ETA: 1169480.6s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.468s, learning 0.162s)
               Value function loss: 11.5975
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 375.63
               Mean episode length: 250.00
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 11.63s
                        Total time: 38334.86s
                               ETA: 1169454.8s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.313s, learning 0.257s)
               Value function loss: 11.6441
                    Surrogate loss: -0.0064
             Mean action noise std: 0.73
                       Mean reward: 374.02
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 11.57s
                        Total time: 38346.43s
                               ETA: 1169427.3s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.654s, learning 0.170s)
               Value function loss: 8.8055
                    Surrogate loss: -0.0115
             Mean action noise std: 0.73
                       Mean reward: 374.47
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 11.82s
                        Total time: 38358.26s
                               ETA: 1169407.5s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.321s, learning 0.201s)
               Value function loss: 9.5471
                    Surrogate loss: -0.0113
             Mean action noise std: 0.73
                       Mean reward: 375.03
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 11.52s
                        Total time: 38369.78s
                               ETA: 1169378.5s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.811s, learning 0.191s)
               Value function loss: 7.2744
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 375.21
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 12.00s
                        Total time: 38381.78s
                               ETA: 1169364.1s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.383s, learning 0.161s)
               Value function loss: 7.6927
                    Surrogate loss: -0.0084
             Mean action noise std: 0.73
                       Mean reward: 372.22
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 11.54s
                        Total time: 38393.32s
                               ETA: 1169335.8s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.576s, learning 0.204s)
               Value function loss: 4.4450
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 372.18
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 11.78s
                        Total time: 38405.10s
                               ETA: 1169314.7s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.361s, learning 0.159s)
               Value function loss: 6.1819
                    Surrogate loss: -0.0083
             Mean action noise std: 0.73
                       Mean reward: 371.54
               Mean episode length: 248.58
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 11.52s
                        Total time: 38416.62s
                               ETA: 1169285.6s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1452 steps/s (collection: 11.114s, learning 0.168s)
               Value function loss: 5.1876
                    Surrogate loss: -0.0098
             Mean action noise std: 0.73
                       Mean reward: 370.50
               Mean episode length: 247.30
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 11.28s
                        Total time: 38427.91s
                               ETA: 1169249.3s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1427 steps/s (collection: 11.312s, learning 0.167s)
               Value function loss: 6.2235
                    Surrogate loss: -0.0111
             Mean action noise std: 0.73
                       Mean reward: 370.48
               Mean episode length: 247.30
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 11.48s
                        Total time: 38439.38s
                               ETA: 1169219.1s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.165s, learning 0.165s)
               Value function loss: 7.4417
                    Surrogate loss: -0.0092
             Mean action noise std: 0.73
                       Mean reward: 370.76
               Mean episode length: 247.42
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 11.33s
                        Total time: 38450.71s
                               ETA: 1169184.3s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.422s, learning 0.239s)
               Value function loss: 5.8400
                    Surrogate loss: -0.0118
             Mean action noise std: 0.73
                       Mean reward: 373.02
               Mean episode length: 248.70
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 11.66s
                        Total time: 38462.38s
                               ETA: 1169159.6s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1460 steps/s (collection: 11.050s, learning 0.165s)
               Value function loss: 5.4785
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 373.41
               Mean episode length: 248.70
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 11.21s
                        Total time: 38473.59s
                               ETA: 1169121.3s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.383s, learning 0.171s)
               Value function loss: 7.1903
                    Surrogate loss: -0.0090
             Mean action noise std: 0.73
                       Mean reward: 374.99
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 11.55s
                        Total time: 38485.14s
                               ETA: 1169093.4s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1473 steps/s (collection: 10.952s, learning 0.168s)
               Value function loss: 7.4867
                    Surrogate loss: -0.0101
             Mean action noise std: 0.73
                       Mean reward: 374.49
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 11.12s
                        Total time: 38496.26s
                               ETA: 1169052.3s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.248s, learning 0.163s)
               Value function loss: 10.2765
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 372.62
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 11.41s
                        Total time: 38507.67s
                               ETA: 1169020.1s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.579s, learning 0.159s)
               Value function loss: 9.2345
                    Surrogate loss: -0.0075
             Mean action noise std: 0.73
                       Mean reward: 371.72
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 11.74s
                        Total time: 38519.41s
                               ETA: 1168997.8s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.164s)
               Value function loss: 11.1106
                    Surrogate loss: -0.0026
             Mean action noise std: 0.73
                       Mean reward: 372.67
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 11.48s
                        Total time: 38530.90s
                               ETA: 1168967.8s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.355s, learning 0.167s)
               Value function loss: 9.5297
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 374.26
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 11.52s
                        Total time: 38542.42s
                               ETA: 1168938.9s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1446 steps/s (collection: 11.124s, learning 0.205s)
               Value function loss: 11.5302
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 375.04
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 11.33s
                        Total time: 38553.75s
                               ETA: 1168904.2s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.206s, learning 0.189s)
               Value function loss: 13.4673
                    Surrogate loss: -0.0050
             Mean action noise std: 0.73
                       Mean reward: 374.22
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 11.39s
                        Total time: 38565.14s
                               ETA: 1168871.6s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.396s, learning 0.266s)
               Value function loss: 14.7985
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 372.91
               Mean episode length: 248.82
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 11.66s
                        Total time: 38576.80s
                               ETA: 1168847.0s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.874s, learning 0.198s)
               Value function loss: 14.5294
                    Surrogate loss: -0.0076
             Mean action noise std: 0.73
                       Mean reward: 374.49
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 12.07s
                        Total time: 38588.88s
                               ETA: 1168834.9s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.387s, learning 0.160s)
               Value function loss: 12.5127
                    Surrogate loss: -0.0007
             Mean action noise std: 0.73
                       Mean reward: 375.10
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 11.55s
                        Total time: 38600.42s
                               ETA: 1168806.8s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.020s, learning 0.226s)
               Value function loss: 12.6986
                    Surrogate loss: -0.0060
             Mean action noise std: 0.73
                       Mean reward: 375.35
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 11.25s
                        Total time: 38611.67s
                               ETA: 1168769.7s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1464 steps/s (collection: 10.989s, learning 0.196s)
               Value function loss: 13.8165
                    Surrogate loss: 0.0008
             Mean action noise std: 0.73
                       Mean reward: 374.13
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 11.18s
                        Total time: 38622.85s
                               ETA: 1168730.7s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.175s, learning 0.170s)
               Value function loss: 15.3065
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 375.27
               Mean episode length: 250.00
                  Mean reward/step: 1.50
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 11.34s
                        Total time: 38634.20s
                               ETA: 1168696.6s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.707s, learning 0.213s)
               Value function loss: 18.2714
                    Surrogate loss: 0.0029
             Mean action noise std: 0.73
                       Mean reward: 374.86
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 11.92s
                        Total time: 38646.12s
                               ETA: 1168679.9s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.164s)
               Value function loss: 15.4860
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 375.29
               Mean episode length: 250.00
                  Mean reward/step: 1.48
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 11.46s
                        Total time: 38657.58s
                               ETA: 1168649.3s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.153s, learning 0.189s)
               Value function loss: 17.4370
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 375.69
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 11.34s
                        Total time: 38668.92s
                               ETA: 1168615.1s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.409s, learning 0.170s)
               Value function loss: 19.6699
                    Surrogate loss: 0.0011
             Mean action noise std: 0.73
                       Mean reward: 374.21
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 11.58s
                        Total time: 38680.50s
                               ETA: 1168588.1s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.231s, learning 0.310s)
               Value function loss: 13.8241
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 376.68
               Mean episode length: 250.00
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 11.54s
                        Total time: 38692.04s
                               ETA: 1168560.0s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.534s, learning 0.193s)
               Value function loss: 14.6933
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 378.02
               Mean episode length: 250.00
                  Mean reward/step: 1.51
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 11.73s
                        Total time: 38703.77s
                               ETA: 1168537.5s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.443s, learning 0.167s)
               Value function loss: 12.5605
                    Surrogate loss: 0.0014
             Mean action noise std: 0.73
                       Mean reward: 378.72
               Mean episode length: 250.00
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 11.61s
                        Total time: 38715.38s
                               ETA: 1168511.5s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.296s, learning 0.260s)
               Value function loss: 10.4125
                    Surrogate loss: -0.0095
             Mean action noise std: 0.73
                       Mean reward: 378.88
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 11.56s
                        Total time: 38726.93s
                               ETA: 1168483.9s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.911s, learning 0.222s)
               Value function loss: 9.9576
                    Surrogate loss: -0.0049
             Mean action noise std: 0.73
                       Mean reward: 379.71
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 12.13s
                        Total time: 38739.07s
                               ETA: 1168473.6s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1471 steps/s (collection: 10.902s, learning 0.231s)
               Value function loss: 9.6817
                    Surrogate loss: -0.0016
             Mean action noise std: 0.73
                       Mean reward: 379.88
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 11.13s
                        Total time: 38750.20s
                               ETA: 1168433.2s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.908s, learning 0.257s)
               Value function loss: 6.5502
                    Surrogate loss: -0.0057
             Mean action noise std: 0.73
                       Mean reward: 380.43
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 12.16s
                        Total time: 38762.37s
                               ETA: 1168424.0s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.675s, learning 0.232s)
               Value function loss: 7.5957
                    Surrogate loss: -0.0094
             Mean action noise std: 0.73
                       Mean reward: 381.03
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 11.91s
                        Total time: 38774.27s
                               ETA: 1168406.9s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.495s, learning 0.165s)
               Value function loss: 7.1149
                    Surrogate loss: -0.0066
             Mean action noise std: 0.73
                       Mean reward: 382.10
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 11.66s
                        Total time: 38785.93s
                               ETA: 1168382.4s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.471s, learning 0.262s)
               Value function loss: 7.8219
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 382.46
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 11.73s
                        Total time: 38797.66s
                               ETA: 1168360.2s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.612s, learning 0.198s)
               Value function loss: 8.1737
                    Surrogate loss: -0.0063
             Mean action noise std: 0.73
                       Mean reward: 382.96
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 11.81s
                        Total time: 38809.48s
                               ETA: 1168340.2s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.349s, learning 0.165s)
               Value function loss: 6.6861
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 383.37
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 11.51s
                        Total time: 38820.99s
                               ETA: 1168311.4s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.577s, learning 0.201s)
               Value function loss: 7.2910
                    Surrogate loss: -0.0059
             Mean action noise std: 0.73
                       Mean reward: 383.74
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 11.78s
                        Total time: 38832.77s
                               ETA: 1168290.5s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.969s, learning 0.155s)
               Value function loss: 11.5784
                    Surrogate loss: -0.0045
             Mean action noise std: 0.73
                       Mean reward: 384.70
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 11.12s
                        Total time: 38843.89s
                               ETA: 1168250.0s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.276s, learning 0.163s)
               Value function loss: 8.9095
                    Surrogate loss: -0.0065
             Mean action noise std: 0.73
                       Mean reward: 385.84
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 11.44s
                        Total time: 38855.33s
                               ETA: 1168218.9s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.276s, learning 0.161s)
               Value function loss: 9.1559
                    Surrogate loss: -0.0085
             Mean action noise std: 0.73
                       Mean reward: 385.62
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 11.44s
                        Total time: 38866.77s
                               ETA: 1168187.8s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1459 steps/s (collection: 11.065s, learning 0.162s)
               Value function loss: 9.3268
                    Surrogate loss: -0.0055
             Mean action noise std: 0.73
                       Mean reward: 385.65
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 11.23s
                        Total time: 38877.99s
                               ETA: 1168150.4s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.373s, learning 0.203s)
               Value function loss: 13.5126
                    Surrogate loss: -0.0051
             Mean action noise std: 0.73
                       Mean reward: 387.04
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 11.58s
                        Total time: 38889.57s
                               ETA: 1168123.5s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.076s, learning 0.162s)
               Value function loss: 9.8544
                    Surrogate loss: -0.0096
             Mean action noise std: 0.73
                       Mean reward: 387.03
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 11.24s
                        Total time: 38900.81s
                               ETA: 1168086.4s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.604s, learning 0.187s)
               Value function loss: 12.9023
                    Surrogate loss: -0.0048
             Mean action noise std: 0.73
                       Mean reward: 386.74
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 11.79s
                        Total time: 38912.60s
                               ETA: 1168065.9s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.555s, learning 0.202s)
               Value function loss: 17.7817
                    Surrogate loss: -0.0052
             Mean action noise std: 0.73
                       Mean reward: 387.37
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 11.76s
                        Total time: 38924.36s
                               ETA: 1168044.5s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.414s, learning 0.160s)
               Value function loss: 18.8359
                    Surrogate loss: -0.0041
             Mean action noise std: 0.73
                       Mean reward: 388.40
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 11.57s
                        Total time: 38935.93s
                               ETA: 1168017.6s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.551s, learning 0.164s)
               Value function loss: 20.3943
                    Surrogate loss: -0.0042
             Mean action noise std: 0.73
                       Mean reward: 388.67
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 11.72s
                        Total time: 38947.65s
                               ETA: 1167994.9s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.529s, learning 0.169s)
               Value function loss: 16.7172
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 389.22
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 11.70s
                        Total time: 38959.35s
                               ETA: 1167971.7s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.730s, learning 0.204s)
               Value function loss: 16.1235
                    Surrogate loss: -0.0091
             Mean action noise std: 0.73
                       Mean reward: 387.65
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 11.93s
                        Total time: 38971.28s
                               ETA: 1167955.6s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.290s, learning 0.298s)
               Value function loss: 16.3847
                    Surrogate loss: 0.0016
             Mean action noise std: 0.73
                       Mean reward: 389.50
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 11.59s
                        Total time: 38982.87s
                               ETA: 1167929.1s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.548s, learning 0.163s)
               Value function loss: 15.4741
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 390.25
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 11.71s
                        Total time: 38994.58s
                               ETA: 1167906.3s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1472 steps/s (collection: 10.965s, learning 0.163s)
               Value function loss: 22.7916
                    Surrogate loss: 0.0109
             Mean action noise std: 0.73
                       Mean reward: 390.15
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 11.13s
                        Total time: 39005.71s
                               ETA: 1167866.1s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.341s, learning 0.239s)
               Value function loss: 18.5577
                    Surrogate loss: -0.0003
             Mean action noise std: 0.73
                       Mean reward: 390.65
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 11.58s
                        Total time: 39017.29s
                               ETA: 1167839.4s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.526s, learning 0.162s)
               Value function loss: 20.1390
                    Surrogate loss: -0.0067
             Mean action noise std: 0.73
                       Mean reward: 390.44
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 11.69s
                        Total time: 39028.97s
                               ETA: 1167815.9s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.290s, learning 0.177s)
               Value function loss: 22.8055
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 391.26
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 11.47s
                        Total time: 39040.44s
                               ETA: 1167785.9s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.215s, learning 0.166s)
               Value function loss: 18.2159
                    Surrogate loss: 0.0044
             Mean action noise std: 0.73
                       Mean reward: 391.13
               Mean episode length: 250.00
                  Mean reward/step: 1.53
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 11.38s
                        Total time: 39051.82s
                               ETA: 1167753.2s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.372s, learning 0.345s)
               Value function loss: 14.5017
                    Surrogate loss: -0.0087
             Mean action noise std: 0.73
                       Mean reward: 391.44
               Mean episode length: 250.00
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 11.72s
                        Total time: 39063.54s
                               ETA: 1167730.7s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1457 steps/s (collection: 11.039s, learning 0.201s)
               Value function loss: 14.5174
                    Surrogate loss: -0.0078
             Mean action noise std: 0.73
                       Mean reward: 390.90
               Mean episode length: 250.00
                  Mean reward/step: 1.55
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 11.24s
                        Total time: 39074.78s
                               ETA: 1167693.9s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.310s, learning 0.195s)
               Value function loss: 15.1502
                    Surrogate loss: -0.0107
             Mean action noise std: 0.73
                       Mean reward: 392.34
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 11.51s
                        Total time: 39086.28s
                               ETA: 1167665.0s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.567s, learning 0.173s)
               Value function loss: 11.9808
                    Surrogate loss: -0.0070
             Mean action noise std: 0.73
                       Mean reward: 392.30
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 11.74s
                        Total time: 39098.02s
                               ETA: 1167643.2s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.140s, learning 0.162s)
               Value function loss: 13.7519
                    Surrogate loss: -0.0053
             Mean action noise std: 0.73
                       Mean reward: 392.01
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 11.30s
                        Total time: 39109.33s
                               ETA: 1167608.3s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1442 steps/s (collection: 11.161s, learning 0.197s)
               Value function loss: 9.2514
                    Surrogate loss: -0.0031
             Mean action noise std: 0.73
                       Mean reward: 393.01
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 11.36s
                        Total time: 39120.68s
                               ETA: 1167575.0s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.154s, learning 0.187s)
               Value function loss: 6.6882
                    Surrogate loss: -0.0072
             Mean action noise std: 0.73
                       Mean reward: 392.81
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 11.34s
                        Total time: 39132.02s
                               ETA: 1167541.3s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.664s, learning 0.158s)
               Value function loss: 8.6551
                    Surrogate loss: -0.0062
             Mean action noise std: 0.73
                       Mean reward: 391.90
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 11.82s
                        Total time: 39143.85s
                               ETA: 1167521.9s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.492s, learning 0.182s)
               Value function loss: 7.6291
                    Surrogate loss: -0.0102
             Mean action noise std: 0.73
                       Mean reward: 391.72
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 11.67s
                        Total time: 39155.52s
                               ETA: 1167498.1s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.583s, learning 0.201s)
               Value function loss: 7.7737
                    Surrogate loss: -0.0080
             Mean action noise std: 0.73
                       Mean reward: 391.55
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 11.78s
                        Total time: 39167.30s
                               ETA: 1167477.7s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.326s, learning 0.183s)
               Value function loss: 9.2614
                    Surrogate loss: -0.0081
             Mean action noise std: 0.73
                       Mean reward: 392.15
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 11.51s
                        Total time: 39178.81s
                               ETA: 1167449.0s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.399s, learning 0.215s)
               Value function loss: 8.5357
                    Surrogate loss: -0.0104
             Mean action noise std: 0.73
                       Mean reward: 391.87
               Mean episode length: 250.00
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 11.61s
                        Total time: 39190.43s
                               ETA: 1167423.5s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1467 steps/s (collection: 10.976s, learning 0.188s)
               Value function loss: 10.5871
                    Surrogate loss: -0.0088
             Mean action noise std: 0.73
                       Mean reward: 391.33
               Mean episode length: 250.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 11.16s
                        Total time: 39201.59s
                               ETA: 1167384.5s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.529s, learning 0.172s)
               Value function loss: 12.3874
                    Surrogate loss: -0.0039
             Mean action noise std: 0.73
                       Mean reward: 391.21
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 11.70s
                        Total time: 39213.29s
                               ETA: 1167361.6s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.283s, learning 0.212s)
               Value function loss: 11.7031
                    Surrogate loss: -0.0040
             Mean action noise std: 0.73
                       Mean reward: 391.06
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 11.50s
                        Total time: 39224.79s
                               ETA: 1167332.6s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.563s, learning 0.183s)
               Value function loss: 11.6065
                    Surrogate loss: -0.0035
             Mean action noise std: 0.73
                       Mean reward: 390.67
               Mean episode length: 250.00
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 11.75s
                        Total time: 39236.53s
                               ETA: 1167311.0s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.562s, learning 0.171s)
               Value function loss: 14.5941
                    Surrogate loss: -0.0034
             Mean action noise std: 0.73
                       Mean reward: 389.31
               Mean episode length: 250.00
                  Mean reward/step: 1.57
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 11.73s
                        Total time: 39248.27s
                               ETA: 1167289.1s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.358s, learning 0.175s)
               Value function loss: 11.4517
                    Surrogate loss: -0.0058
             Mean action noise std: 0.73
                       Mean reward: 389.04
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 11.53s
                        Total time: 39259.80s
                               ETA: 1167261.2s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 1431 steps/s (collection: 11.224s, learning 0.223s)
               Value function loss: 14.5189
                    Surrogate loss: -0.0054
             Mean action noise std: 0.73
                       Mean reward: 391.21
               Mean episode length: 250.00
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 11.45s
                        Total time: 39271.25s
                               ETA: 1167230.8s

Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-3mrcci9u
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandCatchUnderarm_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/3mrcci9u
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:5
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Ellipsoid is not natively supported, tesellated mesh will be used
JointSpec type free not yet supported!
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=5, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=5, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:5', seed=None, sim_device='cuda:5', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandCatchUnderarm', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_catch_underarm', 'numEnvs': 2048, 'envSpacing': 0.75, 'episodeLength': 75, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.01, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.01, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.2, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 50, 'transition_scale': 0.05, 'orientation_scale': 0.5, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.65, 'fallPenalty': 0.0, 'objectType': 'egg', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 8, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandCatchUnderarm', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandCatchUnderarm_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 2288
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:5
Sequential(
  (0): Linear(in_features=422, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=422, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 1504 steps/s (collection: 5.458s, learning 5.428s)
               Value function loss: 7.7127
                    Surrogate loss: 0.0507
             Mean action noise std: 0.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 10.89s
                        Total time: 10.89s
                               ETA: 1088691.8s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 3410 steps/s (collection: 3.690s, learning 1.114s)
               Value function loss: 0.5566
                    Surrogate loss: -0.0215
             Mean action noise std: 0.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 4.80s
                        Total time: 15.69s
                               ETA: 784533.7s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 4778 steps/s (collection: 3.261s, learning 0.168s)
               Value function loss: 0.2109
                    Surrogate loss: -0.0412
             Mean action noise std: 0.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 3.43s
                        Total time: 19.12s
                               ETA: 637303.0s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 4454 steps/s (collection: 3.495s, learning 0.183s)
               Value function loss: 0.1249
                    Surrogate loss: -0.0268
             Mean action noise std: 0.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 3.68s
                        Total time: 22.80s
                               ETA: 569927.5s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 3203 steps/s (collection: 4.931s, learning 0.183s)
               Value function loss: 0.0574
                    Surrogate loss: -0.0311
             Mean action noise std: 0.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 5.11s
                        Total time: 27.91s
                               ETA: 558223.7s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 4220 steps/s (collection: 3.691s, learning 0.192s)
               Value function loss: 0.0484
                    Surrogate loss: -0.0281
             Mean action noise std: 0.80
                       Mean reward: 0.48
               Mean episode length: 48.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 3.88s
                        Total time: 31.79s
                               ETA: 529883.4s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 3791 steps/s (collection: 4.156s, learning 0.165s)
               Value function loss: 0.0531
                    Surrogate loss: -0.0336
             Mean action noise std: 0.80
                       Mean reward: 0.60
               Mean episode length: 52.50
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 4.32s
                        Total time: 36.12s
                               ETA: 515903.4s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 3998 steps/s (collection: 3.912s, learning 0.185s)
               Value function loss: 0.0386
                    Surrogate loss: -0.0360
             Mean action noise std: 0.80
                       Mean reward: 0.61
               Mean episode length: 52.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 4.10s
                        Total time: 40.21s
                               ETA: 502621.5s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 3803 steps/s (collection: 4.136s, learning 0.171s)
               Value function loss: 0.0274
                    Surrogate loss: -0.0289
             Mean action noise std: 0.80
                       Mean reward: 0.63
               Mean episode length: 53.70
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 4.31s
                        Total time: 44.52s
                               ETA: 494623.3s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 3791 steps/s (collection: 4.157s, learning 0.164s)
               Value function loss: 0.0686
                    Surrogate loss: -0.0012
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 74.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 4.32s
                        Total time: 48.84s
                               ETA: 488362.4s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 3259 steps/s (collection: 4.863s, learning 0.164s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0213
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 74.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 5.03s
                        Total time: 53.87s
                               ETA: 489658.9s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 3476 steps/s (collection: 4.517s, learning 0.196s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0306
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 74.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 4.71s
                        Total time: 58.58s
                               ETA: 488122.3s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 3692 steps/s (collection: 4.246s, learning 0.191s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0337
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 73.55
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 4.44s
                        Total time: 63.02s
                               ETA: 484694.2s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 3274 steps/s (collection: 4.831s, learning 0.172s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0361
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 71.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 5.00s
                        Total time: 68.02s
                               ETA: 485799.9s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 3511 steps/s (collection: 4.500s, learning 0.166s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0367
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 70.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 4.67s
                        Total time: 72.69s
                               ETA: 484513.8s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 3302 steps/s (collection: 4.787s, learning 0.174s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0223
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 70.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 4.96s
                        Total time: 77.65s
                               ETA: 485229.6s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 2750 steps/s (collection: 5.796s, learning 0.161s)
               Value function loss: 0.0135
                    Surrogate loss: -0.0357
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 69.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 5.96s
                        Total time: 83.60s
                               ETA: 491714.3s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 2870 steps/s (collection: 5.490s, learning 0.218s)
               Value function loss: 0.0195
                    Surrogate loss: -0.0423
             Mean action noise std: 0.80
                       Mean reward: 0.72
               Mean episode length: 69.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 5.71s
                        Total time: 89.31s
                               ETA: 496099.8s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 2475 steps/s (collection: 6.407s, learning 0.210s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0296
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 6.62s
                        Total time: 95.93s
                               ETA: 504805.9s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.091s, learning 0.164s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0169
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 74.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 8.25s
                        Total time: 104.18s
                               ETA: 520823.7s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 1725 steps/s (collection: 9.319s, learning 0.174s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0331
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 74.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 9.49s
                        Total time: 113.68s
                               ETA: 541211.5s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 1497 steps/s (collection: 10.767s, learning 0.175s)
               Value function loss: 0.0158
                    Surrogate loss: -0.0452
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 74.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 10.94s
                        Total time: 124.62s
                               ETA: 566329.2s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 1278 steps/s (collection: 12.647s, learning 0.171s)
               Value function loss: 0.0237
                    Surrogate loss: -0.0363
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 73.67
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 12.82s
                        Total time: 137.44s
                               ETA: 597421.6s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 1109 steps/s (collection: 14.592s, learning 0.169s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0347
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 72.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 14.76s
                        Total time: 152.20s
                               ETA: 634011.9s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 1059 steps/s (collection: 15.306s, learning 0.164s)
               Value function loss: 0.0088
                    Surrogate loss: -0.0445
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 72.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 15.47s
                        Total time: 167.67s
                               ETA: 670511.2s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 1037 steps/s (collection: 15.620s, learning 0.172s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0434
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 72.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 15.79s
                        Total time: 183.46s
                               ETA: 705439.2s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.236s, learning 0.175s)
               Value function loss: 0.0100
                    Surrogate loss: -0.0342
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 71.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 16.41s
                        Total time: 199.87s
                               ETA: 740071.9s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 971 steps/s (collection: 16.697s, learning 0.168s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0368
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 16.87s
                        Total time: 216.74s
                               ETA: 773851.3s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.470s, learning 0.169s)
               Value function loss: 0.0960
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 16.64s
                        Total time: 233.38s
                               ETA: 804521.0s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.272s, learning 0.162s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0367
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 16.43s
                        Total time: 249.81s
                               ETA: 832459.6s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.517s, learning 0.171s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0389
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 74.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 16.69s
                        Total time: 266.50s
                               ETA: 859414.5s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 974 steps/s (collection: 16.626s, learning 0.190s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0396
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 73.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 16.82s
                        Total time: 283.31s
                               ETA: 885081.7s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 962 steps/s (collection: 16.867s, learning 0.163s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0464
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 71.75
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 17.03s
                        Total time: 300.34s
                               ETA: 909843.1s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.466s, learning 0.173s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0518
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 70.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 16.64s
                        Total time: 316.98s
                               ETA: 931999.0s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.273s, learning 0.175s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0461
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 68.85
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 16.45s
                        Total time: 333.43s
                               ETA: 952338.7s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 961 steps/s (collection: 16.880s, learning 0.160s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0463
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 67.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 17.04s
                        Total time: 350.47s
                               ETA: 973192.3s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.407s, learning 0.172s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0493
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 67.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 16.58s
                        Total time: 367.05s
                               ETA: 991673.3s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.595s, learning 0.170s)
               Value function loss: 0.0606
                    Surrogate loss: -0.0187
             Mean action noise std: 0.80
                       Mean reward: 0.82
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 12.77s
                        Total time: 379.82s
                               ETA: 999146.8s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.289s, learning 0.166s)
               Value function loss: 0.0086
                    Surrogate loss: -0.0385
             Mean action noise std: 0.80
                       Mean reward: 0.82
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 8.46s
                        Total time: 388.27s
                               ETA: 995191.0s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.403s, learning 0.173s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0523
             Mean action noise std: 0.80
                       Mean reward: 0.83
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 8.58s
                        Total time: 396.85s
                               ETA: 991733.0s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1999 steps/s (collection: 8.023s, learning 0.170s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0337
             Mean action noise std: 0.80
                       Mean reward: 0.82
               Mean episode length: 74.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 8.19s
                        Total time: 405.04s
                               ETA: 987509.3s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.581s, learning 0.162s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0393
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 72.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 8.74s
                        Total time: 413.78s
                               ETA: 984797.9s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.415s, learning 0.164s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0523
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 69.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 8.58s
                        Total time: 422.36s
                               ETA: 981828.9s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.347s, learning 0.169s)
               Value function loss: 0.0076
                    Surrogate loss: -0.0451
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 67.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 8.52s
                        Total time: 430.88s
                               ETA: 978851.4s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.361s, learning 0.171s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0534
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 66.26
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 8.53s
                        Total time: 439.41s
                               ETA: 976040.8s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.456s, learning 0.168s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0463
             Mean action noise std: 0.80
                       Mean reward: 0.71
               Mean episode length: 66.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 8.62s
                        Total time: 448.04s
                               ETA: 973551.4s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.525s, learning 0.175s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0416
             Mean action noise std: 0.80
                       Mean reward: 0.83
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 8.70s
                        Total time: 456.74s
                               ETA: 971329.6s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.047s, learning 0.164s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0369
             Mean action noise std: 0.80
                       Mean reward: 0.83
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 8.21s
                        Total time: 464.95s
                               ETA: 968184.2s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.496s, learning 0.164s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0461
             Mean action noise std: 0.80
                       Mean reward: 0.83
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 8.66s
                        Total time: 473.61s
                               ETA: 966080.1s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.450s, learning 0.195s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0530
             Mean action noise std: 0.80
                       Mean reward: 0.82
               Mean episode length: 74.70
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 8.64s
                        Total time: 482.25s
                               ETA: 964029.5s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.485s, learning 0.174s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0386
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 69.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 8.66s
                        Total time: 490.91s
                               ETA: 962086.9s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1867 steps/s (collection: 8.610s, learning 0.163s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0424
             Mean action noise std: 0.80
                       Mean reward: 0.70
               Mean episode length: 66.55
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 8.77s
                        Total time: 499.68s
                               ETA: 960438.5s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.531s, learning 0.164s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0507
             Mean action noise std: 0.80
                       Mean reward: 0.63
               Mean episode length: 60.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 8.69s
                        Total time: 508.38s
                               ETA: 958704.5s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.399s, learning 0.179s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0535
             Mean action noise std: 0.80
                       Mean reward: 0.67
               Mean episode length: 62.79
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 8.58s
                        Total time: 516.96s
                               ETA: 956817.2s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1887 steps/s (collection: 8.513s, learning 0.169s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0529
             Mean action noise std: 0.80
                       Mean reward: 0.72
               Mean episode length: 66.05
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 8.68s
                        Total time: 525.64s
                               ETA: 955186.9s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.464s, learning 0.163s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0437
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 71.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 8.63s
                        Total time: 534.26s
                               ETA: 953518.7s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.466s, learning 0.193s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0322
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 8.66s
                        Total time: 542.92s
                               ETA: 951962.3s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.379s, learning 0.191s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0444
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 8.57s
                        Total time: 551.49s
                               ETA: 950306.3s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.164s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0487
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 8.64s
                        Total time: 560.13s
                               ETA: 948823.5s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1857 steps/s (collection: 8.612s, learning 0.207s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0532
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 73.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 8.82s
                        Total time: 568.95s
                               ETA: 947690.9s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.466s, learning 0.170s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0493
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 70.90
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 8.64s
                        Total time: 577.59s
                               ETA: 946294.3s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.429s, learning 0.201s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 0.72
               Mean episode length: 66.15
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 8.63s
                        Total time: 586.22s
                               ETA: 944933.1s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.343s, learning 0.170s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0473
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 67.63
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 8.51s
                        Total time: 594.73s
                               ETA: 943430.3s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.346s, learning 0.164s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 71.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 8.51s
                        Total time: 603.24s
                               ETA: 941967.5s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1888 steps/s (collection: 8.465s, learning 0.211s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0568
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 72.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 8.68s
                        Total time: 611.92s
                               ETA: 940806.4s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1849 steps/s (collection: 8.680s, learning 0.176s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0354
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 8.86s
                        Total time: 620.77s
                               ETA: 939952.3s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.361s, learning 0.181s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0406
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 74.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 8.54s
                        Total time: 629.31s
                               ETA: 938655.1s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.166s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0501
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 73.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 8.46s
                        Total time: 637.78s
                               ETA: 937277.0s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.172s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0425
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 73.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 8.42s
                        Total time: 646.20s
                               ETA: 935880.7s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.531s, learning 0.215s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0532
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 70.62
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 8.75s
                        Total time: 654.94s
                               ETA: 934986.9s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.361s, learning 0.211s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0464
             Mean action noise std: 0.80
                       Mean reward: 0.70
               Mean episode length: 66.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 8.57s
                        Total time: 663.52s
                               ETA: 933874.1s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1869 steps/s (collection: 8.584s, learning 0.178s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0548
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 68.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 8.76s
                        Total time: 672.28s
                               ETA: 933054.9s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.495s, learning 0.202s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0571
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 72.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 8.70s
                        Total time: 680.97s
                               ETA: 932169.4s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.472s, learning 0.160s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0567
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 73.62
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 8.63s
                        Total time: 689.61s
                               ETA: 931219.2s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.418s, learning 0.166s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0460
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 8.58s
                        Total time: 698.19s
                               ETA: 930231.2s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.456s, learning 0.171s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0338
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 8.63s
                        Total time: 706.82s
                               ETA: 929325.8s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.410s, learning 0.161s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0526
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 74.60
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 8.57s
                        Total time: 715.39s
                               ETA: 928369.7s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.360s, learning 0.163s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0567
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 74.31
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 8.52s
                        Total time: 723.91s
                               ETA: 927376.6s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.164s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0546
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 72.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 8.53s
                        Total time: 732.44s
                               ETA: 926420.9s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.366s, learning 0.172s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0583
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 67.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 8.54s
                        Total time: 740.98s
                               ETA: 925496.1s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1893 steps/s (collection: 8.485s, learning 0.166s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0549
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 68.82
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 8.65s
                        Total time: 749.63s
                               ETA: 924733.6s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.170s, learning 0.170s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0565
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 71.71
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 8.34s
                        Total time: 757.97s
                               ETA: 923610.0s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.192s, learning 0.169s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0521
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 72.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 8.36s
                        Total time: 766.33s
                               ETA: 922538.0s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.404s, learning 0.160s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0503
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 73.72
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 8.56s
                        Total time: 774.90s
                               ETA: 921732.3s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.331s, learning 0.181s)
               Value function loss: 0.0136
                    Surrogate loss: -0.0407
             Mean action noise std: 0.80
                       Mean reward: 0.83
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 8.51s
                        Total time: 783.41s
                               ETA: 920885.0s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.221s, learning 0.162s)
               Value function loss: 0.0081
                    Surrogate loss: -0.0516
             Mean action noise std: 0.80
                       Mean reward: 0.84
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 8.38s
                        Total time: 791.79s
                               ETA: 919907.7s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.385s, learning 0.159s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0591
             Mean action noise std: 0.80
                       Mean reward: 0.82
               Mean episode length: 74.19
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 8.54s
                        Total time: 800.34s
                               ETA: 919137.4s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.268s, learning 0.175s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0577
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 72.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 8.44s
                        Total time: 808.78s
                               ETA: 918269.4s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.342s, learning 0.161s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0580
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 66.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 8.50s
                        Total time: 817.28s
                               ETA: 917488.4s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.173s, learning 0.173s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0596
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 66.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 8.35s
                        Total time: 825.63s
                               ETA: 916550.7s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1878 steps/s (collection: 8.547s, learning 0.176s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0585
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 68.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 8.72s
                        Total time: 834.35s
                               ETA: 916046.8s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.432s, learning 0.180s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0607
             Mean action noise std: 0.80
                       Mean reward: 0.77
               Mean episode length: 69.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 8.61s
                        Total time: 842.97s
                               ETA: 915433.1s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.316s, learning 0.190s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0581
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 71.44
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 8.51s
                        Total time: 851.47s
                               ETA: 914718.5s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.167s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 74.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 4.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 8.45s
                        Total time: 859.92s
                               ETA: 913961.9s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.254s, learning 0.160s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0481
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 74.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 8.41s
                        Total time: 868.34s
                               ETA: 913180.2s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.530s, learning 0.159s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0610
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 71.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 8.69s
                        Total time: 877.03s
                               ETA: 912701.2s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.183s, learning 0.165s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0577
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 69.79
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 8.35s
                        Total time: 885.37s
                               ETA: 911880.8s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.165s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0597
             Mean action noise std: 0.80
                       Mean reward: 0.70
               Mean episode length: 62.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 8.44s
                        Total time: 893.81s
                               ETA: 911167.1s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.420s, learning 0.167s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0576
             Mean action noise std: 0.80
                       Mean reward: 0.72
               Mean episode length: 64.73
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 8.59s
                        Total time: 902.40s
                               ETA: 910619.0s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.164s, learning 0.170s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0576
             Mean action noise std: 0.80
                       Mean reward: 0.74
               Mean episode length: 66.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 8.33s
                        Total time: 910.73s
                               ETA: 909829.8s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.162s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0577
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 67.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 8.34s
                        Total time: 919.07s
                               ETA: 909058.1s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.168s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0583
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 70.19
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 8.45s
                        Total time: 927.52s
                               ETA: 908414.1s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.047s, learning 0.160s)
               Value function loss: 0.0113
                    Surrogate loss: -0.0572
             Mean action noise std: 0.80
                       Mean reward: 0.83
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 8.21s
                        Total time: 935.73s
                               ETA: 907545.5s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.398s, learning 0.178s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0497
             Mean action noise std: 0.80
                       Mean reward: 0.80
               Mean episode length: 71.72
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 8.58s
                        Total time: 944.30s
                               ETA: 907047.9s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.048s, learning 0.169s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0567
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 66.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 8.22s
                        Total time: 952.52s
                               ETA: 906218.4s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.402s, learning 0.174s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0544
             Mean action noise std: 0.80
                       Mean reward: 0.69
               Mean episode length: 60.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 8.58s
                        Total time: 961.10s
                               ETA: 905742.2s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.434s, learning 0.167s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0559
             Mean action noise std: 0.80
                       Mean reward: 0.72
               Mean episode length: 62.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 8.60s
                        Total time: 969.70s
                               ETA: 905297.8s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.086s, learning 0.176s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0529
             Mean action noise std: 0.80
                       Mean reward: 0.71
               Mean episode length: 62.81
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 8.26s
                        Total time: 977.96s
                               ETA: 904548.5s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.167s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0517
             Mean action noise std: 0.80
                       Mean reward: 0.75
               Mean episode length: 67.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 8.41s
                        Total time: 986.37s
                               ETA: 903950.0s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.137s, learning 0.173s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0509
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 67.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 8.31s
                        Total time: 994.68s
                               ETA: 903269.0s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1875 steps/s (collection: 8.528s, learning 0.206s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0537
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 70.58
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 8.73s
                        Total time: 1003.41s
                               ETA: 902983.1s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.339s, learning 0.274s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0520
             Mean action noise std: 0.80
                       Mean reward: 0.82
               Mean episode length: 71.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 8.61s
                        Total time: 1012.03s
                               ETA: 902593.9s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.423s, learning 0.215s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0498
             Mean action noise std: 0.80
                       Mean reward: 0.81
               Mean episode length: 72.10
                  Mean reward/step: 0.01
       Mean episode length/episode: 5.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 8.64s
                        Total time: 1020.67s
                               ETA: 902232.7s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 2024 steps/s (collection: 7.928s, learning 0.164s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0436
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 69.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 8.09s
                        Total time: 1028.76s
                               ETA: 901400.3s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1909 steps/s (collection: 8.419s, learning 0.164s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0580
             Mean action noise std: 0.80
                       Mean reward: 0.73
               Mean episode length: 64.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 8.58s
                        Total time: 1037.34s
                               ETA: 901007.6s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.192s, learning 0.166s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0562
             Mean action noise std: 0.80
                       Mean reward: 0.69
               Mean episode length: 60.62
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 8.36s
                        Total time: 1045.70s
                               ETA: 900428.3s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.264s, learning 0.170s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0531
             Mean action noise std: 0.80
                       Mean reward: 0.76
               Mean episode length: 66.80
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 8.43s
                        Total time: 1054.13s
                               ETA: 899924.1s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.097s, learning 0.167s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0517
             Mean action noise std: 0.80
                       Mean reward: 0.78
               Mean episode length: 67.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 8.26s
                        Total time: 1062.40s
                               ETA: 899283.7s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.252s, learning 0.172s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0503
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 70.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 8.42s
                        Total time: 1070.82s
                               ETA: 898788.7s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.099s, learning 0.169s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0507
             Mean action noise std: 0.80
                       Mean reward: 0.79
               Mean episode length: 69.29
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 8.27s
                        Total time: 1079.09s
                               ETA: 898171.5s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.358s, learning 0.214s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0498
             Mean action noise std: 0.79
                       Mean reward: 0.82
               Mean episode length: 71.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 8.57s
                        Total time: 1087.66s
                               ETA: 897816.1s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 2012 steps/s (collection: 7.977s, learning 0.164s)
               Value function loss: 0.0222
                    Surrogate loss: -0.0438
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 75.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 5.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 8.14s
                        Total time: 1095.80s
                               ETA: 897112.4s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.132s, learning 0.167s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0506
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 68.93
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 8.30s
                        Total time: 1104.10s
                               ETA: 896548.6s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.114s, learning 0.175s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0549
             Mean action noise std: 0.79
                       Mean reward: 0.71
               Mean episode length: 62.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 8.29s
                        Total time: 1112.39s
                               ETA: 895986.0s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.386s, learning 0.163s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0556
             Mean action noise std: 0.79
                       Mean reward: 0.74
               Mean episode length: 64.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 8.55s
                        Total time: 1120.94s
                               ETA: 895640.1s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.165s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0538
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 66.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 8.36s
                        Total time: 1129.30s
                               ETA: 895148.5s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.065s, learning 0.167s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0461
             Mean action noise std: 0.79
                       Mean reward: 0.82
               Mean episode length: 67.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 8.23s
                        Total time: 1137.53s
                               ETA: 894565.3s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.188s, learning 0.176s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0469
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 68.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 8.36s
                        Total time: 1145.89s
                               ETA: 894093.5s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.068s, learning 0.170s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0462
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 69.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 8.24s
                        Total time: 1154.13s
                               ETA: 893531.4s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.055s, learning 0.167s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0452
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 71.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 8.22s
                        Total time: 1162.35s
                               ETA: 892965.4s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.129s, learning 0.176s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0485
             Mean action noise std: 0.79
                       Mean reward: 0.83
               Mean episode length: 70.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 8.30s
                        Total time: 1170.66s
                               ETA: 892471.2s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.379s, learning 0.176s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0440
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 71.16
                  Mean reward/step: 0.01
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 8.55s
                        Total time: 1179.21s
                               ETA: 892173.2s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.267s, learning 0.168s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0544
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 66.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 8.43s
                        Total time: 1187.65s
                               ETA: 891789.8s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.013s, learning 0.168s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0500
             Mean action noise std: 0.79
                       Mean reward: 0.77
               Mean episode length: 64.11
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 8.18s
                        Total time: 1195.83s
                               ETA: 891222.3s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.939s, learning 0.213s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0469
             Mean action noise std: 0.79
                       Mean reward: 0.76
               Mean episode length: 65.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 8.15s
                        Total time: 1203.98s
                               ETA: 890642.6s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.132s, learning 0.160s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0453
             Mean action noise std: 0.79
                       Mean reward: 0.83
               Mean episode length: 69.08
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 8.29s
                        Total time: 1212.27s
                               ETA: 890173.3s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.111s, learning 0.165s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0442
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 68.44
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 8.28s
                        Total time: 1220.55s
                               ETA: 889699.7s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.869s, learning 0.170s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0451
             Mean action noise std: 0.79
                       Mean reward: 0.83
               Mean episode length: 70.87
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 8.04s
                        Total time: 1228.59s
                               ETA: 889061.1s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.060s, learning 0.207s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0460
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 70.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 8.27s
                        Total time: 1236.85s
                               ETA: 888595.3s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.946s, learning 0.176s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0470
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 72.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 8.12s
                        Total time: 1244.98s
                               ETA: 888032.9s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.165s)
               Value function loss: 0.0093
                    Surrogate loss: -0.0482
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 72.26
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 8.38s
                        Total time: 1253.35s
                               ETA: 887657.6s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.071s, learning 0.173s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0524
             Mean action noise std: 0.79
                       Mean reward: 0.78
               Mean episode length: 64.28
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 8.24s
                        Total time: 1261.60s
                               ETA: 887195.5s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.112s, learning 0.177s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0516
             Mean action noise std: 0.79
                       Mean reward: 0.80
               Mean episode length: 66.84
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 8.29s
                        Total time: 1269.89s
                               ETA: 886770.8s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.115s, learning 0.171s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0469
             Mean action noise std: 0.79
                       Mean reward: 0.80
               Mean episode length: 67.32
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 8.29s
                        Total time: 1278.17s
                               ETA: 886349.9s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.047s, learning 0.268s)
               Value function loss: 0.0026
                    Surrogate loss: -0.0439
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 67.40
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 8.31s
                        Total time: 1286.49s
                               ETA: 885954.4s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 2002 steps/s (collection: 7.978s, learning 0.202s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0446
             Mean action noise std: 0.79
                       Mean reward: 0.80
               Mean episode length: 69.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 8.18s
                        Total time: 1294.67s
                               ETA: 885471.9s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.234s, learning 0.207s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0416
             Mean action noise std: 0.79
                       Mean reward: 0.83
               Mean episode length: 69.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 8.44s
                        Total time: 1303.11s
                               ETA: 885173.6s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.314s, learning 0.209s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0369
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 70.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 8.52s
                        Total time: 1311.63s
                               ETA: 884934.1s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.209s, learning 0.270s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0466
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 72.29
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 8.48s
                        Total time: 1320.11s
                               ETA: 884668.2s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.119s, learning 0.202s)
               Value function loss: 0.0075
                    Surrogate loss: -0.0545
             Mean action noise std: 0.79
                       Mean reward: 0.88
               Mean episode length: 73.82
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 8.32s
                        Total time: 1328.43s
                               ETA: 884300.7s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.225s, learning 0.170s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0530
             Mean action noise std: 0.79
                       Mean reward: 0.81
               Mean episode length: 65.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 8.40s
                        Total time: 1336.83s
                               ETA: 883987.0s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.160s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0568
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 65.38
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 8.23s
                        Total time: 1345.05s
                               ETA: 883565.9s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.125s, learning 0.184s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0541
             Mean action noise std: 0.79
                       Mean reward: 0.82
               Mean episode length: 67.61
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 8.31s
                        Total time: 1353.36s
                               ETA: 883204.7s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.375s, learning 0.170s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0539
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 68.77
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 8.55s
                        Total time: 1361.91s
                               ETA: 883001.2s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.871s, learning 0.164s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0544
             Mean action noise std: 0.79
                       Mean reward: 0.83
               Mean episode length: 70.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 8.04s
                        Total time: 1369.94s
                               ETA: 882471.8s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.213s, learning 0.254s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0545
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 71.54
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 8.47s
                        Total time: 1378.41s
                               ETA: 882225.7s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.160s)
               Value function loss: 0.0016
                    Surrogate loss: -0.0529
             Mean action noise std: 0.79
                       Mean reward: 0.86
               Mean episode length: 71.46
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 8.16s
                        Total time: 1386.57s
                               ETA: 881784.7s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.194s, learning 0.167s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0548
             Mean action noise std: 0.79
                       Mean reward: 0.90
               Mean episode length: 73.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 8.36s
                        Total time: 1394.93s
                               ETA: 881478.2s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.333s, learning 0.173s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0497
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 70.37
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 8.51s
                        Total time: 1403.43s
                               ETA: 881266.6s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.041s, learning 0.188s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0509
             Mean action noise std: 0.79
                       Mean reward: 0.82
               Mean episode length: 67.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 8.23s
                        Total time: 1411.66s
                               ETA: 880885.2s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.154s, learning 0.165s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0493
             Mean action noise std: 0.79
                       Mean reward: 0.81
               Mean episode length: 65.83
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 8.32s
                        Total time: 1419.98s
                               ETA: 880563.8s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.227s, learning 0.161s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0519
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 69.13
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 8.39s
                        Total time: 1428.37s
                               ETA: 880289.1s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.268s, learning 0.278s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0479
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 68.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 8.55s
                        Total time: 1436.91s
                               ETA: 880114.4s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.254s, learning 0.174s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0505
             Mean action noise std: 0.79
                       Mean reward: 0.86
               Mean episode length: 69.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 8.43s
                        Total time: 1445.34s
                               ETA: 879869.4s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.051s, learning 0.174s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0466
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 67.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 8.23s
                        Total time: 1453.57s
                               ETA: 879504.9s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.379s, learning 0.168s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0475
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 70.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 8.55s
                        Total time: 1462.11s
                               ETA: 879338.0s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.300s, learning 0.162s)
               Value function loss: 0.0021
                    Surrogate loss: -0.0502
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 69.98
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 8.46s
                        Total time: 1470.58s
                               ETA: 879122.6s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.177s, learning 0.212s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0503
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 69.22
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 8.39s
                        Total time: 1478.96s
                               ETA: 878865.8s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.317s, learning 0.212s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0479
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 70.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 8.53s
                        Total time: 1487.49s
                               ETA: 878694.9s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.350s, learning 0.161s)
               Value function loss: 0.0018
                    Surrogate loss: -0.0471
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 66.44
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 8.51s
                        Total time: 1496.00s
                               ETA: 878515.2s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.248s, learning 0.221s)
               Value function loss: 0.0015
                    Surrogate loss: -0.0516
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 66.94
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 8.47s
                        Total time: 1504.47s
                               ETA: 878313.0s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.166s, learning 0.166s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0500
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 69.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 8.33s
                        Total time: 1512.80s
                               ETA: 878033.5s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.214s, learning 0.177s)
               Value function loss: 0.0019
                    Surrogate loss: -0.0474
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 69.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 8.39s
                        Total time: 1521.20s
                               ETA: 877791.3s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.940s, learning 0.169s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0491
             Mean action noise std: 0.79
                       Mean reward: 0.86
               Mean episode length: 69.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 8.11s
                        Total time: 1529.30s
                               ETA: 877390.1s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.408s, learning 0.181s)
               Value function loss: 0.0023
                    Surrogate loss: -0.0496
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 70.23
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 8.59s
                        Total time: 1537.89s
                               ETA: 877266.7s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.395s, learning 0.174s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0487
             Mean action noise std: 0.79
                       Mean reward: 0.86
               Mean episode length: 70.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 8.57s
                        Total time: 1546.46s
                               ETA: 877134.0s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.944s, learning 0.178s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0490
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 70.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 8.12s
                        Total time: 1554.58s
                               ETA: 876750.1s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.335s, learning 0.215s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0523
             Mean action noise std: 0.79
                       Mean reward: 0.94
               Mean episode length: 74.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 8.55s
                        Total time: 1563.13s
                               ETA: 876610.8s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.088s, learning 0.201s)
               Value function loss: 0.0022
                    Surrogate loss: -0.0494
             Mean action noise std: 0.79
                       Mean reward: 0.80
               Mean episode length: 65.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 8.29s
                        Total time: 1571.42s
                               ETA: 876327.0s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.224s, learning 0.169s)
               Value function loss: 0.0017
                    Surrogate loss: -0.0512
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 66.87
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 8.39s
                        Total time: 1579.82s
                               ETA: 876104.5s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.202s, learning 0.174s)
               Value function loss: 0.0020
                    Surrogate loss: -0.0486
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 68.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 8.38s
                        Total time: 1588.19s
                               ETA: 875874.5s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.118s, learning 0.166s)
               Value function loss: 0.0024
                    Surrogate loss: -0.0490
             Mean action noise std: 0.79
                       Mean reward: 0.82
               Mean episode length: 67.65
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 8.28s
                        Total time: 1596.48s
                               ETA: 875596.5s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.168s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0464
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 68.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 8.30s
                        Total time: 1604.78s
                               ETA: 875332.1s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.178s, learning 0.164s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0472
             Mean action noise std: 0.79
                       Mean reward: 0.84
               Mean episode length: 69.27
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 8.34s
                        Total time: 1613.12s
                               ETA: 875091.5s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.147s, learning 0.207s)
               Value function loss: 0.0030
                    Surrogate loss: -0.0453
             Mean action noise std: 0.79
                       Mean reward: 0.91
               Mean episode length: 71.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 8.35s
                        Total time: 1621.47s
                               ETA: 874859.6s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.163s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0419
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 67.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 8.37s
                        Total time: 1629.84s
                               ETA: 874639.0s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.035s, learning 0.174s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0421
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 69.01
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 8.21s
                        Total time: 1638.05s
                               ETA: 874334.5s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.214s, learning 0.212s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0422
             Mean action noise std: 0.79
                       Mean reward: 0.79
               Mean episode length: 63.46
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 8.43s
                        Total time: 1646.48s
                               ETA: 874148.6s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1999 steps/s (collection: 7.989s, learning 0.205s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0374
             Mean action noise std: 0.79
                       Mean reward: 0.78
               Mean episode length: 62.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 8.19s
                        Total time: 1654.67s
                               ETA: 873842.0s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.241s, learning 0.164s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0443
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 69.33
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 8.41s
                        Total time: 1663.08s
                               ETA: 873649.4s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.128s, learning 0.214s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0436
             Mean action noise std: 0.79
                       Mean reward: 0.90
               Mean episode length: 69.34
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 8.34s
                        Total time: 1671.42s
                               ETA: 873426.3s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.337s, learning 0.209s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0462
             Mean action noise std: 0.79
                       Mean reward: 0.90
               Mean episode length: 69.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 8.55s
                        Total time: 1679.97s
                               ETA: 873311.0s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.984s, learning 0.170s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0477
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 67.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 8.15s
                        Total time: 1688.12s
                               ETA: 872994.2s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.918s, learning 0.178s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0466
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 68.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 8.10s
                        Total time: 1696.22s
                               ETA: 872650.6s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.079s, learning 0.217s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0465
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 69.71
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 8.30s
                        Total time: 1704.51s
                               ETA: 872412.9s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.829s, learning 0.165s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0456
             Mean action noise std: 0.79
                       Mean reward: 0.87
               Mean episode length: 69.81
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 7.99s
                        Total time: 1712.51s
                               ETA: 872023.9s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.311s, learning 0.170s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0398
             Mean action noise std: 0.79
                       Mean reward: 0.91
               Mean episode length: 69.42
                  Mean reward/step: 0.01
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 8.48s
                        Total time: 1720.99s
                               ETA: 871885.2s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.238s, learning 0.163s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0395
             Mean action noise std: 0.79
                       Mean reward: 0.81
               Mean episode length: 64.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 8.40s
                        Total time: 1729.39s
                               ETA: 871707.3s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.292s, learning 0.162s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0445
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 66.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 8.45s
                        Total time: 1737.84s
                               ETA: 871557.9s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.169s, learning 0.178s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0429
             Mean action noise std: 0.79
                       Mean reward: 0.85
               Mean episode length: 66.24
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 8.35s
                        Total time: 1746.19s
                               ETA: 871356.4s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.399s, learning 0.173s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0401
             Mean action noise std: 0.79
                       Mean reward: 0.83
               Mean episode length: 66.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 8.57s
                        Total time: 1754.76s
                               ETA: 871268.5s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 2006 steps/s (collection: 8.001s, learning 0.166s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0430
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 67.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 8.17s
                        Total time: 1762.93s
                               ETA: 870981.4s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1897 steps/s (collection: 8.426s, learning 0.208s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0428
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 68.45
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 8.63s
                        Total time: 1771.56s
                               ETA: 870926.8s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.070s, learning 0.163s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0440
             Mean action noise std: 0.78
                       Mean reward: 0.91
               Mean episode length: 68.74
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 8.23s
                        Total time: 1779.79s
                               ETA: 870676.6s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.153s, learning 0.176s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0433
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 68.31
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 8.33s
                        Total time: 1788.12s
                               ETA: 870475.2s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.083s, learning 0.171s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0419
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 67.71
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 8.25s
                        Total time: 1796.38s
                               ETA: 870239.3s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.105s, learning 0.218s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0418
             Mean action noise std: 0.78
                       Mean reward: 0.84
               Mean episode length: 65.41
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 8.32s
                        Total time: 1804.70s
                               ETA: 870039.1s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.220s, learning 0.182s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0435
             Mean action noise std: 0.78
                       Mean reward: 0.89
               Mean episode length: 68.72
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 8.40s
                        Total time: 1813.10s
                               ETA: 869878.4s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.212s, learning 0.163s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0429
             Mean action noise std: 0.78
                       Mean reward: 0.89
               Mean episode length: 68.31
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 8.38s
                        Total time: 1821.48s
                               ETA: 869706.7s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.164s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0444
             Mean action noise std: 0.78
                       Mean reward: 0.88
               Mean episode length: 67.18
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 8.25s
                        Total time: 1829.72s
                               ETA: 869476.0s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.149s, learning 0.179s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0425
             Mean action noise std: 0.78
                       Mean reward: 0.89
               Mean episode length: 66.73
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 8.33s
                        Total time: 1838.05s
                               ETA: 869285.4s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.355s, learning 0.242s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0443
             Mean action noise std: 0.78
                       Mean reward: 0.86
               Mean episode length: 66.45
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 8.60s
                        Total time: 1846.65s
                               ETA: 869223.2s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1997 steps/s (collection: 7.996s, learning 0.208s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0404
             Mean action noise std: 0.78
                       Mean reward: 0.89
               Mean episode length: 65.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 8.20s
                        Total time: 1854.85s
                               ETA: 868977.2s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.050s, learning 0.161s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0393
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 68.30
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 8.21s
                        Total time: 1863.06s
                               ETA: 868736.4s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.167s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0433
             Mean action noise std: 0.78
                       Mean reward: 0.91
               Mean episode length: 69.17
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 8.36s
                        Total time: 1871.43s
                               ETA: 868568.2s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.111s, learning 0.169s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0425
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 64.64
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 8.28s
                        Total time: 1879.71s
                               ETA: 868363.2s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.178s, learning 0.168s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0404
             Mean action noise std: 0.78
                       Mean reward: 0.88
               Mean episode length: 66.86
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 8.35s
                        Total time: 1888.05s
                               ETA: 868190.6s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.245s, learning 0.171s)
               Value function loss: 0.0025
                    Surrogate loss: -0.0399
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 66.57
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 8.42s
                        Total time: 1896.47s
                               ETA: 868051.8s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.261s, learning 0.174s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0423
             Mean action noise std: 0.78
                       Mean reward: 0.89
               Mean episode length: 66.52
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 8.43s
                        Total time: 1904.90s
                               ETA: 867922.5s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.196s, learning 0.187s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0419
             Mean action noise std: 0.78
                       Mean reward: 0.88
               Mean episode length: 66.25
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 8.38s
                        Total time: 1913.29s
                               ETA: 867770.7s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.052s, learning 0.170s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0388
             Mean action noise std: 0.78
                       Mean reward: 0.88
               Mean episode length: 66.48
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 8.22s
                        Total time: 1921.51s
                               ETA: 867547.7s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.445s, learning 0.170s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0397
             Mean action noise std: 0.78
                       Mean reward: 0.87
               Mean episode length: 65.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 8.62s
                        Total time: 1930.12s
                               ETA: 867503.6s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.165s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0402
             Mean action noise std: 0.78
                       Mean reward: 0.88
               Mean episode length: 64.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 8.28s
                        Total time: 1938.40s
                               ETA: 867309.5s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.283s, learning 0.162s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0396
             Mean action noise std: 0.78
                       Mean reward: 0.88
               Mean episode length: 65.89
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 8.45s
                        Total time: 1946.85s
                               ETA: 867190.6s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.041s, learning 0.218s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0388
             Mean action noise std: 0.78
                       Mean reward: 0.91
               Mean episode length: 67.07
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 8.26s
                        Total time: 1955.11s
                               ETA: 866990.2s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.167s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0352
             Mean action noise std: 0.78
                       Mean reward: 0.90
               Mean episode length: 66.67
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 8.28s
                        Total time: 1963.39s
                               ETA: 866801.9s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.037s, learning 0.175s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0389
             Mean action noise std: 0.78
                       Mean reward: 0.89
               Mean episode length: 65.91
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 8.21s
                        Total time: 1971.60s
                               ETA: 866584.2s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.999s, learning 0.163s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0405
             Mean action noise std: 0.78
                       Mean reward: 0.92
               Mean episode length: 66.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 8.16s
                        Total time: 1979.76s
                               ETA: 866346.2s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.251s, learning 0.216s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0399
             Mean action noise std: 0.78
                       Mean reward: 0.95
               Mean episode length: 68.00
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 8.47s
                        Total time: 1988.23s
                               ETA: 866243.6s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.218s, learning 0.168s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0400
             Mean action noise std: 0.78
                       Mean reward: 0.97
               Mean episode length: 66.47
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 8.39s
                        Total time: 1996.62s
                               ETA: 866106.2s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.221s, learning 0.169s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0396
             Mean action noise std: 0.78
                       Mean reward: 0.95
               Mean episode length: 68.39
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 8.39s
                        Total time: 2005.01s
                               ETA: 865971.9s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.215s, learning 0.180s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0411
             Mean action noise std: 0.78
                       Mean reward: 0.95
               Mean episode length: 68.43
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 8.39s
                        Total time: 2013.40s
                               ETA: 865840.7s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.187s, learning 0.165s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0432
             Mean action noise std: 0.78
                       Mean reward: 0.93
               Mean episode length: 67.14
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 8.35s
                        Total time: 2021.75s
                               ETA: 865692.3s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 2020 steps/s (collection: 7.947s, learning 0.160s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0419
             Mean action noise std: 0.78
                       Mean reward: 0.95
               Mean episode length: 67.20
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 8.11s
                        Total time: 2029.86s
                               ETA: 865440.5s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.271s, learning 0.167s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0394
             Mean action noise std: 0.78
                       Mean reward: 0.92
               Mean episode length: 67.60
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 8.44s
                        Total time: 2038.30s
                               ETA: 865331.4s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.183s, learning 0.190s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0391
             Mean action noise std: 0.78
                       Mean reward: 0.95
               Mean episode length: 67.75
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 8.37s
                        Total time: 2046.67s
                               ETA: 865195.8s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.095s, learning 0.167s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0380
             Mean action noise std: 0.78
                       Mean reward: 0.98
               Mean episode length: 69.56
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 8.26s
                        Total time: 2054.93s
                               ETA: 865014.2s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.170s, learning 0.276s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0393
             Mean action noise std: 0.78
                       Mean reward: 0.96
               Mean episode length: 66.99
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 8.45s
                        Total time: 2063.38s
                               ETA: 864911.1s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.162s)
               Value function loss: 0.0028
                    Surrogate loss: -0.0439
             Mean action noise std: 0.78
                       Mean reward: 0.97
               Mean episode length: 67.68
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 8.17s
                        Total time: 2071.55s
                               ETA: 864693.1s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.105s, learning 0.164s)
               Value function loss: 0.0027
                    Surrogate loss: -0.0431
             Mean action noise std: 0.78
                       Mean reward: 0.93
               Mean episode length: 65.51
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 8.27s
                        Total time: 2079.82s
                               ETA: 864518.7s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.179s, learning 0.212s)
               Value function loss: 0.0029
                    Surrogate loss: -0.0415
             Mean action noise std: 0.78
                       Mean reward: 0.98
               Mean episode length: 66.96
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 8.39s
                        Total time: 2088.21s
                               ETA: 864396.1s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.317s, learning 0.212s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0411
             Mean action noise std: 0.78
                       Mean reward: 0.95
               Mean episode length: 67.46
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 8.53s
                        Total time: 2096.74s
                               ETA: 864331.8s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.334s, learning 0.163s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0424
             Mean action noise std: 0.78
                       Mean reward: 0.97
               Mean episode length: 66.02
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 8.50s
                        Total time: 2105.23s
                               ETA: 864254.4s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.162s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0414
             Mean action noise std: 0.78
                       Mean reward: 0.97
               Mean episode length: 66.76
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 8.47s
                        Total time: 2113.70s
                               ETA: 864164.6s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.976s, learning 0.175s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0386
             Mean action noise std: 0.78
                       Mean reward: 1.03
               Mean episode length: 68.06
                  Mean reward/step: 0.01
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 8.15s
                        Total time: 2121.85s
                               ETA: 863947.7s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.041s, learning 0.163s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0391
             Mean action noise std: 0.78
                       Mean reward: 0.96
               Mean episode length: 66.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 8.20s
                        Total time: 2130.05s
                               ETA: 863753.7s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1890 steps/s (collection: 8.487s, learning 0.179s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0399
             Mean action noise std: 0.78
                       Mean reward: 0.99
               Mean episode length: 66.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 8.67s
                        Total time: 2138.72s
                               ETA: 863748.2s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.325s, learning 0.162s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0399
             Mean action noise std: 0.78
                       Mean reward: 0.97
               Mean episode length: 65.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 8.49s
                        Total time: 2147.21s
                               ETA: 863670.6s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.060s, learning 0.241s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0391
             Mean action noise std: 0.78
                       Mean reward: 0.98
               Mean episode length: 63.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 8.30s
                        Total time: 2155.51s
                               ETA: 863519.2s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.068s, learning 0.168s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0369
             Mean action noise std: 0.78
                       Mean reward: 0.99
               Mean episode length: 66.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 8.24s
                        Total time: 2163.74s
                               ETA: 863342.7s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.164s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0392
             Mean action noise std: 0.78
                       Mean reward: 1.01
               Mean episode length: 67.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 8.36s
                        Total time: 2172.11s
                               ETA: 863218.3s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.087s, learning 0.177s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0374
             Mean action noise std: 0.78
                       Mean reward: 0.98
               Mean episode length: 62.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 8.26s
                        Total time: 2180.37s
                               ETA: 863055.4s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.994s, learning 0.165s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0430
             Mean action noise std: 0.77
                       Mean reward: 1.05
               Mean episode length: 67.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 8.16s
                        Total time: 2188.53s
                               ETA: 862852.3s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.037s, learning 0.167s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0418
             Mean action noise std: 0.77
                       Mean reward: 1.02
               Mean episode length: 66.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 8.20s
                        Total time: 2196.74s
                               ETA: 862668.6s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.139s, learning 0.179s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0413
             Mean action noise std: 0.77
                       Mean reward: 1.01
               Mean episode length: 64.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 8.32s
                        Total time: 2205.05s
                               ETA: 862530.7s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.164s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0403
             Mean action noise std: 0.77
                       Mean reward: 0.96
               Mean episode length: 63.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 8.36s
                        Total time: 2213.41s
                               ETA: 862410.4s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.104s, learning 0.166s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0400
             Mean action noise std: 0.77
                       Mean reward: 1.05
               Mean episode length: 66.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 8.27s
                        Total time: 2221.68s
                               ETA: 862255.7s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.274s, learning 0.218s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0386
             Mean action noise std: 0.77
                       Mean reward: 1.04
               Mean episode length: 65.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 8.49s
                        Total time: 2230.18s
                               ETA: 862188.0s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.449s, learning 0.170s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0396
             Mean action noise std: 0.77
                       Mean reward: 1.08
               Mean episode length: 66.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 8.62s
                        Total time: 2238.80s
                               ETA: 862169.8s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.071s, learning 0.171s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0410
             Mean action noise std: 0.77
                       Mean reward: 1.07
               Mean episode length: 68.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 8.24s
                        Total time: 2247.04s
                               ETA: 862006.7s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1658 steps/s (collection: 9.691s, learning 0.190s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0410
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 66.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 9.88s
                        Total time: 2256.92s
                               ETA: 862471.2s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.900s, learning 0.178s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 65.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 16.08s
                        Total time: 2273.00s
                               ETA: 865291.3s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.004s, learning 0.212s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0394
             Mean action noise std: 0.77
                       Mean reward: 1.09
               Mean episode length: 65.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 16.22s
                        Total time: 2289.21s
                               ETA: 868142.1s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.564s, learning 0.162s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 1.07
               Mean episode length: 64.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 16.73s
                        Total time: 2305.94s
                               ETA: 871164.1s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.378s, learning 0.160s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0400
             Mean action noise std: 0.77
                       Mean reward: 1.06
               Mean episode length: 64.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 16.54s
                        Total time: 2322.48s
                               ETA: 874092.0s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.146s, learning 0.167s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0415
             Mean action noise std: 0.77
                       Mean reward: 1.06
               Mean episode length: 65.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 16.31s
                        Total time: 2338.79s
                               ETA: 876913.9s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.862s, learning 0.198s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 1.07
               Mean episode length: 63.74
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 16.06s
                        Total time: 2354.85s
                               ETA: 879619.8s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.242s, learning 0.165s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0414
             Mean action noise std: 0.77
                       Mean reward: 1.06
               Mean episode length: 63.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 16.41s
                        Total time: 2371.26s
                               ETA: 882434.8s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.139s, learning 0.168s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0406
             Mean action noise std: 0.77
                       Mean reward: 1.13
               Mean episode length: 64.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 16.31s
                        Total time: 2387.56s
                               ETA: 885191.4s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.072s, learning 0.171s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0406
             Mean action noise std: 0.77
                       Mean reward: 1.10
               Mean episode length: 65.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 16.24s
                        Total time: 2403.81s
                               ETA: 887903.7s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.133s, learning 0.210s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0428
             Mean action noise std: 0.77
                       Mean reward: 1.07
               Mean episode length: 63.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 16.34s
                        Total time: 2420.15s
                               ETA: 890632.5s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.018s, learning 0.173s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0427
             Mean action noise std: 0.77
                       Mean reward: 1.14
               Mean episode length: 65.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 16.19s
                        Total time: 2436.34s
                               ETA: 893285.6s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1026 steps/s (collection: 15.793s, learning 0.163s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0401
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 66.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 15.96s
                        Total time: 2452.30s
                               ETA: 895833.5s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.368s, learning 0.165s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0384
             Mean action noise std: 0.77
                       Mean reward: 1.09
               Mean episode length: 63.35
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 16.53s
                        Total time: 2468.83s
                               ETA: 898572.8s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.049s, learning 0.168s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0399
             Mean action noise std: 0.77
                       Mean reward: 1.09
               Mean episode length: 63.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 16.22s
                        Total time: 2485.05s
                               ETA: 901177.2s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 997 steps/s (collection: 16.260s, learning 0.167s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0373
             Mean action noise std: 0.77
                       Mean reward: 1.14
               Mean episode length: 65.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 16.43s
                        Total time: 2501.47s
                               ETA: 903838.5s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.013s, learning 0.187s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0420
             Mean action noise std: 0.77
                       Mean reward: 1.10
               Mean episode length: 63.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 16.20s
                        Total time: 2517.67s
                               ETA: 906398.8s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.146s, learning 0.165s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0447
             Mean action noise std: 0.77
                       Mean reward: 1.07
               Mean episode length: 60.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 16.31s
                        Total time: 2533.99s
                               ETA: 908980.5s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1031 steps/s (collection: 15.705s, learning 0.172s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0395
             Mean action noise std: 0.77
                       Mean reward: 1.06
               Mean episode length: 57.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 15.88s
                        Total time: 2549.86s
                               ETA: 911388.1s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.123s, learning 0.166s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0374
             Mean action noise std: 0.77
                       Mean reward: 1.13
               Mean episode length: 63.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 16.29s
                        Total time: 2566.15s
                               ETA: 913925.2s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.161s, learning 0.168s)
               Value function loss: 0.0032
                    Surrogate loss: -0.0422
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 60.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 16.33s
                        Total time: 2582.48s
                               ETA: 916458.6s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.108s, learning 0.266s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0400
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 61.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 16.37s
                        Total time: 2598.85s
                               ETA: 918989.7s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.451s, learning 0.168s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0414
             Mean action noise std: 0.77
                       Mean reward: 1.07
               Mean episode length: 61.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 16.62s
                        Total time: 2615.47s
                               ETA: 921589.2s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.858s, learning 0.176s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0416
             Mean action noise std: 0.77
                       Mean reward: 1.15
               Mean episode length: 62.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 16.03s
                        Total time: 2631.51s
                               ETA: 923964.6s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 985 steps/s (collection: 16.465s, learning 0.167s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0394
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 61.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 16.63s
                        Total time: 2648.14s
                               ETA: 926532.7s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.205s, learning 0.177s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0400
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 62.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 16.38s
                        Total time: 2664.52s
                               ETA: 928995.4s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1009 steps/s (collection: 16.058s, learning 0.169s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0402
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 60.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 16.23s
                        Total time: 2680.75s
                               ETA: 931386.9s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.096s, learning 0.169s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0396
             Mean action noise std: 0.77
                       Mean reward: 1.13
               Mean episode length: 61.85
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 16.27s
                        Total time: 2697.01s
                               ETA: 933775.1s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.215s, learning 0.163s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0385
             Mean action noise std: 0.77
                       Mean reward: 1.10
               Mean episode length: 60.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 16.38s
                        Total time: 2713.39s
                               ETA: 936185.5s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.110s, learning 0.168s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0388
             Mean action noise std: 0.77
                       Mean reward: 1.14
               Mean episode length: 62.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 16.28s
                        Total time: 2729.67s
                               ETA: 938544.7s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.310s, learning 0.256s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0388
             Mean action noise std: 0.77
                       Mean reward: 1.12
               Mean episode length: 61.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 16.57s
                        Total time: 2746.23s
                               ETA: 940986.6s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.881s, learning 0.166s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0370
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 59.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 16.05s
                        Total time: 2762.28s
                               ETA: 943234.1s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.092s, learning 0.180s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0400
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 59.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 16.27s
                        Total time: 2778.55s
                               ETA: 945542.8s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.945s, learning 0.209s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0388
             Mean action noise std: 0.77
                       Mean reward: 1.15
               Mean episode length: 60.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 16.15s
                        Total time: 2794.71s
                               ETA: 947795.7s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.202s, learning 0.165s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0389
             Mean action noise std: 0.77
                       Mean reward: 1.09
               Mean episode length: 58.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 16.37s
                        Total time: 2811.07s
                               ETA: 950105.1s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1029 steps/s (collection: 15.737s, learning 0.173s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 1.21
               Mean episode length: 62.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 15.91s
                        Total time: 2826.99s
                               ETA: 952245.2s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.299s, learning 0.165s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0384
             Mean action noise std: 0.77
                       Mean reward: 1.14
               Mean episode length: 59.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 16.46s
                        Total time: 2843.45s
                               ETA: 954556.6s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.031s, learning 0.173s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0384
             Mean action noise std: 0.77
                       Mean reward: 1.16
               Mean episode length: 60.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 16.20s
                        Total time: 2859.65s
                               ETA: 956765.2s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.185s, learning 0.165s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0411
             Mean action noise std: 0.77
                       Mean reward: 1.11
               Mean episode length: 59.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 12.35s
                        Total time: 2872.00s
                               ETA: 957673.9s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.431s, learning 0.174s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0427
             Mean action noise std: 0.77
                       Mean reward: 1.15
               Mean episode length: 60.03
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 8.60s
                        Total time: 2880.61s
                               ETA: 957331.7s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.984s, learning 0.168s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0401
             Mean action noise std: 0.77
                       Mean reward: 1.17
               Mean episode length: 60.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 8.15s
                        Total time: 2888.76s
                               ETA: 956842.1s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.164s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0390
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 61.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 8.34s
                        Total time: 2897.10s
                               ETA: 956418.5s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.136s, learning 0.259s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0397
             Mean action noise std: 0.77
                       Mean reward: 1.14
               Mean episode length: 59.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 8.40s
                        Total time: 2905.50s
                               ETA: 956014.8s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1870 steps/s (collection: 8.594s, learning 0.165s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0391
             Mean action noise std: 0.77
                       Mean reward: 1.21
               Mean episode length: 59.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 8.76s
                        Total time: 2914.26s
                               ETA: 955733.1s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.040s, learning 0.162s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0382
             Mean action noise std: 0.77
                       Mean reward: 1.14
               Mean episode length: 56.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 8.20s
                        Total time: 2922.46s
                               ETA: 955271.1s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.137s, learning 0.171s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0381
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 59.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 8.31s
                        Total time: 2930.77s
                               ETA: 954846.4s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.085s, learning 0.213s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0377
             Mean action noise std: 0.77
                       Mean reward: 1.16
               Mean episode length: 58.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 8.30s
                        Total time: 2939.07s
                               ETA: 954421.2s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.273s, learning 0.168s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0429
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 60.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 8.44s
                        Total time: 2947.51s
                               ETA: 954045.1s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.322s, learning 0.167s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0399
             Mean action noise std: 0.77
                       Mean reward: 1.12
               Mean episode length: 58.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 8.49s
                        Total time: 2956.00s
                               ETA: 953686.8s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.300s, learning 0.172s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0383
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 59.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 8.47s
                        Total time: 2964.47s
                               ETA: 953325.4s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.442s, learning 0.162s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0408
             Mean action noise std: 0.77
                       Mean reward: 1.20
               Mean episode length: 58.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 8.60s
                        Total time: 2973.07s
                               ETA: 953008.4s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.222s, learning 0.163s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0394
             Mean action noise std: 0.77
                       Mean reward: 1.20
               Mean episode length: 58.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 8.39s
                        Total time: 2981.46s
                               ETA: 952623.6s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.071s, learning 0.167s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0369
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 59.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 8.24s
                        Total time: 2989.70s
                               ETA: 952194.5s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.164s, learning 0.247s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0371
             Mean action noise std: 0.77
                       Mean reward: 1.13
               Mean episode length: 56.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 8.41s
                        Total time: 2998.11s
                               ETA: 951822.8s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.094s, learning 0.173s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0385
             Mean action noise std: 0.77
                       Mean reward: 1.17
               Mean episode length: 57.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 8.27s
                        Total time: 3006.37s
                               ETA: 951407.9s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.997s, learning 0.176s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0404
             Mean action noise std: 0.77
                       Mean reward: 1.25
               Mean episode length: 60.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 8.17s
                        Total time: 3014.55s
                               ETA: 950965.6s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.424s, learning 0.165s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0412
             Mean action noise std: 0.77
                       Mean reward: 1.23
               Mean episode length: 57.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 8.59s
                        Total time: 3023.14s
                               ETA: 950657.1s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.356s, learning 0.166s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0386
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 58.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 8.52s
                        Total time: 3031.66s
                               ETA: 950329.7s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.898s, learning 0.162s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0374
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 60.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 8.06s
                        Total time: 3039.72s
                               ETA: 949859.6s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.286s, learning 0.174s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0358
             Mean action noise std: 0.77
                       Mean reward: 1.20
               Mean episode length: 61.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 8.46s
                        Total time: 3048.18s
                               ETA: 949517.0s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.899s, learning 0.256s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0378
             Mean action noise std: 0.77
                       Mean reward: 1.19
               Mean episode length: 59.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 8.16s
                        Total time: 3056.33s
                               ETA: 949082.1s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.131s, learning 0.179s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0379
             Mean action noise std: 0.77
                       Mean reward: 1.21
               Mean episode length: 59.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 8.31s
                        Total time: 3064.64s
                               ETA: 948697.6s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.172s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0375
             Mean action noise std: 0.76
                       Mean reward: 1.25
               Mean episode length: 60.68
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 8.42s
                        Total time: 3073.07s
                               ETA: 948349.9s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.064s, learning 0.180s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0384
             Mean action noise std: 0.76
                       Mean reward: 1.25
               Mean episode length: 59.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 8.24s
                        Total time: 3081.31s
                               ETA: 947949.6s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.924s, learning 0.222s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 62.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 8.15s
                        Total time: 3089.46s
                               ETA: 947521.7s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.161s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 1.24
               Mean episode length: 59.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 8.31s
                        Total time: 3097.77s
                               ETA: 947147.4s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.997s, learning 0.165s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0408
             Mean action noise std: 0.76
                       Mean reward: 1.26
               Mean episode length: 62.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 8.16s
                        Total time: 3105.93s
                               ETA: 946729.3s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.988s, learning 0.178s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0413
             Mean action noise std: 0.76
                       Mean reward: 1.21
               Mean episode length: 59.03
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 8.17s
                        Total time: 3114.10s
                               ETA: 946315.0s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.110s, learning 0.163s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0403
             Mean action noise std: 0.76
                       Mean reward: 1.26
               Mean episode length: 60.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 8.27s
                        Total time: 3122.37s
                               ETA: 945935.8s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 2015 steps/s (collection: 7.964s, learning 0.165s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 1.27
               Mean episode length: 61.27
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 8.13s
                        Total time: 3130.50s
                               ETA: 945515.1s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.228s, learning 0.162s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 1.28
               Mean episode length: 59.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 8.39s
                        Total time: 3138.89s
                               ETA: 945175.5s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.188s, learning 0.171s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 1.31
               Mean episode length: 63.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 8.36s
                        Total time: 3147.25s
                               ETA: 944828.7s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.011s, learning 0.217s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0402
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 59.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 8.23s
                        Total time: 3155.48s
                               ETA: 944444.7s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.220s, learning 0.176s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0406
             Mean action noise std: 0.76
                       Mean reward: 1.29
               Mean episode length: 61.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 8.40s
                        Total time: 3163.87s
                               ETA: 944112.8s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.197s, learning 0.180s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0391
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 60.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 8.38s
                        Total time: 3172.25s
                               ETA: 943777.3s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.190s, learning 0.160s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0372
             Mean action noise std: 0.76
                       Mean reward: 1.29
               Mean episode length: 62.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 8.35s
                        Total time: 3180.60s
                               ETA: 943435.6s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.200s, learning 0.176s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0397
             Mean action noise std: 0.76
                       Mean reward: 1.29
               Mean episode length: 60.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 8.38s
                        Total time: 3188.97s
                               ETA: 943103.5s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.482s, learning 0.174s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0399
             Mean action noise std: 0.76
                       Mean reward: 1.27
               Mean episode length: 62.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 8.66s
                        Total time: 3197.63s
                               ETA: 942856.1s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.863s, learning 0.163s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0407
             Mean action noise std: 0.76
                       Mean reward: 1.27
               Mean episode length: 58.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 8.03s
                        Total time: 3205.66s
                               ETA: 942424.9s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.103s, learning 0.187s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 1.27
               Mean episode length: 61.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 8.29s
                        Total time: 3213.94s
                               ETA: 942073.4s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.204s, learning 0.160s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 1.29
               Mean episode length: 61.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 8.36s
                        Total time: 3222.31s
                               ETA: 941745.8s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.168s, learning 0.168s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 1.31
               Mean episode length: 61.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 8.34s
                        Total time: 3230.65s
                               ETA: 941412.0s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.263s, learning 0.169s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0385
             Mean action noise std: 0.76
                       Mean reward: 1.28
               Mean episode length: 62.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 8.43s
                        Total time: 3239.08s
                               ETA: 941107.7s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.124s, learning 0.165s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 62.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 8.29s
                        Total time: 3247.37s
                               ETA: 940763.7s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.147s, learning 0.172s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0400
             Mean action noise std: 0.76
                       Mean reward: 1.27
               Mean episode length: 60.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 8.32s
                        Total time: 3255.68s
                               ETA: 940430.3s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.372s, learning 0.175s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 1.35
               Mean episode length: 60.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 8.55s
                        Total time: 3264.23s
                               ETA: 940164.6s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.144s, learning 0.211s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 62.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 8.35s
                        Total time: 3272.59s
                               ETA: 939845.2s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.968s, learning 0.169s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0328
             Mean action noise std: 0.76
                       Mean reward: 1.34
               Mean episode length: 61.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 8.14s
                        Total time: 3280.72s
                               ETA: 939465.1s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.171s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0386
             Mean action noise std: 0.76
                       Mean reward: 1.33
               Mean episode length: 62.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 8.54s
                        Total time: 3289.26s
                               ETA: 939201.7s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.176s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0368
             Mean action noise std: 0.76
                       Mean reward: 1.31
               Mean episode length: 60.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 8.46s
                        Total time: 3297.72s
                               ETA: 938917.0s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.204s, learning 0.159s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 1.29
               Mean episode length: 59.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 8.36s
                        Total time: 3306.08s
                               ETA: 938606.9s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.167s, learning 0.174s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 61.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 8.34s
                        Total time: 3314.42s
                               ETA: 938292.3s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.041s, learning 0.173s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0404
             Mean action noise std: 0.76
                       Mean reward: 1.33
               Mean episode length: 58.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 8.21s
                        Total time: 3322.64s
                               ETA: 937943.6s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.107s, learning 0.172s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0383
             Mean action noise std: 0.76
                       Mean reward: 1.37
               Mean episode length: 59.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 8.28s
                        Total time: 3330.92s
                               ETA: 937615.0s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.158s, learning 0.169s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0388
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 57.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 8.33s
                        Total time: 3339.24s
                               ETA: 937301.9s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.146s, learning 0.174s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0392
             Mean action noise std: 0.76
                       Mean reward: 1.34
               Mean episode length: 59.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 8.32s
                        Total time: 3347.56s
                               ETA: 936988.4s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.158s, learning 0.167s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 57.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 8.32s
                        Total time: 3355.89s
                               ETA: 936677.8s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1985 steps/s (collection: 8.091s, learning 0.161s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0395
             Mean action noise std: 0.76
                       Mean reward: 1.35
               Mean episode length: 60.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 8.25s
                        Total time: 3364.14s
                               ETA: 936349.0s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.208s, learning 0.181s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0370
             Mean action noise std: 0.76
                       Mean reward: 1.31
               Mean episode length: 60.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 8.39s
                        Total time: 3372.53s
                               ETA: 936059.8s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.914s, learning 0.165s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0352
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 59.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 8.08s
                        Total time: 3380.61s
                               ETA: 935686.3s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 2028 steps/s (collection: 7.904s, learning 0.174s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0377
             Mean action noise std: 0.76
                       Mean reward: 1.31
               Mean episode length: 57.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 8.08s
                        Total time: 3388.69s
                               ETA: 935314.8s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 2001 steps/s (collection: 7.968s, learning 0.219s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0387
             Mean action noise std: 0.76
                       Mean reward: 1.38
               Mean episode length: 59.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 8.19s
                        Total time: 3396.87s
                               ETA: 934975.0s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.115s, learning 0.211s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 1.31
               Mean episode length: 56.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 8.33s
                        Total time: 3405.20s
                               ETA: 934675.2s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.007s, learning 0.164s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0396
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 57.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 8.17s
                        Total time: 3413.37s
                               ETA: 934334.8s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 2038 steps/s (collection: 7.864s, learning 0.172s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0378
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 57.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 8.04s
                        Total time: 3421.40s
                               ETA: 933959.1s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.350s, learning 0.174s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0380
             Mean action noise std: 0.76
                       Mean reward: 1.26
               Mean episode length: 56.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 8.52s
                        Total time: 3429.93s
                               ETA: 933718.5s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.118s, learning 0.173s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0374
             Mean action noise std: 0.76
                       Mean reward: 1.34
               Mean episode length: 59.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 8.29s
                        Total time: 3438.22s
                               ETA: 933415.9s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.083s, learning 0.192s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0390
             Mean action noise std: 0.76
                       Mean reward: 1.35
               Mean episode length: 59.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 8.28s
                        Total time: 3446.50s
                               ETA: 933110.5s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1919 steps/s (collection: 8.318s, learning 0.219s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0376
             Mean action noise std: 0.76
                       Mean reward: 1.30
               Mean episode length: 58.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 8.54s
                        Total time: 3455.03s
                               ETA: 932877.3s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.355s, learning 0.178s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0389
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 59.51
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 8.53s
                        Total time: 3463.56s
                               ETA: 932644.3s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.404s, learning 0.206s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0365
             Mean action noise std: 0.76
                       Mean reward: 1.37
               Mean episode length: 59.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 8.61s
                        Total time: 3472.17s
                               ETA: 932433.4s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.297s, learning 0.175s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0369
             Mean action noise std: 0.76
                       Mean reward: 1.35
               Mean episode length: 59.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 8.47s
                        Total time: 3480.65s
                               ETA: 932186.5s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.971s, learning 0.184s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0355
             Mean action noise std: 0.76
                       Mean reward: 1.40
               Mean episode length: 59.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 8.15s
                        Total time: 3488.80s
                               ETA: 931856.0s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.074s, learning 0.169s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0346
             Mean action noise std: 0.76
                       Mean reward: 1.40
               Mean episode length: 59.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 8.24s
                        Total time: 3497.04s
                               ETA: 931550.9s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.153s, learning 0.166s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0348
             Mean action noise std: 0.76
                       Mean reward: 1.37
               Mean episode length: 59.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 8.32s
                        Total time: 3505.36s
                               ETA: 931267.4s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.118s, learning 0.207s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0358
             Mean action noise std: 0.76
                       Mean reward: 1.37
               Mean episode length: 61.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 8.33s
                        Total time: 3513.69s
                               ETA: 930987.2s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.272s, learning 0.173s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0372
             Mean action noise std: 0.76
                       Mean reward: 1.37
               Mean episode length: 61.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 8.44s
                        Total time: 3522.13s
                               ETA: 930740.0s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.171s, learning 0.179s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0362
             Mean action noise std: 0.76
                       Mean reward: 1.32
               Mean episode length: 56.48
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 8.35s
                        Total time: 3530.48s
                               ETA: 930469.2s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.137s, learning 0.165s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0393
             Mean action noise std: 0.76
                       Mean reward: 1.38
               Mean episode length: 59.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 8.30s
                        Total time: 3538.79s
                               ETA: 930187.0s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.085s, learning 0.160s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0355
             Mean action noise std: 0.76
                       Mean reward: 1.36
               Mean episode length: 58.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 8.25s
                        Total time: 3547.03s
                               ETA: 929891.4s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.458s, learning 0.182s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0354
             Mean action noise std: 0.76
                       Mean reward: 1.34
               Mean episode length: 60.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 8.64s
                        Total time: 3555.67s
                               ETA: 929700.5s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.100s, learning 0.170s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0345
             Mean action noise std: 0.76
                       Mean reward: 1.33
               Mean episode length: 57.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 8.27s
                        Total time: 3563.94s
                               ETA: 929414.0s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 2013 steps/s (collection: 7.965s, learning 0.174s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0352
             Mean action noise std: 0.76
                       Mean reward: 1.41
               Mean episode length: 61.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 8.14s
                        Total time: 3572.08s
                               ETA: 929094.8s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.152s, learning 0.171s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0371
             Mean action noise std: 0.76
                       Mean reward: 1.35
               Mean episode length: 61.93
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 8.32s
                        Total time: 3580.40s
                               ETA: 928825.1s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.212s, learning 0.211s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0368
             Mean action noise std: 0.76
                       Mean reward: 1.37
               Mean episode length: 61.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 8.42s
                        Total time: 3588.82s
                               ETA: 928582.8s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.087s, learning 0.171s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 60.29
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 8.26s
                        Total time: 3597.08s
                               ETA: 928299.0s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.293s, learning 0.164s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0361
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 59.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 8.46s
                        Total time: 3605.54s
                               ETA: 928067.8s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.244s, learning 0.174s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 57.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 8.42s
                        Total time: 3613.96s
                               ETA: 927827.7s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.164s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0364
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 59.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 8.34s
                        Total time: 3622.30s
                               ETA: 927568.9s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 2006 steps/s (collection: 7.995s, learning 0.170s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0346
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 61.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 8.17s
                        Total time: 3630.46s
                               ETA: 927266.7s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1983 steps/s (collection: 8.014s, learning 0.245s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0341
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 60.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 8.26s
                        Total time: 3638.72s
                               ETA: 926990.0s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.073s, learning 0.211s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 60.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 8.28s
                        Total time: 3647.01s
                               ETA: 926721.1s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.418s, learning 0.172s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0350
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 61.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 8.59s
                        Total time: 3655.60s
                               ETA: 926530.9s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.001s, learning 0.187s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0338
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 61.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 8.19s
                        Total time: 3663.78s
                               ETA: 926239.8s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 2035 steps/s (collection: 7.886s, learning 0.164s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0331
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 62.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 8.05s
                        Total time: 3671.83s
                               ETA: 925915.7s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.288s, learning 0.188s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 62.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 8.48s
                        Total time: 3680.31s
                               ETA: 925700.1s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.077s, learning 0.169s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0350
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 63.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 8.25s
                        Total time: 3688.56s
                               ETA: 925428.0s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.189s, learning 0.207s)
               Value function loss: 0.0084
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 59.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 8.40s
                        Total time: 3696.95s
                               ETA: 925194.7s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.327s, learning 0.219s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 61.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 8.55s
                        Total time: 3705.50s
                               ETA: 925000.2s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.376s, learning 0.173s)
               Value function loss: 0.0098
                    Surrogate loss: -0.0351
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 62.12
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 8.55s
                        Total time: 3714.05s
                               ETA: 924807.1s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.257s, learning 0.176s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 1.38
               Mean episode length: 62.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 8.43s
                        Total time: 3722.48s
                               ETA: 924586.2s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.158s, learning 0.168s)
               Value function loss: 0.0082
                    Surrogate loss: -0.0326
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 61.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 8.33s
                        Total time: 3730.81s
                               ETA: 924339.8s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.277s, learning 0.171s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 58.83
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 8.45s
                        Total time: 3739.25s
                               ETA: 924124.7s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.082s, learning 0.168s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 63.55
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 8.25s
                        Total time: 3747.50s
                               ETA: 923861.7s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.181s, learning 0.167s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 62.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 8.35s
                        Total time: 3755.85s
                               ETA: 923624.4s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.137s, learning 0.176s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 60.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 8.31s
                        Total time: 3764.17s
                               ETA: 923379.4s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.026s, learning 0.176s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0361
             Mean action noise std: 0.75
                       Mean reward: 1.42
               Mean episode length: 64.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 8.20s
                        Total time: 3772.37s
                               ETA: 923108.7s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.165s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0367
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 61.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 8.36s
                        Total time: 3780.73s
                               ETA: 922878.7s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.237s, learning 0.173s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0329
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 63.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 8.41s
                        Total time: 3789.14s
                               ETA: 922660.7s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 2004 steps/s (collection: 7.997s, learning 0.178s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 63.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 8.18s
                        Total time: 3797.32s
                               ETA: 922386.9s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.217s, learning 0.177s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0365
             Mean action noise std: 0.75
                       Mean reward: 1.41
               Mean episode length: 66.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 8.39s
                        Total time: 3805.71s
                               ETA: 922167.5s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.041s, learning 0.162s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 1.40
               Mean episode length: 61.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 8.20s
                        Total time: 3813.92s
                               ETA: 921902.9s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.210s, learning 0.175s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 63.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 8.39s
                        Total time: 3822.30s
                               ETA: 921683.4s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.346s, learning 0.176s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0363
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 62.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 8.52s
                        Total time: 3830.82s
                               ETA: 921497.8s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.358s, learning 0.169s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.38
               Mean episode length: 63.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 8.53s
                        Total time: 3839.35s
                               ETA: 921314.3s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.160s, learning 0.166s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0388
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 61.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 8.33s
                        Total time: 3847.68s
                               ETA: 921083.6s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.030s, learning 0.168s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 1.41
               Mean episode length: 62.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 8.20s
                        Total time: 3855.87s
                               ETA: 920823.1s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.165s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0385
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 61.29
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 8.36s
                        Total time: 3864.23s
                               ETA: 920603.1s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.097s, learning 0.160s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0358
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 61.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 8.26s
                        Total time: 3872.49s
                               ETA: 920359.2s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.363s, learning 0.215s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 60.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 8.58s
                        Total time: 3881.07s
                               ETA: 920192.4s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.179s, learning 0.163s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0312
             Mean action noise std: 0.75
                       Mean reward: 1.41
               Mean episode length: 62.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 8.34s
                        Total time: 3889.41s
                               ETA: 919970.6s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.215s, learning 0.167s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0385
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 61.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 8.38s
                        Total time: 3897.79s
                               ETA: 919759.4s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.105s, learning 0.175s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0401
             Mean action noise std: 0.75
                       Mean reward: 1.34
               Mean episode length: 59.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 8.28s
                        Total time: 3906.07s
                               ETA: 919525.1s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.383s, learning 0.169s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 61.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 8.55s
                        Total time: 3914.63s
                               ETA: 919355.5s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.300s, learning 0.175s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0342
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 62.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 8.47s
                        Total time: 3923.10s
                               ETA: 919168.7s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.991s, learning 0.162s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 60.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 8.15s
                        Total time: 3931.25s
                               ETA: 918907.6s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.172s, learning 0.165s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0349
             Mean action noise std: 0.75
                       Mean reward: 1.41
               Mean episode length: 63.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 8.34s
                        Total time: 3939.59s
                               ETA: 918690.4s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.148s, learning 0.171s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0381
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 63.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 8.32s
                        Total time: 3947.91s
                               ETA: 918470.1s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.154s, learning 0.176s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0368
             Mean action noise std: 0.75
                       Mean reward: 1.41
               Mean episode length: 62.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 8.33s
                        Total time: 3956.24s
                               ETA: 918253.3s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.234s, learning 0.166s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0367
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 58.68
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 8.40s
                        Total time: 3964.64s
                               ETA: 918053.6s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.012s, learning 0.162s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0350
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 62.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 8.17s
                        Total time: 3972.81s
                               ETA: 917802.7s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.297s, learning 0.167s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 1.39
               Mean episode length: 63.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 8.46s
                        Total time: 3981.28s
                               ETA: 917619.7s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.459s, learning 0.167s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0380
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 60.27
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 8.63s
                        Total time: 3989.90s
                               ETA: 917474.8s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.224s, learning 0.164s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 1.38
               Mean episode length: 61.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 8.39s
                        Total time: 3998.29s
                               ETA: 917275.9s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.121s, learning 0.169s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0366
             Mean action noise std: 0.75
                       Mean reward: 1.39
               Mean episode length: 63.85
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 8.29s
                        Total time: 4006.58s
                               ETA: 917055.5s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.114s, learning 0.166s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0346
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 64.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 8.28s
                        Total time: 4014.86s
                               ETA: 916833.9s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.326s, learning 0.166s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0383
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 62.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 8.49s
                        Total time: 4023.35s
                               ETA: 916661.6s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.238s, learning 0.171s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0347
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 65.43
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 8.41s
                        Total time: 4031.76s
                               ETA: 916471.1s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.170s, learning 0.225s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 1.30
               Mean episode length: 63.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 8.39s
                        Total time: 4040.16s
                               ETA: 916278.1s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.169s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 1.30
               Mean episode length: 63.18
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 8.41s
                        Total time: 4048.57s
                               ETA: 916090.3s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.068s, learning 0.165s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 61.93
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 8.23s
                        Total time: 4056.80s
                               ETA: 915862.5s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.162s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 1.39
               Mean episode length: 64.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 8.31s
                        Total time: 4065.12s
                               ETA: 915653.9s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.208s, learning 0.167s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 1.34
               Mean episode length: 62.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 8.38s
                        Total time: 4073.49s
                               ETA: 915460.0s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.127s, learning 0.172s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0340
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 60.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 8.30s
                        Total time: 4081.79s
                               ETA: 915249.8s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.105s, learning 0.195s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0370
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 63.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 8.30s
                        Total time: 4090.09s
                               ETA: 915040.6s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.375s, learning 0.214s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0375
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 61.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 8.59s
                        Total time: 4098.68s
                               ETA: 914896.9s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 2029 steps/s (collection: 7.900s, learning 0.173s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0348
             Mean action noise std: 0.75
                       Mean reward: 1.28
               Mean episode length: 61.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 8.07s
                        Total time: 4106.75s
                               ETA: 914639.0s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.343s, learning 0.174s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 1.37
               Mean episode length: 64.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 8.52s
                        Total time: 4115.27s
                               ETA: 914480.9s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.408s, learning 0.179s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 1.29
               Mean episode length: 64.03
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 8.59s
                        Total time: 4123.86s
                               ETA: 914338.8s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.190s, learning 0.170s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0372
             Mean action noise std: 0.75
                       Mean reward: 1.44
               Mean episode length: 65.28
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 8.36s
                        Total time: 4132.22s
                               ETA: 914147.1s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.169s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 65.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 8.22s
                        Total time: 4140.43s
                               ETA: 913924.5s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.139s, learning 0.209s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.39
               Mean episode length: 67.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 8.35s
                        Total time: 4148.78s
                               ETA: 913731.8s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.269s, learning 0.162s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0357
             Mean action noise std: 0.75
                       Mean reward: 1.36
               Mean episode length: 63.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 8.43s
                        Total time: 4157.21s
                               ETA: 913558.3s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.282s, learning 0.162s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0348
             Mean action noise std: 0.75
                       Mean reward: 1.34
               Mean episode length: 63.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 8.44s
                        Total time: 4165.65s
                               ETA: 913388.4s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.163s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0367
             Mean action noise std: 0.75
                       Mean reward: 1.32
               Mean episode length: 64.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 8.48s
                        Total time: 4174.13s
                               ETA: 913226.5s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.148s, learning 0.172s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0348
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 63.75
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 8.32s
                        Total time: 4182.45s
                               ETA: 913030.9s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.234s, learning 0.162s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0368
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 66.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 8.40s
                        Total time: 4190.85s
                               ETA: 912852.7s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.448s, learning 0.163s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0376
             Mean action noise std: 0.75
                       Mean reward: 1.34
               Mean episode length: 65.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 8.61s
                        Total time: 4199.46s
                               ETA: 912721.9s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.012s, learning 0.163s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0362
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 66.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 8.17s
                        Total time: 4207.63s
                               ETA: 912497.0s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.163s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0351
             Mean action noise std: 0.75
                       Mean reward: 1.35
               Mean episode length: 63.98
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 8.16s
                        Total time: 4215.79s
                               ETA: 912269.6s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.192s, learning 0.160s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.30
               Mean episode length: 66.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 8.35s
                        Total time: 4224.14s
                               ETA: 912085.0s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.161s, learning 0.165s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0352
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 64.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 8.33s
                        Total time: 4232.47s
                               ETA: 911895.4s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.436s, learning 0.220s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0362
             Mean action noise std: 0.75
                       Mean reward: 1.30
               Mean episode length: 66.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 8.66s
                        Total time: 4241.12s
                               ETA: 911777.7s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.164s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 66.30
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 8.39s
                        Total time: 4249.51s
                               ETA: 911603.2s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.095s, learning 0.171s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0359
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 70.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 8.27s
                        Total time: 4257.78s
                               ETA: 911403.1s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.164s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0324
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 67.74
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 8.41s
                        Total time: 4266.19s
                               ETA: 911233.7s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.177s, learning 0.174s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0364
             Mean action noise std: 0.75
                       Mean reward: 1.33
               Mean episode length: 68.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 8.35s
                        Total time: 4274.54s
                               ETA: 911053.2s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.032s, learning 0.171s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 65.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 8.20s
                        Total time: 4282.74s
                               ETA: 910841.9s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.286s, learning 0.177s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 1.28
               Mean episode length: 68.59
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 8.46s
                        Total time: 4291.20s
                               ETA: 910686.7s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.122s, learning 0.164s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0365
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 68.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 8.29s
                        Total time: 4299.49s
                               ETA: 910494.8s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.105s, learning 0.162s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0356
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 67.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 8.27s
                        Total time: 4307.76s
                               ETA: 910299.5s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.248s, learning 0.163s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 70.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 8.41s
                        Total time: 4316.17s
                               ETA: 910135.4s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.205s, learning 0.162s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0374
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 69.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 8.37s
                        Total time: 4324.53s
                               ETA: 909962.6s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.076s, learning 0.165s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0353
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 68.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 8.24s
                        Total time: 4332.78s
                               ETA: 909764.0s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.222s, learning 0.186s)
               Value function loss: 0.0063
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 1.27
               Mean episode length: 69.12
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 8.41s
                        Total time: 4341.18s
                               ETA: 909601.2s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.330s, learning 0.169s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0355
             Mean action noise std: 0.75
                       Mean reward: 1.31
               Mean episode length: 69.28
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 8.50s
                        Total time: 4349.68s
                               ETA: 909458.1s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.014s, learning 0.220s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 1.23
               Mean episode length: 67.85
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 8.23s
                        Total time: 4357.92s
                               ETA: 909260.3s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.139s, learning 0.165s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0386
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 69.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 8.30s
                        Total time: 4366.22s
                               ETA: 909078.1s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.100s, learning 0.170s)
               Value function loss: 0.0074
                    Surrogate loss: -0.0360
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 67.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 8.27s
                        Total time: 4374.49s
                               ETA: 908889.3s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.214s, learning 0.163s)
               Value function loss: 0.0067
                    Surrogate loss: -0.0363
             Mean action noise std: 0.75
                       Mean reward: 1.23
               Mean episode length: 69.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 8.38s
                        Total time: 4382.87s
                               ETA: 908723.6s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.157s, learning 0.177s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0379
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 71.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 8.33s
                        Total time: 4391.20s
                               ETA: 908549.6s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.037s, learning 0.165s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0369
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 68.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 8.20s
                        Total time: 4399.40s
                               ETA: 908349.0s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.982s, learning 0.164s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0386
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 69.33
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 8.15s
                        Total time: 4407.55s
                               ETA: 908137.7s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 1957 steps/s (collection: 8.206s, learning 0.163s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0368
             Mean action noise std: 0.75
                       Mean reward: 1.15
               Mean episode length: 66.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 8.37s
                        Total time: 4415.92s
                               ETA: 907972.9s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.159s, learning 0.163s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0375
             Mean action noise std: 0.75
                       Mean reward: 1.21
               Mean episode length: 67.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 8.32s
                        Total time: 4424.24s
                               ETA: 907799.3s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.236s, learning 0.172s)
               Value function loss: 0.0077
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 1.23
               Mean episode length: 70.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 8.41s
                        Total time: 4432.65s
                               ETA: 907643.9s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.217s, learning 0.215s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 1.16
               Mean episode length: 70.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 8.43s
                        Total time: 4441.08s
                               ETA: 907494.2s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 1902 steps/s (collection: 8.381s, learning 0.229s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0377
             Mean action noise std: 0.75
                       Mean reward: 1.22
               Mean episode length: 69.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 8.61s
                        Total time: 4449.69s
                               ETA: 907381.2s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.039s, learning 0.216s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0391
             Mean action noise std: 0.75
                       Mean reward: 1.19
               Mean episode length: 70.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 8.25s
                        Total time: 4457.94s
                               ETA: 907196.2s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.243s, learning 0.210s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0387
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 69.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 8.45s
                        Total time: 4466.40s
                               ETA: 907052.3s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.168s, learning 0.180s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0385
             Mean action noise std: 0.75
                       Mean reward: 1.21
               Mean episode length: 71.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 8.35s
                        Total time: 4474.74s
                               ETA: 906887.7s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 2022 steps/s (collection: 7.928s, learning 0.174s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0418
             Mean action noise std: 0.75
                       Mean reward: 1.25
               Mean episode length: 70.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 8.10s
                        Total time: 4482.85s
                               ETA: 906673.8s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.250s, learning 0.164s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0392
             Mean action noise std: 0.75
                       Mean reward: 1.23
               Mean episode length: 67.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 8.41s
                        Total time: 4491.26s
                               ETA: 906524.0s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.220s, learning 0.159s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0408
             Mean action noise std: 0.75
                       Mean reward: 1.26
               Mean episode length: 68.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 8.38s
                        Total time: 4499.64s
                               ETA: 906367.7s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.285s, learning 0.219s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0424
             Mean action noise std: 0.75
                       Mean reward: 1.19
               Mean episode length: 71.35
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 8.50s
                        Total time: 4508.14s
                               ETA: 906237.0s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.222s, learning 0.211s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0400
             Mean action noise std: 0.75
                       Mean reward: 1.21
               Mean episode length: 70.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 8.43s
                        Total time: 4516.58s
                               ETA: 906092.5s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.163s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0391
             Mean action noise std: 0.75
                       Mean reward: 1.19
               Mean episode length: 68.18
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 8.33s
                        Total time: 4524.90s
                               ETA: 905927.3s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.052s, learning 0.176s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0354
             Mean action noise std: 0.75
                       Mean reward: 1.24
               Mean episode length: 70.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 8.23s
                        Total time: 4533.13s
                               ETA: 905743.0s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.145s, learning 0.175s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0445
             Mean action noise std: 0.74
                       Mean reward: 1.23
               Mean episode length: 69.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 8.32s
                        Total time: 4541.45s
                               ETA: 905577.8s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.019s, learning 0.167s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 1.24
               Mean episode length: 68.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 8.19s
                        Total time: 4549.64s
                               ETA: 905386.6s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.074s, learning 0.208s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0450
             Mean action noise std: 0.74
                       Mean reward: 1.24
               Mean episode length: 68.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 8.28s
                        Total time: 4557.92s
                               ETA: 905215.1s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 2032 steps/s (collection: 7.896s, learning 0.167s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0422
             Mean action noise std: 0.74
                       Mean reward: 1.22
               Mean episode length: 68.85
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 8.06s
                        Total time: 4565.98s
                               ETA: 905000.9s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.047s, learning 0.171s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 1.30
               Mean episode length: 69.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 8.22s
                        Total time: 4574.20s
                               ETA: 904818.2s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 1013 steps/s (collection: 16.006s, learning 0.167s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0440
             Mean action noise std: 0.74
                       Mean reward: 1.18
               Mean episode length: 69.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 16.17s
                        Total time: 4590.37s
                               ETA: 906206.6s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 1016 steps/s (collection: 15.944s, learning 0.170s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0418
             Mean action noise std: 0.74
                       Mean reward: 1.29
               Mean episode length: 70.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 16.11s
                        Total time: 4606.48s
                               ETA: 907577.6s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.873s, learning 0.223s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0399
             Mean action noise std: 0.74
                       Mean reward: 1.25
               Mean episode length: 70.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 16.10s
                        Total time: 4622.58s
                               ETA: 908939.8s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.198s, learning 0.164s)
               Value function loss: 0.0059
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 1.23
               Mean episode length: 70.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 16.36s
                        Total time: 4638.94s
                               ETA: 910348.9s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.122s, learning 0.161s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0428
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 70.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 16.28s
                        Total time: 4655.22s
                               ETA: 911736.7s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1012 steps/s (collection: 15.990s, learning 0.191s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0438
             Mean action noise std: 0.74
                       Mean reward: 1.32
               Mean episode length: 69.55
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 16.18s
                        Total time: 4671.41s
                               ETA: 913099.1s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.354s, learning 0.164s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0440
             Mean action noise std: 0.74
                       Mean reward: 1.29
               Mean episode length: 69.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 16.52s
                        Total time: 4687.92s
                               ETA: 914521.9s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.200s, learning 0.170s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0435
             Mean action noise std: 0.74
                       Mean reward: 1.23
               Mean episode length: 67.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 16.37s
                        Total time: 4704.29s
                               ETA: 915910.1s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.343s, learning 0.170s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 1.26
               Mean episode length: 70.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 16.51s
                        Total time: 4720.81s
                               ETA: 917320.7s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.187s, learning 0.172s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0410
             Mean action noise std: 0.74
                       Mean reward: 1.29
               Mean episode length: 69.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 16.36s
                        Total time: 4737.16s
                               ETA: 918695.9s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1037 steps/s (collection: 15.621s, learning 0.163s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 1.32
               Mean episode length: 69.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 15.78s
                        Total time: 4752.95s
                               ETA: 919954.5s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.003s, learning 0.176s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0423
             Mean action noise std: 0.74
                       Mean reward: 1.31
               Mean episode length: 68.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 16.18s
                        Total time: 4769.13s
                               ETA: 921284.3s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.883s, learning 0.189s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0429
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 69.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 16.07s
                        Total time: 4785.20s
                               ETA: 922588.4s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.033s, learning 0.176s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0447
             Mean action noise std: 0.74
                       Mean reward: 1.36
               Mean episode length: 69.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 16.21s
                        Total time: 4801.41s
                               ETA: 923913.9s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.272s, learning 0.224s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0429
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 66.48
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 16.50s
                        Total time: 4817.91s
                               ETA: 925289.1s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.286s, learning 0.164s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0433
             Mean action noise std: 0.74
                       Mean reward: 1.30
               Mean episode length: 68.74
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 16.45s
                        Total time: 4834.36s
                               ETA: 926650.1s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.992s, learning 0.164s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0436
             Mean action noise std: 0.74
                       Mean reward: 1.31
               Mean episode length: 68.59
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 16.16s
                        Total time: 4850.51s
                               ETA: 927949.5s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1031 steps/s (collection: 15.713s, learning 0.171s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0425
             Mean action noise std: 0.74
                       Mean reward: 1.35
               Mean episode length: 68.15
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 15.88s
                        Total time: 4866.40s
                               ETA: 929192.0s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.088s, learning 0.166s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0440
             Mean action noise std: 0.74
                       Mean reward: 1.27
               Mean episode length: 65.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 16.25s
                        Total time: 4882.65s
                               ETA: 930500.0s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.022s, learning 0.188s)
               Value function loss: 0.0071
                    Surrogate loss: -0.0427
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 68.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 16.21s
                        Total time: 4898.86s
                               ETA: 931794.7s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 983 steps/s (collection: 16.483s, learning 0.172s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0400
             Mean action noise std: 0.74
                       Mean reward: 1.33
               Mean episode length: 67.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 16.66s
                        Total time: 4915.51s
                               ETA: 933168.9s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.202s, learning 0.173s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 1.31
               Mean episode length: 66.36
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 16.38s
                        Total time: 4931.89s
                               ETA: 934484.9s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.287s, learning 0.172s)
               Value function loss: 0.0066
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 1.33
               Mean episode length: 67.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 16.46s
                        Total time: 4948.35s
                               ETA: 935811.5s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.118s, learning 0.177s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 68.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 16.30s
                        Total time: 4964.64s
                               ETA: 937102.2s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.131s, learning 0.163s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0435
             Mean action noise std: 0.74
                       Mean reward: 1.33
               Mean episode length: 67.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 16.29s
                        Total time: 4980.94s
                               ETA: 938387.6s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1030 steps/s (collection: 15.730s, learning 0.167s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 67.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 15.90s
                        Total time: 4996.83s
                               ETA: 939593.6s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.080s, learning 0.168s)
               Value function loss: 0.0064
                    Surrogate loss: -0.0409
             Mean action noise std: 0.74
                       Mean reward: 1.35
               Mean episode length: 66.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 16.25s
                        Total time: 5013.08s
                               ETA: 940860.9s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.044s, learning 0.164s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 1.36
               Mean episode length: 66.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 16.21s
                        Total time: 5029.29s
                               ETA: 942115.9s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.506s, learning 0.168s)
               Value function loss: 0.0070
                    Surrogate loss: -0.0398
             Mean action noise std: 0.74
                       Mean reward: 1.36
               Mean episode length: 68.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 16.67s
                        Total time: 5045.96s
                               ETA: 943453.1s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.367s, learning 0.176s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 68.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 16.54s
                        Total time: 5062.51s
                               ETA: 944760.8s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 986 steps/s (collection: 16.375s, learning 0.234s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0391
             Mean action noise std: 0.74
                       Mean reward: 1.35
               Mean episode length: 68.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 16.61s
                        Total time: 5079.12s
                               ETA: 946075.9s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.030s, learning 0.174s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 69.05
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 16.20s
                        Total time: 5095.32s
                               ETA: 947310.7s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.193s, learning 0.198s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 64.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 16.39s
                        Total time: 5111.71s
                               ETA: 948575.4s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.149s, learning 0.164s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0416
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 64.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 16.31s
                        Total time: 5128.03s
                               ETA: 949821.0s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1020 steps/s (collection: 15.884s, learning 0.165s)
               Value function loss: 0.0062
                    Surrogate loss: -0.0419
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 66.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 16.05s
                        Total time: 5144.07s
                               ETA: 951013.1s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.278s, learning 0.163s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0394
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 67.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 16.44s
                        Total time: 5160.51s
                               ETA: 952272.9s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 987 steps/s (collection: 16.423s, learning 0.175s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0404
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 66.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 16.60s
                        Total time: 5177.11s
                               ETA: 953557.0s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1196 steps/s (collection: 13.521s, learning 0.167s)
               Value function loss: 0.0061
                    Surrogate loss: -0.0402
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 66.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 13.69s
                        Total time: 5190.80s
                               ETA: 954301.3s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.238s, learning 0.163s)
               Value function loss: 0.0065
                    Surrogate loss: -0.0386
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 66.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 8.40s
                        Total time: 5199.20s
                               ETA: 954072.6s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.178s, learning 0.174s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0429
             Mean action noise std: 0.74
                       Mean reward: 1.36
               Mean episode length: 66.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 8.35s
                        Total time: 5207.55s
                               ETA: 953835.6s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.298s, learning 0.166s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0403
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 66.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 8.46s
                        Total time: 5216.02s
                               ETA: 953620.0s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.249s, learning 0.174s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 64.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 8.42s
                        Total time: 5224.44s
                               ETA: 953397.7s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.164s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 65.72
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 8.52s
                        Total time: 5232.96s
                               ETA: 953194.1s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.271s, learning 0.178s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0427
             Mean action noise std: 0.74
                       Mean reward: 1.46
               Mean episode length: 65.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 8.45s
                        Total time: 5241.41s
                               ETA: 952978.2s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.376s, learning 0.175s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 63.90
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 8.55s
                        Total time: 5249.96s
                               ETA: 952781.4s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.486s, learning 0.163s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 64.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 8.65s
                        Total time: 5258.61s
                               ETA: 952603.2s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.428s, learning 0.215s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0429
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 62.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 8.64s
                        Total time: 5267.25s
                               ETA: 952424.3s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.332s, learning 0.160s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 65.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 8.49s
                        Total time: 5275.74s
                               ETA: 952218.9s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.139s, learning 0.168s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 63.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 8.31s
                        Total time: 5284.05s
                               ETA: 951980.9s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.148s, learning 0.175s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 62.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 8.32s
                        Total time: 5292.37s
                               ETA: 951746.8s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.216s, learning 0.175s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0421
             Mean action noise std: 0.74
                       Mean reward: 1.46
               Mean episode length: 64.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 8.39s
                        Total time: 5300.76s
                               ETA: 951525.6s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.302s, learning 0.172s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0403
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 63.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 8.47s
                        Total time: 5309.24s
                               ETA: 951319.9s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.162s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0428
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 61.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 8.40s
                        Total time: 5317.63s
                               ETA: 951100.9s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.159s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0434
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 60.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 8.44s
                        Total time: 5326.07s
                               ETA: 950890.2s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.112s, learning 0.174s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0415
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 63.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 8.29s
                        Total time: 5334.36s
                               ETA: 950653.3s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.190s, learning 0.222s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0426
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 62.32
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 8.41s
                        Total time: 5342.77s
                               ETA: 950439.5s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.146s, learning 0.169s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0425
             Mean action noise std: 0.74
                       Mean reward: 1.44
               Mean episode length: 62.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 8.31s
                        Total time: 5351.08s
                               ETA: 950209.2s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.167s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 61.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 8.44s
                        Total time: 5359.52s
                               ETA: 950001.2s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.224s, learning 0.163s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0408
             Mean action noise std: 0.74
                       Mean reward: 1.42
               Mean episode length: 61.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 8.39s
                        Total time: 5367.91s
                               ETA: 949785.3s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.166s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0423
             Mean action noise std: 0.74
                       Mean reward: 1.44
               Mean episode length: 61.30
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 8.53s
                        Total time: 5376.44s
                               ETA: 949595.5s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.338s, learning 0.172s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0412
             Mean action noise std: 0.74
                       Mean reward: 1.45
               Mean episode length: 64.18
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 8.51s
                        Total time: 5384.95s
                               ETA: 949402.6s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.434s, learning 0.163s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0417
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 62.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 8.60s
                        Total time: 5393.55s
                               ETA: 949225.7s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.225s, learning 0.158s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0413
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 60.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 8.38s
                        Total time: 5401.93s
                               ETA: 949011.9s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.270s, learning 0.279s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0398
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 63.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 8.55s
                        Total time: 5410.48s
                               ETA: 948827.9s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.078s, learning 0.222s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0372
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 61.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 8.30s
                        Total time: 5418.78s
                               ETA: 948600.8s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 2010 steps/s (collection: 7.985s, learning 0.164s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0390
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 59.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 8.15s
                        Total time: 5426.93s
                               ETA: 948348.2s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.325s, learning 0.175s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 1.42
               Mean episode length: 60.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 8.50s
                        Total time: 5435.43s
                               ETA: 948157.6s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.166s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0405
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 59.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 8.37s
                        Total time: 5443.79s
                               ETA: 947944.5s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.353s, learning 0.165s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0377
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 61.05
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 8.52s
                        Total time: 5452.31s
                               ETA: 947758.4s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.207s, learning 0.165s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0396
             Mean action noise std: 0.74
                       Mean reward: 1.44
               Mean episode length: 62.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 8.37s
                        Total time: 5460.68s
                               ETA: 947547.6s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.234s, learning 0.170s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 60.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 8.40s
                        Total time: 5469.09s
                               ETA: 947343.0s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.259s, learning 0.173s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 59.82
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 8.43s
                        Total time: 5477.52s
                               ETA: 947144.0s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.357s, learning 0.214s)
               Value function loss: 0.0052
                    Surrogate loss: -0.0389
             Mean action noise std: 0.74
                       Mean reward: 1.48
               Mean episode length: 63.36
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 8.57s
                        Total time: 5486.09s
                               ETA: 946969.7s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.166s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 60.16
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 8.33s
                        Total time: 5494.42s
                               ETA: 946753.5s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.269s, learning 0.168s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0427
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 58.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 8.44s
                        Total time: 5502.85s
                               ETA: 946557.3s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.358s, learning 0.168s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0402
             Mean action noise std: 0.74
                       Mean reward: 1.42
               Mean episode length: 60.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 8.53s
                        Total time: 5511.38s
                               ETA: 946377.1s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.257s, learning 0.164s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 60.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 8.42s
                        Total time: 5519.80s
                               ETA: 946179.5s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.977s, learning 0.177s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 62.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 8.15s
                        Total time: 5527.96s
                               ETA: 945936.9s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 2026 steps/s (collection: 7.881s, learning 0.202s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0423
             Mean action noise std: 0.74
                       Mean reward: 1.40
               Mean episode length: 60.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 8.08s
                        Total time: 5536.04s
                               ETA: 945682.9s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.204s, learning 0.176s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0396
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 61.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 8.38s
                        Total time: 5544.42s
                               ETA: 945480.4s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.041s, learning 0.164s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0406
             Mean action noise std: 0.74
                       Mean reward: 1.36
               Mean episode length: 59.25
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 8.21s
                        Total time: 5552.62s
                               ETA: 945248.7s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1862 steps/s (collection: 8.617s, learning 0.179s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0411
             Mean action noise std: 0.74
                       Mean reward: 1.45
               Mean episode length: 61.68
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 8.80s
                        Total time: 5561.42s
                               ETA: 945118.3s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.095s, learning 0.163s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0388
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 58.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 8.26s
                        Total time: 5569.68s
                               ETA: 944897.0s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.218s, learning 0.176s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0387
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 58.71
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 8.39s
                        Total time: 5578.07s
                               ETA: 944699.4s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.112s, learning 0.170s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0399
             Mean action noise std: 0.74
                       Mean reward: 1.49
               Mean episode length: 59.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 8.28s
                        Total time: 5586.35s
                               ETA: 944483.5s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.215s, learning 0.201s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 60.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 8.42s
                        Total time: 5594.77s
                               ETA: 944291.0s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.225s, learning 0.172s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 1.44
               Mean episode length: 60.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 8.40s
                        Total time: 5603.17s
                               ETA: 944095.8s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.169s, learning 0.172s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0367
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 59.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 8.34s
                        Total time: 5611.51s
                               ETA: 943891.9s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 2049 steps/s (collection: 7.835s, learning 0.160s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0417
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 58.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 8.00s
                        Total time: 5619.50s
                               ETA: 943630.5s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.242s, learning 0.166s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0394
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 58.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 8.41s
                        Total time: 5627.91s
                               ETA: 943439.4s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.295s, learning 0.207s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0380
             Mean action noise std: 0.74
                       Mean reward: 1.32
               Mean episode length: 55.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 8.50s
                        Total time: 5636.41s
                               ETA: 943264.4s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.192s, learning 0.215s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0398
             Mean action noise std: 0.74
                       Mean reward: 1.42
               Mean episode length: 59.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 8.41s
                        Total time: 5644.82s
                               ETA: 943074.2s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.320s, learning 0.175s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 57.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 8.49s
                        Total time: 5653.32s
                               ETA: 942899.2s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.309s, learning 0.240s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 1.43
               Mean episode length: 60.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 8.55s
                        Total time: 5661.86s
                               ETA: 942733.7s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.286s, learning 0.209s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0375
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 55.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 8.49s
                        Total time: 5670.36s
                               ETA: 942559.8s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.157s, learning 0.163s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0393
             Mean action noise std: 0.74
                       Mean reward: 1.35
               Mean episode length: 58.12
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 8.32s
                        Total time: 5678.68s
                               ETA: 942357.4s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.989s, learning 0.165s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0397
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 57.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 8.15s
                        Total time: 5686.83s
                               ETA: 942128.2s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.271s, learning 0.175s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0417
             Mean action noise std: 0.74
                       Mean reward: 1.35
               Mean episode length: 56.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 8.45s
                        Total time: 5695.28s
                               ETA: 941948.1s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 2021 steps/s (collection: 7.946s, learning 0.159s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0403
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 57.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 8.10s
                        Total time: 5703.38s
                               ETA: 941712.2s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 2016 steps/s (collection: 7.916s, learning 0.211s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 1.37
               Mean episode length: 56.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 8.13s
                        Total time: 5711.51s
                               ETA: 941480.6s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.191s, learning 0.203s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0427
             Mean action noise std: 0.74
                       Mean reward: 1.39
               Mean episode length: 57.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 8.39s
                        Total time: 5719.91s
                               ETA: 941293.8s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.083s, learning 0.159s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0424
             Mean action noise std: 0.74
                       Mean reward: 1.44
               Mean episode length: 58.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 8.24s
                        Total time: 5728.15s
                               ETA: 941082.6s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.355s, learning 0.165s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0401
             Mean action noise std: 0.74
                       Mean reward: 1.38
               Mean episode length: 59.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 8.52s
                        Total time: 5736.67s
                               ETA: 940917.6s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.040s, learning 0.161s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 1.41
               Mean episode length: 59.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 8.20s
                        Total time: 5744.87s
                               ETA: 940701.0s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.098s, learning 0.169s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0407
             Mean action noise std: 0.74
                       Mean reward: 1.34
               Mean episode length: 59.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 8.27s
                        Total time: 5753.14s
                               ETA: 940495.9s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.048s, learning 0.170s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 60.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 8.22s
                        Total time: 5761.35s
                               ETA: 940283.3s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.332s, learning 0.169s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 56.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 8.50s
                        Total time: 5769.86s
                               ETA: 940117.6s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.134s, learning 0.168s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 56.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 8.30s
                        Total time: 5778.16s
                               ETA: 939919.9s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.116s, learning 0.162s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0426
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 57.02
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 8.28s
                        Total time: 5786.44s
                               ETA: 939719.0s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.170s, learning 0.169s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0437
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 56.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 8.34s
                        Total time: 5794.77s
                               ETA: 939528.5s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.178s, learning 0.172s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 59.28
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 8.35s
                        Total time: 5803.12s
                               ETA: 939340.4s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1916 steps/s (collection: 8.343s, learning 0.206s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 55.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 8.55s
                        Total time: 5811.67s
                               ETA: 939185.2s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.159s, learning 0.162s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0379
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 55.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 8.32s
                        Total time: 5819.99s
                               ETA: 938993.5s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.369s, learning 0.205s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0382
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 58.27
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 8.57s
                        Total time: 5828.57s
                               ETA: 938843.3s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.078s, learning 0.170s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 57.60
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 8.25s
                        Total time: 5836.81s
                               ETA: 938641.1s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.112s, learning 0.168s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0426
             Mean action noise std: 0.73
                       Mean reward: 1.42
               Mean episode length: 59.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 8.28s
                        Total time: 5845.09s
                               ETA: 938444.6s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 2041 steps/s (collection: 7.863s, learning 0.164s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0422
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 57.97
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 8.03s
                        Total time: 5853.12s
                               ETA: 938208.2s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.147s, learning 0.167s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 56.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 8.31s
                        Total time: 5861.44s
                               ETA: 938018.4s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.068s, learning 0.210s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 52.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 8.28s
                        Total time: 5869.71s
                               ETA: 937823.4s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.041s, learning 0.163s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0421
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 53.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 8.20s
                        Total time: 5877.92s
                               ETA: 937617.5s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.105s, learning 0.168s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 53.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 8.27s
                        Total time: 5886.19s
                               ETA: 937423.0s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.264s, learning 0.168s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 55.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.25
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 8.43s
                        Total time: 5894.62s
                               ETA: 937254.4s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.206s, learning 0.168s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 52.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 8.37s
                        Total time: 5903.00s
                               ETA: 937077.1s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.327s, learning 0.168s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 56.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 8.50s
                        Total time: 5911.49s
                               ETA: 936919.5s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.165s, learning 0.166s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 54.26
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 8.33s
                        Total time: 5919.82s
                               ETA: 936736.4s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.168s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 56.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 8.42s
                        Total time: 5928.24s
                               ETA: 936567.9s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.166s, learning 0.171s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 56.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 8.34s
                        Total time: 5936.58s
                               ETA: 936386.8s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.229s, learning 0.176s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0402
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 53.33
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 8.40s
                        Total time: 5944.98s
                               ETA: 936217.1s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.174s, learning 0.183s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0361
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 54.83
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 8.36s
                        Total time: 5953.34s
                               ETA: 936040.3s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.278s, learning 0.188s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0394
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 56.45
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 8.47s
                        Total time: 5961.81s
                               ETA: 935881.1s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.166s, learning 0.173s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0413
             Mean action noise std: 0.73
                       Mean reward: 1.28
               Mean episode length: 51.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 8.34s
                        Total time: 5970.14s
                               ETA: 935702.5s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.082s, learning 0.159s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0413
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 54.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 8.24s
                        Total time: 5978.39s
                               ETA: 935509.2s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.144s, learning 0.163s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 54.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 8.31s
                        Total time: 5986.69s
                               ETA: 935326.7s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 2018 steps/s (collection: 7.948s, learning 0.169s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 54.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 8.12s
                        Total time: 5994.81s
                               ETA: 935115.3s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.171s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0412
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 56.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 8.51s
                        Total time: 6003.32s
                               ETA: 934965.6s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.166s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0403
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 53.27
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 8.30s
                        Total time: 6011.62s
                               ETA: 934783.8s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.412s, learning 0.212s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0416
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 54.79
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 8.62s
                        Total time: 6020.25s
                               ETA: 934652.8s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.163s, learning 0.165s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 54.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 8.33s
                        Total time: 6028.58s
                               ETA: 934476.3s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.346s, learning 0.218s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 52.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 8.56s
                        Total time: 6037.14s
                               ETA: 934336.7s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.043s, learning 0.167s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 53.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 8.21s
                        Total time: 6045.35s
                               ETA: 934142.9s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1988 steps/s (collection: 8.031s, learning 0.210s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 53.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 8.24s
                        Total time: 6053.59s
                               ETA: 933954.3s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1882 steps/s (collection: 8.541s, learning 0.161s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0375
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 55.28
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 8.70s
                        Total time: 6062.29s
                               ETA: 933837.5s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.190s, learning 0.166s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0408
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 55.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 8.36s
                        Total time: 6070.65s
                               ETA: 933667.7s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 2009 steps/s (collection: 7.935s, learning 0.216s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0407
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 56.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 8.15s
                        Total time: 6078.80s
                               ETA: 933466.9s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.195s, learning 0.162s)
               Value function loss: 0.0046
                    Surrogate loss: -0.0419
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 54.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 8.36s
                        Total time: 6087.16s
                               ETA: 933298.3s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.223s, learning 0.156s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0425
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 53.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 8.38s
                        Total time: 6095.54s
                               ETA: 933133.6s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.256s, learning 0.174s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 54.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 8.43s
                        Total time: 6103.97s
                               ETA: 932977.1s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.253s, learning 0.163s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0393
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 55.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 8.42s
                        Total time: 6112.38s
                               ETA: 932818.9s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.165s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0389
             Mean action noise std: 0.73
                       Mean reward: 1.29
               Mean episode length: 54.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 8.35s
                        Total time: 6120.73s
                               ETA: 932651.2s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 2002 steps/s (collection: 8.011s, learning 0.171s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0424
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 53.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 8.18s
                        Total time: 6128.91s
                               ETA: 932458.3s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.055s, learning 0.171s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 52.28
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 8.23s
                        Total time: 6137.14s
                               ETA: 932272.7s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.144s, learning 0.167s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 53.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 8.31s
                        Total time: 6145.45s
                               ETA: 932100.5s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.106s, learning 0.213s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0398
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 53.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 8.32s
                        Total time: 6153.77s
                               ETA: 931930.1s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.223s, learning 0.195s)
               Value function loss: 0.0033
                    Surrogate loss: -0.0385
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 54.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 8.42s
                        Total time: 6162.19s
                               ETA: 931775.1s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.076s, learning 0.173s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0384
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 52.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 8.25s
                        Total time: 6170.44s
                               ETA: 931595.1s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.193s, learning 0.173s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 52.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 8.37s
                        Total time: 6178.80s
                               ETA: 931433.1s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.203s, learning 0.174s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 53.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 8.38s
                        Total time: 6187.18s
                               ETA: 931273.2s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 2061 steps/s (collection: 7.776s, learning 0.170s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0361
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 51.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 7.95s
                        Total time: 6195.12s
                               ETA: 931049.2s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.190s, learning 0.211s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0355
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 55.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 8.40s
                        Total time: 6203.52s
                               ETA: 930894.2s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1927 steps/s (collection: 8.333s, learning 0.165s)
               Value function loss: 0.0031
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 51.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 8.50s
                        Total time: 6212.02s
                               ETA: 930754.0s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.475s, learning 0.170s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 53.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 8.65s
                        Total time: 6220.67s
                               ETA: 930636.3s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.101s, learning 0.170s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0396
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 51.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 8.27s
                        Total time: 6228.94s
                               ETA: 930463.0s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.160s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 53.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 8.34s
                        Total time: 6237.27s
                               ETA: 930299.8s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.253s, learning 0.176s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 52.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 8.43s
                        Total time: 6245.70s
                               ETA: 930150.9s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.163s, learning 0.158s)
               Value function loss: 0.0035
                    Surrogate loss: -0.0376
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 54.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 8.32s
                        Total time: 6254.02s
                               ETA: 929986.5s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.174s)
               Value function loss: 0.0036
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 50.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 8.46s
                        Total time: 6262.48s
                               ETA: 929842.4s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.262s, learning 0.167s)
               Value function loss: 0.0037
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 52.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 8.43s
                        Total time: 6270.91s
                               ETA: 929694.8s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.463s, learning 0.167s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0354
             Mean action noise std: 0.73
                       Mean reward: 1.38
               Mean episode length: 52.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 8.63s
                        Total time: 6279.54s
                               ETA: 929577.4s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.264s, learning 0.170s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0401
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 51.68
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 8.43s
                        Total time: 6287.97s
                               ETA: 929431.3s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.285s, learning 0.209s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 52.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 8.49s
                        Total time: 6296.47s
                               ETA: 929294.7s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.160s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 53.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 8.36s
                        Total time: 6304.83s
                               ETA: 929138.5s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.335s, learning 0.158s)
               Value function loss: 0.0045
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 53.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 8.49s
                        Total time: 6313.32s
                               ETA: 929002.5s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 2007 steps/s (collection: 7.995s, learning 0.166s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0411
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 51.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 8.16s
                        Total time: 6321.48s
                               ETA: 928817.9s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.415s, learning 0.169s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 54.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 8.58s
                        Total time: 6330.06s
                               ETA: 928695.9s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.173s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 50.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 8.42s
                        Total time: 6338.49s
                               ETA: 928550.7s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.022s, learning 0.164s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0399
             Mean action noise std: 0.73
                       Mean reward: 1.30
               Mean episode length: 50.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 8.19s
                        Total time: 6346.67s
                               ETA: 928371.4s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.234s, learning 0.175s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0411
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 51.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 8.41s
                        Total time: 6355.08s
                               ETA: 928225.1s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.234s, learning 0.160s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0405
             Mean action noise std: 0.73
                       Mean reward: 1.32
               Mean episode length: 50.28
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 8.39s
                        Total time: 6363.47s
                               ETA: 928076.8s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.237s, learning 0.169s)
               Value function loss: 0.0038
                    Surrogate loss: -0.0374
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 50.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 8.41s
                        Total time: 6371.88s
                               ETA: 927930.9s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1986 steps/s (collection: 8.072s, learning 0.174s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0380
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 49.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 8.25s
                        Total time: 6380.13s
                               ETA: 927762.1s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.317s, learning 0.165s)
               Value function loss: 0.0034
                    Surrogate loss: -0.0428
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 50.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 8.48s
                        Total time: 6388.61s
                               ETA: 927627.9s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.193s, learning 0.164s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.31
               Mean episode length: 50.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 8.36s
                        Total time: 6396.97s
                               ETA: 927476.1s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.139s, learning 0.172s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0404
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 49.90
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 8.31s
                        Total time: 6405.28s
                               ETA: 927318.0s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.168s, learning 0.169s)
               Value function loss: 0.0041
                    Surrogate loss: -0.0409
             Mean action noise std: 0.73
                       Mean reward: 1.33
               Mean episode length: 50.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 8.34s
                        Total time: 6413.61s
                               ETA: 927164.1s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.408s, learning 0.166s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0410
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 50.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 8.57s
                        Total time: 6422.19s
                               ETA: 927044.8s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.164s)
               Value function loss: 0.0040
                    Surrogate loss: -0.0400
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 51.00
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 8.51s
                        Total time: 6430.70s
                               ETA: 926916.7s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.076s, learning 0.188s)
               Value function loss: 0.0043
                    Surrogate loss: -0.0397
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 51.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 8.26s
                        Total time: 6438.96s
                               ETA: 926753.6s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.092s, learning 0.272s)
               Value function loss: 0.0039
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 49.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 8.36s
                        Total time: 6447.33s
                               ETA: 926605.1s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.147s, learning 0.171s)
               Value function loss: 0.0042
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 49.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 8.32s
                        Total time: 6455.65s
                               ETA: 926450.5s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 2008 steps/s (collection: 7.995s, learning 0.164s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0388
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 49.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 8.16s
                        Total time: 6463.81s
                               ETA: 926273.6s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.249s, learning 0.210s)
               Value function loss: 0.0050
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 49.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 8.46s
                        Total time: 6472.26s
                               ETA: 926140.0s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.361s, learning 0.179s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 48.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 8.54s
                        Total time: 6480.80s
                               ETA: 926018.3s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.187s, learning 0.159s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0361
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 49.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 8.35s
                        Total time: 6489.15s
                               ETA: 925869.3s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.198s, learning 0.176s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0386
             Mean action noise std: 0.73
                       Mean reward: 1.37
               Mean episode length: 48.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 8.37s
                        Total time: 6497.52s
                               ETA: 925724.6s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.356s, learning 0.286s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0367
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 48.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 8.64s
                        Total time: 6506.17s
                               ETA: 925618.6s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.224s, learning 0.175s)
               Value function loss: 0.0044
                    Surrogate loss: -0.0368
             Mean action noise std: 0.73
                       Mean reward: 1.34
               Mean episode length: 46.81
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 8.40s
                        Total time: 6514.56s
                               ETA: 925478.3s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.304s, learning 0.158s)
               Value function loss: 0.0048
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.35
               Mean episode length: 47.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 8.46s
                        Total time: 6523.03s
                               ETA: 925347.3s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.183s, learning 0.164s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0420
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 48.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 8.35s
                        Total time: 6531.37s
                               ETA: 925200.4s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.268s, learning 0.166s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 1.41
               Mean episode length: 48.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 8.43s
                        Total time: 6539.81s
                               ETA: 925066.1s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.253s, learning 0.172s)
               Value function loss: 0.0054
                    Surrogate loss: -0.0364
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 46.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 8.42s
                        Total time: 6548.23s
                               ETA: 924930.9s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.136s, learning 0.175s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0369
             Mean action noise std: 0.73
                       Mean reward: 1.40
               Mean episode length: 50.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 8.31s
                        Total time: 6556.54s
                               ETA: 924780.0s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.163s)
               Value function loss: 0.0060
                    Surrogate loss: -0.0406
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 49.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 8.33s
                        Total time: 6564.87s
                               ETA: 924631.5s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.039s, learning 0.167s)
               Value function loss: 0.0053
                    Surrogate loss: -0.0392
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 48.40
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 8.21s
                        Total time: 6573.07s
                               ETA: 924466.6s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1914 steps/s (collection: 8.397s, learning 0.161s)
               Value function loss: 0.0049
                    Surrogate loss: -0.0395
             Mean action noise std: 0.73
                       Mean reward: 1.36
               Mean episode length: 46.80
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 8.56s
                        Total time: 6581.63s
                               ETA: 924351.7s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1947 steps/s (collection: 8.242s, learning 0.170s)
               Value function loss: 0.0047
                    Surrogate loss: -0.0415
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 46.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 8.41s
                        Total time: 6590.05s
                               ETA: 924216.6s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.322s, learning 0.157s)
               Value function loss: 0.0051
                    Surrogate loss: -0.0377
             Mean action noise std: 0.73
                       Mean reward: 1.44
               Mean episode length: 47.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 8.48s
                        Total time: 6598.52s
                               ETA: 924091.2s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.937s, learning 0.162s)
               Value function loss: 0.0058
                    Surrogate loss: -0.0383
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 47.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 8.10s
                        Total time: 6606.62s
                               ETA: 923913.0s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.199s, learning 0.207s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0335
             Mean action noise std: 0.73
                       Mean reward: 1.39
               Mean episode length: 46.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 8.41s
                        Total time: 6615.03s
                               ETA: 923778.0s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.411s, learning 0.168s)
               Value function loss: 0.0057
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.46
               Mean episode length: 48.54
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 16.58s
                        Total time: 6631.61s
                               ETA: 924783.2s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.209s, learning 0.168s)
               Value function loss: 0.0055
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 1.44
               Mean episode length: 49.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 16.38s
                        Total time: 6647.98s
                               ETA: 925757.4s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.336s, learning 0.162s)
               Value function loss: 0.0056
                    Surrogate loss: -0.0417
             Mean action noise std: 0.73
                       Mean reward: 1.46
               Mean episode length: 48.43
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 16.50s
                        Total time: 6664.48s
                               ETA: 926745.7s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1017 steps/s (collection: 15.880s, learning 0.216s)
               Value function loss: 0.0068
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 1.47
               Mean episode length: 45.88
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 16.10s
                        Total time: 6680.58s
                               ETA: 927675.4s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.035s, learning 0.171s)
               Value function loss: 0.0069
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 47.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 16.21s
                        Total time: 6696.79s
                               ETA: 928617.7s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.198s, learning 0.250s)
               Value function loss: 0.0072
                    Surrogate loss: -0.0363
             Mean action noise std: 0.73
                       Mean reward: 1.45
               Mean episode length: 45.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 16.45s
                        Total time: 6713.23s
                               ETA: 929590.7s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 973 steps/s (collection: 16.657s, learning 0.167s)
               Value function loss: 0.0078
                    Surrogate loss: -0.0334
             Mean action noise std: 0.73
                       Mean reward: 1.43
               Mean episode length: 46.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 16.82s
                        Total time: 6730.06s
                               ETA: 930613.1s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.312s, learning 0.160s)
               Value function loss: 0.0085
                    Surrogate loss: -0.0354
             Mean action noise std: 0.73
                       Mean reward: 1.48
               Mean episode length: 47.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 16.47s
                        Total time: 6746.53s
                               ETA: 931583.8s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.467s, learning 0.169s)
               Value function loss: 0.0095
                    Surrogate loss: -0.0357
             Mean action noise std: 0.73
                       Mean reward: 1.53
               Mean episode length: 47.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 16.64s
                        Total time: 6763.16s
                               ETA: 932574.5s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.295s, learning 0.172s)
               Value function loss: 0.0083
                    Surrogate loss: -0.0387
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 47.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 16.47s
                        Total time: 6779.63s
                               ETA: 933539.2s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.336s, learning 0.168s)
               Value function loss: 0.0097
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 1.66
               Mean episode length: 47.81
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 16.50s
                        Total time: 6796.13s
                               ETA: 934506.1s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.380s, learning 0.165s)
               Value function loss: 0.0089
                    Surrogate loss: -0.0390
             Mean action noise std: 0.73
                       Mean reward: 1.49
               Mean episode length: 47.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 16.54s
                        Total time: 6812.68s
                               ETA: 935475.9s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.181s, learning 0.160s)
               Value function loss: 0.0105
                    Surrogate loss: -0.0362
             Mean action noise std: 0.73
                       Mean reward: 1.51
               Mean episode length: 48.07
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 16.34s
                        Total time: 6829.02s
                               ETA: 936415.1s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.292s, learning 0.173s)
               Value function loss: 0.0106
                    Surrogate loss: -0.0356
             Mean action noise std: 0.73
                       Mean reward: 1.53
               Mean episode length: 46.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 16.46s
                        Total time: 6845.48s
                               ETA: 937368.7s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.322s, learning 0.166s)
               Value function loss: 0.0103
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 47.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 16.49s
                        Total time: 6861.97s
                               ETA: 938322.6s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.310s, learning 0.215s)
               Value function loss: 0.0102
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.49
               Mean episode length: 47.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 16.52s
                        Total time: 6878.50s
                               ETA: 939279.0s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 967 steps/s (collection: 16.718s, learning 0.211s)
               Value function loss: 0.0116
                    Surrogate loss: -0.0394
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 46.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 16.93s
                        Total time: 6895.42s
                               ETA: 940287.7s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.291s, learning 0.166s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 49.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 16.46s
                        Total time: 6911.88s
                               ETA: 941229.5s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 992 steps/s (collection: 16.337s, learning 0.175s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 46.53
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 16.51s
                        Total time: 6928.39s
                               ETA: 942176.1s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.298s, learning 0.163s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 46.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 16.46s
                        Total time: 6944.85s
                               ETA: 943113.1s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 994 steps/s (collection: 16.299s, learning 0.171s)
               Value function loss: 0.0172
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 46.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 16.47s
                        Total time: 6961.32s
                               ETA: 944048.8s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.197s, learning 0.167s)
               Value function loss: 0.0161
                    Surrogate loss: -0.0359
             Mean action noise std: 0.72
                       Mean reward: 1.81
               Mean episode length: 48.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 16.36s
                        Total time: 6977.69s
                               ETA: 944967.5s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 982 steps/s (collection: 16.507s, learning 0.165s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 48.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 16.67s
                        Total time: 6994.36s
                               ETA: 945925.2s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.157s, learning 0.167s)
               Value function loss: 0.0155
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 47.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 16.32s
                        Total time: 7010.68s
                               ETA: 946833.3s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 984 steps/s (collection: 16.467s, learning 0.167s)
               Value function loss: 0.0183
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 48.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 16.63s
                        Total time: 7027.32s
                               ETA: 947780.9s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.531s, learning 0.213s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0347
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 47.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 16.74s
                        Total time: 7044.06s
                               ETA: 948740.5s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 995 steps/s (collection: 16.282s, learning 0.175s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 48.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 16.46s
                        Total time: 7060.52s
                               ETA: 949658.9s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.360s, learning 0.162s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 1.86
               Mean episode length: 47.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 16.52s
                        Total time: 7077.04s
                               ETA: 950583.5s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1032 steps/s (collection: 15.699s, learning 0.169s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.68
               Mean episode length: 46.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 15.87s
                        Total time: 7092.91s
                               ETA: 951417.9s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.242s, learning 0.167s)
               Value function loss: 0.0296
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 49.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 16.41s
                        Total time: 7109.32s
                               ETA: 952322.5s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.112s, learning 0.164s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 47.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 16.28s
                        Total time: 7125.59s
                               ETA: 953206.7s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.329s, learning 0.166s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.72
               Mean episode length: 47.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 16.49s
                        Total time: 7142.09s
                               ETA: 954117.8s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 998 steps/s (collection: 16.227s, learning 0.176s)
               Value function loss: 0.0218
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 46.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 16.40s
                        Total time: 7158.49s
                               ETA: 955014.0s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 989 steps/s (collection: 16.391s, learning 0.167s)
               Value function loss: 0.0188
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 48.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 16.56s
                        Total time: 7175.05s
                               ETA: 955928.5s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.195s, learning 0.191s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0335
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 48.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 16.39s
                        Total time: 7191.44s
                               ETA: 956817.7s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.353s, learning 0.169s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 47.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 16.52s
                        Total time: 7207.96s
                               ETA: 957722.3s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 990 steps/s (collection: 16.376s, learning 0.162s)
               Value function loss: 0.0198
                    Surrogate loss: -0.0344
             Mean action noise std: 0.72
                       Mean reward: 1.88
               Mean episode length: 48.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 16.54s
                        Total time: 7224.50s
                               ETA: 958626.8s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1215 steps/s (collection: 13.315s, learning 0.166s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 1.69
               Mean episode length: 48.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 13.48s
                        Total time: 7237.98s
                               ETA: 959123.6s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.267s, learning 0.167s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 1.75
               Mean episode length: 48.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 8.43s
                        Total time: 7246.41s
                               ETA: 958951.3s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.149s, learning 0.191s)
               Value function loss: 0.0215
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 47.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 8.34s
                        Total time: 7254.75s
                               ETA: 958766.9s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.172s)
               Value function loss: 0.0285
                    Surrogate loss: -0.0365
             Mean action noise std: 0.72
                       Mean reward: 2.01
               Mean episode length: 48.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 8.42s
                        Total time: 7263.17s
                               ETA: 958594.0s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1883 steps/s (collection: 8.476s, learning 0.221s)
               Value function loss: 0.0275
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 2.12
               Mean episode length: 50.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 8.70s
                        Total time: 7271.87s
                               ETA: 958457.6s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.339s, learning 0.212s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 2.15
               Mean episode length: 47.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 8.55s
                        Total time: 7280.42s
                               ETA: 958302.3s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.125s, learning 0.284s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0369
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 48.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 8.41s
                        Total time: 7288.83s
                               ETA: 958128.8s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.393s, learning 0.168s)
               Value function loss: 0.0430
                    Surrogate loss: -0.0325
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 49.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 8.56s
                        Total time: 7297.39s
                               ETA: 957975.7s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.322s, learning 0.167s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0339
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 49.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 8.49s
                        Total time: 7305.88s
                               ETA: 957813.5s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 2000 steps/s (collection: 8.025s, learning 0.165s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0332
             Mean action noise std: 0.72
                       Mean reward: 1.83
               Mean episode length: 47.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 8.19s
                        Total time: 7314.07s
                               ETA: 957612.6s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.158s, learning 0.170s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 2.13
               Mean episode length: 48.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 8.33s
                        Total time: 7322.40s
                               ETA: 957430.1s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.166s)
               Value function loss: 0.0376
                    Surrogate loss: -0.0291
             Mean action noise std: 0.72
                       Mean reward: 1.93
               Mean episode length: 48.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 8.38s
                        Total time: 7330.78s
                               ETA: 957255.5s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.265s, learning 0.165s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0326
             Mean action noise std: 0.72
                       Mean reward: 2.00
               Mean episode length: 48.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 8.43s
                        Total time: 7339.21s
                               ETA: 957087.4s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.141s, learning 0.164s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 47.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 8.30s
                        Total time: 7347.52s
                               ETA: 956903.3s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.310s, learning 0.177s)
               Value function loss: 0.0243
                    Surrogate loss: -0.0360
             Mean action noise std: 0.72
                       Mean reward: 1.98
               Mean episode length: 47.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 8.49s
                        Total time: 7356.01s
                               ETA: 956743.5s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.164s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 1.89
               Mean episode length: 48.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 8.44s
                        Total time: 7364.44s
                               ETA: 956577.2s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1896 steps/s (collection: 8.472s, learning 0.168s)
               Value function loss: 0.0283
                    Surrogate loss: -0.0340
             Mean action noise std: 0.72
                       Mean reward: 1.90
               Mean episode length: 48.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 8.64s
                        Total time: 7373.08s
                               ETA: 956438.0s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1945 steps/s (collection: 8.255s, learning 0.167s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 1.82
               Mean episode length: 47.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 8.42s
                        Total time: 7381.50s
                               ETA: 956270.8s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.035s, learning 0.209s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0292
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 47.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 8.24s
                        Total time: 7389.75s
                               ETA: 956081.0s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.934s, learning 0.163s)
               Value function loss: 0.0346
                    Surrogate loss: -0.0357
             Mean action noise std: 0.72
                       Mean reward: 1.69
               Mean episode length: 48.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 8.10s
                        Total time: 7397.84s
                               ETA: 955872.7s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1930 steps/s (collection: 8.315s, learning 0.170s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0326
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 48.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 8.49s
                        Total time: 7406.33s
                               ETA: 955715.0s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1995 steps/s (collection: 8.039s, learning 0.171s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0328
             Mean action noise std: 0.72
                       Mean reward: 2.52
               Mean episode length: 49.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 8.21s
                        Total time: 7414.54s
                               ETA: 955522.2s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1894 steps/s (collection: 8.471s, learning 0.176s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0321
             Mean action noise std: 0.72
                       Mean reward: 1.86
               Mean episode length: 48.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 8.65s
                        Total time: 7423.19s
                               ETA: 955386.2s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.171s)
               Value function loss: 0.0415
                    Surrogate loss: -0.0266
             Mean action noise std: 0.72
                       Mean reward: 2.10
               Mean episode length: 49.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 8.37s
                        Total time: 7431.56s
                               ETA: 955215.4s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.165s)
               Value function loss: 0.0353
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 1.85
               Mean episode length: 46.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 8.42s
                        Total time: 7439.98s
                               ETA: 955051.5s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.244s, learning 0.235s)
               Value function loss: 0.0284
                    Surrogate loss: -0.0354
             Mean action noise std: 0.72
                       Mean reward: 1.99
               Mean episode length: 47.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 8.48s
                        Total time: 7448.46s
                               ETA: 954895.1s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.406s, learning 0.167s)
               Value function loss: 0.0316
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 1.81
               Mean episode length: 46.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 8.57s
                        Total time: 7457.04s
                               ETA: 954750.9s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1873 steps/s (collection: 8.580s, learning 0.168s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0328
             Mean action noise std: 0.72
                       Mean reward: 1.85
               Mean episode length: 49.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 8.75s
                        Total time: 7465.78s
                               ETA: 954629.5s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.194s, learning 0.160s)
               Value function loss: 0.0417
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 2.20
               Mean episode length: 49.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 8.35s
                        Total time: 7474.14s
                               ETA: 954458.1s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.283s, learning 0.168s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 1.73
               Mean episode length: 48.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 8.45s
                        Total time: 7482.59s
                               ETA: 954299.5s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.117s, learning 0.168s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 2.06
               Mean episode length: 48.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 8.28s
                        Total time: 7490.87s
                               ETA: 954120.1s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.286s, learning 0.166s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 2.20
               Mean episode length: 48.63
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 8.45s
                        Total time: 7499.33s
                               ETA: 953962.4s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.165s, learning 0.177s)
               Value function loss: 0.0393
                    Surrogate loss: -0.0305
             Mean action noise std: 0.72
                       Mean reward: 2.17
               Mean episode length: 49.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 8.34s
                        Total time: 7507.67s
                               ETA: 953791.1s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.184s, learning 0.167s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 1.84
               Mean episode length: 48.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 8.35s
                        Total time: 7516.02s
                               ETA: 953621.4s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.251s, learning 0.165s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 2.09
               Mean episode length: 48.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 8.42s
                        Total time: 7524.44s
                               ETA: 953460.3s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.162s)
               Value function loss: 0.0407
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 2.24
               Mean episode length: 47.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 8.40s
                        Total time: 7532.84s
                               ETA: 953298.1s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.091s, learning 0.180s)
               Value function loss: 0.0623
                    Surrogate loss: -0.0297
             Mean action noise std: 0.72
                       Mean reward: 2.05
               Mean episode length: 47.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 8.27s
                        Total time: 7541.11s
                               ETA: 953119.5s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1972 steps/s (collection: 8.142s, learning 0.163s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0307
             Mean action noise std: 0.72
                       Mean reward: 2.18
               Mean episode length: 47.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 8.31s
                        Total time: 7549.42s
                               ETA: 952945.7s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.169s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 1.93
               Mean episode length: 49.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 8.57s
                        Total time: 7557.98s
                               ETA: 952805.5s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1905 steps/s (collection: 8.431s, learning 0.166s)
               Value function loss: 0.0445
                    Surrogate loss: -0.0314
             Mean action noise std: 0.72
                       Mean reward: 2.01
               Mean episode length: 47.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 8.60s
                        Total time: 7566.58s
                               ETA: 952669.2s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 2037 steps/s (collection: 7.881s, learning 0.162s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0344
             Mean action noise std: 0.72
                       Mean reward: 2.06
               Mean episode length: 47.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 8.04s
                        Total time: 7574.63s
                               ETA: 952463.6s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.345s, learning 0.169s)
               Value function loss: 0.0490
                    Surrogate loss: -0.0299
             Mean action noise std: 0.72
                       Mean reward: 2.19
               Mean episode length: 48.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 8.51s
                        Total time: 7583.14s
                               ETA: 952317.6s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1966 steps/s (collection: 8.104s, learning 0.229s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0368
             Mean action noise std: 0.72
                       Mean reward: 2.09
               Mean episode length: 49.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 8.33s
                        Total time: 7591.47s
                               ETA: 952149.2s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.169s)
               Value function loss: 56.1940
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 1.87
               Mean episode length: 49.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 8.43s
                        Total time: 7599.90s
                               ETA: 951993.2s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.147s, learning 0.177s)
               Value function loss: 0.6029
                    Surrogate loss: -0.0255
             Mean action noise std: 0.72
                       Mean reward: 2.01
               Mean episode length: 48.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 8.32s
                        Total time: 7608.23s
                               ETA: 951824.5s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.190s, learning 0.169s)
               Value function loss: 0.2022
                    Surrogate loss: -0.0272
             Mean action noise std: 0.72
                       Mean reward: 1.94
               Mean episode length: 48.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 8.36s
                        Total time: 7616.58s
                               ETA: 951660.6s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.276s, learning 0.176s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0272
             Mean action noise std: 0.72
                       Mean reward: 7.31
               Mean episode length: 50.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 8.45s
                        Total time: 7625.04s
                               ETA: 951508.5s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.340s, learning 0.176s)
               Value function loss: 0.1100
                    Surrogate loss: -0.0300
             Mean action noise std: 0.72
                       Mean reward: 2.01
               Mean episode length: 48.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 8.52s
                        Total time: 7633.55s
                               ETA: 951365.0s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1885 steps/s (collection: 8.525s, learning 0.164s)
               Value function loss: 0.1289
                    Surrogate loss: -0.0260
             Mean action noise std: 0.72
                       Mean reward: 2.25
               Mean episode length: 49.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 8.69s
                        Total time: 7642.24s
                               ETA: 951243.4s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.974s, learning 0.171s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0297
             Mean action noise std: 0.72
                       Mean reward: 2.08
               Mean episode length: 48.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 8.15s
                        Total time: 7650.39s
                               ETA: 951054.4s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.164s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0321
             Mean action noise std: 0.72
                       Mean reward: 1.98
               Mean episode length: 48.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 8.51s
                        Total time: 7658.90s
                               ETA: 950911.2s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1974 steps/s (collection: 8.125s, learning 0.171s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 2.23
               Mean episode length: 49.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 8.30s
                        Total time: 7667.19s
                               ETA: 950741.7s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.210s, learning 0.263s)
               Value function loss: 0.0658
                    Surrogate loss: -0.0349
             Mean action noise std: 0.72
                       Mean reward: 2.24
               Mean episode length: 50.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 8.47s
                        Total time: 7675.67s
                               ETA: 950594.6s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.378s, learning 0.206s)
               Value function loss: 0.0666
                    Surrogate loss: -0.0289
             Mean action noise std: 0.72
                       Mean reward: 2.51
               Mean episode length: 49.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 8.58s
                        Total time: 7684.25s
                               ETA: 950461.4s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1895 steps/s (collection: 8.429s, learning 0.213s)
               Value function loss: 0.0519
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 1.95
               Mean episode length: 49.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 8.64s
                        Total time: 7692.89s
                               ETA: 950335.8s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.266s, learning 0.213s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 2.09
               Mean episode length: 49.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 8.48s
                        Total time: 7701.37s
                               ETA: 950190.3s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1901 steps/s (collection: 8.352s, learning 0.265s)
               Value function loss: 0.0616
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 2.16
               Mean episode length: 48.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 8.62s
                        Total time: 7709.99s
                               ETA: 950062.3s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1997 steps/s (collection: 8.038s, learning 0.166s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0320
             Mean action noise std: 0.72
                       Mean reward: 2.45
               Mean episode length: 50.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 8.20s
                        Total time: 7718.19s
                               ETA: 949883.7s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 2023 steps/s (collection: 7.928s, learning 0.170s)
               Value function loss: 0.0682
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 2.26
               Mean episode length: 49.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 8.10s
                        Total time: 7726.29s
                               ETA: 949692.4s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.147s, learning 0.209s)
               Value function loss: 0.0720
                    Surrogate loss: -0.0307
             Mean action noise std: 0.72
                       Mean reward: 2.21
               Mean episode length: 48.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 8.36s
                        Total time: 7734.65s
                               ETA: 949533.3s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1952 steps/s (collection: 8.229s, learning 0.162s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 2.00
               Mean episode length: 48.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 8.39s
                        Total time: 7743.04s
                               ETA: 949378.9s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.414s, learning 0.173s)
               Value function loss: 0.0664
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 2.05
               Mean episode length: 49.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 8.59s
                        Total time: 7751.63s
                               ETA: 949248.8s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.123s, learning 0.189s)
               Value function loss: 128.5790
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 2.72
               Mean episode length: 50.51
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 8.31s
                        Total time: 7759.94s
                               ETA: 949085.3s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.388s, learning 0.176s)
               Value function loss: 0.5471
                    Surrogate loss: -0.0284
             Mean action noise std: 0.72
                       Mean reward: 2.18
               Mean episode length: 50.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 8.56s
                        Total time: 7768.50s
                               ETA: 948953.0s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1971 steps/s (collection: 8.116s, learning 0.194s)
               Value function loss: 0.4438
                    Surrogate loss: -0.0266
             Mean action noise std: 0.72
                       Mean reward: 2.18
               Mean episode length: 50.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 8.31s
                        Total time: 7776.81s
                               ETA: 948790.1s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.277s, learning 0.169s)
               Value function loss: 0.3367
                    Surrogate loss: -0.0274
             Mean action noise std: 0.72
                       Mean reward: 2.48
               Mean episode length: 48.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 8.45s
                        Total time: 7785.26s
                               ETA: 948644.2s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1991 steps/s (collection: 8.051s, learning 0.175s)
               Value function loss: 0.2742
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 2.76
               Mean episode length: 50.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 8.23s
                        Total time: 7793.48s
                               ETA: 948471.8s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.345s, learning 0.168s)
               Value function loss: 0.2314
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 2.62
               Mean episode length: 51.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 8.51s
                        Total time: 7802.00s
                               ETA: 948334.7s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.110s, learning 0.177s)
               Value function loss: 0.1931
                    Surrogate loss: -0.0290
             Mean action noise std: 0.72
                       Mean reward: 2.48
               Mean episode length: 49.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 8.29s
                        Total time: 7810.28s
                               ETA: 948170.5s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1928 steps/s (collection: 8.281s, learning 0.213s)
               Value function loss: 0.1317
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 1.98
               Mean episode length: 49.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 8.49s
                        Total time: 7818.78s
                               ETA: 948031.7s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1891 steps/s (collection: 8.454s, learning 0.209s)
               Value function loss: 0.1180
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 2.22
               Mean episode length: 49.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 8.66s
                        Total time: 7827.44s
                               ETA: 947913.6s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.173s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0314
             Mean action noise std: 0.72
                       Mean reward: 2.15
               Mean episode length: 51.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 8.33s
                        Total time: 7835.77s
                               ETA: 947755.3s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.146s, learning 0.171s)
               Value function loss: 120.1876
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 2.16
               Mean episode length: 50.49
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 8.32s
                        Total time: 7844.09s
                               ETA: 947596.0s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.208s, learning 0.166s)
               Value function loss: 0.3792
                    Surrogate loss: -0.0286
             Mean action noise std: 0.72
                       Mean reward: 2.12
               Mean episode length: 48.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 8.37s
                        Total time: 7852.46s
                               ETA: 947444.0s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1932 steps/s (collection: 8.318s, learning 0.159s)
               Value function loss: 69.2326
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 2.36
               Mean episode length: 50.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 8.48s
                        Total time: 7860.94s
                               ETA: 947304.8s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1962 steps/s (collection: 8.184s, learning 0.165s)
               Value function loss: 0.2209
                    Surrogate loss: -0.0299
             Mean action noise std: 0.72
                       Mean reward: 2.11
               Mean episode length: 49.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 8.35s
                        Total time: 7869.28s
                               ETA: 947150.5s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.343s, learning 0.172s)
               Value function loss: 0.1519
                    Surrogate loss: -0.0264
             Mean action noise std: 0.72
                       Mean reward: 2.06
               Mean episode length: 49.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 8.51s
                        Total time: 7877.80s
                               ETA: 947016.5s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.125s, learning 0.161s)
               Value function loss: 0.1190
                    Surrogate loss: -0.0266
             Mean action noise std: 0.72
                       Mean reward: 2.41
               Mean episode length: 50.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 8.29s
                        Total time: 7886.09s
                               ETA: 946855.4s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.314s, learning 0.158s)
               Value function loss: 0.0936
                    Surrogate loss: -0.0321
             Mean action noise std: 0.72
                       Mean reward: 2.04
               Mean episode length: 49.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 8.47s
                        Total time: 7894.56s
                               ETA: 946716.9s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.962s, learning 0.182s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 2.16
               Mean episode length: 48.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 8.14s
                        Total time: 7902.70s
                               ETA: 946539.5s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.068s, learning 0.164s)
               Value function loss: 0.0922
                    Surrogate loss: -0.0316
             Mean action noise std: 0.72
                       Mean reward: 2.50
               Mean episode length: 51.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 8.23s
                        Total time: 7910.93s
                               ETA: 946372.9s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 2005 steps/s (collection: 8.006s, learning 0.165s)
               Value function loss: 70.7337
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 2.36
               Mean episode length: 50.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 8.17s
                        Total time: 7919.10s
                               ETA: 946199.4s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1931 steps/s (collection: 8.303s, learning 0.178s)
               Value function loss: 0.1274
                    Surrogate loss: -0.0305
             Mean action noise std: 0.72
                       Mean reward: 2.25
               Mean episode length: 48.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 8.48s
                        Total time: 7927.59s
                               ETA: 946063.4s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.366s, learning 0.164s)
               Value function loss: 0.1097
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 2.83
               Mean episode length: 52.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 8.53s
                        Total time: 7936.12s
                               ETA: 945933.4s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.414s, learning 0.163s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 2.43
               Mean episode length: 50.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 8.58s
                        Total time: 7944.69s
                               ETA: 945809.4s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1944 steps/s (collection: 8.264s, learning 0.161s)
               Value function loss: 0.0884
                    Surrogate loss: -0.0306
             Mean action noise std: 0.72
                       Mean reward: 2.31
               Mean episode length: 49.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 8.42s
                        Total time: 7953.12s
                               ETA: 945667.6s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1899 steps/s (collection: 8.432s, learning 0.194s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0297
             Mean action noise std: 0.72
                       Mean reward: 2.33
               Mean episode length: 49.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 8.63s
                        Total time: 7961.74s
                               ETA: 945550.0s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1982 steps/s (collection: 8.102s, learning 0.162s)
               Value function loss: 0.1098
                    Surrogate loss: -0.0291
             Mean action noise std: 0.72
                       Mean reward: 2.44
               Mean episode length: 51.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 8.26s
                        Total time: 7970.01s
                               ETA: 945389.7s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1961 steps/s (collection: 8.178s, learning 0.173s)
               Value function loss: 0.0936
                    Surrogate loss: -0.0306
             Mean action noise std: 0.72
                       Mean reward: 2.16
               Mean episode length: 50.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 8.35s
                        Total time: 7978.36s
                               ETA: 945240.1s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.295s, learning 0.169s)
               Value function loss: 0.0807
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 1.99
               Mean episode length: 48.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 8.46s
                        Total time: 7986.82s
                               ETA: 945104.2s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.451s, learning 0.177s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 2.33
               Mean episode length: 49.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 8.63s
                        Total time: 7995.45s
                               ETA: 944987.9s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1908 steps/s (collection: 8.408s, learning 0.177s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0311
             Mean action noise std: 0.72
                       Mean reward: 2.00
               Mean episode length: 49.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 8.59s
                        Total time: 8004.04s
                               ETA: 944867.0s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.139s, learning 0.164s)
               Value function loss: 119.2541
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 2.68
               Mean episode length: 49.02
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 8.30s
                        Total time: 8012.34s
                               ETA: 944712.9s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.275s, learning 0.174s)
               Value function loss: 0.3133
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 2.67
               Mean episode length: 49.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 8.45s
                        Total time: 8020.79s
                               ETA: 944576.4s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.258s, learning 0.205s)
               Value function loss: 0.1992
                    Surrogate loss: -0.0296
             Mean action noise std: 0.72
                       Mean reward: 2.30
               Mean episode length: 49.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 8.46s
                        Total time: 8029.25s
                               ETA: 944441.9s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.292s, learning 0.161s)
               Value function loss: 0.1805
                    Surrogate loss: -0.0277
             Mean action noise std: 0.72
                       Mean reward: 2.56
               Mean episode length: 50.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 8.45s
                        Total time: 8037.70s
                               ETA: 944306.5s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.167s)
               Value function loss: 0.1430
                    Surrogate loss: -0.0278
             Mean action noise std: 0.72
                       Mean reward: 2.23
               Mean episode length: 49.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 8.41s
                        Total time: 8046.11s
                               ETA: 944165.9s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1881 steps/s (collection: 8.541s, learning 0.169s)
               Value function loss: 0.1177
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 2.55
               Mean episode length: 48.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 8.71s
                        Total time: 8054.82s
                               ETA: 944061.2s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1929 steps/s (collection: 8.329s, learning 0.164s)
               Value function loss: 0.1258
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 2.45
               Mean episode length: 51.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 8.49s
                        Total time: 8063.31s
                               ETA: 943931.3s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.121s, learning 0.163s)
               Value function loss: 0.0864
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 2.54
               Mean episode length: 51.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 8.28s
                        Total time: 8071.60s
                               ETA: 943777.3s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1960 steps/s (collection: 8.195s, learning 0.161s)
               Value function loss: 0.0746
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 2.41
               Mean episode length: 50.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 8.36s
                        Total time: 8079.95s
                               ETA: 943632.0s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1911 steps/s (collection: 8.414s, learning 0.157s)
               Value function loss: 0.0694
                    Surrogate loss: -0.0314
             Mean action noise std: 0.72
                       Mean reward: 2.41
               Mean episode length: 50.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 8.57s
                        Total time: 8088.52s
                               ETA: 943512.2s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.071s, learning 0.204s)
               Value function loss: 90.3174
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 2.81
               Mean episode length: 50.73
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 8.28s
                        Total time: 8096.80s
                               ETA: 943358.1s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.163s)
               Value function loss: 0.1493
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 2.33
               Mean episode length: 49.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 8.36s
                        Total time: 8105.16s
                               ETA: 943214.7s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.233s, learning 0.168s)
               Value function loss: 0.1534
                    Surrogate loss: -0.0287
             Mean action noise std: 0.72
                       Mean reward: 2.46
               Mean episode length: 49.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 8.40s
                        Total time: 8113.57s
                               ETA: 943075.9s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1978 steps/s (collection: 8.108s, learning 0.173s)
               Value function loss: 0.1222
                    Surrogate loss: -0.0274
             Mean action noise std: 0.72
                       Mean reward: 2.58
               Mean episode length: 51.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 8.28s
                        Total time: 8121.85s
                               ETA: 942923.6s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.167s)
               Value function loss: 0.1071
                    Surrogate loss: -0.0301
             Mean action noise std: 0.72
                       Mean reward: 2.27
               Mean episode length: 48.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 8.55s
                        Total time: 8130.40s
                               ETA: 942803.1s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1950 steps/s (collection: 8.217s, learning 0.184s)
               Value function loss: 0.1112
                    Surrogate loss: -0.0305
             Mean action noise std: 0.72
                       Mean reward: 2.58
               Mean episode length: 49.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 8.40s
                        Total time: 8138.80s
                               ETA: 942665.3s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.282s, learning 0.162s)
               Value function loss: 0.1012
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 2.31
               Mean episode length: 49.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 8.44s
                        Total time: 8147.24s
                               ETA: 942532.6s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 2001 steps/s (collection: 8.012s, learning 0.176s)
               Value function loss: 52.2216
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 2.48
               Mean episode length: 49.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 8.19s
                        Total time: 8155.43s
                               ETA: 942370.7s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.176s)
               Value function loss: 0.1858
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 2.41
               Mean episode length: 48.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 8.46s
                        Total time: 8163.89s
                               ETA: 942240.4s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.363s, learning 0.166s)
               Value function loss: 0.1616
                    Surrogate loss: -0.0293
             Mean action noise std: 0.72
                       Mean reward: 2.38
               Mean episode length: 47.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 8.53s
                        Total time: 8172.42s
                               ETA: 942118.4s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.442s, learning 0.163s)
               Value function loss: 0.1520
                    Surrogate loss: -0.0280
             Mean action noise std: 0.72
                       Mean reward: 2.28
               Mean episode length: 49.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 8.60s
                        Total time: 8181.02s
                               ETA: 942005.4s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1965 steps/s (collection: 8.167s, learning 0.168s)
               Value function loss: 0.1453
                    Surrogate loss: -0.0282
             Mean action noise std: 0.72
                       Mean reward: 2.72
               Mean episode length: 49.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 8.34s
                        Total time: 8189.36s
                               ETA: 941861.8s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.277s, learning 0.161s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0279
             Mean action noise std: 0.72
                       Mean reward: 2.38
               Mean episode length: 49.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 8.44s
                        Total time: 8197.80s
                               ETA: 941730.3s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.306s, learning 0.210s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 2.50
               Mean episode length: 49.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 8.52s
                        Total time: 8206.31s
                               ETA: 941608.0s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.162s, learning 0.270s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0315
             Mean action noise std: 0.72
                       Mean reward: 2.50
               Mean episode length: 49.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 8.43s
                        Total time: 8214.75s
                               ETA: 941476.3s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.172s)
               Value function loss: 106.6814
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 2.88
               Mean episode length: 50.47
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 8.42s
                        Total time: 8223.16s
                               ETA: 941343.4s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.254s, learning 0.215s)
               Value function loss: 0.2281
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 2.58
               Mean episode length: 49.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 8.47s
                        Total time: 8231.63s
                               ETA: 941216.6s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.221s, learning 0.162s)
               Value function loss: 0.1820
                    Surrogate loss: -0.0279
             Mean action noise std: 0.72
                       Mean reward: 2.42
               Mean episode length: 49.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 8.38s
                        Total time: 8240.02s
                               ETA: 941080.2s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1970 steps/s (collection: 8.139s, learning 0.177s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0341
             Mean action noise std: 0.72
                       Mean reward: 2.55
               Mean episode length: 49.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 8.32s
                        Total time: 8248.33s
                               ETA: 940936.5s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.371s, learning 0.167s)
               Value function loss: 0.1189
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 2.78
               Mean episode length: 50.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 8.54s
                        Total time: 8256.87s
                               ETA: 940818.4s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.170s)
               Value function loss: 0.1291
                    Surrogate loss: -0.0292
             Mean action noise std: 0.72
                       Mean reward: 2.20
               Mean episode length: 49.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 8.46s
                        Total time: 8265.33s
                               ETA: 940691.2s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.271s, learning 0.183s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0324
             Mean action noise std: 0.72
                       Mean reward: 2.68
               Mean episode length: 50.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 8.45s
                        Total time: 8273.78s
                               ETA: 940564.0s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1940 steps/s (collection: 8.279s, learning 0.165s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0294
             Mean action noise std: 0.72
                       Mean reward: 2.37
               Mean episode length: 47.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 8.44s
                        Total time: 8282.23s
                               ETA: 940435.8s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1955 steps/s (collection: 8.162s, learning 0.217s)
               Value function loss: 0.1540
                    Surrogate loss: -0.0283
             Mean action noise std: 0.72
                       Mean reward: 2.42
               Mean episode length: 49.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 8.38s
                        Total time: 8290.60s
                               ETA: 940300.7s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1989 steps/s (collection: 8.056s, learning 0.178s)
               Value function loss: 0.1441
                    Surrogate loss: -0.0296
             Mean action noise std: 0.72
                       Mean reward: 2.24
               Mean episode length: 49.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 8.23s
                        Total time: 8298.84s
                               ETA: 940149.3s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.300s, learning 0.165s)
               Value function loss: 0.1341
                    Surrogate loss: -0.0304
             Mean action noise std: 0.72
                       Mean reward: 2.64
               Mean episode length: 49.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 8.47s
                        Total time: 8307.30s
                               ETA: 940024.5s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1913 steps/s (collection: 8.399s, learning 0.164s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0338
             Mean action noise std: 0.72
                       Mean reward: 2.60
               Mean episode length: 50.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 8.56s
                        Total time: 8315.87s
                               ETA: 939911.0s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.274s, learning 0.165s)
               Value function loss: 0.1314
                    Surrogate loss: -0.0319
             Mean action noise std: 0.72
                       Mean reward: 3.08
               Mean episode length: 51.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 8.44s
                        Total time: 8324.31s
                               ETA: 939783.7s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.215s, learning 0.174s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0316
             Mean action noise std: 0.72
                       Mean reward: 2.73
               Mean episode length: 50.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 8.39s
                        Total time: 8332.69s
                               ETA: 939651.1s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1953 steps/s (collection: 8.214s, learning 0.172s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 2.94
               Mean episode length: 50.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 8.39s
                        Total time: 8341.08s
                               ETA: 939518.4s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 2004 steps/s (collection: 8.011s, learning 0.162s)
               Value function loss: 0.1129
                    Surrogate loss: -0.0306
             Mean action noise std: 0.72
                       Mean reward: 2.87
               Mean episode length: 50.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 8.17s
                        Total time: 8349.25s
                               ETA: 939362.0s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1951 steps/s (collection: 8.234s, learning 0.161s)
               Value function loss: 0.1125
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 2.80
               Mean episode length: 50.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 8.39s
                        Total time: 8357.65s
                               ETA: 939231.0s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.223s, learning 0.160s)
               Value function loss: 0.1264
                    Surrogate loss: -0.0287
             Mean action noise std: 0.72
                       Mean reward: 2.46
               Mean episode length: 48.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 8.38s
                        Total time: 8366.03s
                               ETA: 939098.8s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1921 steps/s (collection: 8.361s, learning 0.166s)
               Value function loss: 0.0960
                    Surrogate loss: -0.0339
             Mean action noise std: 0.72
                       Mean reward: 2.74
               Mean episode length: 51.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 8.53s
                        Total time: 8374.56s
                               ETA: 938983.2s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1994 steps/s (collection: 8.054s, learning 0.160s)
               Value function loss: 46.6830
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 2.73
               Mean episode length: 50.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 8.21s
                        Total time: 8382.77s
                               ETA: 938832.7s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1924 steps/s (collection: 8.306s, learning 0.208s)
               Value function loss: 0.2076
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 2.73
               Mean episode length: 50.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 8.51s
                        Total time: 8391.29s
                               ETA: 938716.1s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.203s, learning 0.170s)
               Value function loss: 0.1909
                    Surrogate loss: -0.0297
             Mean action noise std: 0.72
                       Mean reward: 8.23
               Mean episode length: 51.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 8.37s
                        Total time: 8399.66s
                               ETA: 938584.0s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.146s, learning 0.173s)
               Value function loss: 0.1697
                    Surrogate loss: -0.0292
             Mean action noise std: 0.72
                       Mean reward: 2.57
               Mean episode length: 49.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 8.32s
                        Total time: 8407.98s
                               ETA: 938446.1s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.163s)
               Value function loss: 89.4332
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 2.61
               Mean episode length: 49.93
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 8.25s
                        Total time: 8416.23s
                               ETA: 938300.2s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1892 steps/s (collection: 8.495s, learning 0.165s)
               Value function loss: 0.4000
                    Surrogate loss: -0.0307
             Mean action noise std: 0.72
                       Mean reward: 2.58
               Mean episode length: 49.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 8.66s
                        Total time: 8424.88s
                               ETA: 938200.8s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.399s, learning 0.169s)
               Value function loss: 0.3083
                    Surrogate loss: -0.0273
             Mean action noise std: 0.72
                       Mean reward: 10.18
               Mean episode length: 49.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 8.57s
                        Total time: 8433.45s
                               ETA: 938091.3s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1976 steps/s (collection: 8.124s, learning 0.167s)
               Value function loss: 0.2251
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 2.47
               Mean episode length: 48.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 8.29s
                        Total time: 8441.74s
                               ETA: 937951.4s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.185s, learning 0.190s)
               Value function loss: 0.2537
                    Surrogate loss: -0.0285
             Mean action noise std: 0.72
                       Mean reward: 2.67
               Mean episode length: 49.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 8.37s
                        Total time: 8450.12s
                               ETA: 937821.0s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1956 steps/s (collection: 8.180s, learning 0.194s)
               Value function loss: 0.1927
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 2.91
               Mean episode length: 50.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 8.37s
                        Total time: 8458.49s
                               ETA: 937691.0s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.373s, learning 0.216s)
               Value function loss: 0.1806
                    Surrogate loss: -0.0305
             Mean action noise std: 0.72
                       Mean reward: 2.65
               Mean episode length: 49.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 8.59s
                        Total time: 8467.08s
                               ETA: 937585.0s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1501 steps/s (collection: 10.725s, learning 0.190s)
               Value function loss: 0.1695
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 2.42
               Mean episode length: 51.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 10.92s
                        Total time: 8478.00s
                               ETA: 937736.4s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1023 steps/s (collection: 15.845s, learning 0.168s)
               Value function loss: 13.3442
                    Surrogate loss: 0.0011
             Mean action noise std: 0.72
                       Mean reward: 2.68
               Mean episode length: 49.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 16.01s
                        Total time: 8494.01s
                               ETA: 938450.7s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 993 steps/s (collection: 16.309s, learning 0.174s)
               Value function loss: 0.2637
                    Surrogate loss: -0.0327
             Mean action noise std: 0.72
                       Mean reward: 2.78
               Mean episode length: 49.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 16.48s
                        Total time: 8510.49s
                               ETA: 939215.3s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1012 steps/s (collection: 16.016s, learning 0.160s)
               Value function loss: 0.2163
                    Surrogate loss: -0.0322
             Mean action noise std: 0.72
                       Mean reward: 2.34
               Mean episode length: 49.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 16.18s
                        Total time: 8526.67s
                               ETA: 939944.3s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1013 steps/s (collection: 16.001s, learning 0.164s)
               Value function loss: 0.1970
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 2.66
               Mean episode length: 49.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 16.17s
                        Total time: 8542.83s
                               ETA: 940670.5s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.017s, learning 0.187s)
               Value function loss: 0.1921
                    Surrogate loss: -0.0301
             Mean action noise std: 0.72
                       Mean reward: 2.84
               Mean episode length: 50.10
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 16.20s
                        Total time: 8559.04s
                               ETA: 941399.2s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.178s, learning 0.171s)
               Value function loss: 0.1638
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 2.86
               Mean episode length: 51.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 16.35s
                        Total time: 8575.39s
                               ETA: 942142.2s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.082s, learning 0.210s)
               Value function loss: 69.8421
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 52.58
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 16.29s
                        Total time: 8591.68s
                               ETA: 942877.4s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.148s, learning 0.214s)
               Value function loss: 0.1752
                    Surrogate loss: -0.0352
             Mean action noise std: 0.72
                       Mean reward: 2.58
               Mean episode length: 50.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 16.36s
                        Total time: 8608.04s
                               ETA: 943618.5s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.082s, learning 0.171s)
               Value function loss: 0.1793
                    Surrogate loss: -0.0320
             Mean action noise std: 0.72
                       Mean reward: 2.74
               Mean episode length: 51.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 16.25s
                        Total time: 8624.29s
                               ETA: 944346.0s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.043s, learning 0.273s)
               Value function loss: 0.1708
                    Surrogate loss: -0.0311
             Mean action noise std: 0.72
                       Mean reward: 2.93
               Mean episode length: 49.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 16.32s
                        Total time: 8640.61s
                               ETA: 945078.8s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1011 steps/s (collection: 16.026s, learning 0.169s)
               Value function loss: 0.2072
                    Surrogate loss: -0.0291
             Mean action noise std: 0.72
                       Mean reward: 2.78
               Mean episode length: 50.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 16.19s
                        Total time: 8656.81s
                               ETA: 945796.6s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 978 steps/s (collection: 16.531s, learning 0.211s)
               Value function loss: 0.1659
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 3.13
               Mean episode length: 52.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 16.74s
                        Total time: 8673.55s
                               ETA: 946572.5s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1004 steps/s (collection: 16.130s, learning 0.176s)
               Value function loss: 0.1688
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 3.03
               Mean episode length: 50.72
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 16.31s
                        Total time: 8689.85s
                               ETA: 947299.2s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1000 steps/s (collection: 16.218s, learning 0.163s)
               Value function loss: 0.1396
                    Surrogate loss: -0.0341
             Mean action noise std: 0.72
                       Mean reward: 3.02
               Mean episode length: 50.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 16.38s
                        Total time: 8706.24s
                               ETA: 948032.5s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.130s, learning 0.162s)
               Value function loss: 0.1608
                    Surrogate loss: -0.0315
             Mean action noise std: 0.72
                       Mean reward: 3.16
               Mean episode length: 51.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 16.29s
                        Total time: 8722.53s
                               ETA: 948754.3s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 981 steps/s (collection: 16.533s, learning 0.163s)
               Value function loss: 0.1401
                    Surrogate loss: -0.0327
             Mean action noise std: 0.72
                       Mean reward: 2.68
               Mean episode length: 50.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 16.70s
                        Total time: 8739.22s
                               ETA: 949518.5s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1002 steps/s (collection: 16.173s, learning 0.171s)
               Value function loss: 0.1526
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 2.85
               Mean episode length: 49.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 16.34s
                        Total time: 8755.57s
                               ETA: 950242.8s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1019 steps/s (collection: 15.894s, learning 0.179s)
               Value function loss: 0.1532
                    Surrogate loss: -0.0300
             Mean action noise std: 0.72
                       Mean reward: 2.99
               Mean episode length: 50.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 16.07s
                        Total time: 8771.64s
                               ETA: 950936.0s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 988 steps/s (collection: 16.407s, learning 0.163s)
               Value function loss: 0.1581
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 2.61
               Mean episode length: 49.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 16.57s
                        Total time: 8788.21s
                               ETA: 951681.5s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 999 steps/s (collection: 16.174s, learning 0.213s)
               Value function loss: 0.1418
                    Surrogate loss: -0.0299
             Mean action noise std: 0.72
                       Mean reward: 3.04
               Mean episode length: 50.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 16.39s
                        Total time: 8804.60s
                               ETA: 952405.5s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 991 steps/s (collection: 16.354s, learning 0.173s)
               Value function loss: 0.1478
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 3.25
               Mean episode length: 51.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 16.53s
                        Total time: 8821.12s
                               ETA: 953143.1s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.080s, learning 0.222s)
               Value function loss: 0.1678
                    Surrogate loss: -0.0324
             Mean action noise std: 0.72
                       Mean reward: 3.17
               Mean episode length: 51.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 16.30s
                        Total time: 8837.43s
                               ETA: 953854.7s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1003 steps/s (collection: 16.141s, learning 0.181s)
               Value function loss: 63.5463
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 3.05
               Mean episode length: 49.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 16.32s
                        Total time: 8853.75s
                               ETA: 954567.0s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.914s, learning 0.170s)
               Value function loss: 0.4780
                    Surrogate loss: -0.0293
             Mean action noise std: 0.72
                       Mean reward: 3.68
               Mean episode length: 51.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 16.08s
                        Total time: 8869.83s
                               ETA: 955251.9s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1018 steps/s (collection: 15.909s, learning 0.174s)
               Value function loss: 0.3486
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 3.11
               Mean episode length: 51.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 16.08s
                        Total time: 8885.91s
                               ETA: 955935.2s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1007 steps/s (collection: 16.086s, learning 0.179s)
               Value function loss: 0.3046
                    Surrogate loss: -0.0297
             Mean action noise std: 0.72
                       Mean reward: 8.25
               Mean episode length: 51.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 16.27s
                        Total time: 8902.18s
                               ETA: 956636.6s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1021 steps/s (collection: 15.867s, learning 0.175s)
               Value function loss: 0.2480
                    Surrogate loss: -0.0310
             Mean action noise std: 0.72
                       Mean reward: 3.46
               Mean episode length: 51.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 16.04s
                        Total time: 8918.22s
                               ETA: 957312.5s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1006 steps/s (collection: 16.106s, learning 0.168s)
               Value function loss: 0.2025
                    Surrogate loss: -0.0341
             Mean action noise std: 0.72
                       Mean reward: 3.10
               Mean episode length: 50.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 16.27s
                        Total time: 8934.49s
                               ETA: 958011.8s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1010 steps/s (collection: 16.032s, learning 0.182s)
               Value function loss: 0.1843
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 51.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 16.21s
                        Total time: 8950.71s
                               ETA: 958703.0s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 979 steps/s (collection: 16.556s, learning 0.170s)
               Value function loss: 29.1306
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.16
               Mean episode length: 50.56
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 16.73s
                        Total time: 8967.43s
                               ETA: 959447.6s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1028 steps/s (collection: 15.757s, learning 0.167s)
               Value function loss: 0.2190
                    Surrogate loss: -0.0333
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 50.40
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 15.92s
                        Total time: 8983.36s
                               ETA: 960104.8s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1033 steps/s (collection: 15.679s, learning 0.172s)
               Value function loss: 0.2303
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 8.26
               Mean episode length: 51.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 15.85s
                        Total time: 8999.21s
                               ETA: 960752.7s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1001 steps/s (collection: 16.135s, learning 0.228s)
               Value function loss: 0.2001
                    Surrogate loss: -0.0334
             Mean action noise std: 0.72
                       Mean reward: 3.04
               Mean episode length: 50.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 16.36s
                        Total time: 9015.57s
                               ETA: 961453.9s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1014 steps/s (collection: 15.979s, learning 0.169s)
               Value function loss: 0.2072
                    Surrogate loss: -0.0336
             Mean action noise std: 0.72
                       Mean reward: 3.19
               Mean episode length: 50.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 16.15s
                        Total time: 9031.72s
                               ETA: 962130.7s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1005 steps/s (collection: 16.080s, learning 0.209s)
               Value function loss: 0.1977
                    Surrogate loss: -0.0330
             Mean action noise std: 0.72
                       Mean reward: 2.87
               Mean episode length: 48.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 16.29s
                        Total time: 9048.01s
                               ETA: 962820.9s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1008 steps/s (collection: 16.066s, learning 0.173s)
               Value function loss: 0.2028
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 3.27
               Mean episode length: 50.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 16.24s
                        Total time: 9064.25s
                               ETA: 963504.4s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 996 steps/s (collection: 16.252s, learning 0.197s)
               Value function loss: 69.3507
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 52.43
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 16.45s
                        Total time: 9080.70s
                               ETA: 964208.5s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.503s, learning 0.197s)
               Value function loss: 40.0547
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 3.34
               Mean episode length: 51.61
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 11.70s
                        Total time: 9092.40s
                               ETA: 964407.5s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.170s, learning 0.174s)
               Value function loss: 5.1484
                    Surrogate loss: -0.0173
             Mean action noise std: 0.72
                       Mean reward: 3.52
               Mean episode length: 52.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 8.34s
                        Total time: 9100.74s
                               ETA: 964250.4s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 1969 steps/s (collection: 8.156s, learning 0.165s)
               Value function loss: 0.9136
                    Surrogate loss: -0.0296
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 50.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 8.32s
                        Total time: 9109.06s
                               ETA: 964091.1s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.248s, learning 0.162s)
               Value function loss: 13.7175
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 3.09
               Mean episode length: 49.95
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 8.41s
                        Total time: 9117.47s
                               ETA: 963941.6s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 1941 steps/s (collection: 8.273s, learning 0.164s)
               Value function loss: 0.7479
                    Surrogate loss: -0.0289
             Mean action noise std: 0.72
                       Mean reward: 3.44
               Mean episode length: 51.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 8.44s
                        Total time: 9125.91s
                               ETA: 963795.4s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.352s, learning 0.166s)
               Value function loss: 265.2279
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 2.79
               Mean episode length: 50.52
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 8.52s
                        Total time: 9134.43s
                               ETA: 963657.9s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 1968 steps/s (collection: 8.121s, learning 0.201s)
               Value function loss: 0.9746
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 3.92
               Mean episode length: 50.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 8.32s
                        Total time: 9142.75s
                               ETA: 963500.0s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 1979 steps/s (collection: 8.108s, learning 0.169s)
               Value function loss: 0.4837
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 2.78
               Mean episode length: 49.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 8.28s
                        Total time: 9151.03s
                               ETA: 963337.8s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.165s)
               Value function loss: 0.3844
                    Surrogate loss: -0.0314
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 52.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 8.57s
                        Total time: 9159.60s
                               ETA: 963206.5s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 1910 steps/s (collection: 8.350s, learning 0.225s)
               Value function loss: 0.3477
                    Surrogate loss: -0.0326
             Mean action noise std: 0.72
                       Mean reward: 2.97
               Mean episode length: 48.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 8.58s
                        Total time: 9168.17s
                               ETA: 963076.2s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 1925 steps/s (collection: 8.331s, learning 0.175s)
               Value function loss: 0.3139
                    Surrogate loss: -0.0311
             Mean action noise std: 0.72
                       Mean reward: 4.13
               Mean episode length: 52.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 8.51s
                        Total time: 9176.68s
                               ETA: 962938.9s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 1938 steps/s (collection: 8.293s, learning 0.158s)
               Value function loss: 3.9750
                    Surrogate loss: -0.0046
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 51.32
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 8.45s
                        Total time: 9185.13s
                               ETA: 962796.2s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.059s, learning 0.164s)
               Value function loss: 0.2698
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 3.81
               Mean episode length: 51.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 8.22s
                        Total time: 9193.35s
                               ETA: 962629.7s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 1936 steps/s (collection: 8.290s, learning 0.172s)
               Value function loss: 120.1665
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.01
               Mean episode length: 49.64
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 8.46s
                        Total time: 9201.82s
                               ETA: 962488.6s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 1973 steps/s (collection: 8.131s, learning 0.171s)
               Value function loss: 0.3018
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 3.49
               Mean episode length: 49.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 8.30s
                        Total time: 9210.12s
                               ETA: 962331.0s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 2005 steps/s (collection: 8.006s, learning 0.164s)
               Value function loss: 0.2981
                    Surrogate loss: -0.0324
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 52.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 8.17s
                        Total time: 9218.29s
                               ETA: 962160.0s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 1906 steps/s (collection: 8.383s, learning 0.210s)
               Value function loss: 0.2949
                    Surrogate loss: -0.0323
             Mean action noise std: 0.72
                       Mean reward: 3.59
               Mean episode length: 52.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 8.59s
                        Total time: 9226.88s
                               ETA: 962033.4s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 1900 steps/s (collection: 8.449s, learning 0.170s)
               Value function loss: 52.7388
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 3.31
               Mean episode length: 50.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 8.62s
                        Total time: 9235.50s
                               ETA: 961909.9s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 1889 steps/s (collection: 8.505s, learning 0.164s)
               Value function loss: 82.8746
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 2.87
               Mean episode length: 50.15
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 8.67s
                        Total time: 9244.17s
                               ETA: 961791.8s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.175s)
               Value function loss: 45.0030
                    Surrogate loss: -0.0072
             Mean action noise std: 0.72
                       Mean reward: 8.08
               Mean episode length: 51.34
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 8.46s
                        Total time: 9252.63s
                               ETA: 961651.8s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.313s, learning 0.160s)
               Value function loss: 18.4658
                    Surrogate loss: -0.0068
             Mean action noise std: 0.72
                       Mean reward: 18.36
               Mean episode length: 51.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 8.47s
                        Total time: 9261.10s
                               ETA: 961513.8s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.164s)
               Value function loss: 28.7192
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 3.96
               Mean episode length: 53.03
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 8.37s
                        Total time: 9269.47s
                               ETA: 961364.9s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.303s, learning 0.173s)
               Value function loss: 0.9777
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 3.23
               Mean episode length: 51.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 8.48s
                        Total time: 9277.94s
                               ETA: 961227.6s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 1907 steps/s (collection: 8.413s, learning 0.174s)
               Value function loss: 13.5283
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 3.73
               Mean episode length: 52.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 8.59s
                        Total time: 9286.53s
                               ETA: 961102.2s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 1998 steps/s (collection: 8.030s, learning 0.168s)
               Value function loss: 0.8544
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 3.70
               Mean episode length: 52.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 8.20s
                        Total time: 9294.73s
                               ETA: 960936.9s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 1981 steps/s (collection: 8.099s, learning 0.167s)
               Value function loss: 0.5436
                    Surrogate loss: -0.0312
             Mean action noise std: 0.72
                       Mean reward: 3.18
               Mean episode length: 51.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 8.27s
                        Total time: 9302.99s
                               ETA: 960779.0s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 1876 steps/s (collection: 8.572s, learning 0.161s)
               Value function loss: 0.4258
                    Surrogate loss: -0.0345
             Mean action noise std: 0.72
                       Mean reward: 6.22
               Mean episode length: 53.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 8.73s
                        Total time: 9311.73s
                               ETA: 960669.4s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.288s, learning 0.159s)
               Value function loss: 131.0339
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 52.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 8.45s
                        Total time: 9320.17s
                               ETA: 960530.6s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 1898 steps/s (collection: 8.460s, learning 0.171s)
               Value function loss: 0.7875
                    Surrogate loss: -0.0317
             Mean action noise std: 0.72
                       Mean reward: 2.96
               Mean episode length: 51.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 8.63s
                        Total time: 9328.80s
                               ETA: 960411.0s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 2017 steps/s (collection: 7.962s, learning 0.157s)
               Value function loss: 18.1388
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.04
               Mean episode length: 51.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 8.12s
                        Total time: 9336.92s
                               ETA: 960239.0s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 1954 steps/s (collection: 8.223s, learning 0.159s)
               Value function loss: 229.5094
                    Surrogate loss: -0.0018
             Mean action noise std: 0.72
                       Mean reward: 3.78
               Mean episode length: 53.09
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 8.38s
                        Total time: 9345.30s
                               ETA: 960094.3s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.174s, learning 0.166s)
               Value function loss: 86.3857
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 3.03
               Mean episode length: 50.91
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 8.34s
                        Total time: 9353.64s
                               ETA: 959945.5s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 1964 steps/s (collection: 8.177s, learning 0.163s)
               Value function loss: 7.3454
                    Surrogate loss: -0.0191
             Mean action noise std: 0.72
                       Mean reward: 8.62
               Mean episode length: 51.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 8.34s
                        Total time: 9361.98s
                               ETA: 959797.2s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 1917 steps/s (collection: 8.302s, learning 0.242s)
               Value function loss: 45.5095
                    Surrogate loss: 0.0020
             Mean action noise std: 0.72
                       Mean reward: 2.73
               Mean episode length: 50.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 8.54s
                        Total time: 9370.53s
                               ETA: 959670.0s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 1993 steps/s (collection: 8.052s, learning 0.167s)
               Value function loss: 1.5427
                    Surrogate loss: -0.0254
             Mean action noise std: 0.72
                       Mean reward: 10.55
               Mean episode length: 51.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 8.22s
                        Total time: 9378.75s
                               ETA: 959509.8s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 1977 steps/s (collection: 8.121s, learning 0.163s)
               Value function loss: 18.5310
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 8.49
               Mean episode length: 53.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 8.28s
                        Total time: 9387.03s
                               ETA: 959356.6s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 1937 steps/s (collection: 8.288s, learning 0.171s)
               Value function loss: 4.9884
                    Surrogate loss: -0.0136
             Mean action noise std: 0.72
                       Mean reward: 2.98
               Mean episode length: 50.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 8.46s
                        Total time: 9395.49s
                               ETA: 959221.4s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 1904 steps/s (collection: 8.438s, learning 0.164s)
               Value function loss: 0.9246
                    Surrogate loss: -0.0300
             Mean action noise std: 0.72
                       Mean reward: 3.04
               Mean episode length: 50.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 8.60s
                        Total time: 9404.09s
                               ETA: 959101.2s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 1997 steps/s (collection: 7.997s, learning 0.204s)
               Value function loss: 61.9371
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 7.95
               Mean episode length: 50.15
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 8.20s
                        Total time: 9412.29s
                               ETA: 958940.3s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.285s, learning 0.165s)
               Value function loss: 1.0292
                    Surrogate loss: -0.0300
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 51.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 8.45s
                        Total time: 9420.74s
                               ETA: 958805.0s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.292s, learning 0.174s)
               Value function loss: 38.2942
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.03
               Mean episode length: 52.14
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 8.47s
                        Total time: 9429.21s
                               ETA: 958671.7s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1980 steps/s (collection: 8.107s, learning 0.166s)
               Value function loss: 114.5549
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 51.61
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 8.27s
                        Total time: 9437.48s
                               ETA: 958519.0s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1996 steps/s (collection: 8.046s, learning 0.160s)
               Value function loss: 1.4149
                    Surrogate loss: -0.0226
             Mean action noise std: 0.72
                       Mean reward: 3.47
               Mean episode length: 51.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 8.21s
                        Total time: 9445.69s
                               ETA: 958359.8s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1920 steps/s (collection: 8.365s, learning 0.168s)
               Value function loss: 0.7696
                    Surrogate loss: -0.0253
             Mean action noise std: 0.72
                       Mean reward: 3.33
               Mean episode length: 51.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 8.53s
                        Total time: 9454.22s
                               ETA: 958234.0s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.178s, learning 0.251s)
               Value function loss: 17.6024
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 3.57
               Mean episode length: 50.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 8.43s
                        Total time: 9462.65s
                               ETA: 958098.0s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.351s, learning 0.166s)
               Value function loss: 0.5775
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 3.03
               Mean episode length: 51.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 8.52s
                        Total time: 9471.17s
                               ETA: 957971.1s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1935 steps/s (collection: 8.301s, learning 0.163s)
               Value function loss: 0.5872
                    Surrogate loss: -0.0287
             Mean action noise std: 0.72
                       Mean reward: 3.28
               Mean episode length: 50.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 8.46s
                        Total time: 9479.63s
                               ETA: 957839.2s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1923 steps/s (collection: 8.345s, learning 0.172s)
               Value function loss: 0.4402
                    Surrogate loss: -0.0315
             Mean action noise std: 0.72
                       Mean reward: 6.31
               Mean episode length: 53.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 8.52s
                        Total time: 9488.15s
                               ETA: 957712.9s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1933 steps/s (collection: 8.300s, learning 0.172s)
               Value function loss: 0.3035
                    Surrogate loss: -0.0304
             Mean action noise std: 0.72
                       Mean reward: 3.77
               Mean episode length: 52.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 8.47s
                        Total time: 9496.62s
                               ETA: 957582.3s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1987 steps/s (collection: 8.068s, learning 0.175s)
               Value function loss: 0.3003
                    Surrogate loss: -0.0322
             Mean action noise std: 0.72
                       Mean reward: 4.33
               Mean episode length: 52.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 8.24s
                        Total time: 9504.86s
                               ETA: 957428.8s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.049s, learning 0.175s)
               Value function loss: 17.6244
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.21
               Mean episode length: 51.51
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 8.22s
                        Total time: 9513.09s
                               ETA: 957273.6s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1958 steps/s (collection: 8.193s, learning 0.172s)
               Value function loss: 4.1728
                    Surrogate loss: -0.0091
             Mean action noise std: 0.72
                       Mean reward: 3.10
               Mean episode length: 51.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 8.37s
                        Total time: 9521.45s
                               ETA: 957133.0s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1990 steps/s (collection: 8.066s, learning 0.167s)
               Value function loss: 0.3754
                    Surrogate loss: -0.0347
             Mean action noise std: 0.72
                       Mean reward: 3.58
               Mean episode length: 52.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 8.23s
                        Total time: 9529.68s
                               ETA: 956979.4s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.170s)
               Value function loss: 0.3240
                    Surrogate loss: -0.0302
             Mean action noise std: 0.72
                       Mean reward: 3.25
               Mean episode length: 50.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 8.35s
                        Total time: 9538.03s
                               ETA: 956837.4s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.167s)
               Value function loss: 98.0996
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 8.40
               Mean episode length: 52.33
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 8.47s
                        Total time: 9546.50s
                               ETA: 956708.2s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1926 steps/s (collection: 8.328s, learning 0.175s)
               Value function loss: 18.1217
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 3.91
               Mean episode length: 52.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 8.50s
                        Total time: 9555.00s
                               ETA: 956582.5s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1884 steps/s (collection: 8.482s, learning 0.212s)
               Value function loss: 0.4164
                    Surrogate loss: -0.0301
             Mean action noise std: 0.72
                       Mean reward: 3.98
               Mean episode length: 53.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 8.69s
                        Total time: 9563.70s
                               ETA: 956476.1s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 2011 steps/s (collection: 7.984s, learning 0.161s)
               Value function loss: 120.0322
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 16.37
               Mean episode length: 52.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 8.15s
                        Total time: 9571.84s
                               ETA: 956315.1s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1918 steps/s (collection: 8.377s, learning 0.164s)
               Value function loss: 186.9799
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 52.19
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 8.54s
                        Total time: 9580.38s
                               ETA: 956193.8s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1943 steps/s (collection: 8.264s, learning 0.166s)
               Value function loss: 5.1691
                    Surrogate loss: -0.0279
             Mean action noise std: 0.72
                       Mean reward: 2.70
               Mean episode length: 50.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 8.43s
                        Total time: 9588.81s
                               ETA: 956061.8s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1912 steps/s (collection: 8.392s, learning 0.177s)
               Value function loss: 104.8673
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 10.99
               Mean episode length: 53.02
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 8.57s
                        Total time: 9597.38s
                               ETA: 955943.8s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1992 steps/s (collection: 8.048s, learning 0.174s)
               Value function loss: 1.6419
                    Surrogate loss: -0.0256
             Mean action noise std: 0.72
                       Mean reward: 15.74
               Mean episode length: 53.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 8.22s
                        Total time: 9605.61s
                               ETA: 955791.5s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 2027 steps/s (collection: 7.905s, learning 0.177s)
               Value function loss: 1.1719
                    Surrogate loss: -0.0237
             Mean action noise std: 0.72
                       Mean reward: 3.88
               Mean episode length: 52.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 8.08s
                        Total time: 9613.69s
                               ETA: 955625.6s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1949 steps/s (collection: 8.245s, learning 0.160s)
               Value function loss: 0.6377
                    Surrogate loss: -0.0264
             Mean action noise std: 0.72
                       Mean reward: 3.36
               Mean episode length: 51.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 8.40s
                        Total time: 9622.09s
                               ETA: 955492.1s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1948 steps/s (collection: 8.238s, learning 0.170s)
               Value function loss: 0.4844
                    Surrogate loss: -0.0273
             Mean action noise std: 0.72
                       Mean reward: 3.27
               Mean episode length: 51.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 8.41s
                        Total time: 9630.50s
                               ETA: 955359.1s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1984 steps/s (collection: 8.086s, learning 0.170s)
               Value function loss: 0.4132
                    Surrogate loss: -0.0286
             Mean action noise std: 0.72
                       Mean reward: 3.07
               Mean episode length: 50.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 8.26s
                        Total time: 9638.76s
                               ETA: 955211.3s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.165s)
               Value function loss: 46.1862
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 3.39
               Mean episode length: 51.15
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 8.45s
                        Total time: 9647.20s
                               ETA: 955082.9s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.230s, learning 0.160s)
               Value function loss: 0.5178
                    Surrogate loss: -0.0302
             Mean action noise std: 0.72
                       Mean reward: 3.19
               Mean episode length: 52.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 8.39s
                        Total time: 9655.59s
                               ETA: 954948.8s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.867s, learning 0.221s)
               Value function loss: 279.0891
                    Surrogate loss: -0.0005
             Mean action noise std: 0.72
                       Mean reward: 3.25
               Mean episode length: 51.70
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 8.09s
                        Total time: 9663.68s
                               ETA: 954785.2s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.282s, learning 0.187s)
               Value function loss: 0.7000
                    Surrogate loss: -0.0295
             Mean action noise std: 0.72
                       Mean reward: 3.35
               Mean episode length: 50.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 8.47s
                        Total time: 9672.15s
                               ETA: 954659.5s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.168s, learning 0.179s)
               Value function loss: 133.3463
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.36
               Mean episode length: 52.25
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 8.35s
                        Total time: 9680.50s
                               ETA: 954522.1s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.261s, learning 0.197s)
               Value function loss: 174.2656
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 3.08
               Mean episode length: 51.49
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 8.46s
                        Total time: 9688.96s
                               ETA: 954395.8s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.160s, learning 0.162s)
               Value function loss: 2.2180
                    Surrogate loss: -0.0306
             Mean action noise std: 0.72
                       Mean reward: 3.72
               Mean episode length: 50.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 8.32s
                        Total time: 9697.28s
                               ETA: 954256.4s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.370s, learning 0.162s)
               Value function loss: 1.3877
                    Surrogate loss: -0.0240
             Mean action noise std: 0.72
                       Mean reward: 8.01
               Mean episode length: 49.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 8.53s
                        Total time: 9705.81s
                               ETA: 954138.0s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.458s, learning 0.162s)
               Value function loss: 0.9395
                    Surrogate loss: -0.0267
             Mean action noise std: 0.72
                       Mean reward: 3.61
               Mean episode length: 51.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 8.62s
                        Total time: 9714.43s
                               ETA: 954028.3s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.024s, learning 0.169s)
               Value function loss: 29.6806
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.76
               Mean episode length: 52.59
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 8.19s
                        Total time: 9722.62s
                               ETA: 953877.0s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.422s, learning 0.165s)
               Value function loss: 16.7123
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 3.41
               Mean episode length: 51.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 8.59s
                        Total time: 9731.21s
                               ETA: 953764.5s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.505s, learning 0.165s)
               Value function loss: 1.0164
                    Surrogate loss: -0.0286
             Mean action noise std: 0.72
                       Mean reward: 3.06
               Mean episode length: 50.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 8.67s
                        Total time: 9739.88s
                               ETA: 953660.3s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.166s, learning 0.172s)
               Value function loss: 242.3310
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 3.55
               Mean episode length: 53.12
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 8.34s
                        Total time: 9748.22s
                               ETA: 953524.0s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.176s)
               Value function loss: 2.0339
                    Surrogate loss: -0.0253
             Mean action noise std: 0.72
                       Mean reward: 3.18
               Mean episode length: 51.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 8.70s
                        Total time: 9756.91s
                               ETA: 953423.0s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.291s, learning 0.173s)
               Value function loss: 12.5491
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 52.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 8.46s
                        Total time: 9765.38s
                               ETA: 953299.4s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.036s, learning 0.177s)
               Value function loss: 53.0770
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 8.56
               Mean episode length: 51.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 8.21s
                        Total time: 9773.59s
                               ETA: 953151.4s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.016s, learning 0.162s)
               Value function loss: 75.0405
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 3.50
               Mean episode length: 50.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 8.18s
                        Total time: 9781.77s
                               ETA: 953000.4s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.286s, learning 0.160s)
               Value function loss: 15.6943
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 3.03
               Mean episode length: 50.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 8.45s
                        Total time: 9790.22s
                               ETA: 952875.8s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.405s, learning 0.183s)
               Value function loss: 1.9737
                    Surrogate loss: -0.0250
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 51.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 8.59s
                        Total time: 9798.80s
                               ETA: 952765.1s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.282s, learning 0.206s)
               Value function loss: 148.5735
                    Surrogate loss: 0.0010
             Mean action noise std: 0.72
                       Mean reward: 14.51
               Mean episode length: 53.65
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 8.49s
                        Total time: 9807.29s
                               ETA: 952645.0s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.267s, learning 0.173s)
               Value function loss: 20.6044
                    Surrogate loss: -0.0085
             Mean action noise std: 0.72
                       Mean reward: 3.37
               Mean episode length: 52.16
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 8.44s
                        Total time: 9815.73s
                               ETA: 952520.5s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.198s, learning 0.162s)
               Value function loss: 65.7828
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 3.18
               Mean episode length: 51.43
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 8.36s
                        Total time: 9824.09s
                               ETA: 952388.4s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.166s, learning 0.161s)
               Value function loss: 347.1738
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 13.42
               Mean episode length: 51.07
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 8.33s
                        Total time: 9832.42s
                               ETA: 952253.4s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.486s, learning 0.160s)
               Value function loss: 8.1744
                    Surrogate loss: -0.0254
             Mean action noise std: 0.72
                       Mean reward: 3.09
               Mean episode length: 51.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 8.65s
                        Total time: 9841.06s
                               ETA: 952149.5s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.407s, learning 0.206s)
               Value function loss: 290.5308
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 3.34
               Mean episode length: 50.81
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 8.61s
                        Total time: 9849.68s
                               ETA: 952042.6s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.263s, learning 0.165s)
               Value function loss: 198.6283
                    Surrogate loss: -0.0058
             Mean action noise std: 0.72
                       Mean reward: 13.93
               Mean episode length: 51.73
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 8.43s
                        Total time: 9858.11s
                               ETA: 951918.0s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.013s, learning 0.204s)
               Value function loss: 154.1883
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 2.95
               Mean episode length: 50.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 8.22s
                        Total time: 9866.32s
                               ETA: 951773.3s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.248s, learning 0.170s)
               Value function loss: 251.7178
                    Surrogate loss: -0.0075
             Mean action noise std: 0.72
                       Mean reward: 2.96
               Mean episode length: 50.20
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 8.42s
                        Total time: 9874.74s
                               ETA: 951648.2s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.205s, learning 0.174s)
               Value function loss: 408.6094
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 8.49
               Mean episode length: 50.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 8.38s
                        Total time: 9883.12s
                               ETA: 951519.5s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.506s, learning 0.165s)
               Value function loss: 116.7162
                    Surrogate loss: -0.0129
             Mean action noise std: 0.72
                       Mean reward: 2.58
               Mean episode length: 49.22
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 8.67s
                        Total time: 9891.79s
                               ETA: 951419.3s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.392s, learning 0.165s)
               Value function loss: 121.0340
                    Surrogate loss: -0.0093
             Mean action noise std: 0.72
                       Mean reward: 6.03
               Mean episode length: 50.98
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 8.56s
                        Total time: 9900.35s
                               ETA: 951308.3s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.165s)
               Value function loss: 512.8840
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 8.47
               Mean episode length: 51.78
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 8.48s
                        Total time: 9908.83s
                               ETA: 951189.5s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.325s, learning 0.164s)
               Value function loss: 914.3362
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 2.81
               Mean episode length: 50.13
                  Mean reward/step: 0.38
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 8.49s
                        Total time: 9917.31s
                               ETA: 951072.4s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.180s, learning 0.184s)
               Value function loss: 871.5220
                    Surrogate loss: -0.0041
             Mean action noise std: 0.72
                       Mean reward: 36.37
               Mean episode length: 51.63
                  Mean reward/step: 0.38
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 8.36s
                        Total time: 9925.68s
                               ETA: 950943.4s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.265s, learning 0.163s)
               Value function loss: 327.1526
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 20.69
               Mean episode length: 50.56
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 8.43s
                        Total time: 9934.11s
                               ETA: 950820.8s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.318s, learning 0.170s)
               Value function loss: 319.0984
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 5.58
               Mean episode length: 50.49
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0196
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 8.49s
                        Total time: 9942.59s
                               ETA: 950704.1s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.182s, learning 0.171s)
               Value function loss: 193.3921
                    Surrogate loss: -0.0073
             Mean action noise std: 0.72
                       Mean reward: 12.93
               Mean episode length: 49.84
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0200
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 8.35s
                        Total time: 9950.95s
                               ETA: 950574.8s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.358s, learning 0.172s)
               Value function loss: 977.2769
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 50.51
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0203
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 8.53s
                        Total time: 9959.48s
                               ETA: 950462.5s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.523s, learning 0.168s)
               Value function loss: 503.6877
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 3.53
               Mean episode length: 50.70
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0200
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 8.69s
                        Total time: 9968.17s
                               ETA: 950365.8s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.170s)
               Value function loss: 314.8245
                    Surrogate loss: -0.0101
             Mean action noise std: 0.72
                       Mean reward: 5.58
               Mean episode length: 49.64
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 8.59s
                        Total time: 9976.75s
                               ETA: 950259.4s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.305s, learning 0.181s)
               Value function loss: 34.5515
                    Surrogate loss: -0.0155
             Mean action noise std: 0.72
                       Mean reward: 15.52
               Mean episode length: 52.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 8.49s
                        Total time: 9985.24s
                               ETA: 950143.5s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.192s, learning 0.161s)
               Value function loss: 483.3791
                    Surrogate loss: 0.0009
             Mean action noise std: 0.72
                       Mean reward: 3.20
               Mean episode length: 52.09
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 8.35s
                        Total time: 9993.59s
                               ETA: 950015.2s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.281s, learning 0.206s)
               Value function loss: 187.3523
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 2.46
               Mean episode length: 50.53
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 8.49s
                        Total time: 10002.08s
                               ETA: 949899.9s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.261s, learning 0.165s)
               Value function loss: 30.5520
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 2.69
               Mean episode length: 51.61
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0213
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 8.43s
                        Total time: 10010.50s
                               ETA: 949779.0s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.401s, learning 0.214s)
               Value function loss: 27.1800
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 2.92
               Mean episode length: 50.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 8.61s
                        Total time: 10019.12s
                               ETA: 949676.3s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.168s, learning 0.169s)
               Value function loss: 110.9691
                    Surrogate loss: -0.0089
             Mean action noise std: 0.72
                       Mean reward: 3.38
               Mean episode length: 51.38
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0203
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 8.34s
                        Total time: 10027.46s
                               ETA: 949547.3s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.134s, learning 0.208s)
               Value function loss: 42.5392
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 5.38
               Mean episode length: 50.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0199
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 8.34s
                        Total time: 10035.80s
                               ETA: 949419.2s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.098s, learning 0.161s)
               Value function loss: 152.5158
                    Surrogate loss: -0.0071
             Mean action noise std: 0.72
                       Mean reward: 3.07
               Mean episode length: 51.33
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 8.26s
                        Total time: 10044.06s
                               ETA: 949283.3s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.427s, learning 0.188s)
               Value function loss: 547.2655
                    Surrogate loss: -0.0078
             Mean action noise std: 0.72
                       Mean reward: 2.71
               Mean episode length: 51.11
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 8.62s
                        Total time: 10052.67s
                               ETA: 949181.4s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.970s, learning 0.172s)
               Value function loss: 49.6705
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 3.20
               Mean episode length: 51.46
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 8.14s
                        Total time: 10060.81s
                               ETA: 949034.9s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.068s, learning 0.163s)
               Value function loss: 487.0200
                    Surrogate loss: 0.0028
             Mean action noise std: 0.72
                       Mean reward: 5.66
               Mean episode length: 51.62
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 8.23s
                        Total time: 10069.04s
                               ETA: 948897.1s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.168s)
               Value function loss: 68.2615
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 2.92
               Mean episode length: 50.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0207
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 8.41s
                        Total time: 10077.46s
                               ETA: 948776.8s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.610s, learning 0.166s)
               Value function loss: 8.0089
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 2.87
               Mean episode length: 51.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 8.78s
                        Total time: 10086.23s
                               ETA: 948690.8s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.367s, learning 0.178s)
               Value function loss: 203.6691
                    Surrogate loss: -0.0000
             Mean action noise std: 0.72
                       Mean reward: 15.54
               Mean episode length: 51.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 8.54s
                        Total time: 10094.78s
                               ETA: 948583.2s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.588s, learning 0.163s)
               Value function loss: 27.9195
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 3.20
               Mean episode length: 51.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0194
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 8.75s
                        Total time: 10103.53s
                               ETA: 948495.2s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.266s, learning 0.170s)
               Value function loss: 41.6340
                    Surrogate loss: -0.0076
             Mean action noise std: 0.72
                       Mean reward: 2.66
               Mean episode length: 50.12
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 8.44s
                        Total time: 10111.97s
                               ETA: 948377.8s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.147s, learning 0.174s)
               Value function loss: 253.7969
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 2.62
               Mean episode length: 50.78
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 8.32s
                        Total time: 10120.29s
                               ETA: 948249.7s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.214s, learning 0.165s)
               Value function loss: 162.4771
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 3.62
               Mean episode length: 51.07
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 8.38s
                        Total time: 10128.67s
                               ETA: 948127.4s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.036s, learning 0.165s)
               Value function loss: 31.9409
                    Surrogate loss: -0.0090
             Mean action noise std: 0.72
                       Mean reward: 3.26
               Mean episode length: 51.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 8.20s
                        Total time: 10136.87s
                               ETA: 947988.6s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.252s, learning 0.187s)
               Value function loss: 113.4610
                    Surrogate loss: -0.0054
             Mean action noise std: 0.72
                       Mean reward: 5.46
               Mean episode length: 50.91
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0159
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 8.44s
                        Total time: 10145.30s
                               ETA: 947872.2s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.271s, learning 0.195s)
               Value function loss: 98.4670
                    Surrogate loss: -0.0012
             Mean action noise std: 0.72
                       Mean reward: 2.98
               Mean episode length: 50.38
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 8.47s
                        Total time: 10153.77s
                               ETA: 947758.7s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.279s, learning 0.174s)
               Value function loss: 5.0234
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 10.63
               Mean episode length: 50.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 8.45s
                        Total time: 10162.22s
                               ETA: 947644.1s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.336s, learning 0.160s)
               Value function loss: 3.2767
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 2.49
               Mean episode length: 50.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 8.50s
                        Total time: 10170.72s
                               ETA: 947533.7s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.204s, learning 0.167s)
               Value function loss: 392.4432
                    Surrogate loss: 0.0017
             Mean action noise std: 0.72
                       Mean reward: 10.16
               Mean episode length: 50.92
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 8.37s
                        Total time: 10179.09s
                               ETA: 947411.9s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.164s)
               Value function loss: 163.0140
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 2.54
               Mean episode length: 49.79
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 8.55s
                        Total time: 10187.64s
                               ETA: 947307.3s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.349s, learning 0.211s)
               Value function loss: 202.0321
                    Surrogate loss: -0.0047
             Mean action noise std: 0.72
                       Mean reward: 2.67
               Mean episode length: 52.42
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 8.56s
                        Total time: 10196.20s
                               ETA: 947203.4s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.126s, learning 0.174s)
               Value function loss: 109.4242
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 3.18
               Mean episode length: 51.08
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 8.30s
                        Total time: 10204.50s
                               ETA: 947075.6s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.435s, learning 0.161s)
               Value function loss: 42.7570
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 10.46
               Mean episode length: 52.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 8.60s
                        Total time: 10213.10s
                               ETA: 946975.4s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.236s, learning 0.167s)
               Value function loss: 159.1987
                    Surrogate loss: -0.0064
             Mean action noise std: 0.72
                       Mean reward: 2.56
               Mean episode length: 50.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 8.40s
                        Total time: 10221.50s
                               ETA: 946857.5s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.135s, learning 0.158s)
               Value function loss: 21.7893
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 2.28
               Mean episode length: 50.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 8.29s
                        Total time: 10229.79s
                               ETA: 946729.7s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.161s)
               Value function loss: 33.2572
                    Surrogate loss: -0.0059
             Mean action noise std: 0.72
                       Mean reward: 2.34
               Mean episode length: 51.06
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 8.40s
                        Total time: 10238.20s
                               ETA: 946612.1s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.310s, learning 0.163s)
               Value function loss: 10.2106
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 15.53
               Mean episode length: 51.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 8.47s
                        Total time: 10246.67s
                               ETA: 946501.4s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.092s, learning 0.170s)
               Value function loss: 1.7944
                    Surrogate loss: -0.0169
             Mean action noise std: 0.72
                       Mean reward: 2.57
               Mean episode length: 50.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 8.26s
                        Total time: 10254.93s
                               ETA: 946371.4s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.962s, learning 0.164s)
               Value function loss: 132.8900
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 2.93
               Mean episode length: 51.71
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 8.13s
                        Total time: 10263.06s
                               ETA: 946229.0s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.234s, learning 0.174s)
               Value function loss: 237.1934
                    Surrogate loss: -0.0040
             Mean action noise std: 0.72
                       Mean reward: 2.50
               Mean episode length: 52.30
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 8.41s
                        Total time: 10271.47s
                               ETA: 946112.9s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.955s, learning 0.169s)
               Value function loss: 69.3069
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 3.09
               Mean episode length: 52.29
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 8.12s
                        Total time: 10279.59s
                               ETA: 945970.8s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.205s, learning 0.161s)
               Value function loss: 2.2037
                    Surrogate loss: -0.0262
             Mean action noise std: 0.72
                       Mean reward: 2.30
               Mean episode length: 52.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 8.37s
                        Total time: 10287.95s
                               ETA: 945851.2s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.109s, learning 0.212s)
               Value function loss: 552.0819
                    Surrogate loss: -0.0015
             Mean action noise std: 0.72
                       Mean reward: 3.32
               Mean episode length: 52.30
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 8.32s
                        Total time: 10296.28s
                               ETA: 945727.7s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.883s, learning 0.164s)
               Value function loss: 2.8404
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 3.14
               Mean episode length: 53.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 8.05s
                        Total time: 10304.32s
                               ETA: 945579.3s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.395s, learning 0.172s)
               Value function loss: 124.2680
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 3.22
               Mean episode length: 52.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 8.57s
                        Total time: 10312.89s
                               ETA: 945478.8s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.513s, learning 0.161s)
               Value function loss: 131.3427
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 28.31
               Mean episode length: 52.00
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 8.67s
                        Total time: 10321.56s
                               ETA: 945388.4s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.155s, learning 0.173s)
               Value function loss: 57.7177
                    Surrogate loss: -0.0103
             Mean action noise std: 0.72
                       Mean reward: 2.30
               Mean episode length: 50.05
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 8.33s
                        Total time: 10329.89s
                               ETA: 945266.4s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.306s, learning 0.160s)
               Value function loss: 24.9274
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 2.49
               Mean episode length: 51.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 8.47s
                        Total time: 10338.36s
                               ETA: 945157.2s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.176s, learning 0.235s)
               Value function loss: 163.0248
                    Surrogate loss: -0.0033
             Mean action noise std: 0.72
                       Mean reward: 5.70
               Mean episode length: 51.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 8.41s
                        Total time: 10346.77s
                               ETA: 945043.2s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.323s, learning 0.207s)
               Value function loss: 8.1458
                    Surrogate loss: -0.0189
             Mean action noise std: 0.72
                       Mean reward: 2.89
               Mean episode length: 51.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 8.53s
                        Total time: 10355.30s
                               ETA: 944940.3s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.325s, learning 0.164s)
               Value function loss: 209.7646
                    Surrogate loss: 0.0015
             Mean action noise std: 0.72
                       Mean reward: 8.00
               Mean episode length: 52.94
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 8.49s
                        Total time: 10363.79s
                               ETA: 944833.8s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.336s, learning 0.161s)
               Value function loss: 382.9960
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 8.17
               Mean episode length: 52.58
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 8.50s
                        Total time: 10372.29s
                               ETA: 944728.1s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.078s, learning 0.245s)
               Value function loss: 17.7980
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 2.98
               Mean episode length: 53.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 8.32s
                        Total time: 10380.61s
                               ETA: 944606.9s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.136s, learning 0.205s)
               Value function loss: 4.5047
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 2.50
               Mean episode length: 52.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 8.34s
                        Total time: 10388.95s
                               ETA: 944487.5s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.391s, learning 0.161s)
               Value function loss: 1.2650
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 27.75
               Mean episode length: 50.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 8.55s
                        Total time: 10397.50s
                               ETA: 944387.5s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.448s, learning 0.267s)
               Value function loss: 1.4477
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 2.80
               Mean episode length: 52.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 16.71s
                        Total time: 10414.22s
                               ETA: 945028.3s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.105s, learning 0.170s)
               Value function loss: 1.2386
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: 2.61
               Mean episode length: 54.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 16.27s
                        Total time: 10430.49s
                               ETA: 945628.0s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.177s, learning 0.161s)
               Value function loss: 0.6311
                    Surrogate loss: -0.0132
             Mean action noise std: 0.72
                       Mean reward: 2.52
               Mean episode length: 51.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 16.34s
                        Total time: 10446.83s
                               ETA: 946232.3s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.450s, learning 0.168s)
               Value function loss: 0.6506
                    Surrogate loss: -0.0157
             Mean action noise std: 0.72
                       Mean reward: 2.41
               Mean episode length: 53.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 16.62s
                        Total time: 10463.45s
                               ETA: 946860.8s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.183s, learning 0.177s)
               Value function loss: 0.4207
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 2.30
               Mean episode length: 51.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 16.36s
                        Total time: 10479.81s
                               ETA: 947464.8s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.216s, learning 0.177s)
               Value function loss: 0.3512
                    Surrogate loss: -0.0202
             Mean action noise std: 0.72
                       Mean reward: 2.78
               Mean episode length: 53.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 16.39s
                        Total time: 10496.20s
                               ETA: 948070.7s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.166s, learning 0.165s)
               Value function loss: 132.9038
                    Surrogate loss: -0.0011
             Mean action noise std: 0.72
                       Mean reward: 3.23
               Mean episode length: 52.08
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 16.33s
                        Total time: 10512.53s
                               ETA: 948669.9s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.243s, learning 0.164s)
               Value function loss: 17.6149
                    Surrogate loss: -0.0024
             Mean action noise std: 0.72
                       Mean reward: 1.94
               Mean episode length: 51.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 16.41s
                        Total time: 10528.94s
                               ETA: 949274.7s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.986s, learning 0.162s)
               Value function loss: 504.8482
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 2.79
               Mean episode length: 51.90
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 16.15s
                        Total time: 10545.09s
                               ETA: 949855.2s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.272s, learning 0.162s)
               Value function loss: 80.4308
                    Surrogate loss: -0.0114
             Mean action noise std: 0.72
                       Mean reward: 2.08
               Mean episode length: 52.97
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 16.43s
                        Total time: 10561.52s
                               ETA: 950460.2s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.072s, learning 0.205s)
               Value function loss: 9.3304
                    Surrogate loss: 0.0047
             Mean action noise std: 0.72
                       Mean reward: 1.97
               Mean episode length: 50.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 16.28s
                        Total time: 10577.80s
                               ETA: 951050.1s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.330s, learning 0.170s)
               Value function loss: 247.6777
                    Surrogate loss: 0.0021
             Mean action noise std: 0.72
                       Mean reward: 5.00
               Mean episode length: 51.91
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 16.50s
                        Total time: 10594.30s
                               ETA: 951658.8s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.287s, learning 0.171s)
               Value function loss: 92.8970
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 2.31
               Mean episode length: 52.63
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 16.46s
                        Total time: 10610.76s
                               ETA: 952262.7s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.138s, learning 0.171s)
               Value function loss: 6.5168
                    Surrogate loss: -0.0147
             Mean action noise std: 0.72
                       Mean reward: 10.08
               Mean episode length: 54.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 16.31s
                        Total time: 10627.07s
                               ETA: 952852.0s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.073s, learning 0.170s)
               Value function loss: 293.6830
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 2.42
               Mean episode length: 53.87
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 16.24s
                        Total time: 10643.31s
                               ETA: 953434.3s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1028 steps/s (collection: 15.756s, learning 0.168s)
               Value function loss: 7.8194
                    Surrogate loss: -0.0148
             Mean action noise std: 0.72
                       Mean reward: 2.44
               Mean episode length: 53.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 15.92s
                        Total time: 10659.23s
                               ETA: 953987.0s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.221s, learning 0.268s)
               Value function loss: 1.7608
                    Surrogate loss: -0.0170
             Mean action noise std: 0.72
                       Mean reward: 2.69
               Mean episode length: 54.82
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 16.49s
                        Total time: 10675.72s
                               ETA: 954589.3s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.415s, learning 0.167s)
               Value function loss: 18.9318
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 2.09
               Mean episode length: 53.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 16.58s
                        Total time: 10692.31s
                               ETA: 955198.7s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.096s, learning 0.198s)
               Value function loss: 4.6228
                    Surrogate loss: -0.0094
             Mean action noise std: 0.72
                       Mean reward: 2.57
               Mean episode length: 52.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 16.29s
                        Total time: 10708.60s
                               ETA: 955781.3s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.407s, learning 0.166s)
               Value function loss: 0.6532
                    Surrogate loss: -0.0198
             Mean action noise std: 0.72
                       Mean reward: 2.24
               Mean episode length: 54.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 16.57s
                        Total time: 10725.17s
                               ETA: 956387.7s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.193s, learning 0.166s)
               Value function loss: 0.5321
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 7.34
               Mean episode length: 52.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 16.36s
                        Total time: 10741.53s
                               ETA: 956973.8s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.115s, learning 0.162s)
               Value function loss: 0.4547
                    Surrogate loss: -0.0119
             Mean action noise std: 0.72
                       Mean reward: 2.14
               Mean episode length: 53.35
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 16.28s
                        Total time: 10757.81s
                               ETA: 957551.6s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.388s, learning 0.170s)
               Value function loss: 0.3737
                    Surrogate loss: -0.0128
             Mean action noise std: 0.72
                       Mean reward: 2.00
               Mean episode length: 53.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 16.56s
                        Total time: 10774.37s
                               ETA: 958153.2s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.800s, learning 0.168s)
               Value function loss: 54.7379
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 54.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 15.97s
                        Total time: 10790.33s
                               ETA: 958701.3s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.281s, learning 0.169s)
               Value function loss: 0.2553
                    Surrogate loss: -0.0227
             Mean action noise std: 0.72
                       Mean reward: 1.92
               Mean episode length: 53.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 16.45s
                        Total time: 10806.78s
                               ETA: 959291.3s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.938s, learning 0.171s)
               Value function loss: 0.2830
                    Surrogate loss: -0.0201
             Mean action noise std: 0.72
                       Mean reward: 2.20
               Mean episode length: 51.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 16.11s
                        Total time: 10822.89s
                               ETA: 959850.0s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.319s, learning 0.168s)
               Value function loss: 120.9262
                    Surrogate loss: -0.0003
             Mean action noise std: 0.72
                       Mean reward: 1.85
               Mean episode length: 52.15
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 16.49s
                        Total time: 10839.38s
                               ETA: 960441.0s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.032s, learning 0.170s)
               Value function loss: 28.8937
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 54.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 16.20s
                        Total time: 10855.58s
                               ETA: 961005.7s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.059s, learning 0.167s)
               Value function loss: 0.2755
                    Surrogate loss: -0.0019
             Mean action noise std: 0.72
                       Mean reward: 2.14
               Mean episode length: 52.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 16.23s
                        Total time: 10871.81s
                               ETA: 961571.6s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.823s, learning 0.164s)
               Value function loss: 195.5295
                    Surrogate loss: 0.0018
             Mean action noise std: 0.72
                       Mean reward: 1.91
               Mean episode length: 54.67
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 15.99s
                        Total time: 10887.80s
                               ETA: 962115.4s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.700s, learning 0.213s)
               Value function loss: 538.3784
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 6.92
               Mean episode length: 52.78
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 15.91s
                        Total time: 10903.71s
                               ETA: 962651.5s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.890s, learning 0.166s)
               Value function loss: 15.7560
                    Surrogate loss: -0.0097
             Mean action noise std: 0.72
                       Mean reward: 1.58
               Mean episode length: 52.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 16.06s
                        Total time: 10919.76s
                               ETA: 963199.2s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.076s, learning 0.164s)
               Value function loss: 204.0527
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 53.02
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 16.24s
                        Total time: 10936.01s
                               ETA: 963762.3s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.000s, learning 0.176s)
               Value function loss: 316.7088
                    Surrogate loss: -0.0042
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 54.11
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 16.18s
                        Total time: 10952.18s
                               ETA: 964318.7s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.707s, learning 0.172s)
               Value function loss: 13.7971
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 6.91
               Mean episode length: 53.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 15.88s
                        Total time: 10968.06s
                               ETA: 964847.8s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.144s, learning 0.177s)
               Value function loss: 438.3944
                    Surrogate loss: 0.0004
             Mean action noise std: 0.72
                       Mean reward: 1.82
               Mean episode length: 53.55
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 16.32s
                        Total time: 10984.38s
                               ETA: 965414.9s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.220s, learning 0.176s)
               Value function loss: 247.0370
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 1.69
               Mean episode length: 53.59
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 16.40s
                        Total time: 11000.78s
                               ETA: 965987.5s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1257 steps/s (collection: 12.849s, learning 0.177s)
               Value function loss: 15.9841
                    Surrogate loss: -0.0130
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 54.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 13.03s
                        Total time: 11013.80s
                               ETA: 966263.4s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.215s, learning 0.167s)
               Value function loss: 21.2051
                    Surrogate loss: -0.0083
             Mean action noise std: 0.72
                       Mean reward: 1.64
               Mean episode length: 52.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 8.38s
                        Total time: 11022.19s
                               ETA: 966131.7s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.307s, learning 0.160s)
               Value function loss: 9.8849
                    Surrogate loss: -0.0109
             Mean action noise std: 0.72
                       Mean reward: 2.42
               Mean episode length: 54.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 8.47s
                        Total time: 11030.65s
                               ETA: 966007.6s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.975s, learning 0.172s)
               Value function loss: 8.6732
                    Surrogate loss: -0.0100
             Mean action noise std: 0.72
                       Mean reward: 2.32
               Mean episode length: 54.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 8.15s
                        Total time: 11038.80s
                               ETA: 965855.8s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.112s, learning 0.170s)
               Value function loss: 10.6195
                    Surrogate loss: -0.0061
             Mean action noise std: 0.72
                       Mean reward: 1.70
               Mean episode length: 52.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 8.28s
                        Total time: 11047.08s
                               ETA: 965716.0s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.123s, learning 0.169s)
               Value function loss: 47.5358
                    Surrogate loss: -0.0016
             Mean action noise std: 0.72
                       Mean reward: 1.82
               Mean episode length: 53.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 8.29s
                        Total time: 11055.37s
                               ETA: 965577.4s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.167s, learning 0.169s)
               Value function loss: 10.3028
                    Surrogate loss: -0.0043
             Mean action noise std: 0.72
                       Mean reward: 4.73
               Mean episode length: 53.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 8.34s
                        Total time: 11063.71s
                               ETA: 965442.8s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.170s)
               Value function loss: 450.0658
                    Surrogate loss: -0.0022
             Mean action noise std: 0.72
                       Mean reward: 1.78
               Mean episode length: 53.04
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 8.38s
                        Total time: 11072.09s
                               ETA: 965312.4s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.344s, learning 0.174s)
               Value function loss: 261.5783
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 54.87
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 8.52s
                        Total time: 11080.61s
                               ETA: 965194.1s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.164s)
               Value function loss: 118.7210
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: 1.91
               Mean episode length: 53.15
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 8.48s
                        Total time: 11089.09s
                               ETA: 965072.7s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.295s, learning 0.160s)
               Value function loss: 1.3353
                    Surrogate loss: -0.0257
             Mean action noise std: 0.72
                       Mean reward: 11.85
               Mean episode length: 53.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 8.45s
                        Total time: 11097.54s
                               ETA: 964949.3s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.163s)
               Value function loss: 25.1079
                    Surrogate loss: 0.0029
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 53.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 8.23s
                        Total time: 11105.77s
                               ETA: 964806.5s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.244s, learning 0.170s)
               Value function loss: 381.4482
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 22.17
               Mean episode length: 54.22
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 8.41s
                        Total time: 11114.18s
                               ETA: 964680.0s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.402s, learning 0.186s)
               Value function loss: 106.1055
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 1.90
               Mean episode length: 53.88
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 8.59s
                        Total time: 11122.77s
                               ETA: 964568.8s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.091s, learning 0.181s)
               Value function loss: 2.0903
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 9.46
               Mean episode length: 52.95
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 8.27s
                        Total time: 11131.04s
                               ETA: 964430.4s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.182s, learning 0.171s)
               Value function loss: 11.2557
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 14.55
               Mean episode length: 51.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 8.35s
                        Total time: 11139.40s
                               ETA: 964299.3s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.083s, learning 0.168s)
               Value function loss: 164.3586
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 2.02
               Mean episode length: 51.74
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 8.25s
                        Total time: 11147.65s
                               ETA: 964159.5s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.219s, learning 0.177s)
               Value function loss: 682.7661
                    Surrogate loss: -0.0037
             Mean action noise std: 0.72
                       Mean reward: 2.02
               Mean episode length: 53.16
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 8.40s
                        Total time: 11156.04s
                               ETA: 964032.5s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.162s)
               Value function loss: 489.5442
                    Surrogate loss: -0.0044
             Mean action noise std: 0.72
                       Mean reward: 46.99
               Mean episode length: 52.25
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 8.40s
                        Total time: 11164.45s
                               ETA: 963906.3s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.021s, learning 0.165s)
               Value function loss: 20.8318
                    Surrogate loss: -0.0225
             Mean action noise std: 0.72
                       Mean reward: 2.03
               Mean episode length: 51.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 8.19s
                        Total time: 11172.63s
                               ETA: 963761.5s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.166s)
               Value function loss: 11.4203
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 1.87
               Mean episode length: 51.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 8.20s
                        Total time: 11180.83s
                               ETA: 963618.1s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.154s, learning 0.180s)
               Value function loss: 64.3171
                    Surrogate loss: 0.0047
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 52.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 8.33s
                        Total time: 11189.16s
                               ETA: 963486.5s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.331s, learning 0.168s)
               Value function loss: 404.7925
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 53.93
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 8.50s
                        Total time: 11197.66s
                               ETA: 963369.5s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.101s, learning 0.170s)
               Value function loss: 13.6908
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 1.92
               Mean episode length: 53.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 8.27s
                        Total time: 11205.93s
                               ETA: 963232.9s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.954s, learning 0.169s)
               Value function loss: 82.7402
                    Surrogate loss: 0.0027
             Mean action noise std: 0.72
                       Mean reward: 2.21
               Mean episode length: 53.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 8.12s
                        Total time: 11214.06s
                               ETA: 963084.0s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.240s, learning 0.166s)
               Value function loss: 105.4702
                    Surrogate loss: -0.0065
             Mean action noise std: 0.72
                       Mean reward: 7.25
               Mean episode length: 54.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 8.41s
                        Total time: 11222.46s
                               ETA: 962959.6s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.422s, learning 0.166s)
               Value function loss: 224.0006
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 1.85
               Mean episode length: 53.47
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 8.59s
                        Total time: 11231.05s
                               ETA: 962850.9s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.161s, learning 0.205s)
               Value function loss: 419.1467
                    Surrogate loss: -0.0050
             Mean action noise std: 0.72
                       Mean reward: 6.79
               Mean episode length: 52.70
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 8.37s
                        Total time: 11239.42s
                               ETA: 962723.5s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.369s, learning 0.173s)
               Value function loss: 8.2888
                    Surrogate loss: -0.0229
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 52.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 8.54s
                        Total time: 11247.96s
                               ETA: 962611.2s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.232s, learning 0.168s)
               Value function loss: 3.7466
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 1.80
               Mean episode length: 54.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 8.40s
                        Total time: 11256.36s
                               ETA: 962487.0s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.212s, learning 0.158s)
               Value function loss: 66.0435
                    Surrogate loss: 0.0007
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 54.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 8.37s
                        Total time: 11264.73s
                               ETA: 962360.5s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.169s, learning 0.159s)
               Value function loss: 2.3908
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 2.03
               Mean episode length: 53.67
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 8.33s
                        Total time: 11273.06s
                               ETA: 962230.6s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.166s, learning 0.212s)
               Value function loss: 68.0511
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 54.44
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 8.38s
                        Total time: 11281.44s
                               ETA: 962105.2s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.372s, learning 0.166s)
               Value function loss: 61.2507
                    Surrogate loss: -0.0039
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 53.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 8.54s
                        Total time: 11289.98s
                               ETA: 961993.6s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.170s)
               Value function loss: 573.1981
                    Surrogate loss: -0.0025
             Mean action noise std: 0.72
                       Mean reward: 6.94
               Mean episode length: 53.87
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 8.67s
                        Total time: 11298.64s
                               ETA: 961893.3s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.195s, learning 0.172s)
               Value function loss: 29.0717
                    Surrogate loss: -0.0127
             Mean action noise std: 0.72
                       Mean reward: 9.21
               Mean episode length: 53.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 8.37s
                        Total time: 11307.01s
                               ETA: 961767.4s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.205s, learning 0.161s)
               Value function loss: 9.2238
                    Surrogate loss: -0.0163
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 53.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 8.37s
                        Total time: 11315.38s
                               ETA: 961641.7s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.113s, learning 0.160s)
               Value function loss: 3.7860
                    Surrogate loss: -0.0188
             Mean action noise std: 0.72
                       Mean reward: 9.58
               Mean episode length: 54.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 8.27s
                        Total time: 11323.65s
                               ETA: 961508.3s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.201s, learning 0.161s)
               Value function loss: 1.2824
                    Surrogate loss: -0.0177
             Mean action noise std: 0.72
                       Mean reward: 1.77
               Mean episode length: 53.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 8.36s
                        Total time: 11332.01s
                               ETA: 961382.7s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.214s, learning 0.164s)
               Value function loss: 16.0681
                    Surrogate loss: 0.0033
             Mean action noise std: 0.72
                       Mean reward: 2.06
               Mean episode length: 54.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 8.38s
                        Total time: 11340.39s
                               ETA: 961258.7s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.986s, learning 0.162s)
               Value function loss: 17.2532
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 53.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 8.15s
                        Total time: 11348.54s
                               ETA: 961115.3s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.037s, learning 0.161s)
               Value function loss: 18.2235
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 54.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 8.20s
                        Total time: 11356.74s
                               ETA: 960976.4s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.125s, learning 0.163s)
               Value function loss: 586.0877
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 1.74
               Mean episode length: 54.41
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 8.29s
                        Total time: 11365.03s
                               ETA: 960845.3s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.395s, learning 0.183s)
               Value function loss: 582.4819
                    Surrogate loss: -0.0035
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 54.29
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 8.58s
                        Total time: 11373.60s
                               ETA: 960738.9s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.202s, learning 0.175s)
               Value function loss: 70.8718
                    Surrogate loss: -0.0143
             Mean action noise std: 0.72
                       Mean reward: 19.16
               Mean episode length: 53.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 8.38s
                        Total time: 11381.98s
                               ETA: 960615.8s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.201s, learning 0.208s)
               Value function loss: 9.5374
                    Surrogate loss: -0.0154
             Mean action noise std: 0.72
                       Mean reward: 21.78
               Mean episode length: 54.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 8.41s
                        Total time: 11390.39s
                               ETA: 960495.5s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.057s, learning 0.228s)
               Value function loss: 17.7601
                    Surrogate loss: -0.0053
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 54.82
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 8.29s
                        Total time: 11398.67s
                               ETA: 960365.1s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.263s, learning 0.170s)
               Value function loss: 4.9616
                    Surrogate loss: -0.0158
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 54.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 8.43s
                        Total time: 11407.11s
                               ETA: 960247.3s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.668s, learning 0.177s)
               Value function loss: 0.9547
                    Surrogate loss: -0.0162
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 52.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 8.85s
                        Total time: 11415.95s
                               ETA: 960164.3s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.271s, learning 0.189s)
               Value function loss: 1.5594
                    Surrogate loss: -0.0113
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 53.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 8.46s
                        Total time: 11424.41s
                               ETA: 960049.0s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.210s, learning 0.176s)
               Value function loss: 0.9015
                    Surrogate loss: -0.0165
             Mean action noise std: 0.72
                       Mean reward: 1.72
               Mean episode length: 54.65
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 8.39s
                        Total time: 11432.80s
                               ETA: 959927.8s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.236s, learning 0.173s)
               Value function loss: 0.9658
                    Surrogate loss: -0.0069
             Mean action noise std: 0.72
                       Mean reward: 1.65
               Mean episode length: 54.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 8.41s
                        Total time: 11441.21s
                               ETA: 959808.6s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.335s, learning 0.184s)
               Value function loss: 53.5805
                    Surrogate loss: -0.0007
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 53.72
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 8.52s
                        Total time: 11449.73s
                               ETA: 959698.8s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.110s, learning 0.188s)
               Value function loss: 0.6016
                    Surrogate loss: -0.0175
             Mean action noise std: 0.72
                       Mean reward: 1.57
               Mean episode length: 53.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 8.30s
                        Total time: 11458.02s
                               ETA: 959570.7s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.545s, learning 0.179s)
               Value function loss: 0.3638
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 6.80
               Mean episode length: 53.58
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 8.72s
                        Total time: 11466.75s
                               ETA: 959478.5s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.130s, learning 0.177s)
               Value function loss: 473.3876
                    Surrogate loss: -0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.76
               Mean episode length: 55.83
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 8.31s
                        Total time: 11475.06s
                               ETA: 959351.6s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.246s, learning 0.182s)
               Value function loss: 95.4065
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 54.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 8.43s
                        Total time: 11483.48s
                               ETA: 959234.9s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.161s)
               Value function loss: 54.3659
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 53.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 8.36s
                        Total time: 11491.84s
                               ETA: 959112.5s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.182s, learning 0.162s)
               Value function loss: 17.8870
                    Surrogate loss: -0.0117
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 54.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 8.34s
                        Total time: 11500.18s
                               ETA: 958989.2s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.303s, learning 0.171s)
               Value function loss: 131.5174
                    Surrogate loss: -0.0038
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 55.02
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 8.47s
                        Total time: 11508.66s
                               ETA: 958877.0s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.451s, learning 0.163s)
               Value function loss: 139.8481
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 53.52
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 8.61s
                        Total time: 11517.27s
                               ETA: 958776.6s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.225s, learning 0.168s)
               Value function loss: 42.9191
                    Surrogate loss: -0.0088
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 54.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 8.39s
                        Total time: 11525.67s
                               ETA: 958657.9s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.112s, learning 0.172s)
               Value function loss: 6.0236
                    Surrogate loss: -0.0144
             Mean action noise std: 0.72
                       Mean reward: 1.19
               Mean episode length: 54.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 8.28s
                        Total time: 11533.95s
                               ETA: 958530.4s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.323s, learning 0.168s)
               Value function loss: 109.5239
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 54.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 8.49s
                        Total time: 11542.44s
                               ETA: 958420.3s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.341s, learning 0.165s)
               Value function loss: 5.9114
                    Surrogate loss: -0.0171
             Mean action noise std: 0.72
                       Mean reward: 6.53
               Mean episode length: 54.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 8.51s
                        Total time: 11550.95s
                               ETA: 958311.5s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.234s, learning 0.166s)
               Value function loss: 3.7688
                    Surrogate loss: 0.0043
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 54.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 8.40s
                        Total time: 11559.35s
                               ETA: 958194.3s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.241s, learning 0.166s)
               Value function loss: 2.3915
                    Surrogate loss: -0.0151
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 56.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 8.41s
                        Total time: 11567.75s
                               ETA: 958077.6s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.162s)
               Value function loss: 0.6505
                    Surrogate loss: -0.0205
             Mean action noise std: 0.72
                       Mean reward: 1.19
               Mean episode length: 53.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 8.48s
                        Total time: 11576.23s
                               ETA: 957967.1s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.290s, learning 0.168s)
               Value function loss: 0.6418
                    Surrogate loss: -0.0110
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 54.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 8.46s
                        Total time: 11584.69s
                               ETA: 957855.1s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.421s, learning 0.166s)
               Value function loss: 0.2823
                    Surrogate loss: -0.0233
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 55.05
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 8.59s
                        Total time: 11593.28s
                               ETA: 957753.9s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.120s, learning 0.162s)
               Value function loss: 657.7839
                    Surrogate loss: -0.0010
             Mean action noise std: 0.72
                       Mean reward: 1.54
               Mean episode length: 53.63
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 8.28s
                        Total time: 11601.56s
                               ETA: 957627.7s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.224s, learning 0.162s)
               Value function loss: 94.0639
                    Surrogate loss: -0.0021
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 53.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 8.39s
                        Total time: 11609.94s
                               ETA: 957510.3s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.195s, learning 0.164s)
               Value function loss: 0.7215
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 55.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 8.36s
                        Total time: 11618.30s
                               ETA: 957390.8s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.489s, learning 0.170s)
               Value function loss: 109.6055
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 19.05
               Mean episode length: 56.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 8.66s
                        Total time: 11626.96s
                               ETA: 957296.3s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.060s, learning 0.163s)
               Value function loss: 230.3678
                    Surrogate loss: -0.0029
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 53.68
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 8.22s
                        Total time: 11635.19s
                               ETA: 957166.0s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.167s)
               Value function loss: 1.1741
                    Surrogate loss: -0.0230
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 53.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 8.37s
                        Total time: 11643.55s
                               ETA: 957047.8s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.219s, learning 0.168s)
               Value function loss: 1.1023
                    Surrogate loss: -0.0178
             Mean action noise std: 0.72
                       Mean reward: 6.77
               Mean episode length: 53.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 8.39s
                        Total time: 11651.94s
                               ETA: 956931.3s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.040s, learning 0.167s)
               Value function loss: 447.4371
                    Surrogate loss: 0.0002
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 54.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 8.21s
                        Total time: 11660.15s
                               ETA: 956800.3s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.263s, learning 0.164s)
               Value function loss: 302.5886
                    Surrogate loss: -0.0032
             Mean action noise std: 0.72
                       Mean reward: 1.61
               Mean episode length: 55.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 8.43s
                        Total time: 11668.57s
                               ETA: 956687.5s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.247s, learning 0.164s)
               Value function loss: 3.1617
                    Surrogate loss: -0.0102
             Mean action noise std: 0.72
                       Mean reward: 14.55
               Mean episode length: 54.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 8.41s
                        Total time: 11676.99s
                               ETA: 956573.6s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.337s, learning 0.168s)
               Value function loss: 2.0707
                    Surrogate loss: -0.0184
             Mean action noise std: 0.72
                       Mean reward: 21.96
               Mean episode length: 55.25
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 8.50s
                        Total time: 11685.49s
                               ETA: 956467.5s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.176s, learning 0.189s)
               Value function loss: 0.7089
                    Surrogate loss: -0.0180
             Mean action noise std: 0.72
                       Mean reward: 1.81
               Mean episode length: 53.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 8.36s
                        Total time: 11693.85s
                               ETA: 956350.1s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.183s, learning 0.216s)
               Value function loss: 0.4657
                    Surrogate loss: -0.0194
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 53.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 8.40s
                        Total time: 11702.25s
                               ETA: 956235.8s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.249s, learning 0.165s)
               Value function loss: 1.3905
                    Surrogate loss: -0.0080
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 52.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 8.41s
                        Total time: 11710.67s
                               ETA: 956122.8s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.167s, learning 0.173s)
               Value function loss: 0.5532
                    Surrogate loss: -0.0131
             Mean action noise std: 0.72
                       Mean reward: 1.95
               Mean episode length: 53.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 8.34s
                        Total time: 11719.01s
                               ETA: 956004.0s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.064s, learning 0.166s)
               Value function loss: 0.2856
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 53.63
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 8.23s
                        Total time: 11727.24s
                               ETA: 955876.3s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.194s, learning 0.159s)
               Value function loss: 0.1722
                    Surrogate loss: -0.0251
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 53.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 8.35s
                        Total time: 11735.59s
                               ETA: 955758.9s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.056s, learning 0.166s)
               Value function loss: 0.1544
                    Surrogate loss: -0.0238
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 54.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 8.22s
                        Total time: 11743.81s
                               ETA: 955631.0s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.151s, learning 0.168s)
               Value function loss: 0.1068
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 53.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 8.32s
                        Total time: 11752.13s
                               ETA: 955511.2s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.183s, learning 0.219s)
               Value function loss: 59.5369
                    Surrogate loss: -0.0008
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 54.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 8.40s
                        Total time: 11760.53s
                               ETA: 955398.2s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.186s, learning 0.268s)
               Value function loss: 134.1543
                    Surrogate loss: -0.0028
             Mean action noise std: 0.72
                       Mean reward: 1.60
               Mean episode length: 53.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 8.45s
                        Total time: 11768.99s
                               ETA: 955289.7s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.191s, learning 0.173s)
               Value function loss: 17.2903
                    Surrogate loss: -0.0051
             Mean action noise std: 0.72
                       Mean reward: 1.20
               Mean episode length: 53.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 8.36s
                        Total time: 11777.35s
                               ETA: 955174.0s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.165s)
               Value function loss: 57.4195
                    Surrogate loss: 0.0001
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 53.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 8.53s
                        Total time: 11785.88s
                               ETA: 955072.2s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.046s, learning 0.171s)
               Value function loss: 1.5134
                    Surrogate loss: -0.0206
             Mean action noise std: 0.72
                       Mean reward: 1.47
               Mean episode length: 53.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 8.22s
                        Total time: 11794.10s
                               ETA: 954945.0s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.169s)
               Value function loss: 58.0457
                    Surrogate loss: 0.0008
             Mean action noise std: 0.72
                       Mean reward: 1.46
               Mean episode length: 53.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 8.55s
                        Total time: 11802.65s
                               ETA: 954844.6s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.329s, learning 0.164s)
               Value function loss: 1.4452
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 1.71
               Mean episode length: 53.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 8.49s
                        Total time: 11811.14s
                               ETA: 954740.1s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.189s, learning 0.170s)
               Value function loss: 29.3778
                    Surrogate loss: 0.0021
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 55.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 8.36s
                        Total time: 11819.50s
                               ETA: 954624.9s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.562s, learning 0.161s)
               Value function loss: 215.4375
                    Surrogate loss: -0.0023
             Mean action noise std: 0.72
                       Mean reward: 1.59
               Mean episode length: 54.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 8.72s
                        Total time: 11828.22s
                               ETA: 954539.3s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.203s, learning 0.173s)
               Value function loss: 63.1417
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 1.40
               Mean episode length: 54.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 8.38s
                        Total time: 11836.60s
                               ETA: 954425.8s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.526s, learning 0.160s)
               Value function loss: 47.1130
                    Surrogate loss: -0.0036
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 54.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 8.69s
                        Total time: 11845.28s
                               ETA: 954337.6s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.163s)
               Value function loss: 321.2958
                    Surrogate loss: -0.0004
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 53.81
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 8.34s
                        Total time: 11853.62s
                               ETA: 954221.4s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.262s, learning 0.172s)
               Value function loss: 25.9605
                    Surrogate loss: -0.0077
             Mean action noise std: 0.72
                       Mean reward: 1.23
               Mean episode length: 53.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 8.43s
                        Total time: 11862.06s
                               ETA: 954113.1s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.144s, learning 0.209s)
               Value function loss: 66.2289
                    Surrogate loss: -0.0006
             Mean action noise std: 0.72
                       Mean reward: 2.13
               Mean episode length: 55.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 8.35s
                        Total time: 11870.41s
                               ETA: 953998.4s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.225s, learning 0.166s)
               Value function loss: 47.1905
                    Surrogate loss: -0.0057
             Mean action noise std: 0.72
                       Mean reward: 1.56
               Mean episode length: 55.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 8.39s
                        Total time: 11878.80s
                               ETA: 953886.9s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.210s, learning 0.167s)
               Value function loss: 360.3724
                    Surrogate loss: -0.0001
             Mean action noise std: 0.72
                       Mean reward: 1.44
               Mean episode length: 56.54
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 8.38s
                        Total time: 11887.18s
                               ETA: 953774.6s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.135s, learning 0.164s)
               Value function loss: 64.8951
                    Surrogate loss: -0.0067
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 56.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 8.30s
                        Total time: 11895.48s
                               ETA: 953656.1s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.252s, learning 0.165s)
               Value function loss: 98.3443
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 55.79
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 8.42s
                        Total time: 11903.89s
                               ETA: 953547.2s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.143s, learning 0.221s)
               Value function loss: 34.2657
                    Surrogate loss: -0.0087
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 57.20
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 8.36s
                        Total time: 11912.26s
                               ETA: 953434.3s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.230s, learning 0.164s)
               Value function loss: 26.2597
                    Surrogate loss: -0.0096
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 54.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 8.39s
                        Total time: 11920.65s
                               ETA: 953324.0s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.271s, learning 0.160s)
               Value function loss: 4.8316
                    Surrogate loss: -0.0049
             Mean action noise std: 0.72
                       Mean reward: 1.34
               Mean episode length: 55.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 8.43s
                        Total time: 11929.08s
                               ETA: 953216.8s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.181s, learning 0.167s)
               Value function loss: 6.8580
                    Surrogate loss: -0.0062
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 55.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 8.35s
                        Total time: 11937.43s
                               ETA: 953103.1s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.285s, learning 0.175s)
               Value function loss: 93.5585
                    Surrogate loss: -0.0009
             Mean action noise std: 0.72
                       Mean reward: 1.12
               Mean episode length: 57.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 8.46s
                        Total time: 11945.89s
                               ETA: 952998.4s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.169s, learning 0.166s)
               Value function loss: 3.8623
                    Surrogate loss: -0.0030
             Mean action noise std: 0.72
                       Mean reward: 1.20
               Mean episode length: 55.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 8.34s
                        Total time: 11954.23s
                               ETA: 952884.0s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.250s, learning 0.159s)
               Value function loss: 0.2374
                    Surrogate loss: -0.0321
             Mean action noise std: 0.72
                       Mean reward: 1.15
               Mean episode length: 57.88
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 8.41s
                        Total time: 11962.64s
                               ETA: 952775.7s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.325s, learning 0.159s)
               Value function loss: 0.1420
                    Surrogate loss: -0.0331
             Mean action noise std: 0.72
                       Mean reward: 1.17
               Mean episode length: 55.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 8.48s
                        Total time: 11971.12s
                               ETA: 952673.5s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.173s, learning 0.169s)
               Value function loss: 0.1539
                    Surrogate loss: -0.0256
             Mean action noise std: 0.72
                       Mean reward: 1.10
               Mean episode length: 57.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 8.34s
                        Total time: 11979.46s
                               ETA: 952560.2s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1873 steps/s (collection: 8.579s, learning 0.168s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0293
             Mean action noise std: 0.72
                       Mean reward: 1.08
               Mean episode length: 58.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 8.75s
                        Total time: 11988.21s
                               ETA: 952479.1s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.467s, learning 0.171s)
               Value function loss: 0.2283
                    Surrogate loss: -0.0167
             Mean action noise std: 0.72
                       Mean reward: 1.12
               Mean episode length: 57.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 8.64s
                        Total time: 11996.85s
                               ETA: 952389.6s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.237s, learning 0.167s)
               Value function loss: 0.2582
                    Surrogate loss: -0.0156
             Mean action noise std: 0.72
                       Mean reward: 1.07
               Mean episode length: 56.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 8.40s
                        Total time: 12005.25s
                               ETA: 952281.6s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.123s, learning 0.172s)
               Value function loss: 0.0919
                    Surrogate loss: -0.0275
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 54.40
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 8.29s
                        Total time: 12013.55s
                               ETA: 952165.2s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.302s, learning 0.166s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.17
               Mean episode length: 56.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 8.47s
                        Total time: 12022.02s
                               ETA: 952062.6s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.168s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0316
             Mean action noise std: 0.72
                       Mean reward: 1.23
               Mean episode length: 58.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 8.37s
                        Total time: 12030.39s
                               ETA: 951952.6s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.501s, learning 0.159s)
               Value function loss: 0.0508
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 0.99
               Mean episode length: 56.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 8.66s
                        Total time: 12039.05s
                               ETA: 951865.5s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.350s, learning 0.175s)
               Value function loss: 0.0517
                    Surrogate loss: -0.0398
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 56.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 8.53s
                        Total time: 12047.57s
                               ETA: 951767.9s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.099s, learning 0.162s)
               Value function loss: 0.0412
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 1.20
               Mean episode length: 56.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 8.26s
                        Total time: 12055.83s
                               ETA: 951649.6s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.372s, learning 0.173s)
               Value function loss: 0.0392
                    Surrogate loss: -0.0432
             Mean action noise std: 0.72
                       Mean reward: 1.07
               Mean episode length: 56.85
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 8.54s
                        Total time: 12064.38s
                               ETA: 951553.8s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.373s, learning 0.162s)
               Value function loss: 0.0394
                    Surrogate loss: -0.0414
             Mean action noise std: 0.72
                       Mean reward: 1.15
               Mean episode length: 57.08
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 8.53s
                        Total time: 12072.91s
                               ETA: 951457.3s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.574s, learning 0.160s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0386
             Mean action noise std: 0.72
                       Mean reward: 1.12
               Mean episode length: 57.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 8.73s
                        Total time: 12081.65s
                               ETA: 951376.7s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.289s, learning 0.175s)
               Value function loss: 0.0355
                    Surrogate loss: -0.0411
             Mean action noise std: 0.72
                       Mean reward: 1.24
               Mean episode length: 57.55
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 8.46s
                        Total time: 12090.11s
                               ETA: 951275.1s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.279s, learning 0.159s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0203
             Mean action noise std: 0.72
                       Mean reward: 1.16
               Mean episode length: 56.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 8.44s
                        Total time: 12098.55s
                               ETA: 951171.4s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.936s, learning 0.161s)
               Value function loss: 0.1043
                    Surrogate loss: -0.0152
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 57.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 8.10s
                        Total time: 12106.65s
                               ETA: 951041.2s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.141s, learning 0.162s)
               Value function loss: 0.0781
                    Surrogate loss: -0.0216
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 54.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 8.30s
                        Total time: 12114.95s
                               ETA: 950927.3s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1994 steps/s (collection: 7.960s, learning 0.255s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0396
             Mean action noise std: 0.72
                       Mean reward: 1.15
               Mean episode length: 54.31
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 8.22s
                        Total time: 12123.17s
                               ETA: 950806.7s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.411s, learning 0.208s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0420
             Mean action noise std: 0.72
                       Mean reward: 1.52
               Mean episode length: 55.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 8.62s
                        Total time: 12131.78s
                               ETA: 950717.8s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.162s)
               Value function loss: 0.0238
                    Surrogate loss: -0.0440
             Mean action noise std: 0.72
                       Mean reward: 1.15
               Mean episode length: 55.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 8.36s
                        Total time: 12140.15s
                               ETA: 950609.0s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.374s, learning 0.206s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0451
             Mean action noise std: 0.72
                       Mean reward: 1.17
               Mean episode length: 57.10
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 8.58s
                        Total time: 12148.73s
                               ETA: 950517.5s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.408s, learning 0.163s)
               Value function loss: 0.0406
                    Surrogate loss: -0.0306
             Mean action noise std: 0.72
                       Mean reward: 1.15
               Mean episode length: 57.13
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 8.57s
                        Total time: 12157.30s
                               ETA: 950425.3s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.230s, learning 0.169s)
               Value function loss: 0.0366
                    Surrogate loss: -0.0281
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 55.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 8.40s
                        Total time: 12165.70s
                               ETA: 950319.9s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.161s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0443
             Mean action noise std: 0.72
                       Mean reward: 1.24
               Mean episode length: 55.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 8.31s
                        Total time: 12174.01s
                               ETA: 950207.9s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.392s, learning 0.167s)
               Value function loss: 0.0253
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 1.20
               Mean episode length: 55.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 8.56s
                        Total time: 12182.57s
                               ETA: 950115.3s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.306s, learning 0.163s)
               Value function loss: 0.0250
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 1.35
               Mean episode length: 55.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 8.47s
                        Total time: 12191.04s
                               ETA: 950015.8s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.298s, learning 0.164s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0460
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 56.42
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 8.46s
                        Total time: 12199.50s
                               ETA: 949915.8s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.057s, learning 0.211s)
               Value function loss: 0.0263
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 57.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 8.27s
                        Total time: 12207.77s
                               ETA: 949800.9s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.059s, learning 0.168s)
               Value function loss: 0.0464
                    Surrogate loss: -0.0251
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 56.24
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 8.23s
                        Total time: 12215.99s
                               ETA: 949683.0s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.305s, learning 0.177s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0347
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 55.96
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 8.48s
                        Total time: 12224.48s
                               ETA: 949585.1s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.051s, learning 0.213s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0376
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 56.73
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 8.26s
                        Total time: 12232.74s
                               ETA: 949470.3s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.458s, learning 0.198s)
               Value function loss: 0.0659
                    Surrogate loss: -0.0145
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 55.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 8.66s
                        Total time: 12241.40s
                               ETA: 949386.2s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.455s, learning 0.280s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0380
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 55.83
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 8.73s
                        Total time: 12250.13s
                               ETA: 949308.2s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.585s, learning 0.207s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0363
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 56.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 8.79s
                        Total time: 12258.92s
                               ETA: 949234.9s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.416s, learning 0.180s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 56.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 8.60s
                        Total time: 12267.52s
                               ETA: 949146.4s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.256s, learning 0.173s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 57.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 8.43s
                        Total time: 12275.95s
                               ETA: 949045.1s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.339s, learning 0.174s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0395
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 56.71
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 8.51s
                        Total time: 12284.46s
                               ETA: 948950.6s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.513s, learning 0.175s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 55.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 8.69s
                        Total time: 12293.15s
                               ETA: 948869.6s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.336s, learning 0.173s)
               Value function loss: 0.0266
                    Surrogate loss: -0.0355
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 55.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 8.51s
                        Total time: 12301.66s
                               ETA: 948775.0s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.216s, learning 0.167s)
               Value function loss: 0.0339
                    Surrogate loss: -0.0329
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 56.28
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 8.38s
                        Total time: 12310.04s
                               ETA: 948670.8s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.320s, learning 0.169s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 1.53
               Mean episode length: 53.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 8.49s
                        Total time: 12318.53s
                               ETA: 948574.9s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.164s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0392
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 53.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 8.66s
                        Total time: 12327.19s
                               ETA: 948492.2s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.152s, learning 0.215s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.55
               Mean episode length: 52.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 8.37s
                        Total time: 12335.56s
                               ETA: 948387.1s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 2038 steps/s (collection: 7.839s, learning 0.200s)
               Value function loss: 0.0300
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.63
               Mean episode length: 53.30
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 8.04s
                        Total time: 12343.60s
                               ETA: 948257.1s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.565s, learning 0.191s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0265
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 54.45
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 8.76s
                        Total time: 12352.35s
                               ETA: 948182.2s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1506 steps/s (collection: 10.701s, learning 0.178s)
               Value function loss: 16.4511
                    Surrogate loss: -0.0013
             Mean action noise std: 0.72
                       Mean reward: 1.66
               Mean episode length: 53.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 10.88s
                        Total time: 12363.23s
                               ETA: 948270.3s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 969 steps/s (collection: 16.730s, learning 0.169s)
               Value function loss: 475.5034
                    Surrogate loss: -0.0026
             Mean action noise std: 0.72
                       Mean reward: 1.38
               Mean episode length: 54.16
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 16.90s
                        Total time: 12380.13s
                               ETA: 948819.6s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.915s, learning 0.160s)
               Value function loss: 67.4796
                    Surrogate loss: -0.0125
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 54.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 16.08s
                        Total time: 12396.20s
                               ETA: 949304.9s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.405s, learning 0.178s)
               Value function loss: 32.4257
                    Surrogate loss: -0.0138
             Mean action noise std: 0.72
                       Mean reward: 1.37
               Mean episode length: 52.91
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 16.58s
                        Total time: 12412.79s
                               ETA: 949828.3s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 974 steps/s (collection: 16.649s, learning 0.167s)
               Value function loss: 1.7721
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 53.66
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 16.82s
                        Total time: 12429.60s
                               ETA: 950368.7s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.500s, learning 0.173s)
               Value function loss: 0.6079
                    Surrogate loss: -0.0309
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 54.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 16.67s
                        Total time: 12446.27s
                               ETA: 950897.3s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.456s, learning 0.167s)
               Value function loss: 0.3941
                    Surrogate loss: -0.0273
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 53.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 16.62s
                        Total time: 12462.90s
                               ETA: 951421.3s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.172s, learning 0.274s)
               Value function loss: 0.2115
                    Surrogate loss: -0.0315
             Mean action noise std: 0.72
                       Mean reward: 1.14
               Mean episode length: 54.39
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 16.45s
                        Total time: 12479.34s
                               ETA: 951930.9s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.566s, learning 0.214s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0369
             Mean action noise std: 0.72
                       Mean reward: 1.26
               Mean episode length: 54.03
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 16.78s
                        Total time: 12496.12s
                               ETA: 952465.2s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 954 steps/s (collection: 16.958s, learning 0.214s)
               Value function loss: 0.0787
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.29
               Mean episode length: 54.89
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 17.17s
                        Total time: 12513.30s
                               ETA: 953028.4s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.239s, learning 0.171s)
               Value function loss: 0.1182
                    Surrogate loss: -0.0222
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 55.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 16.41s
                        Total time: 12529.70s
                               ETA: 953532.8s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.338s, learning 0.181s)
               Value function loss: 0.0550
                    Surrogate loss: -0.0336
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 56.06
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 16.52s
                        Total time: 12546.22s
                               ETA: 954044.6s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.405s, learning 0.169s)
               Value function loss: 0.0442
                    Surrogate loss: -0.0318
             Mean action noise std: 0.72
                       Mean reward: 1.10
               Mean episode length: 56.83
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 16.57s
                        Total time: 12562.80s
                               ETA: 954559.9s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.259s, learning 0.207s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0369
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 55.72
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 16.47s
                        Total time: 12579.26s
                               ETA: 955066.2s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.120s, learning 0.170s)
               Value function loss: 0.0497
                    Surrogate loss: -0.0261
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 55.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 16.29s
                        Total time: 12595.55s
                               ETA: 955558.2s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.194s, learning 0.215s)
               Value function loss: 0.0704
                    Surrogate loss: -0.0197
             Mean action noise std: 0.72
                       Mean reward: 1.24
               Mean episode length: 56.34
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 16.41s
                        Total time: 12611.96s
                               ETA: 956058.5s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.308s, learning 0.169s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0140
             Mean action noise std: 0.72
                       Mean reward: 1.20
               Mean episode length: 55.97
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 16.48s
                        Total time: 12628.44s
                               ETA: 956563.1s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.282s, learning 0.166s)
               Value function loss: 0.0260
                    Surrogate loss: -0.0375
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 56.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 16.45s
                        Total time: 12644.89s
                               ETA: 957064.8s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.132s, learning 0.212s)
               Value function loss: 70.5928
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 1.13
               Mean episode length: 55.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 16.34s
                        Total time: 12661.23s
                               ETA: 957557.8s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.535s, learning 0.168s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0238
             Mean action noise std: 0.72
                       Mean reward: 1.30
               Mean episode length: 55.44
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 16.70s
                        Total time: 12677.94s
                               ETA: 958077.2s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.510s, learning 0.174s)
               Value function loss: 0.0681
                    Surrogate loss: -0.0185
             Mean action noise std: 0.72
                       Mean reward: 1.17
               Mean episode length: 55.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 16.68s
                        Total time: 12694.62s
                               ETA: 958594.3s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.463s, learning 0.175s)
               Value function loss: 0.0373
                    Surrogate loss: -0.0299
             Mean action noise std: 0.72
                       Mean reward: 1.22
               Mean episode length: 55.93
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 16.64s
                        Total time: 12711.26s
                               ETA: 959107.1s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.499s, learning 0.185s)
               Value function loss: 0.0546
                    Surrogate loss: -0.0166
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 56.49
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 16.68s
                        Total time: 12727.94s
                               ETA: 959622.6s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.203s, learning 0.164s)
               Value function loss: 0.0483
                    Surrogate loss: -0.0220
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 57.01
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 16.37s
                        Total time: 12744.31s
                               ETA: 960113.5s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.224s, learning 0.164s)
               Value function loss: 0.0196
                    Surrogate loss: -0.0409
             Mean action noise std: 0.72
                       Mean reward: 1.15
               Mean episode length: 55.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 16.39s
                        Total time: 12760.70s
                               ETA: 960605.0s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.173s, learning 0.171s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0414
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 56.47
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 16.34s
                        Total time: 12777.04s
                               ETA: 961092.5s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.401s, learning 0.162s)
               Value function loss: 0.0219
                    Surrogate loss: -0.0313
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 56.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 16.56s
                        Total time: 12793.60s
                               ETA: 961595.7s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.279s, learning 0.175s)
               Value function loss: 0.0209
                    Surrogate loss: -0.0337
             Mean action noise std: 0.72
                       Mean reward: 0.98
               Mean episode length: 58.09
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 16.45s
                        Total time: 12810.06s
                               ETA: 962089.9s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.227s, learning 0.170s)
               Value function loss: 0.0192
                    Surrogate loss: -0.0364
             Mean action noise std: 0.72
                       Mean reward: 1.14
               Mean episode length: 56.84
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 16.40s
                        Total time: 12826.46s
                               ETA: 962579.1s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.419s, learning 0.172s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0303
             Mean action noise std: 0.72
                       Mean reward: 1.06
               Mean episode length: 56.63
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 16.59s
                        Total time: 12843.05s
                               ETA: 963082.1s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.343s, learning 0.164s)
               Value function loss: 0.0179
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.14
               Mean episode length: 54.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 16.51s
                        Total time: 12859.55s
                               ETA: 963578.0s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.497s, learning 0.171s)
               Value function loss: 0.0184
                    Surrogate loss: -0.0377
             Mean action noise std: 0.72
                       Mean reward: 1.01
               Mean episode length: 56.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 16.67s
                        Total time: 12876.22s
                               ETA: 964085.2s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.200s, learning 0.222s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0358
             Mean action noise std: 0.72
                       Mean reward: 1.13
               Mean episode length: 54.35
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 16.42s
                        Total time: 12892.64s
                               ETA: 964573.1s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.849s, learning 0.163s)
               Value function loss: 0.0154
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 56.53
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 16.01s
                        Total time: 12908.66s
                               ETA: 965029.7s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.246s, learning 0.219s)
               Value function loss: 0.0381
                    Surrogate loss: -0.0164
             Mean action noise std: 0.72
                       Mean reward: 1.11
               Mean episode length: 55.72
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 16.46s
                        Total time: 12925.12s
                               ETA: 965519.3s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.366s, learning 0.165s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0308
             Mean action noise std: 0.72
                       Mean reward: 1.12
               Mean episode length: 58.19
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 16.53s
                        Total time: 12941.65s
                               ETA: 966013.2s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.949s, learning 0.163s)
               Value function loss: 0.0114
                    Surrogate loss: -0.0444
             Mean action noise std: 0.72
                       Mean reward: 1.01
               Mean episode length: 55.37
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 16.11s
                        Total time: 12957.77s
                               ETA: 966474.9s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.063s, learning 0.172s)
               Value function loss: 0.0122
                    Surrogate loss: -0.0452
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 53.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 16.23s
                        Total time: 12974.00s
                               ETA: 966945.1s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.719s, learning 0.178s)
               Value function loss: 0.0120
                    Surrogate loss: -0.0416
             Mean action noise std: 0.72
                       Mean reward: 1.07
               Mean episode length: 55.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 11.90s
                        Total time: 12985.90s
                               ETA: 967091.6s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 2023 steps/s (collection: 7.930s, learning 0.165s)
               Value function loss: 0.0130
                    Surrogate loss: -0.0427
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 55.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 8.10s
                        Total time: 12993.99s
                               ETA: 966954.9s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.333s, learning 0.169s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0356
             Mean action noise std: 0.72
                       Mean reward: 1.05
               Mean episode length: 57.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 8.50s
                        Total time: 13002.49s
                               ETA: 966848.6s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.218s, learning 0.161s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0383
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 55.29
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 8.38s
                        Total time: 13010.87s
                               ETA: 966733.3s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.213s, learning 0.161s)
               Value function loss: 0.0171
                    Surrogate loss: -0.0384
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 56.23
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 8.37s
                        Total time: 13019.25s
                               ETA: 966617.8s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.201s, learning 0.167s)
               Value function loss: 16.6341
                    Surrogate loss: -0.0014
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 54.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 8.37s
                        Total time: 13027.62s
                               ETA: 966502.1s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.206s, learning 0.171s)
               Value function loss: 0.0264
                    Surrogate loss: -0.0417
             Mean action noise std: 0.72
                       Mean reward: 1.21
               Mean episode length: 56.41
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 8.38s
                        Total time: 13035.99s
                               ETA: 966387.3s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.383s, learning 0.172s)
               Value function loss: 0.0236
                    Surrogate loss: -0.0424
             Mean action noise std: 0.72
                       Mean reward: 1.18
               Mean episode length: 56.87
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 8.56s
                        Total time: 13044.55s
                               ETA: 966285.7s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.544s, learning 0.170s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0362
             Mean action noise std: 0.72
                       Mean reward: 4.01
               Mean episode length: 53.62
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 8.71s
                        Total time: 13053.26s
                               ETA: 966196.0s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.516s, learning 0.168s)
               Value function loss: 0.0193
                    Surrogate loss: -0.0429
             Mean action noise std: 0.72
                       Mean reward: 1.25
               Mean episode length: 57.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 8.68s
                        Total time: 13061.95s
                               ETA: 966104.3s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.386s, learning 0.209s)
               Value function loss: 0.0206
                    Surrogate loss: -0.0399
             Mean action noise std: 0.72
                       Mean reward: 1.27
               Mean episode length: 55.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 8.60s
                        Total time: 13070.54s
                               ETA: 966006.1s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.415s, learning 0.187s)
               Value function loss: 0.0187
                    Surrogate loss: -0.0372
             Mean action noise std: 0.72
                       Mean reward: 1.50
               Mean episode length: 53.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 8.60s
                        Total time: 13079.14s
                               ETA: 965908.5s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.511s, learning 0.162s)
               Value function loss: 0.0202
                    Surrogate loss: -0.0346
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 55.72
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 8.67s
                        Total time: 13087.82s
                               ETA: 965816.3s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.260s, learning 0.223s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0418
             Mean action noise std: 0.72
                       Mean reward: 1.43
               Mean episode length: 52.81
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 8.48s
                        Total time: 13096.30s
                               ETA: 965710.2s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.337s, learning 0.165s)
               Value function loss: 0.0210
                    Surrogate loss: -0.0361
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 54.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 8.50s
                        Total time: 13104.80s
                               ETA: 965605.6s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.425s, learning 0.163s)
               Value function loss: 0.0199
                    Surrogate loss: -0.0401
             Mean action noise std: 0.72
                       Mean reward: 1.33
               Mean episode length: 54.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 8.59s
                        Total time: 13113.39s
                               ETA: 965507.6s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.347s, learning 0.166s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0421
             Mean action noise std: 0.72
                       Mean reward: 1.39
               Mean episode length: 53.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 8.51s
                        Total time: 13121.90s
                               ETA: 965404.1s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.691s, learning 0.163s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0391
             Mean action noise std: 0.72
                       Mean reward: 1.48
               Mean episode length: 55.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 8.85s
                        Total time: 13130.76s
                               ETA: 965325.9s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.342s, learning 0.164s)
               Value function loss: 0.0176
                    Surrogate loss: -0.0393
             Mean action noise std: 0.72
                       Mean reward: 1.36
               Mean episode length: 54.83
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 8.51s
                        Total time: 13139.26s
                               ETA: 965222.2s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.247s, learning 0.169s)
               Value function loss: 0.0244
                    Surrogate loss: -0.0310
             Mean action noise std: 0.72
                       Mean reward: 1.32
               Mean episode length: 54.11
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 8.42s
                        Total time: 13147.68s
                               ETA: 965112.0s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.274s, learning 0.168s)
               Value function loss: 0.0297
                    Surrogate loss: -0.0268
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 53.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 8.44s
                        Total time: 13156.12s
                               ETA: 965003.9s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.450s, learning 0.174s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0289
             Mean action noise std: 0.72
                       Mean reward: 1.42
               Mean episode length: 55.86
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 8.62s
                        Total time: 13164.74s
                               ETA: 964909.3s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.402s, learning 0.159s)
               Value function loss: 0.0211
                    Surrogate loss: -0.0378
             Mean action noise std: 0.72
                       Mean reward: 1.31
               Mean episode length: 53.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 8.56s
                        Total time: 13173.31s
                               ETA: 964810.2s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.269s, learning 0.177s)
               Value function loss: 0.0226
                    Surrogate loss: -0.0343
             Mean action noise std: 0.72
                       Mean reward: 1.51
               Mean episode length: 52.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 8.45s
                        Total time: 13181.75s
                               ETA: 964702.8s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.162s, learning 0.169s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0389
             Mean action noise std: 0.72
                       Mean reward: 1.62
               Mean episode length: 54.02
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 8.33s
                        Total time: 13190.08s
                               ETA: 964587.2s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.159s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0350
             Mean action noise std: 0.72
                       Mean reward: 1.28
               Mean episode length: 54.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 8.47s
                        Total time: 13198.55s
                               ETA: 964481.7s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.133s, learning 0.164s)
               Value function loss: 0.0239
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 1.55
               Mean episode length: 54.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 8.30s
                        Total time: 13206.85s
                               ETA: 964363.9s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.117s, learning 0.177s)
               Value function loss: 0.0276
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 1.48
               Mean episode length: 53.99
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 8.29s
                        Total time: 13215.14s
                               ETA: 964246.0s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.258s, learning 0.186s)
               Value function loss: 192.1938
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 1.21
               Mean episode length: 54.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 8.44s
                        Total time: 13223.59s
                               ETA: 964139.2s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.029s, learning 0.171s)
               Value function loss: 0.1168
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 1.54
               Mean episode length: 54.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 8.20s
                        Total time: 13231.79s
                               ETA: 964014.8s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.162s)
               Value function loss: 0.0488
                    Surrogate loss: -0.0323
             Mean action noise std: 0.71
                       Mean reward: 1.56
               Mean episode length: 53.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 8.55s
                        Total time: 13240.33s
                               ETA: 963915.8s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.942s, learning 0.213s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 1.58
               Mean episode length: 54.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 8.15s
                        Total time: 13248.49s
                               ETA: 963788.4s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.234s, learning 0.194s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0339
             Mean action noise std: 0.71
                       Mean reward: 1.48
               Mean episode length: 53.23
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 8.43s
                        Total time: 13256.92s
                               ETA: 963681.1s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.789s, learning 0.173s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 1.72
               Mean episode length: 54.19
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 7.96s
                        Total time: 13264.88s
                               ETA: 963540.1s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.164s, learning 0.171s)
               Value function loss: 0.0304
                    Surrogate loss: -0.0363
             Mean action noise std: 0.71
                       Mean reward: 1.50
               Mean episode length: 52.87
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 8.34s
                        Total time: 13273.21s
                               ETA: 963426.3s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.466s, learning 0.164s)
               Value function loss: 0.0254
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 1.69
               Mean episode length: 52.32
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 8.63s
                        Total time: 13281.84s
                               ETA: 963334.1s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.523s, learning 0.159s)
               Value function loss: 0.0282
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 1.56
               Mean episode length: 52.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 8.68s
                        Total time: 13290.53s
                               ETA: 963245.8s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.509s, learning 0.222s)
               Value function loss: 0.0293
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 54.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 8.73s
                        Total time: 13299.26s
                               ETA: 963161.1s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.450s, learning 0.179s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0328
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 53.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 8.63s
                        Total time: 13307.89s
                               ETA: 963069.2s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.407s, learning 0.161s)
               Value function loss: 0.0270
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 52.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 8.57s
                        Total time: 13316.45s
                               ETA: 962973.0s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.411s, learning 0.172s)
               Value function loss: 0.0302
                    Surrogate loss: -0.0338
             Mean action noise std: 0.71
                       Mean reward: 1.63
               Mean episode length: 54.97
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 8.58s
                        Total time: 13325.04s
                               ETA: 962878.0s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.184s, learning 0.197s)
               Value function loss: 0.0370
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 53.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 8.38s
                        Total time: 13333.42s
                               ETA: 962768.5s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.349s, learning 0.174s)
               Value function loss: 0.0688
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 54.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 8.52s
                        Total time: 13341.94s
                               ETA: 962669.4s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.368s, learning 0.167s)
               Value function loss: 11.9104
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 53.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 8.54s
                        Total time: 13350.48s
                               ETA: 962571.3s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.244s, learning 0.164s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0404
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 54.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 8.41s
                        Total time: 13358.88s
                               ETA: 962464.2s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.098s, learning 0.182s)
               Value function loss: 0.0426
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 4.42
               Mean episode length: 52.72
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 8.28s
                        Total time: 13367.16s
                               ETA: 962348.0s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.112s, learning 0.164s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 52.38
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 8.28s
                        Total time: 13375.44s
                               ETA: 962231.7s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.163s)
               Value function loss: 0.0367
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 52.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 8.61s
                        Total time: 13384.05s
                               ETA: 962139.8s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.273s, learning 0.167s)
               Value function loss: 0.0378
                    Surrogate loss: -0.0330
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 52.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 8.44s
                        Total time: 13392.49s
                               ETA: 962035.6s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.342s, learning 0.168s)
               Value function loss: 0.0405
                    Surrogate loss: -0.0346
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 53.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 8.51s
                        Total time: 13401.00s
                               ETA: 961936.6s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.151s, learning 0.167s)
               Value function loss: 0.0359
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 53.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 8.32s
                        Total time: 13409.32s
                               ETA: 961823.9s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.434s, learning 0.169s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 50.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 8.60s
                        Total time: 13417.93s
                               ETA: 961731.8s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.466s, learning 0.163s)
               Value function loss: 0.0351
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 51.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 8.63s
                        Total time: 13426.55s
                               ETA: 961641.6s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.448s, learning 0.218s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 51.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 8.67s
                        Total time: 13435.22s
                               ETA: 961554.3s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.236s, learning 0.217s)
               Value function loss: 0.0591
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 51.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 8.45s
                        Total time: 13443.67s
                               ETA: 961451.8s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1993 steps/s (collection: 7.989s, learning 0.229s)
               Value function loss: 0.0506
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 49.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 8.22s
                        Total time: 13451.89s
                               ETA: 961332.7s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.614s, learning 0.165s)
               Value function loss: 0.0514
                    Surrogate loss: -0.0310
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 51.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 8.78s
                        Total time: 13460.67s
                               ETA: 961253.8s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1850 steps/s (collection: 8.693s, learning 0.161s)
               Value function loss: 0.0382
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 49.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 8.85s
                        Total time: 13469.53s
                               ETA: 961180.3s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.505s, learning 0.214s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 51.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 8.72s
                        Total time: 13478.24s
                               ETA: 961097.3s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.379s, learning 0.161s)
               Value function loss: 0.0460
                    Surrogate loss: -0.0315
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 51.53
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 8.54s
                        Total time: 13486.79s
                               ETA: 961001.6s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.138s, learning 0.223s)
               Value function loss: 47.0141
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 50.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 8.36s
                        Total time: 13495.15s
                               ETA: 960893.4s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.215s, learning 0.281s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 48.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 8.50s
                        Total time: 13503.64s
                               ETA: 960794.8s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.167s)
               Value function loss: 0.0651
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 49.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 8.62s
                        Total time: 13512.26s
                               ETA: 960705.1s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.389s, learning 0.186s)
               Value function loss: 0.0722
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 48.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 8.57s
                        Total time: 13520.83s
                               ETA: 960612.4s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.619s, learning 0.170s)
               Value function loss: 0.0628
                    Surrogate loss: -0.0287
             Mean action noise std: 0.71
                       Mean reward: 2.55
               Mean episode length: 50.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 8.79s
                        Total time: 13529.62s
                               ETA: 960535.1s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.190s, learning 0.182s)
               Value function loss: 0.0839
                    Surrogate loss: -0.0266
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 49.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 8.37s
                        Total time: 13538.00s
                               ETA: 960428.3s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.339s, learning 0.161s)
               Value function loss: 0.0589
                    Surrogate loss: -0.0297
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 51.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 8.50s
                        Total time: 13546.50s
                               ETA: 960330.7s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.267s, learning 0.184s)
               Value function loss: 0.0538
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 49.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 8.45s
                        Total time: 13554.95s
                               ETA: 960229.7s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.483s, learning 0.218s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0311
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 50.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 8.70s
                        Total time: 13563.65s
                               ETA: 960146.6s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.323s, learning 0.262s)
               Value function loss: 13.6568
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 49.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 8.59s
                        Total time: 13572.23s
                               ETA: 960055.3s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.378s, learning 0.165s)
               Value function loss: 0.0749
                    Surrogate loss: -0.0337
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 49.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 8.54s
                        Total time: 13580.78s
                               ETA: 959961.3s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.165s, learning 0.165s)
               Value function loss: 0.0677
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 49.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 8.33s
                        Total time: 13589.11s
                               ETA: 959852.3s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.305s, learning 0.164s)
               Value function loss: 0.0718
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 49.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 8.47s
                        Total time: 13597.57s
                               ETA: 959753.2s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.395s, learning 0.171s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0270
             Mean action noise std: 0.71
                       Mean reward: 2.71
               Mean episode length: 51.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 8.57s
                        Total time: 13606.14s
                               ETA: 959661.1s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.279s, learning 0.171s)
               Value function loss: 0.0808
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 2.63
               Mean episode length: 50.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 8.45s
                        Total time: 13614.59s
                               ETA: 959561.0s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.405s, learning 0.166s)
               Value function loss: 0.0747
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 48.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 8.57s
                        Total time: 13623.16s
                               ETA: 959469.5s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.167s)
               Value function loss: 0.0691
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 2.42
               Mean episode length: 49.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 8.47s
                        Total time: 13631.63s
                               ETA: 959371.0s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.575s, learning 0.187s)
               Value function loss: 191.6111
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 48.62
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 8.76s
                        Total time: 13640.39s
                               ETA: 959293.2s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.161s, learning 0.166s)
               Value function loss: 0.1132
                    Surrogate loss: -0.0290
             Mean action noise std: 0.71
                       Mean reward: 2.49
               Mean episode length: 49.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 8.33s
                        Total time: 13648.72s
                               ETA: 959184.9s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.067s, learning 0.207s)
               Value function loss: 132.0521
                    Surrogate loss: -0.0007
             Mean action noise std: 0.71
                       Mean reward: 2.46
               Mean episode length: 50.36
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 8.27s
                        Total time: 13656.99s
                               ETA: 959073.1s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.170s)
               Value function loss: 0.1633
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 49.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 8.35s
                        Total time: 13665.34s
                               ETA: 958966.4s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.117s, learning 0.178s)
               Value function loss: 0.1456
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 48.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 8.30s
                        Total time: 13673.64s
                               ETA: 958856.4s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.088s, learning 0.175s)
               Value function loss: 0.1261
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 2.70
               Mean episode length: 51.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 8.26s
                        Total time: 13681.90s
                               ETA: 958744.2s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.160s, learning 0.184s)
               Value function loss: 131.1435
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 2.60
               Mean episode length: 50.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 8.34s
                        Total time: 13690.24s
                               ETA: 958637.9s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.415s, learning 0.180s)
               Value function loss: 63.0719
                    Surrogate loss: -0.0034
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 48.65
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 8.60s
                        Total time: 13698.84s
                               ETA: 958549.2s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.337s, learning 0.179s)
               Value function loss: 3.3103
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 2.54
               Mean episode length: 49.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 8.52s
                        Total time: 13707.35s
                               ETA: 958455.1s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.390s, learning 0.187s)
               Value function loss: 2.5861
                    Surrogate loss: -0.0097
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 47.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 8.58s
                        Total time: 13715.93s
                               ETA: 958365.4s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.423s, learning 0.165s)
               Value function loss: 312.3003
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 48.87
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 8.59s
                        Total time: 13724.52s
                               ETA: 958276.6s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.168s, learning 0.174s)
               Value function loss: 217.4313
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 48.89
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 8.34s
                        Total time: 13732.86s
                               ETA: 958170.8s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.422s, learning 0.173s)
               Value function loss: 6.1556
                    Surrogate loss: -0.0159
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 47.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 8.60s
                        Total time: 13741.46s
                               ETA: 958082.7s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.398s, learning 0.206s)
               Value function loss: 1.5012
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 10.51
               Mean episode length: 49.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 8.60s
                        Total time: 13750.06s
                               ETA: 957995.3s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 1856 steps/s (collection: 8.615s, learning 0.211s)
               Value function loss: 296.1608
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 10.00
               Mean episode length: 47.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 8.83s
                        Total time: 13758.89s
                               ETA: 957923.6s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.432s, learning 0.207s)
               Value function loss: 34.0383
                    Surrogate loss: -0.0047
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 46.17
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 8.64s
                        Total time: 13767.53s
                               ETA: 957838.9s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.338s, learning 0.196s)
               Value function loss: 8.8426
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 48.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 8.53s
                        Total time: 13776.06s
                               ETA: 957747.0s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.153s, learning 0.169s)
               Value function loss: 1.4836
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 2.54
               Mean episode length: 49.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 8.32s
                        Total time: 13784.38s
                               ETA: 957640.5s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.350s, learning 0.165s)
               Value function loss: 1.0254
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 5.13
               Mean episode length: 49.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 8.52s
                        Total time: 13792.90s
                               ETA: 957547.6s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.362s, learning 0.160s)
               Value function loss: 0.7560
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 48.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 8.52s
                        Total time: 13801.42s
                               ETA: 957455.2s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.412s, learning 0.212s)
               Value function loss: 0.3228
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 2.59
               Mean episode length: 50.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 8.62s
                        Total time: 13810.04s
                               ETA: 957370.1s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.381s, learning 0.177s)
               Value function loss: 0.2306
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 2.54
               Mean episode length: 50.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 8.56s
                        Total time: 13818.60s
                               ETA: 957280.4s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.074s, learning 0.173s)
               Value function loss: 212.6615
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 49.11
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 8.25s
                        Total time: 13826.85s
                               ETA: 957169.3s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.351s, learning 0.182s)
               Value function loss: 4.0023
                    Surrogate loss: -0.0064
             Mean action noise std: 0.71
                       Mean reward: 2.40
               Mean episode length: 50.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 8.53s
                        Total time: 13835.38s
                               ETA: 957078.2s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.167s)
               Value function loss: 11.6380
                    Surrogate loss: 0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.59
               Mean episode length: 52.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 8.36s
                        Total time: 13843.74s
                               ETA: 956975.4s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.037s, learning 0.223s)
               Value function loss: 16.6187
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 2.58
               Mean episode length: 51.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 8.26s
                        Total time: 13852.00s
                               ETA: 956865.7s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.268s, learning 0.221s)
               Value function loss: 0.2951
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 50.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 8.49s
                        Total time: 13860.49s
                               ETA: 956771.8s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.302s, learning 0.237s)
               Value function loss: 15.2591
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 5.23
               Mean episode length: 50.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 8.54s
                        Total time: 13869.03s
                               ETA: 956681.6s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.164s)
               Value function loss: 18.0448
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.51
               Mean episode length: 50.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 8.37s
                        Total time: 13877.40s
                               ETA: 956579.9s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 1865 steps/s (collection: 8.570s, learning 0.211s)
               Value function loss: 0.2865
                    Surrogate loss: -0.0191
             Mean action noise std: 0.71
                       Mean reward: 2.52
               Mean episode length: 49.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 8.78s
                        Total time: 13886.18s
                               ETA: 956506.6s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.553s, learning 0.214s)
               Value function loss: 215.5961
                    Surrogate loss: -0.0017
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 50.37
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 8.77s
                        Total time: 13894.95s
                               ETA: 956432.5s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.170s, learning 0.251s)
               Value function loss: 17.8408
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 50.88
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 8.42s
                        Total time: 13903.37s
                               ETA: 956334.6s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.993s, learning 0.172s)
               Value function loss: 0.8233
                    Surrogate loss: -0.0197
             Mean action noise std: 0.71
                       Mean reward: 2.95
               Mean episode length: 51.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 8.17s
                        Total time: 13911.54s
                               ETA: 956219.3s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.345s, learning 0.167s)
               Value function loss: 0.4774
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 48.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 8.51s
                        Total time: 13920.05s
                               ETA: 956127.9s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.463s, learning 0.172s)
               Value function loss: 0.3142
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 2.50
               Mean episode length: 50.09
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 8.63s
                        Total time: 13928.68s
                               ETA: 956045.0s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.362s, learning 0.207s)
               Value function loss: 0.2024
                    Surrogate loss: -0.0147
             Mean action noise std: 0.71
                       Mean reward: 2.37
               Mean episode length: 50.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 8.57s
                        Total time: 13937.25s
                               ETA: 955957.8s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.246s, learning 0.218s)
               Value function loss: 0.2653
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 49.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 8.46s
                        Total time: 13945.72s
                               ETA: 955863.5s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.398s, learning 0.208s)
               Value function loss: 0.1703
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 2.33
               Mean episode length: 49.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 8.61s
                        Total time: 13954.32s
                               ETA: 955779.0s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.337s, learning 0.182s)
               Value function loss: 64.3405
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 50.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 8.52s
                        Total time: 13962.84s
                               ETA: 955688.6s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.357s, learning 0.259s)
               Value function loss: 396.7552
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 50.32
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 8.62s
                        Total time: 13971.46s
                               ETA: 955605.0s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.234s, learning 0.206s)
               Value function loss: 248.3779
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 4.89
               Mean episode length: 50.33
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 8.44s
                        Total time: 13979.90s
                               ETA: 955509.5s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.115s, learning 0.164s)
               Value function loss: 6.9195
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 49.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 8.28s
                        Total time: 13988.18s
                               ETA: 955403.1s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.264s, learning 0.169s)
               Value function loss: 3.0117
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 20.05
               Mean episode length: 50.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 8.43s
                        Total time: 13996.61s
                               ETA: 955307.3s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.220s, learning 0.162s)
               Value function loss: 2.4593
                    Surrogate loss: -0.0172
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 50.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 8.38s
                        Total time: 14004.99s
                               ETA: 955208.2s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.376s, learning 0.167s)
               Value function loss: 0.8463
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 48.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 8.54s
                        Total time: 14013.53s
                               ETA: 955120.2s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.112s, learning 0.175s)
               Value function loss: 0.6525
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 49.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 8.29s
                        Total time: 14021.82s
                               ETA: 955014.9s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.283s, learning 0.186s)
               Value function loss: 0.5020
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 50.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 8.47s
                        Total time: 14030.29s
                               ETA: 954922.0s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.432s, learning 0.174s)
               Value function loss: 16.6273
                    Surrogate loss: 0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 50.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 8.61s
                        Total time: 14038.90s
                               ETA: 954838.7s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.525s, learning 0.161s)
               Value function loss: 0.2476
                    Surrogate loss: -0.0304
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 50.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 8.69s
                        Total time: 14047.58s
                               ETA: 954760.8s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.351s, learning 0.162s)
               Value function loss: 0.2501
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 51.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 8.51s
                        Total time: 14056.09s
                               ETA: 954671.3s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.448s, learning 0.167s)
               Value function loss: 0.2266
                    Surrogate loss: -0.0252
             Mean action noise std: 0.71
                       Mean reward: 2.56
               Mean episode length: 51.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 8.62s
                        Total time: 14064.71s
                               ETA: 954588.9s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.483s, learning 0.184s)
               Value function loss: 0.1714
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 50.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 8.67s
                        Total time: 14073.38s
                               ETA: 954510.1s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.258s, learning 0.173s)
               Value function loss: 0.1576
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 51.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 8.43s
                        Total time: 14081.81s
                               ETA: 954415.3s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.369s, learning 0.169s)
               Value function loss: 64.1099
                    Surrogate loss: -0.0015
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 49.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 8.54s
                        Total time: 14090.35s
                               ETA: 954328.0s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.161s)
               Value function loss: 133.7751
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 51.20
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 8.44s
                        Total time: 14098.78s
                               ETA: 954234.0s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.597s, learning 0.169s)
               Value function loss: 196.8215
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 2.13
               Mean episode length: 51.21
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 8.77s
                        Total time: 14107.55s
                               ETA: 954162.3s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.229s, learning 0.168s)
               Value function loss: 148.0579
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 2.60
               Mean episode length: 51.13
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 8.40s
                        Total time: 14115.95s
                               ETA: 954065.7s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.166s)
               Value function loss: 20.4412
                    Surrogate loss: -0.0085
             Mean action noise std: 0.71
                       Mean reward: 2.31
               Mean episode length: 51.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 8.48s
                        Total time: 14124.43s
                               ETA: 953974.9s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.170s, learning 0.170s)
               Value function loss: 15.1279
                    Surrogate loss: -0.0062
             Mean action noise std: 0.71
                       Mean reward: 14.91
               Mean episode length: 50.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 8.34s
                        Total time: 14132.77s
                               ETA: 953874.7s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.296s, learning 0.168s)
               Value function loss: 77.2536
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 2.43
               Mean episode length: 50.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 8.46s
                        Total time: 14141.23s
                               ETA: 953783.0s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.331s, learning 0.167s)
               Value function loss: 4.7065
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 10.03
               Mean episode length: 51.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 8.50s
                        Total time: 14149.73s
                               ETA: 953693.7s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.257s, learning 0.178s)
               Value function loss: 4.4736
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 2.35
               Mean episode length: 50.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 8.44s
                        Total time: 14158.16s
                               ETA: 953600.3s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.501s, learning 0.161s)
               Value function loss: 2.3859
                    Surrogate loss: -0.0143
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 50.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 8.66s
                        Total time: 14166.83s
                               ETA: 953522.3s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.161s)
               Value function loss: 0.5825
                    Surrogate loss: 0.0036
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 49.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 8.43s
                        Total time: 14175.26s
                               ETA: 953428.7s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.235s, learning 0.159s)
               Value function loss: 0.4341
                    Surrogate loss: -0.0207
             Mean action noise std: 0.71
                       Mean reward: 2.38
               Mean episode length: 52.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 8.39s
                        Total time: 14183.65s
                               ETA: 953332.9s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.165s)
               Value function loss: 0.2861
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 2.26
               Mean episode length: 51.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 8.28s
                        Total time: 14191.94s
                               ETA: 953229.9s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.950s, learning 0.217s)
               Value function loss: 0.2725
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 2.39
               Mean episode length: 51.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 8.17s
                        Total time: 14200.10s
                               ETA: 953119.1s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.938s, learning 0.167s)
               Value function loss: 131.8358
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.50
               Mean episode length: 50.33
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 8.11s
                        Total time: 14208.21s
                               ETA: 953004.2s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.289s, learning 0.169s)
               Value function loss: 0.4427
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 51.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 8.46s
                        Total time: 14216.67s
                               ETA: 952913.2s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.282s, learning 0.177s)
               Value function loss: 0.2651
                    Surrogate loss: -0.0158
             Mean action noise std: 0.71
                       Mean reward: 2.41
               Mean episode length: 50.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 8.46s
                        Total time: 14225.13s
                               ETA: 952822.3s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.372s, learning 0.216s)
               Value function loss: 0.1807
                    Surrogate loss: -0.0154
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 51.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 8.59s
                        Total time: 14233.71s
                               ETA: 952740.2s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.303s, learning 0.206s)
               Value function loss: 0.1053
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 52.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 8.51s
                        Total time: 14242.22s
                               ETA: 952652.9s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.392s, learning 0.209s)
               Value function loss: 0.1001
                    Surrogate loss: -0.0271
             Mean action noise std: 0.71
                       Mean reward: 2.45
               Mean episode length: 53.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 8.60s
                        Total time: 14250.82s
                               ETA: 952571.8s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.362s, learning 0.159s)
               Value function loss: 0.1027
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 52.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 8.52s
                        Total time: 14259.34s
                               ETA: 952485.5s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.166s, learning 0.176s)
               Value function loss: 3.9895
                    Surrogate loss: 0.0013
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 51.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 8.34s
                        Total time: 14267.69s
                               ETA: 952387.4s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.169s)
               Value function loss: 0.0950
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 4.53
               Mean episode length: 52.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 8.48s
                        Total time: 14276.17s
                               ETA: 952298.7s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.054s, learning 0.167s)
               Value function loss: 0.0971
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 53.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 8.22s
                        Total time: 14284.39s
                               ETA: 952192.7s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.336s, learning 0.184s)
               Value function loss: 0.0830
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 51.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 8.52s
                        Total time: 14292.91s
                               ETA: 952106.8s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.168s, learning 0.166s)
               Value function loss: 0.0754
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 51.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 8.33s
                        Total time: 14301.24s
                               ETA: 952008.6s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.192s, learning 0.160s)
               Value function loss: 47.2938
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 2.19
               Mean episode length: 53.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 8.35s
                        Total time: 14309.59s
                               ETA: 951911.7s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.964s, learning 0.195s)
               Value function loss: 0.0821
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 52.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 8.16s
                        Total time: 14317.75s
                               ETA: 951802.1s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.167s, learning 0.258s)
               Value function loss: 0.0638
                    Surrogate loss: -0.0361
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 52.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 8.43s
                        Total time: 14326.18s
                               ETA: 951710.4s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/spaces/box.py:84: UserWarning: [33mWARN: Box bound precision lowered by casting to float32[0m
  logger.warn(f"Box bound precision lowered by casting to {self.dtype}")
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.127s, learning 0.160s)
               Value function loss: 63.3131
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 50.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 8.29s
                        Total time: 14334.47s
                               ETA: 951609.5s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.394s, learning 0.192s)
               Value function loss: 352.8001
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 2.28
               Mean episode length: 52.66
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 8.59s
                        Total time: 14343.05s
                               ETA: 951528.7s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.338s, learning 0.166s)
               Value function loss: 4.6504
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 52.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 8.50s
                        Total time: 14351.56s
                               ETA: 951442.4s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.484s, learning 0.237s)
               Value function loss: 2.7314
                    Surrogate loss: -0.0115
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 52.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 8.72s
                        Total time: 14360.28s
                               ETA: 951370.7s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.275s, learning 0.210s)
               Value function loss: 1.5997
                    Surrogate loss: -0.0171
             Mean action noise std: 0.71
                       Mean reward: 2.21
               Mean episode length: 52.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 8.48s
                        Total time: 14368.76s
                               ETA: 951283.4s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.180s, learning 0.167s)
               Value function loss: 1.1285
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 52.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 8.35s
                        Total time: 14377.11s
                               ETA: 951187.1s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.534s, learning 0.207s)
               Value function loss: 193.1960
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 52.86
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 8.74s
                        Total time: 14385.85s
                               ETA: 951117.0s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 977 steps/s (collection: 16.602s, learning 0.161s)
               Value function loss: 0.9606
                    Surrogate loss: -0.0134
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 52.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 16.76s
                        Total time: 14402.61s
                               ETA: 951577.0s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.185s, learning 0.180s)
               Value function loss: 0.6707
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 52.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 16.36s
                        Total time: 14418.98s
                               ETA: 952010.0s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.888s, learning 0.165s)
               Value function loss: 0.4540
                    Surrogate loss: -0.0164
             Mean action noise std: 0.71
                       Mean reward: 12.21
               Mean episode length: 51.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 16.05s
                        Total time: 14435.03s
                               ETA: 952421.9s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.232s, learning 0.190s)
               Value function loss: 62.7994
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 52.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 16.42s
                        Total time: 14451.45s
                               ETA: 952857.5s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.196s, learning 0.177s)
               Value function loss: 4.3303
                    Surrogate loss: -0.0043
             Mean action noise std: 0.71
                       Mean reward: 2.25
               Mean episode length: 52.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 16.37s
                        Total time: 14467.82s
                               ETA: 953289.3s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.150s, learning 0.172s)
               Value function loss: 0.6203
                    Surrogate loss: -0.0162
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 52.36
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 16.32s
                        Total time: 14484.15s
                               ETA: 953717.2s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.119s, learning 0.184s)
               Value function loss: 130.1359
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 51.93
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 16.30s
                        Total time: 14500.45s
                               ETA: 954143.2s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1011 steps/s (collection: 15.983s, learning 0.211s)
               Value function loss: 4.1485
                    Surrogate loss: -0.0066
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 52.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 16.19s
                        Total time: 14516.64s
                               ETA: 954561.4s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.278s, learning 0.175s)
               Value function loss: 0.3725
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 52.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 16.45s
                        Total time: 14533.10s
                               ETA: 954996.0s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.118s, learning 0.212s)
               Value function loss: 0.1809
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 53.36
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 16.33s
                        Total time: 14549.43s
                               ETA: 955422.0s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.192s, learning 0.172s)
               Value function loss: 9.2735
                    Surrogate loss: 0.0009
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 54.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 16.36s
                        Total time: 14565.79s
                               ETA: 955849.7s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.339s, learning 0.167s)
               Value function loss: 0.2334
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 53.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 16.51s
                        Total time: 14582.30s
                               ETA: 956286.0s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.380s, learning 0.164s)
               Value function loss: 189.7646
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 52.19
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 16.54s
                        Total time: 14598.84s
                               ETA: 956724.2s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.472s, learning 0.173s)
               Value function loss: 0.3487
                    Surrogate loss: -0.0136
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 52.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 16.64s
                        Total time: 14615.48s
                               ETA: 957168.4s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.040s, learning 0.164s)
               Value function loss: 4.2351
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 1.65
               Mean episode length: 53.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 16.20s
                        Total time: 14631.69s
                               ETA: 957583.2s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.156s, learning 0.179s)
               Value function loss: 0.1824
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 12.16
               Mean episode length: 52.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 16.33s
                        Total time: 14648.02s
                               ETA: 958006.0s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.125s, learning 0.168s)
               Value function loss: 0.1308
                    Surrogate loss: -0.0096
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 52.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 16.29s
                        Total time: 14664.32s
                               ETA: 958425.4s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.278s, learning 0.170s)
               Value function loss: 0.1203
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 2.15
               Mean episode length: 52.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 16.45s
                        Total time: 14680.76s
                               ETA: 958854.4s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.470s, learning 0.169s)
               Value function loss: 0.0904
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 1.72
               Mean episode length: 53.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 16.64s
                        Total time: 14697.40s
                               ETA: 959295.2s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.411s, learning 0.159s)
               Value function loss: 0.0689
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 51.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 16.57s
                        Total time: 14713.97s
                               ETA: 959731.0s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.837s, learning 0.171s)
               Value function loss: 0.0598
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 53.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 16.01s
                        Total time: 14729.98s
                               ETA: 960129.5s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.196s, learning 0.162s)
               Value function loss: 0.1127
                    Surrogate loss: -0.0232
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 53.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 16.36s
                        Total time: 14746.34s
                               ETA: 960550.3s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.071s, learning 0.163s)
               Value function loss: 402.1765
                    Surrogate loss: 0.0055
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 52.28
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 16.23s
                        Total time: 14762.57s
                               ETA: 960962.5s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.184s, learning 0.169s)
               Value function loss: 17.7228
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 52.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 16.35s
                        Total time: 14778.93s
                               ETA: 961381.8s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.095s, learning 0.162s)
               Value function loss: 468.1197
                    Surrogate loss: -0.0036
             Mean action noise std: 0.71
                       Mean reward: 2.34
               Mean episode length: 52.80
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 16.26s
                        Total time: 14795.18s
                               ETA: 961794.3s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.062s, learning 0.165s)
               Value function loss: 487.3531
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 51.68
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 16.23s
                        Total time: 14811.41s
                               ETA: 962204.2s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.945s, learning 0.162s)
               Value function loss: 35.6435
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 52.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 16.11s
                        Total time: 14827.52s
                               ETA: 962605.9s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.805s, learning 0.169s)
               Value function loss: 14.0303
                    Surrogate loss: -0.0104
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 52.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 15.97s
                        Total time: 14843.49s
                               ETA: 962998.3s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.062s, learning 0.161s)
               Value function loss: 7.6136
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 52.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 16.22s
                        Total time: 14859.71s
                               ETA: 963406.4s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.182s, learning 0.166s)
               Value function loss: 6.8077
                    Surrogate loss: -0.0144
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 51.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 16.35s
                        Total time: 14876.06s
                               ETA: 963822.0s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1036 steps/s (collection: 15.594s, learning 0.208s)
               Value function loss: 5.0276
                    Surrogate loss: -0.0124
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 52.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 15.80s
                        Total time: 14891.86s
                               ETA: 964201.7s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.262s, learning 0.176s)
               Value function loss: 2.0359
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 53.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 16.44s
                        Total time: 14908.30s
                               ETA: 964622.0s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.971s, learning 0.164s)
               Value function loss: 51.5585
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 51.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 16.13s
                        Total time: 14924.44s
                               ETA: 965022.1s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.319s, learning 0.165s)
               Value function loss: 1.1624
                    Surrogate loss: -0.0139
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 52.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 16.48s
                        Total time: 14940.92s
                               ETA: 965444.3s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.965s, learning 0.166s)
               Value function loss: 0.4788
                    Surrogate loss: -0.0192
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 16.13s
                        Total time: 14957.05s
                               ETA: 965843.1s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.998s, learning 0.164s)
               Value function loss: 0.3638
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 52.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 16.16s
                        Total time: 14973.21s
                               ETA: 966243.3s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.986s, learning 0.162s)
               Value function loss: 0.3461
                    Surrogate loss: -0.0151
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 52.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 16.15s
                        Total time: 14989.36s
                               ETA: 966642.1s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1200 steps/s (collection: 13.440s, learning 0.209s)
               Value function loss: 0.1298
                    Surrogate loss: -0.0224
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 51.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 13.65s
                        Total time: 15003.01s
                               ETA: 966879.3s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.163s)
               Value function loss: 0.0931
                    Surrogate loss: -0.0226
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 52.82
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 8.28s
                        Total time: 15011.29s
                               ETA: 966770.2s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.393s, learning 0.160s)
               Value function loss: 17.7439
                    Surrogate loss: -0.0000
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 53.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 8.55s
                        Total time: 15019.84s
                               ETA: 966679.0s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.400s, learning 0.165s)
               Value function loss: 0.0643
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 53.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 8.56s
                        Total time: 15028.41s
                               ETA: 966588.7s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.107s, learning 0.174s)
               Value function loss: 0.0777
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 6.62
               Mean episode length: 53.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 8.28s
                        Total time: 15036.69s
                               ETA: 966480.2s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.098s, learning 0.156s)
               Value function loss: 0.0613
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 52.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 8.25s
                        Total time: 15044.94s
                               ETA: 966370.1s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.121s, learning 0.209s)
               Value function loss: 0.0672
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 54.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 8.33s
                        Total time: 15053.27s
                               ETA: 966265.0s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.319s, learning 0.210s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 54.04
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 8.53s
                        Total time: 15061.80s
                               ETA: 966172.8s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.229s, learning 0.169s)
               Value function loss: 0.0428
                    Surrogate loss: -0.0373
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 54.68
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 8.40s
                        Total time: 15070.20s
                               ETA: 966072.4s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.116s, learning 0.160s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 1.65
               Mean episode length: 53.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 8.28s
                        Total time: 15078.48s
                               ETA: 965964.2s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.342s, learning 0.163s)
               Value function loss: 0.0321
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 52.27
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 8.51s
                        Total time: 15086.98s
                               ETA: 965870.8s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.190s, learning 0.161s)
               Value function loss: 0.0404
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 54.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 8.35s
                        Total time: 15095.33s
                               ETA: 965767.7s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.993s, learning 0.164s)
               Value function loss: 3.9003
                    Surrogate loss: 0.0006
             Mean action noise std: 0.71
                       Mean reward: 1.66
               Mean episode length: 53.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 8.16s
                        Total time: 15103.49s
                               ETA: 965652.3s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.300s, learning 0.164s)
               Value function loss: 0.0331
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 1.59
               Mean episode length: 52.79
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 8.46s
                        Total time: 15111.95s
                               ETA: 965556.7s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.923s, learning 0.164s)
               Value function loss: 0.0465
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 4.38
               Mean episode length: 52.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 8.09s
                        Total time: 15120.04s
                               ETA: 965437.1s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.319s, learning 0.166s)
               Value function loss: 131.8005
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 1.56
               Mean episode length: 52.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 8.49s
                        Total time: 15128.52s
                               ETA: 965343.0s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.069s, learning 0.161s)
               Value function loss: 158.6790
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 1.63
               Mean episode length: 52.18
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 8.23s
                        Total time: 15136.75s
                               ETA: 965232.8s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.273s, learning 0.169s)
               Value function loss: 1.1215
                    Surrogate loss: -0.0284
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 8.44s
                        Total time: 15145.20s
                               ETA: 965136.2s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.324s, learning 0.162s)
               Value function loss: 0.4012
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 1.64
               Mean episode length: 53.20
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 8.49s
                        Total time: 15153.68s
                               ETA: 965042.6s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.096s, learning 0.172s)
               Value function loss: 0.3591
                    Surrogate loss: -0.0241
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 52.31
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 8.27s
                        Total time: 15161.95s
                               ETA: 964935.2s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.965s, learning 0.167s)
               Value function loss: 0.2464
                    Surrogate loss: -0.0221
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 52.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 8.13s
                        Total time: 15170.08s
                               ETA: 964819.3s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.263s, learning 0.166s)
               Value function loss: 0.1838
                    Surrogate loss: -0.0196
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 53.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 8.43s
                        Total time: 15178.51s
                               ETA: 964722.4s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.173s)
               Value function loss: 0.1624
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 52.14
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 8.42s
                        Total time: 15186.94s
                               ETA: 964625.2s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.089s, learning 0.158s)
               Value function loss: 17.9884
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 2.05
               Mean episode length: 53.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 8.25s
                        Total time: 15195.18s
                               ETA: 964516.9s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.992s, learning 0.166s)
               Value function loss: 301.8595
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 7.47
               Mean episode length: 54.11
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 8.16s
                        Total time: 15203.34s
                               ETA: 964403.2s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.166s)
               Value function loss: 3.7784
                    Surrogate loss: -0.0042
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 52.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 8.34s
                        Total time: 15211.69s
                               ETA: 964301.4s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.163s)
               Value function loss: 0.2421
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 51.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 8.36s
                        Total time: 15220.04s
                               ETA: 964200.5s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.137s, learning 0.206s)
               Value function loss: 0.1994
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 52.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 8.34s
                        Total time: 15228.39s
                               ETA: 964098.8s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.429s, learning 0.188s)
               Value function loss: 0.1585
                    Surrogate loss: -0.0149
             Mean action noise std: 0.71
                       Mean reward: 2.00
               Mean episode length: 52.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 8.62s
                        Total time: 15237.00s
                               ETA: 964014.6s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.172s, learning 0.216s)
               Value function loss: 301.0694
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 51.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 8.39s
                        Total time: 15245.39s
                               ETA: 963916.0s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.129s, learning 0.168s)
               Value function loss: 63.3225
                    Surrogate loss: -0.0026
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 52.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 8.30s
                        Total time: 15253.69s
                               ETA: 963811.7s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.165s, learning 0.171s)
               Value function loss: 4.2710
                    Surrogate loss: -0.0078
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 52.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 8.34s
                        Total time: 15262.02s
                               ETA: 963710.1s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.164s)
               Value function loss: 13.5146
                    Surrogate loss: -0.0023
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 52.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 8.53s
                        Total time: 15270.55s
                               ETA: 963620.9s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.367s, learning 0.175s)
               Value function loss: 0.5671
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 6.81
               Mean episode length: 52.34
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 8.54s
                        Total time: 15279.10s
                               ETA: 963532.5s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.072s, learning 0.161s)
               Value function loss: 0.2936
                    Surrogate loss: -0.0204
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 50.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 8.23s
                        Total time: 15287.33s
                               ETA: 963424.7s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.145s, learning 0.186s)
               Value function loss: 0.4562
                    Surrogate loss: -0.0153
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 50.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 8.33s
                        Total time: 15295.66s
                               ETA: 963323.3s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.399s, learning 0.173s)
               Value function loss: 0.3543
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 52.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 8.57s
                        Total time: 15304.23s
                               ETA: 963237.1s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.386s, learning 0.171s)
               Value function loss: 0.1286
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 53.46
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 8.56s
                        Total time: 15312.79s
                               ETA: 963150.1s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.928s, learning 0.179s)
               Value function loss: 0.0898
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 53.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 8.11s
                        Total time: 15320.90s
                               ETA: 963034.8s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.236s, learning 0.179s)
               Value function loss: 45.2850
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 2.12
               Mean episode length: 51.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 8.42s
                        Total time: 15329.31s
                               ETA: 962939.1s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.498s, learning 0.271s)
               Value function loss: 0.2951
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 51.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 8.77s
                        Total time: 15338.08s
                               ETA: 962865.7s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.168s)
               Value function loss: 15.9738
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 50.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 8.53s
                        Total time: 15346.62s
                               ETA: 962777.6s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.087s, learning 0.239s)
               Value function loss: 0.2221
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 51.33
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 8.33s
                        Total time: 15354.94s
                               ETA: 962676.6s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.226s, learning 0.176s)
               Value function loss: 0.1616
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 4.55
               Mean episode length: 53.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 8.40s
                        Total time: 15363.34s
                               ETA: 962580.5s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.044s, learning 0.249s)
               Value function loss: 117.7828
                    Surrogate loss: -0.0004
             Mean action noise std: 0.71
                       Mean reward: 2.37
               Mean episode length: 51.81
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 8.29s
                        Total time: 15371.64s
                               ETA: 962477.6s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.567s, learning 0.166s)
               Value function loss: 0.1857
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 51.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 8.73s
                        Total time: 15380.37s
                               ETA: 962402.4s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.117s, learning 0.165s)
               Value function loss: 0.1406
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 51.10
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 8.28s
                        Total time: 15388.65s
                               ETA: 962299.2s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.514s, learning 0.162s)
               Value function loss: 0.1380
                    Surrogate loss: -0.0201
             Mean action noise std: 0.71
                       Mean reward: 12.28
               Mean episode length: 51.30
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 8.68s
                        Total time: 15397.33s
                               ETA: 962220.6s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.993s, learning 0.163s)
               Value function loss: 452.2051
                    Surrogate loss: -0.0018
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 51.56
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 8.16s
                        Total time: 15405.48s
                               ETA: 962109.7s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.073s, learning 0.163s)
               Value function loss: 0.2310
                    Surrogate loss: -0.0188
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 53.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 8.24s
                        Total time: 15413.72s
                               ETA: 962003.9s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.246s, learning 0.201s)
               Value function loss: 0.2586
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 2.36
               Mean episode length: 51.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 8.45s
                        Total time: 15422.17s
                               ETA: 961911.3s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.121s, learning 0.173s)
               Value function loss: 0.1513
                    Surrogate loss: -0.0169
             Mean action noise std: 0.71
                       Mean reward: 2.17
               Mean episode length: 51.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 8.29s
                        Total time: 15430.46s
                               ETA: 961809.3s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.284s, learning 0.180s)
               Value function loss: 0.0891
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 50.70
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 8.46s
                        Total time: 15438.93s
                               ETA: 961718.1s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.531s, learning 0.169s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0253
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 50.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 8.70s
                        Total time: 15447.63s
                               ETA: 961641.6s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.253s, learning 0.176s)
               Value function loss: 0.1025
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 2.07
               Mean episode length: 49.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 8.43s
                        Total time: 15456.05s
                               ETA: 961548.3s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.149s, learning 0.174s)
               Value function loss: 131.5825
                    Surrogate loss: -0.0008
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 50.35
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 8.32s
                        Total time: 15464.38s
                               ETA: 961448.6s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.196s, learning 0.186s)
               Value function loss: 0.2253
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 50.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 8.38s
                        Total time: 15472.76s
                               ETA: 961352.6s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.182s, learning 0.169s)
               Value function loss: 0.1916
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 50.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 8.35s
                        Total time: 15481.11s
                               ETA: 961254.8s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.328s, learning 0.161s)
               Value function loss: 0.1710
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 10.20
               Mean episode length: 52.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 8.49s
                        Total time: 15489.60s
                               ETA: 961165.7s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.178s, learning 0.216s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 51.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 8.39s
                        Total time: 15497.99s
                               ETA: 961070.9s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.162s, learning 0.205s)
               Value function loss: 212.2849
                    Surrogate loss: -0.0009
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 51.78
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 8.37s
                        Total time: 15506.36s
                               ETA: 960974.5s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.970s, learning 0.164s)
               Value function loss: 104.6635
                    Surrogate loss: -0.0028
             Mean action noise std: 0.71
                       Mean reward: 7.21
               Mean episode length: 51.94
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 8.13s
                        Total time: 15514.49s
                               ETA: 960863.7s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.415s, learning 0.170s)
               Value function loss: 123.5969
                    Surrogate loss: -0.0035
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 51.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 8.58s
                        Total time: 15523.08s
                               ETA: 960781.0s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.270s, learning 0.168s)
               Value function loss: 121.3830
                    Surrogate loss: -0.0039
             Mean action noise std: 0.71
                       Mean reward: 27.29
               Mean episode length: 54.88
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 8.44s
                        Total time: 15531.52s
                               ETA: 960689.2s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.438s, learning 0.213s)
               Value function loss: 43.4481
                    Surrogate loss: -0.0059
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 51.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 8.65s
                        Total time: 15540.17s
                               ETA: 960610.8s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.367s, learning 0.218s)
               Value function loss: 16.2034
                    Surrogate loss: -0.0001
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 51.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 8.59s
                        Total time: 15548.75s
                               ETA: 960528.3s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.273s, learning 0.207s)
               Value function loss: 1.1446
                    Surrogate loss: -0.0176
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 51.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 8.48s
                        Total time: 15557.23s
                               ETA: 960439.6s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.426s, learning 0.165s)
               Value function loss: 0.4960
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 52.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 8.59s
                        Total time: 15565.82s
                               ETA: 960357.7s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.399s, learning 0.185s)
               Value function loss: 361.9760
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 1.72
               Mean episode length: 52.82
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 8.58s
                        Total time: 15574.41s
                               ETA: 960275.5s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.232s, learning 0.169s)
               Value function loss: 1.1081
                    Surrogate loss: -0.0122
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 53.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 8.40s
                        Total time: 15582.81s
                               ETA: 960182.1s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.002s, learning 0.166s)
               Value function loss: 0.3655
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 51.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 8.17s
                        Total time: 15590.98s
                               ETA: 960074.4s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.211s, learning 0.164s)
               Value function loss: 52.9669
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 6.87
               Mean episode length: 53.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 8.37s
                        Total time: 15599.35s
                               ETA: 959979.6s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.272s, learning 0.165s)
               Value function loss: 0.3738
                    Surrogate loss: -0.0273
             Mean action noise std: 0.71
                       Mean reward: 1.64
               Mean episode length: 53.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 8.44s
                        Total time: 15607.79s
                               ETA: 959888.8s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.317s, learning 0.167s)
               Value function loss: 0.4140
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 1.63
               Mean episode length: 53.26
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 8.48s
                        Total time: 15616.27s
                               ETA: 959800.9s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.384s, learning 0.161s)
               Value function loss: 0.3019
                    Surrogate loss: -0.0179
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 52.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 8.54s
                        Total time: 15624.82s
                               ETA: 959716.9s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.994s, learning 0.167s)
               Value function loss: 0.1215
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 51.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 8.16s
                        Total time: 15632.98s
                               ETA: 959609.3s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.961s, learning 0.164s)
               Value function loss: 0.0943
                    Surrogate loss: -0.0277
             Mean action noise std: 0.71
                       Mean reward: 1.63
               Mean episode length: 53.84
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 8.13s
                        Total time: 15641.10s
                               ETA: 959499.8s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.253s, learning 0.170s)
               Value function loss: 301.9910
                    Surrogate loss: 0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 51.94
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 8.42s
                        Total time: 15649.53s
                               ETA: 959408.6s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.395s, learning 0.161s)
               Value function loss: 252.8672
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 53.45
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 8.56s
                        Total time: 15658.08s
                               ETA: 959325.6s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.378s, learning 0.212s)
               Value function loss: 5.6772
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 1.57
               Mean episode length: 53.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 8.59s
                        Total time: 15666.67s
                               ETA: 959244.9s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.162s)
               Value function loss: 1.9093
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 1.64
               Mean episode length: 54.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 8.33s
                        Total time: 15675.00s
                               ETA: 959148.5s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.939s, learning 0.175s)
               Value function loss: 0.7220
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 1.45
               Mean episode length: 52.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 8.11s
                        Total time: 15683.12s
                               ETA: 959038.8s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.262s, learning 0.167s)
               Value function loss: 0.2770
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 53.28
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 8.43s
                        Total time: 15691.55s
                               ETA: 958948.5s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.161s)
               Value function loss: 0.1433
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 1.64
               Mean episode length: 51.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 8.44s
                        Total time: 15699.99s
                               ETA: 958859.0s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.166s)
               Value function loss: 0.1208
                    Surrogate loss: -0.0327
             Mean action noise std: 0.71
                       Mean reward: 1.45
               Mean episode length: 53.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 8.46s
                        Total time: 15708.44s
                               ETA: 958770.5s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.357s, learning 0.166s)
               Value function loss: 13.7289
                    Surrogate loss: 0.0024
             Mean action noise std: 0.71
                       Mean reward: 1.53
               Mean episode length: 53.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 8.52s
                        Total time: 15716.97s
                               ETA: 958686.3s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.486s, learning 0.171s)
               Value function loss: 0.1262
                    Surrogate loss: -0.0320
             Mean action noise std: 0.71
                       Mean reward: 1.46
               Mean episode length: 52.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 8.66s
                        Total time: 15725.62s
                               ETA: 958610.3s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.169s, learning 0.168s)
               Value function loss: 0.1152
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 1.58
               Mean episode length: 53.09
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 8.34s
                        Total time: 15733.96s
                               ETA: 958514.9s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.172s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 1.62
               Mean episode length: 53.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 8.48s
                        Total time: 15742.45s
                               ETA: 958428.5s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.926s, learning 0.207s)
               Value function loss: 13.7270
                    Surrogate loss: 0.0003
             Mean action noise std: 0.71
                       Mean reward: 4.16
               Mean episode length: 54.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 8.13s
                        Total time: 15750.58s
                               ETA: 958320.9s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.141s, learning 0.210s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 1.43
               Mean episode length: 53.93
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 8.35s
                        Total time: 15758.93s
                               ETA: 958226.6s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.020s, learning 0.164s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 1.65
               Mean episode length: 53.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 8.18s
                        Total time: 15767.11s
                               ETA: 958122.4s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.168s)
               Value function loss: 0.0388
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 1.32
               Mean episode length: 54.61
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 8.38s
                        Total time: 15775.49s
                               ETA: 958030.2s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.409s, learning 0.167s)
               Value function loss: 0.0374
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 1.39
               Mean episode length: 52.69
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 8.58s
                        Total time: 15784.07s
                               ETA: 957950.0s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.162s)
               Value function loss: 0.0529
                    Surrogate loss: -0.0316
             Mean action noise std: 0.71
                       Mean reward: 1.26
               Mean episode length: 53.70
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 8.66s
                        Total time: 15792.73s
                               ETA: 957874.7s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.292s, learning 0.186s)
               Value function loss: 0.0469
                    Surrogate loss: -0.0284
             Mean action noise std: 0.71
                       Mean reward: 1.34
               Mean episode length: 56.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 8.48s
                        Total time: 15801.21s
                               ETA: 957788.7s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.392s, learning 0.175s)
               Value function loss: 0.0424
                    Surrogate loss: -0.0251
             Mean action noise std: 0.71
                       Mean reward: 1.33
               Mean episode length: 54.38
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 8.57s
                        Total time: 15809.77s
                               ETA: 957708.2s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.050s, learning 0.171s)
               Value function loss: 0.0526
                    Surrogate loss: -0.0236
             Mean action noise std: 0.71
                       Mean reward: 1.24
               Mean episode length: 55.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 8.22s
                        Total time: 15818.00s
                               ETA: 957606.8s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.114s, learning 0.172s)
               Value function loss: 0.0358
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 1.23
               Mean episode length: 55.50
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 8.29s
                        Total time: 15826.28s
                               ETA: 957509.5s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.162s)
               Value function loss: 0.0309
                    Surrogate loss: -0.0264
             Mean action noise std: 0.71
                       Mean reward: 1.28
               Mean episode length: 55.78
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 8.58s
                        Total time: 15834.86s
                               ETA: 957429.7s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.279s, learning 0.173s)
               Value function loss: 0.0307
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 1.54
               Mean episode length: 54.92
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 8.45s
                        Total time: 15843.31s
                               ETA: 957342.6s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.359s, learning 0.220s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 1.22
               Mean episode length: 53.94
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 8.58s
                        Total time: 15851.89s
                               ETA: 957263.3s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.168s, learning 0.170s)
               Value function loss: 0.0214
                    Surrogate loss: -0.0389
             Mean action noise std: 0.71
                       Mean reward: 1.25
               Mean episode length: 56.04
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 8.34s
                        Total time: 15860.23s
                               ETA: 957169.5s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.198s, learning 0.167s)
               Value function loss: 0.0223
                    Surrogate loss: -0.0352
             Mean action noise std: 0.71
                       Mean reward: 1.14
               Mean episode length: 54.57
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 8.36s
                        Total time: 15868.59s
                               ETA: 957077.4s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.096s, learning 0.168s)
               Value function loss: 0.1188
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 1.28
               Mean episode length: 55.14
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 8.26s
                        Total time: 15876.85s
                               ETA: 956979.3s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.458s, learning 0.175s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0405
             Mean action noise std: 0.71
                       Mean reward: 1.30
               Mean episode length: 55.56
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 8.63s
                        Total time: 15885.49s
                               ETA: 956903.6s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.163s, learning 0.170s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0355
             Mean action noise std: 0.71
                       Mean reward: 1.20
               Mean episode length: 54.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 8.33s
                        Total time: 15893.82s
                               ETA: 956809.9s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.161s, learning 0.161s)
               Value function loss: 0.0200
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 1.17
               Mean episode length: 54.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 8.32s
                        Total time: 15902.14s
                               ETA: 956715.6s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.362s, learning 0.167s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0375
             Mean action noise std: 0.71
                       Mean reward: 1.44
               Mean episode length: 54.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 8.53s
                        Total time: 15910.67s
                               ETA: 956633.9s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.099s, learning 0.160s)
               Value function loss: 0.0168
                    Surrogate loss: -0.0345
             Mean action noise std: 0.71
                       Mean reward: 1.21
               Mean episode length: 55.95
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 8.26s
                        Total time: 15918.93s
                               ETA: 956536.1s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.166s)
               Value function loss: 0.0162
                    Surrogate loss: -0.0390
             Mean action noise std: 0.71
                       Mean reward: 1.24
               Mean episode length: 54.07
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 8.21s
                        Total time: 15927.14s
                               ETA: 956435.6s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.180s, learning 0.161s)
               Value function loss: 0.0137
                    Surrogate loss: -0.0401
             Mean action noise std: 0.71
                       Mean reward: 1.35
               Mean episode length: 56.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 8.34s
                        Total time: 15935.48s
                               ETA: 956342.9s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.372s, learning 0.167s)
               Value function loss: 0.0125
                    Surrogate loss: -0.0467
             Mean action noise std: 0.71
                       Mean reward: 1.27
               Mean episode length: 56.54
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 8.54s
                        Total time: 15944.02s
                               ETA: 956262.2s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.978s, learning 0.165s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0421
             Mean action noise std: 0.71
                       Mean reward: 1.34
               Mean episode length: 55.21
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 8.14s
                        Total time: 15952.17s
                               ETA: 956157.9s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.163s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0406
             Mean action noise std: 0.71
                       Mean reward: 1.11
               Mean episode length: 55.76
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 8.17s
                        Total time: 15960.34s
                               ETA: 956055.3s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.191s, learning 0.161s)
               Value function loss: 0.0128
                    Surrogate loss: -0.0427
             Mean action noise std: 0.71
                       Mean reward: 1.13
               Mean episode length: 55.67
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 8.35s
                        Total time: 15968.69s
                               ETA: 955963.6s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.163s)
               Value function loss: 0.0123
                    Surrogate loss: -0.0431
             Mean action noise std: 0.71
                       Mean reward: 1.17
               Mean episode length: 55.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 8.46s
                        Total time: 15977.15s
                               ETA: 955878.5s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.266s, learning 0.163s)
               Value function loss: 0.0197
                    Surrogate loss: -0.0297
             Mean action noise std: 0.71
                       Mean reward: 1.25
               Mean episode length: 55.58
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 8.43s
                        Total time: 15985.58s
                               ETA: 955791.7s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.105s, learning 0.165s)
               Value function loss: 0.0174
                    Surrogate loss: -0.0348
             Mean action noise std: 0.71
                       Mean reward: 1.19
               Mean episode length: 56.86
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 8.27s
                        Total time: 15993.85s
                               ETA: 955695.5s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.117s, learning 0.174s)
               Value function loss: 0.0141
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 1.31
               Mean episode length: 54.11
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 8.29s
                        Total time: 16002.14s
                               ETA: 955600.6s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.490s, learning 0.184s)
               Value function loss: 0.0152
                    Surrogate loss: -0.0385
             Mean action noise std: 0.71
                       Mean reward: 1.23
               Mean episode length: 55.77
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 8.67s
                        Total time: 16010.81s
                               ETA: 955528.8s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.439s, learning 0.228s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0381
             Mean action noise std: 0.71
                       Mean reward: 1.21
               Mean episode length: 54.17
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 8.67s
                        Total time: 16019.48s
                               ETA: 955456.5s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.307s, learning 0.164s)
               Value function loss: 0.0129
                    Surrogate loss: -0.0388
             Mean action noise std: 0.71
                       Mean reward: 1.22
               Mean episode length: 55.33
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 8.47s
                        Total time: 16027.95s
                               ETA: 955372.6s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.162s)
               Value function loss: 0.0149
                    Surrogate loss: -0.0366
             Mean action noise std: 0.71
                       Mean reward: 1.19
               Mean episode length: 57.03
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 8.44s
                        Total time: 16036.39s
                               ETA: 955287.0s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.328s, learning 0.162s)
               Value function loss: 0.0138
                    Surrogate loss: -0.0386
             Mean action noise std: 0.71
                       Mean reward: 1.17
               Mean episode length: 56.52
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 8.49s
                        Total time: 16044.88s
                               ETA: 955204.6s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.153s, learning 0.159s)
               Value function loss: 0.0119
                    Surrogate loss: -0.0428
             Mean action noise std: 0.71
                       Mean reward: 1.16
               Mean episode length: 55.64
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 8.31s
                        Total time: 16053.19s
                               ETA: 955111.5s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.072s, learning 0.165s)
               Value function loss: 0.0133
                    Surrogate loss: -0.0395
             Mean action noise std: 0.71
                       Mean reward: 1.28
               Mean episode length: 56.00
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 8.24s
                        Total time: 16061.43s
                               ETA: 955014.1s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.353s, learning 0.178s)
               Value function loss: 0.0132
                    Surrogate loss: -0.0369
             Mean action noise std: 0.71
                       Mean reward: 1.28
               Mean episode length: 55.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 8.53s
                        Total time: 16069.96s
                               ETA: 954934.3s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.196s, learning 0.174s)
               Value function loss: 0.0126
                    Surrogate loss: -0.0401
             Mean action noise std: 0.71
                       Mean reward: 1.22
               Mean episode length: 55.99
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 8.37s
                        Total time: 16078.33s
                               ETA: 954845.0s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.118s, learning 0.164s)
               Value function loss: 0.0146
                    Surrogate loss: -0.0403
             Mean action noise std: 0.71
                       Mean reward: 1.30
               Mean episode length: 55.46
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 8.28s
                        Total time: 16086.61s
                               ETA: 954750.6s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.243s, learning 0.162s)
               Value function loss: 0.0160
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 1.34
               Mean episode length: 54.22
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 8.40s
                        Total time: 16095.02s
                               ETA: 954663.6s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.346s, learning 0.207s)
               Value function loss: 0.0134
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 1.29
               Mean episode length: 54.65
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 8.55s
                        Total time: 16103.57s
                               ETA: 954585.4s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.196s, learning 0.169s)
               Value function loss: 0.0164
                    Surrogate loss: -0.0368
             Mean action noise std: 0.71
                       Mean reward: 1.24
               Mean episode length: 54.93
                  Mean reward/step: 0.02
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 8.36s
                        Total time: 16111.93s
                               ETA: 954496.2s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.131s, learning 0.161s)
               Value function loss: 109.9162
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 1.38
               Mean episode length: 54.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 8.29s
                        Total time: 16120.23s
                               ETA: 954402.8s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.208s, learning 0.170s)
               Value function loss: 0.0521
                    Surrogate loss: -0.0265
             Mean action noise std: 0.71
                       Mean reward: 1.25
               Mean episode length: 53.36
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 8.38s
                        Total time: 16128.60s
                               ETA: 954314.6s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.168s, learning 0.163s)
               Value function loss: 0.0421
                    Surrogate loss: -0.0264
             Mean action noise std: 0.71
                       Mean reward: 1.30
               Mean episode length: 53.80
                  Mean reward/step: 0.02
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 8.33s
                        Total time: 16136.94s
                               ETA: 954223.7s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1996 steps/s (collection: 7.947s, learning 0.258s)
               Value function loss: 0.0365
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 1.31
               Mean episode length: 55.01
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 8.21s
                        Total time: 16145.14s
                               ETA: 954125.5s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.270s, learning 0.225s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0253
             Mean action noise std: 0.71
                       Mean reward: 1.20
               Mean episode length: 54.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 8.49s
                        Total time: 16153.64s
                               ETA: 954044.4s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.933s, learning 0.162s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 1.43
               Mean episode length: 53.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 8.09s
                        Total time: 16161.73s
                               ETA: 953939.8s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.267s, learning 0.158s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 1.46
               Mean episode length: 53.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 8.42s
                        Total time: 16170.16s
                               ETA: 953854.9s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.105s, learning 0.166s)
               Value function loss: 0.0369
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 1.64
               Mean episode length: 52.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 8.27s
                        Total time: 16178.43s
                               ETA: 953760.9s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.948s, learning 0.164s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 1.54
               Mean episode length: 53.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 8.11s
                        Total time: 16186.54s
                               ETA: 953657.6s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.180s, learning 0.184s)
               Value function loss: 0.0240
                    Surrogate loss: -0.0295
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 51.89
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 8.36s
                        Total time: 16194.90s
                               ETA: 953569.4s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.982s, learning 0.166s)
               Value function loss: 0.0335
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 51.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 8.15s
                        Total time: 16203.05s
                               ETA: 953468.5s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.245s, learning 0.162s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 1.59
               Mean episode length: 53.16
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 8.41s
                        Total time: 16211.46s
                               ETA: 953383.0s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 2090 steps/s (collection: 7.678s, learning 0.161s)
               Value function loss: 0.0269
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 1.54
               Mean episode length: 51.70
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 7.84s
                        Total time: 16219.30s
                               ETA: 953264.2s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.061s, learning 0.209s)
               Value function loss: 0.0242
                    Surrogate loss: -0.0321
             Mean action noise std: 0.71
                       Mean reward: 1.69
               Mean episode length: 53.12
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 8.27s
                        Total time: 16227.57s
                               ETA: 953170.8s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 2022 steps/s (collection: 7.898s, learning 0.203s)
               Value function loss: 0.0178
                    Surrogate loss: -0.0326
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 52.91
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 8.10s
                        Total time: 16235.67s
                               ETA: 953067.6s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.129s, learning 0.161s)
               Value function loss: 0.0185
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 1.68
               Mean episode length: 51.35
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 8.29s
                        Total time: 16243.96s
                               ETA: 952975.6s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.117s, learning 0.161s)
               Value function loss: 0.0232
                    Surrogate loss: -0.0319
             Mean action noise std: 0.71
                       Mean reward: 1.55
               Mean episode length: 53.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 8.28s
                        Total time: 16252.24s
                               ETA: 952883.0s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.286s, learning 0.158s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0340
             Mean action noise std: 0.71
                       Mean reward: 1.60
               Mean episode length: 52.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 8.44s
                        Total time: 16260.68s
                               ETA: 952800.3s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.168s)
               Value function loss: 0.0189
                    Surrogate loss: -0.0358
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 50.66
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 8.41s
                        Total time: 16269.09s
                               ETA: 952715.8s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.363s, learning 0.210s)
               Value function loss: 0.0217
                    Surrogate loss: -0.0353
             Mean action noise std: 0.71
                       Mean reward: 1.65
               Mean episode length: 52.21
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 8.57s
                        Total time: 16277.67s
                               ETA: 952640.8s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.225s, learning 0.173s)
               Value function loss: 0.0181
                    Surrogate loss: -0.0357
             Mean action noise std: 0.71
                       Mean reward: 1.62
               Mean episode length: 52.17
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 8.40s
                        Total time: 16286.06s
                               ETA: 952555.6s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 2007 steps/s (collection: 8.004s, learning 0.159s)
               Value function loss: 0.0212
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 1.49
               Mean episode length: 51.49
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 8.16s
                        Total time: 16294.23s
                               ETA: 952456.7s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.986s, learning 0.178s)
               Value function loss: 0.0220
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 52.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 8.16s
                        Total time: 16302.39s
                               ETA: 952358.0s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.228s, learning 0.171s)
               Value function loss: 0.0249
                    Surrogate loss: -0.0308
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 52.50
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 8.40s
                        Total time: 16310.79s
                               ETA: 952273.2s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.359s, learning 0.167s)
               Value function loss: 0.0227
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 1.69
               Mean episode length: 52.03
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 8.53s
                        Total time: 16319.32s
                               ETA: 952195.9s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.280s, learning 0.159s)
               Value function loss: 0.0255
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 52.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 8.44s
                        Total time: 16327.76s
                               ETA: 952113.6s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.420s, learning 0.173s)
               Value function loss: 0.0294
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 51.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 8.59s
                        Total time: 16336.35s
                               ETA: 952040.3s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.537s, learning 0.178s)
               Value function loss: 0.0279
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 1.69
               Mean episode length: 52.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 8.71s
                        Total time: 16345.07s
                               ETA: 951974.2s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.354s, learning 0.164s)
               Value function loss: 0.0395
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 51.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 8.52s
                        Total time: 16353.58s
                               ETA: 951896.7s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.048s, learning 0.165s)
               Value function loss: 0.0247
                    Surrogate loss: -0.0335
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 49.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 8.21s
                        Total time: 16361.80s
                               ETA: 951801.5s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.211s, learning 0.163s)
               Value function loss: 0.0337
                    Surrogate loss: -0.0300
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 49.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 8.37s
                        Total time: 16370.17s
                               ETA: 951715.8s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.958s, learning 0.162s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0254
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 53.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 8.12s
                        Total time: 16378.29s
                               ETA: 951615.5s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.222s, learning 0.158s)
               Value function loss: 0.0356
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 53.21
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 8.38s
                        Total time: 16386.67s
                               ETA: 951530.4s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.232s, learning 0.168s)
               Value function loss: 301.0348
                    Surrogate loss: -0.0016
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 51.63
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 8.40s
                        Total time: 16395.07s
                               ETA: 951446.4s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.106s, learning 0.212s)
               Value function loss: 159.0584
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 50.17
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 8.32s
                        Total time: 16403.39s
                               ETA: 951357.9s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1132 steps/s (collection: 14.257s, learning 0.208s)
               Value function loss: 1.2197
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 50.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 14.47s
                        Total time: 16417.86s
                               ETA: 951625.8s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.882s, learning 0.192s)
               Value function loss: 0.5443
                    Surrogate loss: -0.0227
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 51.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 16.07s
                        Total time: 16433.93s
                               ETA: 951986.4s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.848s, learning 0.161s)
               Value function loss: 0.2601
                    Surrogate loss: -0.0209
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 49.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 16.01s
                        Total time: 16449.94s
                               ETA: 952342.9s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.021s, learning 0.159s)
               Value function loss: 0.2451
                    Surrogate loss: -0.0165
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 49.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 16.18s
                        Total time: 16466.12s
                               ETA: 952708.8s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.025s, learning 0.161s)
               Value function loss: 0.1192
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 1.77
               Mean episode length: 49.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 16.19s
                        Total time: 16482.30s
                               ETA: 953074.6s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.093s, learning 0.187s)
               Value function loss: 0.0511
                    Surrogate loss: -0.0268
             Mean action noise std: 0.71
                       Mean reward: 1.99
               Mean episode length: 48.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 16.28s
                        Total time: 16498.58s
                               ETA: 953445.5s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.213s, learning 0.159s)
               Value function loss: 0.0653
                    Surrogate loss: -0.0233
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 48.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 16.37s
                        Total time: 16514.95s
                               ETA: 953821.1s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.045s, learning 0.163s)
               Value function loss: 0.0612
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 49.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 16.21s
                        Total time: 16531.16s
                               ETA: 954186.8s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.153s, learning 0.205s)
               Value function loss: 0.0590
                    Surrogate loss: -0.0286
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 50.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 16.36s
                        Total time: 16547.52s
                               ETA: 954560.8s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.943s, learning 0.165s)
               Value function loss: 0.0773
                    Surrogate loss: -0.0184
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 51.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 16.11s
                        Total time: 16563.63s
                               ETA: 954919.9s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.229s, learning 0.281s)
               Value function loss: 0.0371
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 50.54
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 16.51s
                        Total time: 16580.14s
                               ETA: 955301.7s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.383s, learning 0.163s)
               Value function loss: 0.0479
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 50.42
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 16.55s
                        Total time: 16596.68s
                               ETA: 955685.0s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.264s, learning 0.206s)
               Value function loss: 0.0295
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 51.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 16.47s
                        Total time: 16613.15s
                               ETA: 956063.6s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.171s, learning 0.211s)
               Value function loss: 0.0281
                    Surrogate loss: -0.0325
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 50.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 16.38s
                        Total time: 16629.53s
                               ETA: 956436.7s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.354s, learning 0.165s)
               Value function loss: 0.0292
                    Surrogate loss: -0.0305
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 50.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 16.52s
                        Total time: 16646.05s
                               ETA: 956817.1s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.195s, learning 0.160s)
               Value function loss: 0.0399
                    Surrogate loss: -0.0250
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 50.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 16.35s
                        Total time: 16662.41s
                               ETA: 957187.7s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.848s, learning 0.166s)
               Value function loss: 0.0278
                    Surrogate loss: -0.0318
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 50.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 16.01s
                        Total time: 16678.42s
                               ETA: 957538.3s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.713s, learning 0.164s)
               Value function loss: 0.0305
                    Surrogate loss: -0.0292
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 49.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 15.88s
                        Total time: 16694.30s
                               ETA: 957880.6s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.208s, learning 0.177s)
               Value function loss: 0.0311
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 49.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 16.39s
                        Total time: 16710.69s
                               ETA: 958251.6s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.916s, learning 0.167s)
               Value function loss: 0.0342
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 49.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 16.08s
                        Total time: 16726.77s
                               ETA: 958604.8s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.119s, learning 0.161s)
               Value function loss: 0.0360
                    Surrogate loss: -0.0239
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 49.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 16.28s
                        Total time: 16743.05s
                               ETA: 958968.9s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.283s, learning 0.189s)
               Value function loss: 0.0354
                    Surrogate loss: -0.0291
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 51.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 16.47s
                        Total time: 16759.52s
                               ETA: 959343.5s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.183s, learning 0.211s)
               Value function loss: 0.0462
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 2.04
               Mean episode length: 51.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 16.39s
                        Total time: 16775.92s
                               ETA: 959713.2s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.139s, learning 0.161s)
               Value function loss: 13.5782
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 50.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 16.30s
                        Total time: 16792.21s
                               ETA: 960077.0s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.206s, learning 0.163s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 49.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 16.37s
                        Total time: 16808.58s
                               ETA: 960444.4s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.957s, learning 0.202s)
               Value function loss: 0.0753
                    Surrogate loss: -0.0208
             Mean action noise std: 0.71
                       Mean reward: 4.71
               Mean episode length: 50.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 16.16s
                        Total time: 16824.74s
                               ETA: 960799.4s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.064s, learning 0.160s)
               Value function loss: 11.8705
                    Surrogate loss: 0.0019
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 51.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 16.22s
                        Total time: 16840.97s
                               ETA: 961157.6s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.269s, learning 0.169s)
               Value function loss: 0.1373
                    Surrogate loss: -0.0174
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 49.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 16.44s
                        Total time: 16857.40s
                               ETA: 961527.6s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.230s, learning 0.164s)
               Value function loss: 0.1046
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 50.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 16.39s
                        Total time: 16873.80s
                               ETA: 961894.6s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.469s, learning 0.173s)
               Value function loss: 0.0675
                    Surrogate loss: -0.0280
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 49.51
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 16.64s
                        Total time: 16890.44s
                               ETA: 962275.3s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.989s, learning 0.164s)
               Value function loss: 0.0432
                    Surrogate loss: -0.0294
             Mean action noise std: 0.71
                       Mean reward: 5.02
               Mean episode length: 50.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 16.15s
                        Total time: 16906.59s
                               ETA: 962627.8s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.261s, learning 0.169s)
               Value function loss: 0.0543
                    Surrogate loss: -0.0264
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 48.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 16.43s
                        Total time: 16923.02s
                               ETA: 962995.5s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.134s, learning 0.170s)
               Value function loss: 0.0548
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 48.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 16.30s
                        Total time: 16939.33s
                               ETA: 963355.6s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.654s, learning 0.181s)
               Value function loss: 231.1812
                    Surrogate loss: 0.0007
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 49.15
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 16.83s
                        Total time: 16956.16s
                               ETA: 963745.5s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.771s, learning 0.222s)
               Value function loss: 214.4122
                    Surrogate loss: -0.0024
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 50.74
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 15.99s
                        Total time: 16972.16s
                               ETA: 964087.1s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.349s, learning 0.162s)
               Value function loss: 7.1568
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 2.09
               Mean episode length: 52.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 16.51s
                        Total time: 16988.67s
                               ETA: 964457.7s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.109s, learning 0.164s)
               Value function loss: 1.1739
                    Surrogate loss: 0.1069
             Mean action noise std: 0.71
                       Mean reward: 14.85
               Mean episode length: 49.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 16.27s
                        Total time: 17004.94s
                               ETA: 964814.4s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1063 steps/s (collection: 15.239s, learning 0.165s)
               Value function loss: 0.6203
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 2.43
               Mean episode length: 50.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 15.40s
                        Total time: 17020.34s
                               ETA: 965121.3s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.425s, learning 0.168s)
               Value function loss: 0.2640
                    Surrogate loss: -0.0116
             Mean action noise std: 0.71
                       Mean reward: 2.32
               Mean episode length: 49.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 8.59s
                        Total time: 17028.94s
                               ETA: 965041.9s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.019s, learning 0.163s)
               Value function loss: 0.2347
                    Surrogate loss: -0.0100
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 48.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 8.18s
                        Total time: 17037.12s
                               ETA: 964939.2s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.161s)
               Value function loss: 0.1394
                    Surrogate loss: -0.0181
             Mean action noise std: 0.71
                       Mean reward: 2.24
               Mean episode length: 49.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 8.49s
                        Total time: 17045.61s
                               ETA: 964854.0s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.103s, learning 0.161s)
               Value function loss: 0.1005
                    Surrogate loss: -0.0203
             Mean action noise std: 0.71
                       Mean reward: 1.98
               Mean episode length: 47.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 8.26s
                        Total time: 17053.87s
                               ETA: 964756.2s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.217s, learning 0.203s)
               Value function loss: 90.7069
                    Surrogate loss: -0.0006
             Mean action noise std: 0.71
                       Mean reward: 2.18
               Mean episode length: 48.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 8.42s
                        Total time: 17062.29s
                               ETA: 964667.3s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.377s, learning 0.214s)
               Value function loss: 63.2336
                    Surrogate loss: -0.0030
             Mean action noise std: 0.71
                       Mean reward: 2.48
               Mean episode length: 49.25
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 8.59s
                        Total time: 17070.88s
                               ETA: 964588.2s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.217s, learning 0.175s)
               Value function loss: 1.9813
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 2.27
               Mean episode length: 49.29
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 8.39s
                        Total time: 17079.27s
                               ETA: 964498.0s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.415s, learning 0.164s)
               Value function loss: 0.4905
                    Surrogate loss: -0.0243
             Mean action noise std: 0.71
                       Mean reward: 2.03
               Mean episode length: 49.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 8.58s
                        Total time: 17087.85s
                               ETA: 964418.4s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.209s, learning 0.221s)
               Value function loss: 70.7802
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 49.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 8.43s
                        Total time: 17096.28s
                               ETA: 964330.4s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.228s, learning 0.283s)
               Value function loss: 0.2338
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 2.20
               Mean episode length: 49.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 8.51s
                        Total time: 17104.79s
                               ETA: 964247.1s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.291s, learning 0.207s)
               Value function loss: 0.1919
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 2.16
               Mean episode length: 49.72
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 8.50s
                        Total time: 17113.29s
                               ETA: 964163.2s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.440s, learning 0.178s)
               Value function loss: 0.1412
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 2.06
               Mean episode length: 49.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 8.62s
                        Total time: 17121.91s
                               ETA: 964086.1s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.136s, learning 0.186s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0234
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 51.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 8.32s
                        Total time: 17130.23s
                               ETA: 963992.5s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.351s, learning 0.162s)
               Value function loss: 0.1169
                    Surrogate loss: -0.0189
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 50.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 8.51s
                        Total time: 17138.74s
                               ETA: 963909.6s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 2000 steps/s (collection: 7.987s, learning 0.205s)
               Value function loss: 0.0869
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 2.29
               Mean episode length: 51.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 8.19s
                        Total time: 17146.94s
                               ETA: 963808.8s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.512s, learning 0.167s)
               Value function loss: 0.0532
                    Surrogate loss: -0.0302
             Mean action noise std: 0.71
                       Mean reward: 2.23
               Mean episode length: 50.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 8.68s
                        Total time: 17155.62s
                               ETA: 963735.6s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.162s)
               Value function loss: 0.0711
                    Surrogate loss: -0.0260
             Mean action noise std: 0.71
                       Mean reward: 2.11
               Mean episode length: 51.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 8.25s
                        Total time: 17163.86s
                               ETA: 963638.0s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.262s, learning 0.168s)
               Value function loss: 0.0457
                    Surrogate loss: -0.0249
             Mean action noise std: 0.71
                       Mean reward: 2.22
               Mean episode length: 50.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 8.43s
                        Total time: 17172.29s
                               ETA: 963550.9s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.081s, learning 0.161s)
               Value function loss: 213.7474
                    Surrogate loss: -0.0011
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 51.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 8.24s
                        Total time: 17180.53s
                               ETA: 963453.3s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.298s, learning 0.171s)
               Value function loss: 4.0402
                    Surrogate loss: -0.0032
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 51.40
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 8.47s
                        Total time: 17189.00s
                               ETA: 963368.6s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.212s, learning 0.167s)
               Value function loss: 0.1698
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 14.80
               Mean episode length: 50.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 8.38s
                        Total time: 17197.38s
                               ETA: 963278.9s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.277s, learning 0.172s)
               Value function loss: 0.1661
                    Surrogate loss: -0.0106
             Mean action noise std: 0.71
                       Mean reward: 2.08
               Mean episode length: 50.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 8.45s
                        Total time: 17205.83s
                               ETA: 963193.2s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.164s)
               Value function loss: 0.0818
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 50.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 8.55s
                        Total time: 17214.39s
                               ETA: 963113.6s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.375s, learning 0.163s)
               Value function loss: 0.0649
                    Surrogate loss: -0.0210
             Mean action noise std: 0.71
                       Mean reward: 2.14
               Mean episode length: 53.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 8.54s
                        Total time: 17222.92s
                               ETA: 963033.0s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.470s, learning 0.159s)
               Value function loss: 0.0547
                    Surrogate loss: -0.0267
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 50.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 8.63s
                        Total time: 17231.55s
                               ETA: 962957.6s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.099s, learning 0.167s)
               Value function loss: 0.0492
                    Surrogate loss: -0.0235
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 51.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 8.27s
                        Total time: 17239.82s
                               ETA: 962862.0s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.540s, learning 0.166s)
               Value function loss: 0.0446
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 50.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 8.71s
                        Total time: 17248.53s
                               ETA: 962791.1s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.161s, learning 0.165s)
               Value function loss: 0.0350
                    Surrogate loss: -0.0244
             Mean action noise std: 0.71
                       Mean reward: 1.97
               Mean episode length: 51.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 8.33s
                        Total time: 17256.85s
                               ETA: 962699.1s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.076s, learning 0.162s)
               Value function loss: 7.0330
                    Surrogate loss: 0.0015
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 50.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 8.24s
                        Total time: 17265.09s
                               ETA: 962602.2s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.069s, learning 0.164s)
               Value function loss: 0.0461
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 1.68
               Mean episode length: 50.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 8.23s
                        Total time: 17273.32s
                               ETA: 962505.2s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.165s)
               Value function loss: 0.0632
                    Surrogate loss: -0.0190
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 50.95
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 8.42s
                        Total time: 17281.75s
                               ETA: 962418.9s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.030s, learning 0.173s)
               Value function loss: 0.0485
                    Surrogate loss: -0.0185
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 51.55
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 8.20s
                        Total time: 17289.95s
                               ETA: 962320.4s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.509s, learning 0.161s)
               Value function loss: 0.0467
                    Surrogate loss: -0.0213
             Mean action noise std: 0.71
                       Mean reward: 1.66
               Mean episode length: 52.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 8.67s
                        Total time: 17298.62s
                               ETA: 962248.0s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.162s)
               Value function loss: 0.0400
                    Surrogate loss: -0.0230
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 51.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 8.45s
                        Total time: 17307.07s
                               ETA: 962163.4s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.229s, learning 0.161s)
               Value function loss: 0.0473
                    Surrogate loss: -0.0237
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 53.18
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 8.39s
                        Total time: 17315.46s
                               ETA: 962075.5s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.211s, learning 0.162s)
               Value function loss: 0.0411
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 52.75
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 8.37s
                        Total time: 17323.83s
                               ETA: 961986.9s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.408s, learning 0.167s)
               Value function loss: 0.0257
                    Surrogate loss: -0.0322
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 52.76
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 8.57s
                        Total time: 17332.41s
                               ETA: 961909.4s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.237s, learning 0.174s)
               Value function loss: 0.0228
                    Surrogate loss: -0.0349
             Mean action noise std: 0.71
                       Mean reward: 1.66
               Mean episode length: 53.24
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 8.41s
                        Total time: 17340.82s
                               ETA: 961823.0s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.079s, learning 0.171s)
               Value function loss: 0.0231
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 52.05
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 8.25s
                        Total time: 17349.07s
                               ETA: 961727.8s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.333s, learning 0.191s)
               Value function loss: 0.0273
                    Surrogate loss: -0.0281
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 53.36
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 8.52s
                        Total time: 17357.59s
                               ETA: 961647.9s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.254s, learning 0.170s)
               Value function loss: 0.0207
                    Surrogate loss: -0.0350
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 53.14
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 8.42s
                        Total time: 17366.02s
                               ETA: 961562.4s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.113s, learning 0.218s)
               Value function loss: 0.0205
                    Surrogate loss: -0.0329
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 51.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 8.33s
                        Total time: 17374.35s
                               ETA: 961471.9s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.180s, learning 0.178s)
               Value function loss: 0.0173
                    Surrogate loss: -0.0364
             Mean action noise std: 0.71
                       Mean reward: 1.58
               Mean episode length: 52.39
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 8.36s
                        Total time: 17382.71s
                               ETA: 961383.0s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.417s, learning 0.257s)
               Value function loss: 0.0241
                    Surrogate loss: -0.0274
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 52.22
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 8.67s
                        Total time: 17391.38s
                               ETA: 961311.7s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.133s, learning 0.205s)
               Value function loss: 0.0208
                    Surrogate loss: -0.0309
             Mean action noise std: 0.71
                       Mean reward: 1.59
               Mean episode length: 51.98
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 8.34s
                        Total time: 17399.72s
                               ETA: 961221.8s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.411s, learning 0.173s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 52.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 8.58s
                        Total time: 17408.30s
                               ETA: 961145.7s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.337s, learning 0.163s)
               Value function loss: 0.0322
                    Surrogate loss: -0.0231
             Mean action noise std: 0.71
                       Mean reward: 1.60
               Mean episode length: 51.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 8.50s
                        Total time: 17416.80s
                               ETA: 961065.0s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.162s, learning 0.177s)
               Value function loss: 0.0229
                    Surrogate loss: -0.0298
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 53.37
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 8.34s
                        Total time: 17425.14s
                               ETA: 960975.5s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.396s, learning 0.174s)
               Value function loss: 0.0268
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 1.50
               Mean episode length: 53.41
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 8.57s
                        Total time: 17433.71s
                               ETA: 960898.7s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.301s, learning 0.166s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0288
             Mean action noise std: 0.71
                       Mean reward: 1.55
               Mean episode length: 53.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 8.47s
                        Total time: 17442.18s
                               ETA: 960816.5s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.358s, learning 0.164s)
               Value function loss: 0.0235
                    Surrogate loss: -0.0296
             Mean action noise std: 0.71
                       Mean reward: 1.81
               Mean episode length: 52.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 8.52s
                        Total time: 17450.70s
                               ETA: 960737.3s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.142s, learning 0.172s)
               Value function loss: 0.0234
                    Surrogate loss: -0.0283
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.67
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 8.31s
                        Total time: 17459.01s
                               ETA: 960646.8s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.160s)
               Value function loss: 0.0177
                    Surrogate loss: -0.0347
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 8.44s
                        Total time: 17467.45s
                               ETA: 960563.1s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.074s, learning 0.162s)
               Value function loss: 17.3063
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 51.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 8.24s
                        Total time: 17475.69s
                               ETA: 960468.5s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 2027 steps/s (collection: 7.918s, learning 0.162s)
               Value function loss: 0.0631
                    Surrogate loss: -0.0306
             Mean action noise std: 0.71
                       Mean reward: 1.64
               Mean episode length: 51.81
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 8.08s
                        Total time: 17483.77s
                               ETA: 960365.4s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.000s, learning 0.166s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0199
             Mean action noise std: 0.71
                       Mean reward: 1.63
               Mean episode length: 50.62
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 8.17s
                        Total time: 17491.93s
                               ETA: 960267.1s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.396s, learning 0.220s)
               Value function loss: 0.0375
                    Surrogate loss: -0.0242
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 51.92
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 8.62s
                        Total time: 17500.55s
                               ETA: 960193.6s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.175s, learning 0.207s)
               Value function loss: 0.0299
                    Surrogate loss: -0.0222
             Mean action noise std: 0.71
                       Mean reward: 1.68
               Mean episode length: 52.44
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 8.38s
                        Total time: 17508.93s
                               ETA: 960107.3s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.305s, learning 0.263s)
               Value function loss: 0.0216
                    Surrogate loss: -0.0313
             Mean action noise std: 0.71
                       Mean reward: 1.77
               Mean episode length: 50.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 8.57s
                        Total time: 17517.50s
                               ETA: 960031.3s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.231s, learning 0.169s)
               Value function loss: 0.0248
                    Surrogate loss: -0.0299
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 51.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 8.40s
                        Total time: 17525.90s
                               ETA: 959946.2s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.578s, learning 0.176s)
               Value function loss: 17.6200
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 50.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 8.75s
                        Total time: 17534.65s
                               ETA: 959880.5s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.166s)
               Value function loss: 0.0629
                    Surrogate loss: -0.0276
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 52.69
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 8.54s
                        Total time: 17543.19s
                               ETA: 959803.0s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.160s, learning 0.160s)
               Value function loss: 0.0552
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 1.61
               Mean episode length: 52.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 8.32s
                        Total time: 17551.51s
                               ETA: 959713.8s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.214s, learning 0.163s)
               Value function loss: 0.0319
                    Surrogate loss: -0.0278
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 8.38s
                        Total time: 17559.89s
                               ETA: 959627.8s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.167s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0285
             Mean action noise std: 0.71
                       Mean reward: 1.71
               Mean episode length: 51.59
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 8.45s
                        Total time: 17568.34s
                               ETA: 959545.9s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.226s, learning 0.163s)
               Value function loss: 0.0414
                    Surrogate loss: -0.0266
             Mean action noise std: 0.71
                       Mean reward: 1.68
               Mean episode length: 50.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 8.39s
                        Total time: 17576.73s
                               ETA: 959460.7s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.148s, learning 0.169s)
               Value function loss: 0.0288
                    Surrogate loss: -0.0293
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 51.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 8.32s
                        Total time: 17585.04s
                               ETA: 959371.6s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.138s, learning 0.162s)
               Value function loss: 17.6672
                    Surrogate loss: -0.0003
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 8.30s
                        Total time: 17593.34s
                               ETA: 959281.7s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.178s, learning 0.170s)
               Value function loss: 0.0545
                    Surrogate loss: -0.0275
             Mean action noise std: 0.71
                       Mean reward: 1.89
               Mean episode length: 51.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 8.35s
                        Total time: 17601.69s
                               ETA: 959194.6s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.284s, learning 0.161s)
               Value function loss: 0.0707
                    Surrogate loss: -0.0202
             Mean action noise std: 0.71
                       Mean reward: 1.75
               Mean episode length: 50.36
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 8.44s
                        Total time: 17610.14s
                               ETA: 959112.7s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.166s)
               Value function loss: 0.0571
                    Surrogate loss: -0.0261
             Mean action noise std: 0.71
                       Mean reward: 4.46
               Mean episode length: 51.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 8.49s
                        Total time: 17618.62s
                               ETA: 959033.3s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.162s)
               Value function loss: 13.9345
                    Surrogate loss: 0.0002
             Mean action noise std: 0.71
                       Mean reward: 1.85
               Mean episode length: 50.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 8.46s
                        Total time: 17627.08s
                               ETA: 958952.4s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.164s)
               Value function loss: 0.0870
                    Surrogate loss: -0.0220
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 53.18
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 8.46s
                        Total time: 17635.55s
                               ETA: 958871.8s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.265s, learning 0.269s)
               Value function loss: 14.7268
                    Surrogate loss: 0.0020
             Mean action noise std: 0.71
                       Mean reward: 1.92
               Mean episode length: 51.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 8.53s
                        Total time: 17644.08s
                               ETA: 958795.1s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.166s)
               Value function loss: 52.2761
                    Surrogate loss: -0.0021
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 49.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 8.46s
                        Total time: 17652.54s
                               ETA: 958714.6s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.163s)
               Value function loss: 0.8800
                    Surrogate loss: -0.0150
             Mean action noise std: 0.71
                       Mean reward: 1.71
               Mean episode length: 51.73
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 8.39s
                        Total time: 17660.93s
                               ETA: 958630.2s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.419s, learning 0.159s)
               Value function loss: 16.2687
                    Surrogate loss: -0.0014
             Mean action noise std: 0.71
                       Mean reward: 7.02
               Mean episode length: 52.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 8.58s
                        Total time: 17669.51s
                               ETA: 958556.2s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.235s, learning 0.168s)
               Value function loss: 0.2112
                    Surrogate loss: -0.0211
             Mean action noise std: 0.71
                       Mean reward: 1.77
               Mean episode length: 50.81
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 8.40s
                        Total time: 17677.91s
                               ETA: 958472.8s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.900s, learning 0.170s)
               Value function loss: 0.1305
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 50.46
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 8.07s
                        Total time: 17685.98s
                               ETA: 958371.3s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.473s, learning 0.160s)
               Value function loss: 0.0594
                    Surrogate loss: -0.0219
             Mean action noise std: 0.71
                       Mean reward: 1.72
               Mean episode length: 52.48
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 8.63s
                        Total time: 17694.61s
                               ETA: 958300.5s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.349s, learning 0.214s)
               Value function loss: 0.0344
                    Surrogate loss: -0.0289
             Mean action noise std: 0.71
                       Mean reward: 4.30
               Mean episode length: 51.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 8.56s
                        Total time: 17703.18s
                               ETA: 958226.0s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.179s, learning 0.212s)
               Value function loss: 0.0573
                    Surrogate loss: -0.0242
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 50.47
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 8.39s
                        Total time: 17711.57s
                               ETA: 958142.2s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.335s, learning 0.164s)
               Value function loss: 17.9763
                    Surrogate loss: 0.0001
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 52.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 8.50s
                        Total time: 17720.07s
                               ETA: 958064.4s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.148s, learning 0.160s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0270
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 50.94
                  Mean reward/step: 0.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 8.31s
                        Total time: 17728.38s
                               ETA: 957976.3s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.250s, learning 0.173s)
               Value function loss: 0.0313
                    Surrogate loss: -0.0287
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 51.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 8.42s
                        Total time: 17736.80s
                               ETA: 957894.5s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.047s, learning 0.166s)
               Value function loss: 0.0230
                    Surrogate loss: -0.0312
             Mean action noise std: 0.71
                       Mean reward: 1.67
               Mean episode length: 51.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 8.21s
                        Total time: 17745.01s
                               ETA: 957801.5s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.326s, learning 0.167s)
               Value function loss: 0.0341
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 52.10
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 8.49s
                        Total time: 17753.51s
                               ETA: 957723.6s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.350s, learning 0.213s)
               Value function loss: 90.8143
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 51.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 8.56s
                        Total time: 17762.07s
                               ETA: 957649.6s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.103s, learning 0.260s)
               Value function loss: 0.0409
                    Surrogate loss: -0.0246
             Mean action noise std: 0.71
                       Mean reward: 1.62
               Mean episode length: 50.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 8.36s
                        Total time: 17770.43s
                               ETA: 957564.9s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.315s, learning 0.172s)
               Value function loss: 0.0557
                    Surrogate loss: -0.0157
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 51.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 8.49s
                        Total time: 17778.92s
                               ETA: 957487.0s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.478s, learning 0.213s)
               Value function loss: 316.0314
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.74
               Mean episode length: 51.38
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 8.69s
                        Total time: 17787.61s
                               ETA: 957420.1s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.178s, learning 0.159s)
               Value function loss: 17.8493
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 50.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 8.34s
                        Total time: 17795.95s
                               ETA: 957334.3s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.346s, learning 0.166s)
               Value function loss: 0.4102
                    Surrogate loss: -0.0127
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 51.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 8.51s
                        Total time: 17804.46s
                               ETA: 957257.9s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.087s, learning 0.162s)
               Value function loss: 0.2480
                    Surrogate loss: -0.0107
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 50.85
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 8.25s
                        Total time: 17812.71s
                               ETA: 957167.5s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.087s, learning 0.167s)
               Value function loss: 0.1845
                    Surrogate loss: -0.0146
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 50.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 8.25s
                        Total time: 17820.96s
                               ETA: 957077.4s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.381s, learning 0.167s)
               Value function loss: 28.9445
                    Surrogate loss: 0.0031
             Mean action noise std: 0.71
                       Mean reward: 1.94
               Mean episode length: 50.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 8.55s
                        Total time: 17829.51s
                               ETA: 957003.2s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.126s, learning 0.187s)
               Value function loss: 91.7636
                    Surrogate loss: -0.0019
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 50.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 8.31s
                        Total time: 17837.83s
                               ETA: 956916.5s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.273s, learning 0.222s)
               Value function loss: 363.4027
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 2.10
               Mean episode length: 50.76
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 8.49s
                        Total time: 17846.32s
                               ETA: 956839.6s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.027s, learning 0.165s)
               Value function loss: 19.3081
                    Surrogate loss: -0.0077
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 50.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 8.19s
                        Total time: 17854.51s
                               ETA: 956746.5s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.330s, learning 0.176s)
               Value function loss: 0.4055
                    Surrogate loss: -0.0045
             Mean action noise std: 0.71
                       Mean reward: 17.51
               Mean episode length: 51.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 8.51s
                        Total time: 17863.02s
                               ETA: 956670.4s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.113s, learning 0.166s)
               Value function loss: 0.2756
                    Surrogate loss: -0.0108
             Mean action noise std: 0.71
                       Mean reward: 1.82
               Mean episode length: 51.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 8.28s
                        Total time: 17871.30s
                               ETA: 956582.1s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.290s, learning 0.220s)
               Value function loss: 79.6033
                    Surrogate loss: -0.0025
             Mean action noise std: 0.71
                       Mean reward: 1.87
               Mean episode length: 50.61
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 8.51s
                        Total time: 17879.81s
                               ETA: 956506.3s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.187s, learning 0.175s)
               Value function loss: 154.4520
                    Surrogate loss: -0.0020
             Mean action noise std: 0.71
                       Mean reward: 1.96
               Mean episode length: 50.46
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 8.36s
                        Total time: 17888.17s
                               ETA: 956422.7s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.539s, learning 0.169s)
               Value function loss: 6.8128
                    Surrogate loss: -0.0110
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 51.13
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 8.71s
                        Total time: 17896.88s
                               ETA: 956357.7s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.125s, learning 0.189s)
               Value function loss: 1.6830
                    Surrogate loss: -0.0166
             Mean action noise std: 0.71
                       Mean reward: 1.71
               Mean episode length: 51.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 8.31s
                        Total time: 17905.19s
                               ETA: 956271.6s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.439s, learning 0.162s)
               Value function loss: 0.7031
                    Surrogate loss: -0.0195
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 50.43
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 8.60s
                        Total time: 17913.79s
                               ETA: 956201.0s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.184s, learning 0.179s)
               Value function loss: 17.4449
                    Surrogate loss: 0.0044
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 50.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 8.36s
                        Total time: 17922.16s
                               ETA: 956117.8s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.332s, learning 0.170s)
               Value function loss: 0.4919
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 50.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 8.50s
                        Total time: 17930.66s
                               ETA: 956042.1s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.041s, learning 0.171s)
               Value function loss: 0.2910
                    Surrogate loss: -0.0206
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 51.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 8.21s
                        Total time: 17938.87s
                               ETA: 955950.9s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.190s, learning 0.168s)
               Value function loss: 0.1342
                    Surrogate loss: -0.0214
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 50.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 8.36s
                        Total time: 17947.23s
                               ETA: 955867.7s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.021s, learning 0.158s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0218
             Mean action noise std: 0.71
                       Mean reward: 4.60
               Mean episode length: 51.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 8.18s
                        Total time: 17955.41s
                               ETA: 955775.0s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.163s, learning 0.264s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0226
             Mean action noise std: 0.71
                       Mean reward: 1.68
               Mean episode length: 51.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 8.43s
                        Total time: 17963.84s
                               ETA: 955695.6s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.351s, learning 0.207s)
               Value function loss: 0.0647
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 51.88
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 8.56s
                        Total time: 17972.39s
                               ETA: 955623.2s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.244s, learning 0.188s)
               Value function loss: 0.0878
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 50.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 8.43s
                        Total time: 17980.83s
                               ETA: 955544.1s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.471s, learning 0.216s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0161
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 50.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 8.69s
                        Total time: 17989.51s
                               ETA: 955478.7s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.259s, learning 0.204s)
               Value function loss: 342.8160
                    Surrogate loss: 0.0018
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 51.88
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 8.46s
                        Total time: 17997.98s
                               ETA: 955401.5s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.235s, learning 0.166s)
               Value function loss: 196.9621
                    Surrogate loss: -0.0031
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 50.84
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 8.40s
                        Total time: 18006.38s
                               ETA: 955321.1s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.986s, learning 0.167s)
               Value function loss: 0.9644
                    Surrogate loss: -0.0183
             Mean action noise std: 0.71
                       Mean reward: 1.91
               Mean episode length: 52.15
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 8.15s
                        Total time: 18014.53s
                               ETA: 955227.6s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.114s, learning 0.160s)
               Value function loss: 18.3734
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 2.02
               Mean episode length: 51.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 8.27s
                        Total time: 18022.80s
                               ETA: 955140.5s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.085s, learning 0.164s)
               Value function loss: 261.9401
                    Surrogate loss: -0.0022
             Mean action noise std: 0.71
                       Mean reward: 1.90
               Mean episode length: 52.66
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 8.25s
                        Total time: 18031.05s
                               ETA: 955052.3s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.132s, learning 0.221s)
               Value function loss: 60.6722
                    Surrogate loss: -0.0010
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 50.68
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 8.35s
                        Total time: 18039.41s
                               ETA: 954969.6s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.080s, learning 0.170s)
               Value function loss: 7.8755
                    Surrogate loss: -0.0111
             Mean action noise std: 0.71
                       Mean reward: 1.56
               Mean episode length: 50.04
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 8.25s
                        Total time: 18047.66s
                               ETA: 954881.6s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.123s, learning 0.163s)
               Value function loss: 2.4355
                    Surrogate loss: -0.0090
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 51.66
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 8.29s
                        Total time: 18055.94s
                               ETA: 954795.6s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.169s, learning 0.165s)
               Value function loss: 0.6404
                    Surrogate loss: -0.0084
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 51.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 8.33s
                        Total time: 18064.28s
                               ETA: 954712.1s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.297s, learning 0.168s)
               Value function loss: 0.2617
                    Surrogate loss: -0.0163
             Mean action noise std: 0.71
                       Mean reward: 1.78
               Mean episode length: 52.82
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 8.47s
                        Total time: 18072.74s
                               ETA: 954635.7s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.992s, learning 0.168s)
               Value function loss: 0.1912
                    Surrogate loss: -0.0200
             Mean action noise std: 0.71
                       Mean reward: 1.86
               Mean episode length: 50.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 8.16s
                        Total time: 18080.90s
                               ETA: 954543.3s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.987s, learning 0.163s)
               Value function loss: 0.1597
                    Surrogate loss: -0.0168
             Mean action noise std: 0.71
                       Mean reward: 1.93
               Mean episode length: 52.08
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 8.15s
                        Total time: 18089.05s
                               ETA: 954450.4s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.373s, learning 0.170s)
               Value function loss: 0.1497
                    Surrogate loss: -0.0145
             Mean action noise std: 0.71
                       Mean reward: 1.73
               Mean episode length: 51.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 8.54s
                        Total time: 18097.60s
                               ETA: 954378.3s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.009s, learning 0.160s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0198
             Mean action noise std: 0.71
                       Mean reward: 1.95
               Mean episode length: 51.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 8.17s
                        Total time: 18105.76s
                               ETA: 954286.5s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.153s, learning 0.169s)
               Value function loss: 0.0810
                    Surrogate loss: -0.0225
             Mean action noise std: 0.71
                       Mean reward: 1.83
               Mean episode length: 50.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 8.32s
                        Total time: 18114.09s
                               ETA: 954203.0s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.068s, learning 0.207s)
               Value function loss: 0.0795
                    Surrogate loss: -0.0033
             Mean action noise std: 0.71
                       Mean reward: 1.80
               Mean episode length: 50.74
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 8.28s
                        Total time: 18122.36s
                               ETA: 954117.0s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.388s, learning 0.165s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0215
             Mean action noise std: 0.71
                       Mean reward: 1.76
               Mean episode length: 50.51
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 8.55s
                        Total time: 18130.91s
                               ETA: 954045.8s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.251s, learning 0.163s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0177
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 51.56
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 8.41s
                        Total time: 18139.33s
                               ETA: 953967.3s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.244s, learning 0.168s)
               Value function loss: 0.0627
                    Surrogate loss: -0.0193
             Mean action noise std: 0.71
                       Mean reward: 2.01
               Mean episode length: 51.78
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 8.41s
                        Total time: 18147.74s
                               ETA: 953888.8s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.983s, learning 0.163s)
               Value function loss: 0.0597
                    Surrogate loss: -0.0178
             Mean action noise std: 0.71
                       Mean reward: 1.84
               Mean episode length: 51.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 8.15s
                        Total time: 18155.89s
                               ETA: 953796.4s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.194s, learning 0.166s)
               Value function loss: 0.0380
                    Surrogate loss: -0.0262
             Mean action noise std: 0.71
                       Mean reward: 1.79
               Mean episode length: 50.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 8.36s
                        Total time: 18164.25s
                               ETA: 953715.2s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.109s, learning 0.210s)
               Value function loss: 0.0539
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 1.76
               Mean episode length: 50.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 8.32s
                        Total time: 18172.57s
                               ETA: 953632.1s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.278s, learning 0.269s)
               Value function loss: 0.0560
                    Surrogate loss: -0.0247
             Mean action noise std: 0.71
                       Mean reward: 1.77
               Mean episode length: 51.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 8.55s
                        Total time: 18181.11s
                               ETA: 953561.0s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 2009 steps/s (collection: 7.990s, learning 0.163s)
               Value function loss: 301.4245
                    Surrogate loss: -0.0012
             Mean action noise std: 0.71
                       Mean reward: 1.70
               Mean episode length: 50.05
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 8.15s
                        Total time: 18189.27s
                               ETA: 953469.3s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.981s, learning 0.176s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0140
             Mean action noise std: 0.71
                       Mean reward: 1.88
               Mean episode length: 51.78
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 8.16s
                        Total time: 18197.42s
                               ETA: 953377.8s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.267s, learning 0.161s)
               Value function loss: 0.1215
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 51.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 8.43s
                        Total time: 18205.85s
                               ETA: 953300.7s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.205s, learning 0.210s)
               Value function loss: 0.0585
                    Surrogate loss: -0.0279
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 50.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 8.42s
                        Total time: 18214.27s
                               ETA: 953222.9s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.086s, learning 0.165s)
               Value function loss: 0.0525
                    Surrogate loss: -0.0264
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 51.01
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 8.25s
                        Total time: 18222.52s
                               ETA: 953136.7s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.205s, learning 0.173s)
               Value function loss: 0.0829
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 1.78
               Mean episode length: 49.96
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 8.38s
                        Total time: 18230.89s
                               ETA: 953057.1s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 1998 steps/s (collection: 7.930s, learning 0.267s)
               Value function loss: 24.8848
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 1.79
               Mean episode length: 51.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 8.20s
                        Total time: 18239.09s
                               ETA: 952968.2s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.174s, learning 0.207s)
               Value function loss: 0.0817
                    Surrogate loss: -0.0184
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 51.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 8.38s
                        Total time: 18247.47s
                               ETA: 952889.0s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.077s, learning 0.228s)
               Value function loss: 0.0840
                    Surrogate loss: -0.0171
             Mean action noise std: 0.70
                       Mean reward: 1.95
               Mean episode length: 51.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 8.30s
                        Total time: 18255.78s
                               ETA: 952805.8s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.446s, learning 0.170s)
               Value function loss: 0.0717
                    Surrogate loss: -0.0177
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 50.52
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 8.62s
                        Total time: 18264.39s
                               ETA: 952739.0s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.056s, learning 0.168s)
               Value function loss: 0.0599
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 1.80
               Mean episode length: 50.45
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 8.22s
                        Total time: 18272.61s
                               ETA: 952651.8s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.147s, learning 0.159s)
               Value function loss: 0.0410
                    Surrogate loss: -0.0261
             Mean action noise std: 0.70
                       Mean reward: 1.96
               Mean episode length: 50.62
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 8.31s
                        Total time: 18280.92s
                               ETA: 952569.0s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.101s, learning 0.162s)
               Value function loss: 0.0700
                    Surrogate loss: -0.0160
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 51.02
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 8.26s
                        Total time: 18289.18s
                               ETA: 952484.0s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.270s, learning 0.188s)
               Value function loss: 0.0533
                    Surrogate loss: -0.0154
             Mean action noise std: 0.70
                       Mean reward: 1.73
               Mean episode length: 50.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 8.46s
                        Total time: 18297.64s
                               ETA: 952409.3s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.395s, learning 0.175s)
               Value function loss: 0.0723
                    Surrogate loss: -0.0198
             Mean action noise std: 0.70
                       Mean reward: 1.78
               Mean episode length: 51.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 8.57s
                        Total time: 18306.21s
                               ETA: 952340.4s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.255s, learning 0.179s)
               Value function loss: 0.0495
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 50.87
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 8.43s
                        Total time: 18314.65s
                               ETA: 952264.5s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.066s, learning 0.169s)
               Value function loss: 0.0650
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 1.86
               Mean episode length: 50.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 8.24s
                        Total time: 18322.88s
                               ETA: 952178.4s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.127s, learning 0.167s)
               Value function loss: 216.9048
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 50.79
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 8.29s
                        Total time: 18331.18s
                               ETA: 952095.5s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.105s, learning 0.172s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0220
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 51.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 8.28s
                        Total time: 18339.45s
                               ETA: 952011.7s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 1296 steps/s (collection: 12.478s, learning 0.161s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0140
             Mean action noise std: 0.70
                       Mean reward: 4.61
               Mean episode length: 51.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 12.64s
                        Total time: 18352.09s
                               ETA: 952154.3s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.817s, learning 0.168s)
               Value function loss: 0.0850
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 11.97
               Mean episode length: 50.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 15.98s
                        Total time: 18368.08s
                               ETA: 952470.2s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.629s, learning 0.196s)
               Value function loss: 0.0541
                    Surrogate loss: -0.0219
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 51.29
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 15.82s
                        Total time: 18383.90s
                               ETA: 952777.5s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 1035 steps/s (collection: 15.620s, learning 0.206s)
               Value function loss: 0.0660
                    Surrogate loss: -0.0224
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 50.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 15.83s
                        Total time: 18399.73s
                               ETA: 953084.5s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.218s, learning 0.166s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0171
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 49.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 16.38s
                        Total time: 18416.11s
                               ETA: 953420.0s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.913s, learning 0.165s)
               Value function loss: 0.1271
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 51.17
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 16.08s
                        Total time: 18432.19s
                               ETA: 953739.4s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.893s, learning 0.177s)
               Value function loss: 17.7562
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 1.92
               Mean episode length: 50.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 16.07s
                        Total time: 18448.26s
                               ETA: 954058.0s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.001s, learning 0.167s)
               Value function loss: 0.1217
                    Surrogate loss: -0.0210
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 52.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 16.17s
                        Total time: 18464.43s
                               ETA: 954381.3s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.761s, learning 0.193s)
               Value function loss: 34.8822
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 51.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 15.95s
                        Total time: 18480.38s
                               ETA: 954693.2s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.896s, learning 0.163s)
               Value function loss: 61.7513
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 14.79
               Mean episode length: 51.86
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 16.06s
                        Total time: 18496.44s
                               ETA: 955010.1s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.163s, learning 0.165s)
               Value function loss: 0.9692
                    Surrogate loss: -0.0159
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 49.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 16.33s
                        Total time: 18512.77s
                               ETA: 955340.6s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.098s, learning 0.171s)
               Value function loss: 0.2513
                    Surrogate loss: -0.0146
             Mean action noise std: 0.70
                       Mean reward: 4.87
               Mean episode length: 51.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 16.27s
                        Total time: 18529.04s
                               ETA: 955667.7s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.152s, learning 0.175s)
               Value function loss: 0.1097
                    Surrogate loss: -0.0178
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 50.60
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 16.33s
                        Total time: 18545.36s
                               ETA: 955997.4s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.344s, learning 0.181s)
               Value function loss: 0.1227
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 51.12
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 16.52s
                        Total time: 18561.89s
                               ETA: 956336.9s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.028s, learning 0.167s)
               Value function loss: 0.0918
                    Surrogate loss: -0.0202
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 50.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 16.20s
                        Total time: 18578.08s
                               ETA: 956659.1s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.945s, learning 0.208s)
               Value function loss: 53.9230
                    Surrogate loss: 0.0016
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 51.05
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 16.15s
                        Total time: 18594.24s
                               ETA: 956978.8s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.324s, learning 0.160s)
               Value function loss: 293.8623
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 51.77
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 16.48s
                        Total time: 18610.72s
                               ETA: 957315.1s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.308s, learning 0.165s)
               Value function loss: 20.2711
                    Surrogate loss: -0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 52.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 16.47s
                        Total time: 18627.19s
                               ETA: 957650.5s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.989s, learning 0.170s)
               Value function loss: 3.1065
                    Surrogate loss: -0.0117
             Mean action noise std: 0.70
                       Mean reward: 7.35
               Mean episode length: 51.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 16.16s
                        Total time: 18643.35s
                               ETA: 957969.4s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.050s, learning 0.169s)
               Value function loss: 59.7985
                    Surrogate loss: 0.0000
             Mean action noise std: 0.70
                       Mean reward: 4.58
               Mean episode length: 51.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 16.22s
                        Total time: 18659.57s
                               ETA: 958291.0s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.690s, learning 0.190s)
               Value function loss: 18.9783
                    Surrogate loss: -0.0041
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 50.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 15.88s
                        Total time: 18675.45s
                               ETA: 958594.9s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1031 steps/s (collection: 15.712s, learning 0.170s)
               Value function loss: 224.9334
                    Surrogate loss: 0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 50.52
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 15.88s
                        Total time: 18691.33s
                               ETA: 958898.6s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.926s, learning 0.167s)
               Value function loss: 1.1355
                    Surrogate loss: -0.0200
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 50.90
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 16.09s
                        Total time: 18707.43s
                               ETA: 959212.7s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.015s, learning 0.219s)
               Value function loss: 191.9154
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 4.59
               Mean episode length: 50.53
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 16.23s
                        Total time: 18723.66s
                               ETA: 959533.8s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.156s, learning 0.168s)
               Value function loss: 1.0132
                    Surrogate loss: -0.0187
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 50.64
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 16.32s
                        Total time: 18739.98s
                               ETA: 959859.1s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.164s, learning 0.174s)
               Value function loss: 13.1790
                    Surrogate loss: 0.0070
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 51.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 16.34s
                        Total time: 18756.32s
                               ETA: 960184.7s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.379s, learning 0.171s)
               Value function loss: 0.4378
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 51.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 16.55s
                        Total time: 18772.87s
                               ETA: 960520.8s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.962s, learning 0.164s)
               Value function loss: 0.4821
                    Surrogate loss: -0.0078
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 52.09
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 16.13s
                        Total time: 18789.00s
                               ETA: 960834.8s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.887s, learning 0.163s)
               Value function loss: 0.2933
                    Surrogate loss: 0.0012
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 51.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 16.05s
                        Total time: 18805.05s
                               ETA: 961144.7s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.150s, learning 0.173s)
               Value function loss: 0.2267
                    Surrogate loss: -0.0136
             Mean action noise std: 0.70
                       Mean reward: 1.73
               Mean episode length: 50.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 16.32s
                        Total time: 18821.37s
                               ETA: 961468.1s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.213s, learning 0.163s)
               Value function loss: 4.0992
                    Surrogate loss: 0.0021
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 51.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 16.38s
                        Total time: 18837.75s
                               ETA: 961793.9s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.169s, learning 0.169s)
               Value function loss: 0.2499
                    Surrogate loss: -0.0119
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 51.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 16.34s
                        Total time: 18854.08s
                               ETA: 962117.4s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.127s, learning 0.170s)
               Value function loss: 0.1051
                    Surrogate loss: -0.0074
             Mean action noise std: 0.70
                       Mean reward: 4.34
               Mean episode length: 52.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 16.30s
                        Total time: 18870.38s
                               ETA: 962438.5s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.858s, learning 0.174s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0162
             Mean action noise std: 0.70
                       Mean reward: 1.82
               Mean episode length: 51.06
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 16.03s
                        Total time: 18886.41s
                               ETA: 962745.7s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.944s, learning 0.163s)
               Value function loss: 0.0875
                    Surrogate loss: -0.0158
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 51.42
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 16.11s
                        Total time: 18902.52s
                               ETA: 963056.4s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.043s, learning 0.168s)
               Value function loss: 0.0789
                    Surrogate loss: -0.0167
             Mean action noise std: 0.70
                       Mean reward: 1.79
               Mean episode length: 52.60
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 16.21s
                        Total time: 18918.73s
                               ETA: 963372.0s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.100s, learning 0.171s)
               Value function loss: 0.0384
                    Surrogate loss: -0.0290
             Mean action noise std: 0.70
                       Mean reward: 1.73
               Mean episode length: 51.22
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 16.27s
                        Total time: 18935.00s
                               ETA: 963690.3s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.862s, learning 0.165s)
               Value function loss: 0.0330
                    Surrogate loss: -0.0285
             Mean action noise std: 0.70
                       Mean reward: 1.81
               Mean episode length: 52.64
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 16.03s
                        Total time: 18951.03s
                               ETA: 963996.0s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1741 steps/s (collection: 9.203s, learning 0.206s)
               Value function loss: 0.0580
                    Surrogate loss: -0.0188
             Mean action noise std: 0.70
                       Mean reward: 1.74
               Mean episode length: 52.15
                  Mean reward/step: 0.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 9.41s
                        Total time: 18960.44s
                               ETA: 963964.8s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.487s, learning 0.159s)
               Value function loss: 0.0472
                    Surrogate loss: -0.0177
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 51.49
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 8.65s
                        Total time: 18969.08s
                               ETA: 963894.8s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.313s, learning 0.162s)
               Value function loss: 0.0434
                    Surrogate loss: -0.0203
             Mean action noise std: 0.70
                       Mean reward: 1.73
               Mean episode length: 52.55
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 8.47s
                        Total time: 18977.56s
                               ETA: 963816.2s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.162s, learning 0.212s)
               Value function loss: 0.0565
                    Surrogate loss: -0.0232
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 51.25
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 8.37s
                        Total time: 18985.93s
                               ETA: 963732.6s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.165s)
               Value function loss: 0.0403
                    Surrogate loss: -0.0296
             Mean action noise std: 0.70
                       Mean reward: 1.72
               Mean episode length: 51.71
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 8.46s
                        Total time: 18994.39s
                               ETA: 963653.4s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1848 steps/s (collection: 8.589s, learning 0.274s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0215
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 51.62
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 8.86s
                        Total time: 19003.25s
                               ETA: 963594.7s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.254s, learning 0.207s)
               Value function loss: 0.0391
                    Surrogate loss: -0.0289
             Mean action noise std: 0.70
                       Mean reward: 1.87
               Mean episode length: 50.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 8.46s
                        Total time: 19011.72s
                               ETA: 963515.7s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.361s, learning 0.206s)
               Value function loss: 0.0318
                    Surrogate loss: -0.0302
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 52.20
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 8.57s
                        Total time: 19020.28s
                               ETA: 963442.2s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.112s, learning 0.170s)
               Value function loss: 0.0444
                    Surrogate loss: -0.0282
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 50.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 8.28s
                        Total time: 19028.57s
                               ETA: 963354.3s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.544s, learning 0.166s)
               Value function loss: 0.0447
                    Surrogate loss: -0.0271
             Mean action noise std: 0.70
                       Mean reward: 1.85
               Mean episode length: 51.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 8.71s
                        Total time: 19037.27s
                               ETA: 963288.1s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.248s, learning 0.161s)
               Value function loss: 0.0336
                    Surrogate loss: -0.0283
             Mean action noise std: 0.70
                       Mean reward: 1.94
               Mean episode length: 51.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 8.41s
                        Total time: 19045.68s
                               ETA: 963206.7s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.406s, learning 0.163s)
               Value function loss: 71.3056
                    Surrogate loss: 0.0027
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 51.36
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 8.57s
                        Total time: 19054.25s
                               ETA: 963133.6s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.134s, learning 0.163s)
               Value function loss: 0.0427
                    Surrogate loss: -0.0300
             Mean action noise std: 0.70
                       Mean reward: 1.77
               Mean episode length: 50.80
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 8.30s
                        Total time: 19062.55s
                               ETA: 963046.7s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.175s, learning 0.172s)
               Value function loss: 0.0437
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 52.07
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 8.35s
                        Total time: 19070.90s
                               ETA: 962962.5s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.983s, learning 0.162s)
               Value function loss: 0.0326
                    Surrogate loss: -0.0263
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 50.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 8.14s
                        Total time: 19079.04s
                               ETA: 962868.1s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.226s, learning 0.217s)
               Value function loss: 0.0377
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 50.11
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 8.44s
                        Total time: 19087.48s
                               ETA: 962788.8s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.125s, learning 0.158s)
               Value function loss: 0.0327
                    Surrogate loss: -0.0263
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 51.48
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 8.28s
                        Total time: 19095.77s
                               ETA: 962701.6s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.239s, learning 0.176s)
               Value function loss: 17.6174
                    Surrogate loss: -0.0003
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 50.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 8.42s
                        Total time: 19104.18s
                               ETA: 962621.1s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.444s, learning 0.164s)
               Value function loss: 110.5564
                    Surrogate loss: -0.0028
             Mean action noise std: 0.70
                       Mean reward: 1.89
               Mean episode length: 49.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 8.61s
                        Total time: 19112.79s
                               ETA: 962550.4s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.951s, learning 0.173s)
               Value function loss: 17.5874
                    Surrogate loss: -0.0032
             Mean action noise std: 0.70
                       Mean reward: 1.81
               Mean episode length: 50.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 8.12s
                        Total time: 19120.91s
                               ETA: 962455.3s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.071s, learning 0.165s)
               Value function loss: 0.8357
                    Surrogate loss: -0.0086
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 50.86
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 8.24s
                        Total time: 19129.15s
                               ETA: 962366.0s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.150s, learning 0.172s)
               Value function loss: 0.5421
                    Surrogate loss: -0.0163
             Mean action noise std: 0.70
                       Mean reward: 1.85
               Mean episode length: 50.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 8.32s
                        Total time: 19137.47s
                               ETA: 962281.2s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.156s, learning 0.210s)
               Value function loss: 61.8685
                    Surrogate loss: 0.0004
             Mean action noise std: 0.70
                       Mean reward: 12.39
               Mean episode length: 50.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 8.37s
                        Total time: 19145.84s
                               ETA: 962198.6s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.249s, learning 0.177s)
               Value function loss: 45.5012
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 50.58
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 8.43s
                        Total time: 19154.26s
                               ETA: 962119.1s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.320s, learning 0.163s)
               Value function loss: 0.9018
                    Surrogate loss: -0.0177
             Mean action noise std: 0.70
                       Mean reward: 14.71
               Mean episode length: 51.40
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 8.48s
                        Total time: 19162.75s
                               ETA: 962042.5s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.090s, learning 0.167s)
               Value function loss: 0.3548
                    Surrogate loss: -0.0087
             Mean action noise std: 0.70
                       Mean reward: 1.83
               Mean episode length: 52.19
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 8.26s
                        Total time: 19171.00s
                               ETA: 961954.7s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.164s, learning 0.163s)
               Value function loss: 0.2107
                    Surrogate loss: -0.0145
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 50.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 8.33s
                        Total time: 19179.33s
                               ETA: 961870.4s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.081s, learning 0.165s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0168
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 50.31
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 8.25s
                        Total time: 19187.58s
                               ETA: 961782.2s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.377s, learning 0.173s)
               Value function loss: 0.1358
                    Surrogate loss: -0.0133
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 50.99
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 8.55s
                        Total time: 19196.13s
                               ETA: 961709.3s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.162s)
               Value function loss: 0.1861
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 1.93
               Mean episode length: 50.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 8.47s
                        Total time: 19204.60s
                               ETA: 961632.6s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.101s, learning 0.166s)
               Value function loss: 0.1809
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 50.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 8.27s
                        Total time: 19212.87s
                               ETA: 961545.7s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.352s, learning 0.162s)
               Value function loss: 0.1414
                    Surrogate loss: -0.0165
             Mean action noise std: 0.70
                       Mean reward: 1.87
               Mean episode length: 50.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 8.51s
                        Total time: 19221.38s
                               ETA: 961471.2s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.166s, learning 0.168s)
               Value function loss: 92.6868
                    Surrogate loss: -0.0009
             Mean action noise std: 0.70
                       Mean reward: 1.80
               Mean episode length: 50.25
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 8.33s
                        Total time: 19229.72s
                               ETA: 961387.7s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.099s, learning 0.165s)
               Value function loss: 0.1749
                    Surrogate loss: -0.0153
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 52.44
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 8.26s
                        Total time: 19237.98s
                               ETA: 961300.9s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.335s, learning 0.162s)
               Value function loss: 0.1126
                    Surrogate loss: -0.0206
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 51.83
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 8.50s
                        Total time: 19246.48s
                               ETA: 961225.7s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.165s)
               Value function loss: 0.1297
                    Surrogate loss: -0.0195
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 51.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 8.44s
                        Total time: 19254.91s
                               ETA: 961147.6s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.349s, learning 0.173s)
               Value function loss: 131.8380
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 51.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 8.52s
                        Total time: 19263.44s
                               ETA: 961073.9s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.163s)
               Value function loss: 373.9803
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 51.04
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 8.37s
                        Total time: 19271.81s
                               ETA: 960992.8s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.122s, learning 0.174s)
               Value function loss: 28.0977
                    Surrogate loss: -0.0112
             Mean action noise std: 0.70
                       Mean reward: 12.02
               Mean episode length: 50.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 8.30s
                        Total time: 19280.10s
                               ETA: 960907.8s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1861 steps/s (collection: 8.637s, learning 0.164s)
               Value function loss: 5.1257
                    Surrogate loss: 0.0054
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 50.84
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 8.80s
                        Total time: 19288.91s
                               ETA: 960848.2s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.346s, learning 0.165s)
               Value function loss: 6.2778
                    Surrogate loss: -0.0064
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 51.10
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 8.51s
                        Total time: 19297.42s
                               ETA: 960774.1s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.234s, learning 0.164s)
               Value function loss: 2.3703
                    Surrogate loss: -0.0081
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 51.35
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 8.40s
                        Total time: 19305.81s
                               ETA: 960694.6s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.116s, learning 0.180s)
               Value function loss: 15.7100
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 4.80
               Mean episode length: 51.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 8.30s
                        Total time: 19314.11s
                               ETA: 960610.0s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.341s, learning 0.163s)
               Value function loss: 62.7983
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 51.89
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 8.50s
                        Total time: 19322.61s
                               ETA: 960535.8s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.098s, learning 0.162s)
               Value function loss: 46.5245
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 50.77
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 8.26s
                        Total time: 19330.87s
                               ETA: 960449.5s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.325s, learning 0.208s)
               Value function loss: 434.9052
                    Surrogate loss: -0.0024
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 50.49
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 8.53s
                        Total time: 19339.41s
                               ETA: 960376.9s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.946s, learning 0.170s)
               Value function loss: 24.9085
                    Surrogate loss: -0.0097
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 50.47
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 8.12s
                        Total time: 19347.52s
                               ETA: 960283.7s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.587s, learning 0.165s)
               Value function loss: 239.4546
                    Surrogate loss: -0.0022
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 51.23
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 8.75s
                        Total time: 19356.28s
                               ETA: 960222.1s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.024s, learning 0.164s)
               Value function loss: 14.6626
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 51.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 8.19s
                        Total time: 19364.46s
                               ETA: 960132.6s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.230s, learning 0.168s)
               Value function loss: 9.8612
                    Surrogate loss: -0.0089
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 50.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 8.40s
                        Total time: 19372.86s
                               ETA: 960053.6s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.196s, learning 0.205s)
               Value function loss: 3.3501
                    Surrogate loss: -0.0126
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 49.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 8.40s
                        Total time: 19381.26s
                               ETA: 959974.8s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.380s, learning 0.172s)
               Value function loss: 3.7625
                    Surrogate loss: -0.0086
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 51.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 8.55s
                        Total time: 19389.81s
                               ETA: 959903.5s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.154s, learning 0.161s)
               Value function loss: 1.3070
                    Surrogate loss: -0.0052
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 51.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 8.31s
                        Total time: 19398.13s
                               ETA: 959820.6s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.312s, learning 0.212s)
               Value function loss: 0.7731
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 3.05
               Mean episode length: 52.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 8.52s
                        Total time: 19406.65s
                               ETA: 959748.1s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.316s, learning 0.165s)
               Value function loss: 0.3843
                    Surrogate loss: -0.0207
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 51.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 8.48s
                        Total time: 19415.13s
                               ETA: 959673.5s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.073s, learning 0.163s)
               Value function loss: 0.3142
                    Surrogate loss: -0.0054
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 50.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 8.24s
                        Total time: 19423.37s
                               ETA: 959586.9s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.399s, learning 0.162s)
               Value function loss: 61.3166
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 51.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 8.56s
                        Total time: 19431.93s
                               ETA: 959516.4s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.212s, learning 0.161s)
               Value function loss: 0.3201
                    Surrogate loss: -0.0123
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 50.89
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 8.37s
                        Total time: 19440.30s
                               ETA: 959436.8s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.111s, learning 0.158s)
               Value function loss: 0.3742
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 51.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 8.27s
                        Total time: 19448.57s
                               ETA: 959352.0s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.234s, learning 0.161s)
               Value function loss: 212.9020
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 52.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 8.39s
                        Total time: 19456.97s
                               ETA: 959273.5s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.337s, learning 0.208s)
               Value function loss: 486.9133
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 50.32
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 8.55s
                        Total time: 19465.51s
                               ETA: 959202.5s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.218s, learning 0.162s)
               Value function loss: 24.3797
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 27.42
               Mean episode length: 51.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 8.38s
                        Total time: 19473.89s
                               ETA: 959123.5s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.264s, learning 0.259s)
               Value function loss: 13.6120
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 51.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 8.52s
                        Total time: 19482.42s
                               ETA: 959051.6s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.433s, learning 0.209s)
               Value function loss: 10.3539
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 50.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 8.64s
                        Total time: 19491.06s
                               ETA: 958985.5s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.953s, learning 0.207s)
               Value function loss: 5.4758
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 2.00
               Mean episode length: 51.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 8.16s
                        Total time: 19499.22s
                               ETA: 958895.8s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.181s, learning 0.158s)
               Value function loss: 5.6615
                    Surrogate loss: 0.0114
             Mean action noise std: 0.70
                       Mean reward: 2.51
               Mean episode length: 50.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 8.34s
                        Total time: 19507.56s
                               ETA: 958815.1s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.326s, learning 0.283s)
               Value function loss: 0.9969
                    Surrogate loss: -0.0186
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 51.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 8.61s
                        Total time: 19516.17s
                               ETA: 958747.6s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.386s, learning 0.168s)
               Value function loss: 0.4276
                    Surrogate loss: -0.0063
             Mean action noise std: 0.70
                       Mean reward: 2.46
               Mean episode length: 51.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 8.55s
                        Total time: 19524.72s
                               ETA: 958677.5s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.126s, learning 0.160s)
               Value function loss: 0.3307
                    Surrogate loss: -0.0197
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 50.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 8.29s
                        Total time: 19533.01s
                               ETA: 958594.4s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.287s, learning 0.206s)
               Value function loss: 0.1475
                    Surrogate loss: -0.0249
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 50.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 8.49s
                        Total time: 19541.50s
                               ETA: 958521.4s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.157s, learning 0.253s)
               Value function loss: 62.8334
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 1.95
               Mean episode length: 50.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 8.41s
                        Total time: 19549.91s
                               ETA: 958444.5s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.238s, learning 0.212s)
               Value function loss: 3.8955
                    Surrogate loss: -0.0029
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 50.69
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 8.45s
                        Total time: 19558.36s
                               ETA: 958369.5s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.104s, learning 0.189s)
               Value function loss: 216.4690
                    Surrogate loss: -0.0015
             Mean action noise std: 0.70
                       Mean reward: 1.90
               Mean episode length: 48.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 8.29s
                        Total time: 19566.65s
                               ETA: 958286.9s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.366s, learning 0.161s)
               Value function loss: 731.2990
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 49.67
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 8.53s
                        Total time: 19575.18s
                               ETA: 958215.9s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.393s, learning 0.261s)
               Value function loss: 205.4444
                    Surrogate loss: -0.0044
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 49.83
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 8.65s
                        Total time: 19583.84s
                               ETA: 958151.1s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.314s, learning 0.209s)
               Value function loss: 17.3765
                    Surrogate loss: -0.0078
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 49.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 8.52s
                        Total time: 19592.36s
                               ETA: 958080.0s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.185s, learning 0.176s)
               Value function loss: 8.8497
                    Surrogate loss: -0.0095
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 50.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 8.36s
                        Total time: 19600.72s
                               ETA: 958001.0s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.050s, learning 0.162s)
               Value function loss: 199.2809
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 50.65
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 8.21s
                        Total time: 19608.93s
                               ETA: 957914.9s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.160s)
               Value function loss: 8.3263
                    Surrogate loss: -0.0063
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 49.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 8.36s
                        Total time: 19617.29s
                               ETA: 957835.9s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.374s, learning 0.215s)
               Value function loss: 3.1276
                    Surrogate loss: -0.0155
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 51.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 8.59s
                        Total time: 19625.88s
                               ETA: 957768.3s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.387s, learning 0.167s)
               Value function loss: 2.7019
                    Surrogate loss: -0.0075
             Mean action noise std: 0.70
                       Mean reward: 12.54
               Mean episode length: 50.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 8.55s
                        Total time: 19634.43s
                               ETA: 957699.0s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.339s, learning 0.158s)
               Value function loss: 1.6846
                    Surrogate loss: -0.0127
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 49.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 8.50s
                        Total time: 19642.93s
                               ETA: 957627.1s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.636s, learning 0.160s)
               Value function loss: 126.5752
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 49.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 8.80s
                        Total time: 19651.73s
                               ETA: 957569.7s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.232s, learning 0.163s)
               Value function loss: 4.8741
                    Surrogate loss: -0.0054
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 49.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 8.39s
                        Total time: 19660.12s
                               ETA: 957492.8s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.295s, learning 0.168s)
               Value function loss: 234.4821
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 2.53
               Mean episode length: 49.52
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 8.46s
                        Total time: 19668.58s
                               ETA: 957419.3s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.283s, learning 0.176s)
               Value function loss: 7.8677
                    Surrogate loss: -0.0092
             Mean action noise std: 0.70
                       Mean reward: 2.01
               Mean episode length: 48.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 8.46s
                        Total time: 19677.04s
                               ETA: 957345.7s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.162s)
               Value function loss: 1.8292
                    Surrogate loss: -0.0075
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 49.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 8.44s
                        Total time: 19685.48s
                               ETA: 957271.3s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.475s, learning 0.185s)
               Value function loss: 1.2174
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 50.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 8.66s
                        Total time: 19694.14s
                               ETA: 957207.6s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.085s, learning 0.169s)
               Value function loss: 0.8618
                    Surrogate loss: -0.0150
             Mean action noise std: 0.70
                       Mean reward: 2.54
               Mean episode length: 51.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 8.25s
                        Total time: 19702.40s
                               ETA: 957124.2s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1995 steps/s (collection: 7.982s, learning 0.230s)
               Value function loss: 30.1696
                    Surrogate loss: 0.0026
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 51.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 8.21s
                        Total time: 19710.61s
                               ETA: 957038.9s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.195s, learning 0.218s)
               Value function loss: 0.3160
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 50.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 8.41s
                        Total time: 19719.02s
                               ETA: 956963.4s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 2011 steps/s (collection: 7.988s, learning 0.159s)
               Value function loss: 131.0408
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 51.87
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 8.15s
                        Total time: 19727.17s
                               ETA: 956875.1s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.330s, learning 0.166s)
               Value function loss: 7.5260
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 51.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 8.50s
                        Total time: 19735.66s
                               ETA: 956803.7s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.286s, learning 0.174s)
               Value function loss: 0.3232
                    Surrogate loss: -0.0173
             Mean action noise std: 0.70
                       Mean reward: 9.86
               Mean episode length: 50.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 8.46s
                        Total time: 19744.12s
                               ETA: 956730.7s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.131s, learning 0.176s)
               Value function loss: 0.2456
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 49.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 8.31s
                        Total time: 19752.43s
                               ETA: 956650.4s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.492s, learning 0.160s)
               Value function loss: 105.4080
                    Surrogate loss: 0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 50.83
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 8.65s
                        Total time: 19761.08s
                               ETA: 956586.7s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.436s, learning 0.164s)
               Value function loss: 0.3336
                    Surrogate loss: -0.0190
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 50.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 8.60s
                        Total time: 19769.68s
                               ETA: 956520.7s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.525s, learning 0.174s)
               Value function loss: 34.5359
                    Surrogate loss: 0.0017
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 49.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 8.70s
                        Total time: 19778.38s
                               ETA: 956459.5s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.275s, learning 0.174s)
               Value function loss: 368.5454
                    Surrogate loss: -0.0024
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 50.09
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 8.45s
                        Total time: 19786.83s
                               ETA: 956386.2s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.309s, learning 0.210s)
               Value function loss: 60.9630
                    Surrogate loss: -0.0036
             Mean action noise std: 0.70
                       Mean reward: 7.60
               Mean episode length: 51.76
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 8.52s
                        Total time: 19795.35s
                               ETA: 956316.4s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.177s, learning 0.164s)
               Value function loss: 3.3023
                    Surrogate loss: -0.0100
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 49.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 8.34s
                        Total time: 19803.69s
                               ETA: 956238.1s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.367s, learning 0.177s)
               Value function loss: 272.1763
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 30.12
               Mean episode length: 50.05
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 8.54s
                        Total time: 19812.23s
                               ETA: 956169.6s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.162s)
               Value function loss: 2.2184
                    Surrogate loss: -0.0146
             Mean action noise std: 0.70
                       Mean reward: 5.12
               Mean episode length: 50.91
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 8.45s
                        Total time: 19820.68s
                               ETA: 956096.7s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.526s, learning 0.173s)
               Value function loss: 129.1621
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 49.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 8.70s
                        Total time: 19829.38s
                               ETA: 956035.8s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.434s, learning 0.208s)
               Value function loss: 214.1795
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 2.67
               Mean episode length: 51.13
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 8.64s
                        Total time: 19838.02s
                               ETA: 955972.2s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.536s, learning 0.260s)
               Value function loss: 21.4816
                    Surrogate loss: -0.0067
             Mean action noise std: 0.70
                       Mean reward: 2.41
               Mean episode length: 49.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 8.80s
                        Total time: 19846.82s
                               ETA: 955916.1s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.119s, learning 0.169s)
               Value function loss: 115.8434
                    Surrogate loss: -0.0048
             Mean action noise std: 0.70
                       Mean reward: 2.51
               Mean episode length: 50.42
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 8.29s
                        Total time: 19855.11s
                               ETA: 955835.7s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.387s, learning 0.217s)
               Value function loss: 130.3231
                    Surrogate loss: -0.0037
             Mean action noise std: 0.70
                       Mean reward: 3.00
               Mean episode length: 51.14
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 8.60s
                        Total time: 19863.71s
                               ETA: 955770.5s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.161s, learning 0.180s)
               Value function loss: 334.2900
                    Surrogate loss: -0.0044
             Mean action noise std: 0.70
                       Mean reward: 14.89
               Mean episode length: 49.42
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 8.34s
                        Total time: 19872.05s
                               ETA: 955692.7s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.310s, learning 0.171s)
               Value function loss: 356.3562
                    Surrogate loss: -0.0039
             Mean action noise std: 0.70
                       Mean reward: 22.76
               Mean episode length: 51.16
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 8.48s
                        Total time: 19880.54s
                               ETA: 955621.6s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.560s, learning 0.173s)
               Value function loss: 49.3904
                    Surrogate loss: -0.0068
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 50.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 8.73s
                        Total time: 19889.27s
                               ETA: 955562.8s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.235s, learning 0.181s)
               Value function loss: 91.9260
                    Surrogate loss: -0.0041
             Mean action noise std: 0.70
                       Mean reward: 15.28
               Mean episode length: 50.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 8.42s
                        Total time: 19897.68s
                               ETA: 955488.8s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.238s, learning 0.173s)
               Value function loss: 4.8811
                    Surrogate loss: -0.0143
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 51.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 8.41s
                        Total time: 19906.10s
                               ETA: 955414.5s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.249s, learning 0.167s)
               Value function loss: 13.8122
                    Surrogate loss: -0.0078
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 50.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 8.42s
                        Total time: 19914.51s
                               ETA: 955340.7s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.389s, learning 0.165s)
               Value function loss: 1.5100
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 50.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 8.55s
                        Total time: 19923.07s
                               ETA: 955273.5s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.220s, learning 0.199s)
               Value function loss: 1.0370
                    Surrogate loss: -0.0069
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 49.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 8.42s
                        Total time: 19931.49s
                               ETA: 955199.8s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.592s, learning 0.157s)
               Value function loss: 1.0051
                    Surrogate loss: -0.0118
             Mean action noise std: 0.70
                       Mean reward: 5.31
               Mean episode length: 50.14
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 8.75s
                        Total time: 19940.23s
                               ETA: 955142.1s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.314s, learning 0.215s)
               Value function loss: 350.5346
                    Surrogate loss: 0.0044
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 50.01
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 8.53s
                        Total time: 19948.76s
                               ETA: 955073.8s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.347s, learning 0.215s)
               Value function loss: 760.4261
                    Surrogate loss: -0.0025
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 50.18
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 8.56s
                        Total time: 19957.33s
                               ETA: 955007.2s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.280s, learning 0.212s)
               Value function loss: 22.1479
                    Surrogate loss: -0.0059
             Mean action noise std: 0.70
                       Mean reward: 12.69
               Mean episode length: 51.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 8.49s
                        Total time: 19965.82s
                               ETA: 954937.4s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.218s, learning 0.207s)
               Value function loss: 314.6119
                    Surrogate loss: -0.0031
             Mean action noise std: 0.70
                       Mean reward: 2.81
               Mean episode length: 51.61
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 8.43s
                        Total time: 19974.24s
                               ETA: 954864.4s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.139s, learning 0.173s)
               Value function loss: 30.0128
                    Surrogate loss: -0.0095
             Mean action noise std: 0.70
                       Mean reward: 12.78
               Mean episode length: 50.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 8.31s
                        Total time: 19982.56s
                               ETA: 954786.0s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.308s, learning 0.177s)
               Value function loss: 89.8446
                    Surrogate loss: -0.0032
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 49.60
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 8.49s
                        Total time: 19991.04s
                               ETA: 954716.0s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.088s, learning 0.173s)
               Value function loss: 73.3427
                    Surrogate loss: -0.0048
             Mean action noise std: 0.70
                       Mean reward: 2.79
               Mean episode length: 51.65
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 8.26s
                        Total time: 19999.30s
                               ETA: 954635.3s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.246s, learning 0.169s)
               Value function loss: 18.1008
                    Surrogate loss: -0.0101
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 51.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 8.41s
                        Total time: 20007.72s
                               ETA: 954562.0s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1851 steps/s (collection: 8.682s, learning 0.167s)
               Value function loss: 5.4576
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 51.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 8.85s
                        Total time: 20016.57s
                               ETA: 954509.5s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.434s, learning 0.215s)
               Value function loss: 1.3343
                    Surrogate loss: -0.0056
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 50.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 8.65s
                        Total time: 20025.21s
                               ETA: 954447.5s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.155s, learning 0.222s)
               Value function loss: 18.1260
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 50.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 8.38s
                        Total time: 20033.59s
                               ETA: 954372.6s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.366s, learning 0.173s)
               Value function loss: 1.2852
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 50.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 8.54s
                        Total time: 20042.13s
                               ETA: 954305.5s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.153s, learning 0.197s)
               Value function loss: 0.7478
                    Surrogate loss: -0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 51.41
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 8.35s
                        Total time: 20050.48s
                               ETA: 954229.5s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.092s, learning 0.165s)
               Value function loss: 0.4827
                    Surrogate loss: -0.0022
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 51.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 8.26s
                        Total time: 20058.74s
                               ETA: 954149.1s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.168s, learning 0.171s)
               Value function loss: 10.2432
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 52.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 8.34s
                        Total time: 20067.08s
                               ETA: 954072.7s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.131s, learning 0.261s)
               Value function loss: 0.2274
                    Surrogate loss: -0.0184
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 50.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 8.39s
                        Total time: 20075.47s
                               ETA: 953998.9s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.370s, learning 0.161s)
               Value function loss: 0.2593
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 51.39
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 8.53s
                        Total time: 20084.00s
                               ETA: 953931.7s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.264s, learning 0.162s)
               Value function loss: 0.2345
                    Surrogate loss: -0.0109
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 50.77
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 8.43s
                        Total time: 20092.43s
                               ETA: 953859.5s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.225s, learning 0.167s)
               Value function loss: 0.1651
                    Surrogate loss: -0.0137
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 51.69
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 8.39s
                        Total time: 20100.82s
                               ETA: 953785.9s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.173s)
               Value function loss: 0.1547
                    Surrogate loss: -0.0129
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 51.76
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 8.67s
                        Total time: 20109.49s
                               ETA: 953725.5s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.164s)
               Value function loss: 0.1202
                    Surrogate loss: -0.0201
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 51.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 8.42s
                        Total time: 20117.91s
                               ETA: 953653.4s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.253s, learning 0.167s)
               Value function loss: 0.1040
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 50.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 8.42s
                        Total time: 20126.33s
                               ETA: 953581.2s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.313s, learning 0.165s)
               Value function loss: 0.1505
                    Surrogate loss: -0.0252
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 50.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 8.48s
                        Total time: 20134.81s
                               ETA: 953511.9s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.537s, learning 0.178s)
               Value function loss: 0.1141
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 51.27
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 8.72s
                        Total time: 20143.53s
                               ETA: 953453.8s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.526s, learning 0.184s)
               Value function loss: 0.2142
                    Surrogate loss: -0.0162
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 50.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 8.71s
                        Total time: 20152.24s
                               ETA: 953395.6s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.521s, learning 0.199s)
               Value function loss: 0.1092
                    Surrogate loss: -0.0239
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 51.05
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 8.72s
                        Total time: 20160.96s
                               ETA: 953337.8s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.310s, learning 0.184s)
               Value function loss: 320.1821
                    Surrogate loss: -0.0009
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 51.05
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 8.49s
                        Total time: 20169.45s
                               ETA: 953269.5s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.406s, learning 0.210s)
               Value function loss: 3.9412
                    Surrogate loss: -0.0020
             Mean action noise std: 0.70
                       Mean reward: 10.48
               Mean episode length: 53.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 8.62s
                        Total time: 20178.07s
                               ETA: 953206.9s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.396s, learning 0.169s)
               Value function loss: 0.2906
                    Surrogate loss: -0.0061
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 51.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 8.56s
                        Total time: 20186.63s
                               ETA: 953142.0s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.374s, learning 0.177s)
               Value function loss: 0.1588
                    Surrogate loss: -0.0185
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 52.37
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 8.55s
                        Total time: 20195.18s
                               ETA: 953076.4s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.320s, learning 0.169s)
               Value function loss: 0.1276
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 51.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 8.49s
                        Total time: 20203.67s
                               ETA: 953008.0s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.042s, learning 0.211s)
               Value function loss: 0.1039
                    Surrogate loss: -0.0206
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 50.73
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 8.25s
                        Total time: 20211.93s
                               ETA: 952928.6s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.155s, learning 0.181s)
               Value function loss: 0.1466
                    Surrogate loss: -0.0209
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 51.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 8.34s
                        Total time: 20220.26s
                               ETA: 952853.1s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 976 steps/s (collection: 16.580s, learning 0.191s)
               Value function loss: 250.0275
                    Surrogate loss: -0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 50.17
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 16.77s
                        Total time: 20237.03s
                               ETA: 953175.0s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.934s, learning 0.166s)
               Value function loss: 92.6759
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.20
               Mean episode length: 51.85
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 16.10s
                        Total time: 20253.13s
                               ETA: 953464.9s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1038 steps/s (collection: 15.613s, learning 0.168s)
               Value function loss: 0.2233
                    Surrogate loss: -0.0169
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 50.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 15.78s
                        Total time: 20268.91s
                               ETA: 953739.5s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.124s, learning 0.189s)
               Value function loss: 0.1711
                    Surrogate loss: -0.0173
             Mean action noise std: 0.70
                       Mean reward: 2.46
               Mean episode length: 52.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 16.31s
                        Total time: 20285.23s
                               ETA: 954038.9s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.236s, learning 0.219s)
               Value function loss: 0.1635
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 2.12
               Mean episode length: 51.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 16.46s
                        Total time: 20301.68s
                               ETA: 954344.7s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.940s, learning 0.177s)
               Value function loss: 0.1961
                    Surrogate loss: -0.0196
             Mean action noise std: 0.70
                       Mean reward: 2.10
               Mean episode length: 49.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 16.12s
                        Total time: 20317.80s
                               ETA: 954634.3s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.090s, learning 0.168s)
               Value function loss: 0.1989
                    Surrogate loss: -0.0207
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 50.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 16.26s
                        Total time: 20334.06s
                               ETA: 954930.2s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.292s, learning 0.168s)
               Value function loss: 0.1399
                    Surrogate loss: -0.0160
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 51.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 16.46s
                        Total time: 20350.52s
                               ETA: 955235.3s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.108s, learning 0.162s)
               Value function loss: 64.9608
                    Surrogate loss: -0.0008
             Mean action noise std: 0.70
                       Mean reward: 2.53
               Mean episode length: 50.14
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 16.27s
                        Total time: 20366.79s
                               ETA: 955531.2s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.236s, learning 0.192s)
               Value function loss: 245.7248
                    Surrogate loss: -0.0019
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 50.71
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 16.43s
                        Total time: 20383.22s
                               ETA: 955834.2s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.368s, learning 0.211s)
               Value function loss: 1.9416
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 50.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 16.58s
                        Total time: 20399.79s
                               ETA: 956143.9s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.223s, learning 0.173s)
               Value function loss: 1.0180
                    Surrogate loss: -0.0159
             Mean action noise std: 0.70
                       Mean reward: 22.55
               Mean episode length: 51.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 16.40s
                        Total time: 20416.19s
                               ETA: 956444.8s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.153s, learning 0.196s)
               Value function loss: 0.5844
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 51.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 16.35s
                        Total time: 20432.54s
                               ETA: 956743.1s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.943s, learning 0.160s)
               Value function loss: 0.3326
                    Surrogate loss: -0.0200
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 50.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 16.10s
                        Total time: 20448.64s
                               ETA: 957029.7s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.919s, learning 0.159s)
               Value function loss: 0.2937
                    Surrogate loss: -0.0134
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 49.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 16.08s
                        Total time: 20464.72s
                               ETA: 957314.8s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.346s, learning 0.213s)
               Value function loss: 0.2241
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 50.93
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 16.56s
                        Total time: 20481.28s
                               ETA: 957622.1s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.226s, learning 0.214s)
               Value function loss: 0.1742
                    Surrogate loss: -0.0229
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 51.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 16.44s
                        Total time: 20497.72s
                               ETA: 957923.5s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.002s, learning 0.169s)
               Value function loss: 0.1355
                    Surrogate loss: -0.0150
             Mean action noise std: 0.70
                       Mean reward: 2.14
               Mean episode length: 50.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 16.17s
                        Total time: 20513.89s
                               ETA: 958212.0s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.096s, learning 0.170s)
               Value function loss: 17.7571
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 51.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 16.27s
                        Total time: 20530.16s
                               ETA: 958504.7s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.885s, learning 0.212s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0258
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 51.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 16.10s
                        Total time: 20546.25s
                               ETA: 958789.2s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.422s, learning 0.174s)
               Value function loss: 11.9993
                    Surrogate loss: 0.0015
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 51.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 16.60s
                        Total time: 20562.85s
                               ETA: 959096.7s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.161s, learning 0.163s)
               Value function loss: 110.2578
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 51.23
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 16.32s
                        Total time: 20579.17s
                               ETA: 959391.2s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.909s, learning 0.212s)
               Value function loss: 0.1681
                    Surrogate loss: -0.0243
             Mean action noise std: 0.70
                       Mean reward: 12.64
               Mean episode length: 51.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 16.12s
                        Total time: 20595.29s
                               ETA: 959676.0s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.138s, learning 0.182s)
               Value function loss: 0.1580
                    Surrogate loss: -0.0125
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 50.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 16.32s
                        Total time: 20611.61s
                               ETA: 959969.7s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.483s, learning 0.165s)
               Value function loss: 0.1451
                    Surrogate loss: -0.0145
             Mean action noise std: 0.70
                       Mean reward: 4.84
               Mean episode length: 50.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 16.65s
                        Total time: 20628.26s
                               ETA: 960278.5s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.417s, learning 0.185s)
               Value function loss: 0.1762
                    Surrogate loss: -0.0174
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 51.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 16.60s
                        Total time: 20644.86s
                               ETA: 960584.7s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.097s, learning 0.161s)
               Value function loss: 90.9537
                    Surrogate loss: -0.0003
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 51.21
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 16.26s
                        Total time: 20661.12s
                               ETA: 960874.7s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.756s, learning 0.165s)
               Value function loss: 0.1437
                    Surrogate loss: -0.0153
             Mean action noise std: 0.70
                       Mean reward: 2.95
               Mean episode length: 52.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 15.92s
                        Total time: 20677.04s
                               ETA: 961148.7s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.180s, learning 0.164s)
               Value function loss: 0.1460
                    Surrogate loss: -0.0167
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 50.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 16.34s
                        Total time: 20693.39s
                               ETA: 961442.1s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.386s, learning 0.166s)
               Value function loss: 117.8821
                    Surrogate loss: -0.0012
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 50.39
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 16.55s
                        Total time: 20709.94s
                               ETA: 961744.8s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.939s, learning 0.216s)
               Value function loss: 0.1570
                    Surrogate loss: -0.0210
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 51.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 16.15s
                        Total time: 20726.09s
                               ETA: 962028.8s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.987s, learning 0.168s)
               Value function loss: 0.1332
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 2.75
               Mean episode length: 51.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 16.16s
                        Total time: 20742.25s
                               ETA: 962312.5s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.355s, learning 0.164s)
               Value function loss: 0.1637
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 50.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 16.52s
                        Total time: 20758.77s
                               ETA: 962612.8s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.337s, learning 0.163s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0255
             Mean action noise std: 0.70
                       Mean reward: 2.83
               Mean episode length: 51.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 16.50s
                        Total time: 20775.27s
                               ETA: 962912.0s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.071s, learning 0.171s)
               Value function loss: 0.1845
                    Surrogate loss: -0.0183
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 51.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 16.24s
                        Total time: 20791.51s
                               ETA: 963198.8s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.097s, learning 0.182s)
               Value function loss: 0.1209
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 50.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 16.28s
                        Total time: 20807.79s
                               ETA: 963487.1s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.314s, learning 0.171s)
               Value function loss: 0.1061
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 51.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 16.49s
                        Total time: 20824.27s
                               ETA: 963784.7s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1245 steps/s (collection: 12.998s, learning 0.161s)
               Value function loss: 0.1002
                    Surrogate loss: -0.0242
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 51.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 13.16s
                        Total time: 20837.43s
                               ETA: 963928.2s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.143s, learning 0.176s)
               Value function loss: 263.3393
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 50.58
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 8.32s
                        Total time: 20845.75s
                               ETA: 963847.6s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.261s, learning 0.171s)
               Value function loss: 0.1450
                    Surrogate loss: -0.0193
             Mean action noise std: 0.70
                       Mean reward: 2.57
               Mean episode length: 51.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 8.43s
                        Total time: 20854.18s
                               ETA: 963772.4s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.498s, learning 0.166s)
               Value function loss: 0.1414
                    Surrogate loss: -0.0138
             Mean action noise std: 0.70
                       Mean reward: 15.10
               Mean episode length: 51.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 8.66s
                        Total time: 20862.85s
                               ETA: 963708.0s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.413s, learning 0.224s)
               Value function loss: 0.1255
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 51.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 8.64s
                        Total time: 20871.48s
                               ETA: 963642.3s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.325s, learning 0.182s)
               Value function loss: 0.0953
                    Surrogate loss: -0.0075
             Mean action noise std: 0.70
                       Mean reward: 2.57
               Mean episode length: 52.08
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 8.51s
                        Total time: 20879.99s
                               ETA: 963570.7s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 2029 steps/s (collection: 7.901s, learning 0.172s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 51.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 8.07s
                        Total time: 20888.06s
                               ETA: 963479.2s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.153s, learning 0.197s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0184
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 50.24
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 8.35s
                        Total time: 20896.41s
                               ETA: 963400.5s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.480s, learning 0.181s)
               Value function loss: 0.1407
                    Surrogate loss: -0.0187
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 51.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 8.66s
                        Total time: 20905.08s
                               ETA: 963336.2s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.492s, learning 0.168s)
               Value function loss: 0.1259
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 51.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 8.66s
                        Total time: 20913.74s
                               ETA: 963271.9s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.406s, learning 0.239s)
               Value function loss: 0.1341
                    Surrogate loss: -0.0145
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 51.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 8.64s
                        Total time: 20922.38s
                               ETA: 963206.9s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.261s, learning 0.165s)
               Value function loss: 0.0814
                    Surrogate loss: -0.0239
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 51.57
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 8.43s
                        Total time: 20930.81s
                               ETA: 963132.0s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.932s, learning 0.162s)
               Value function loss: 0.0783
                    Surrogate loss: -0.0272
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 51.26
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 8.09s
                        Total time: 20938.90s
                               ETA: 963041.8s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.173s, learning 0.216s)
               Value function loss: 0.0903
                    Surrogate loss: -0.0205
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 51.58
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 8.39s
                        Total time: 20947.29s
                               ETA: 962965.3s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.169s, learning 0.169s)
               Value function loss: 0.0822
                    Surrogate loss: -0.0205
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 49.61
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 8.34s
                        Total time: 20955.63s
                               ETA: 962886.4s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.192s, learning 0.172s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0268
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 51.92
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 8.36s
                        Total time: 20963.99s
                               ETA: 962808.9s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.422s, learning 0.175s)
               Value function loss: 0.0748
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.39
               Mean episode length: 51.06
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 8.60s
                        Total time: 20972.59s
                               ETA: 962742.1s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.062s, learning 0.160s)
               Value function loss: 106.5600
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 50.76
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 8.22s
                        Total time: 20980.81s
                               ETA: 962658.2s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.082s, learning 0.181s)
               Value function loss: 0.0892
                    Surrogate loss: -0.0223
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 50.75
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 8.26s
                        Total time: 20989.07s
                               ETA: 962576.2s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.446s, learning 0.167s)
               Value function loss: 0.0934
                    Surrogate loss: -0.0209
             Mean action noise std: 0.70
                       Mean reward: 9.95
               Mean episode length: 50.98
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 8.61s
                        Total time: 20997.69s
                               ETA: 962510.4s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.263s, learning 0.169s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0155
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 50.94
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 8.43s
                        Total time: 21006.12s
                               ETA: 962436.2s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.168s, learning 0.175s)
               Value function loss: 0.0699
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 51.38
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 8.34s
                        Total time: 21014.46s
                               ETA: 962358.1s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.274s, learning 0.179s)
               Value function loss: 0.0786
                    Surrogate loss: -0.0239
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 51.68
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 8.45s
                        Total time: 21022.91s
                               ETA: 962285.1s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.341s, learning 0.176s)
               Value function loss: 0.0641
                    Surrogate loss: -0.0243
             Mean action noise std: 0.70
                       Mean reward: 2.02
               Mean episode length: 51.91
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 8.52s
                        Total time: 21031.43s
                               ETA: 962215.0s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.290s, learning 0.184s)
               Value function loss: 0.0674
                    Surrogate loss: -0.0266
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 51.50
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 8.47s
                        Total time: 21039.90s
                               ETA: 962143.1s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.311s, learning 0.187s)
               Value function loss: 0.0897
                    Surrogate loss: -0.0194
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 51.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 8.50s
                        Total time: 21048.40s
                               ETA: 962072.3s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.158s, learning 0.172s)
               Value function loss: 0.0727
                    Surrogate loss: -0.0206
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 51.56
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 8.33s
                        Total time: 21056.73s
                               ETA: 961993.8s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.935s, learning 0.186s)
               Value function loss: 212.6551
                    Surrogate loss: 0.0033
             Mean action noise std: 0.70
                       Mean reward: 2.09
               Mean episode length: 50.41
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 8.12s
                        Total time: 21064.85s
                               ETA: 961905.9s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.300s, learning 0.188s)
               Value function loss: 3.8535
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 51.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 8.49s
                        Total time: 21073.34s
                               ETA: 961834.9s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.327s, learning 0.186s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0166
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 51.16
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 8.51s
                        Total time: 21081.85s
                               ETA: 961765.0s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.190s, learning 0.170s)
               Value function loss: 0.1280
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 50.03
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 8.36s
                        Total time: 21090.21s
                               ETA: 961688.2s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.142s, learning 0.182s)
               Value function loss: 133.3393
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 51.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 8.32s
                        Total time: 21098.54s
                               ETA: 961609.9s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.526s, learning 0.213s)
               Value function loss: 0.2816
                    Surrogate loss: -0.0147
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 50.97
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 8.74s
                        Total time: 21107.28s
                               ETA: 961550.5s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.116s, learning 0.168s)
               Value function loss: 510.3241
                    Surrogate loss: 0.0073
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 50.57
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 8.28s
                        Total time: 21115.56s
                               ETA: 961470.4s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.461s, learning 0.172s)
               Value function loss: 481.7356
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 9.64
               Mean episode length: 51.32
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 8.63s
                        Total time: 21124.19s
                               ETA: 961406.3s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.957s, learning 0.166s)
               Value function loss: 13.1746
                    Surrogate loss: 0.0035
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 52.81
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 8.12s
                        Total time: 21132.32s
                               ETA: 961319.0s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.289s, learning 0.214s)
               Value function loss: 5.7623
                    Surrogate loss: -0.0095
             Mean action noise std: 0.70
                       Mean reward: 1.88
               Mean episode length: 51.32
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 8.50s
                        Total time: 21140.82s
                               ETA: 961249.1s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.352s, learning 0.262s)
               Value function loss: 4.9510
                    Surrogate loss: -0.0060
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 51.23
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 8.61s
                        Total time: 21149.43s
                               ETA: 961184.3s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.383s, learning 0.211s)
               Value function loss: 4.1526
                    Surrogate loss: -0.0091
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 52.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 8.59s
                        Total time: 21158.03s
                               ETA: 961118.7s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.244s, learning 0.174s)
               Value function loss: 2.3449
                    Surrogate loss: -0.0077
             Mean action noise std: 0.70
                       Mean reward: 2.05
               Mean episode length: 51.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 8.42s
                        Total time: 21166.45s
                               ETA: 961045.1s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.491s, learning 0.170s)
               Value function loss: 0.7582
                    Surrogate loss: 0.0028
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 50.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 8.66s
                        Total time: 21175.11s
                               ETA: 960982.6s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.202s, learning 0.166s)
               Value function loss: 179.2801
                    Surrogate loss: 0.0012
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 50.89
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 8.37s
                        Total time: 21183.48s
                               ETA: 960906.8s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.088s, learning 0.185s)
               Value function loss: 0.5298
                    Surrogate loss: -0.0180
             Mean action noise std: 0.70
                       Mean reward: 7.29
               Mean episode length: 51.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 8.27s
                        Total time: 21191.75s
                               ETA: 960826.8s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.211s, learning 0.175s)
               Value function loss: 0.2258
                    Surrogate loss: -0.0079
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 51.88
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 8.39s
                        Total time: 21200.13s
                               ETA: 960752.0s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.098s, learning 0.171s)
               Value function loss: 0.2791
                    Surrogate loss: -0.0096
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 51.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 8.27s
                        Total time: 21208.40s
                               ETA: 960671.9s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.152s, learning 0.174s)
               Value function loss: 0.2144
                    Surrogate loss: -0.0163
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 49.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 8.33s
                        Total time: 21216.73s
                               ETA: 960594.5s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.031s, learning 0.160s)
               Value function loss: 0.1556
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 52.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 8.19s
                        Total time: 21224.92s
                               ETA: 960511.1s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.237s, learning 0.173s)
               Value function loss: 0.1212
                    Surrogate loss: -0.0093
             Mean action noise std: 0.70
                       Mean reward: 2.51
               Mean episode length: 51.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 8.41s
                        Total time: 21233.33s
                               ETA: 960437.6s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.119s, learning 0.208s)
               Value function loss: 4.0007
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 49.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 8.33s
                        Total time: 21241.66s
                               ETA: 960360.5s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.435s, learning 0.165s)
               Value function loss: 15.4151
                    Surrogate loss: 0.0008
             Mean action noise std: 0.70
                       Mean reward: 2.39
               Mean episode length: 51.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 8.60s
                        Total time: 21250.26s
                               ETA: 960295.7s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.469s, learning 0.223s)
               Value function loss: 0.1251
                    Surrogate loss: -0.0274
             Mean action noise std: 0.70
                       Mean reward: 2.68
               Mean episode length: 50.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 8.69s
                        Total time: 21258.95s
                               ETA: 960235.1s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.332s, learning 0.167s)
               Value function loss: 0.1462
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 49.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 8.50s
                        Total time: 21267.45s
                               ETA: 960165.9s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.623s, learning 0.165s)
               Value function loss: 0.0946
                    Surrogate loss: -0.0270
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 48.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 8.79s
                        Total time: 21276.24s
                               ETA: 960109.8s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.367s, learning 0.160s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0220
             Mean action noise std: 0.70
                       Mean reward: 2.55
               Mean episode length: 49.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 8.53s
                        Total time: 21284.76s
                               ETA: 960042.0s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.474s, learning 0.164s)
               Value function loss: 0.1116
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 49.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 8.64s
                        Total time: 21293.40s
                               ETA: 959979.1s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.335s, learning 0.164s)
               Value function loss: 0.1029
                    Surrogate loss: -0.0215
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 49.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 8.50s
                        Total time: 21301.90s
                               ETA: 959910.1s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.373s, learning 0.215s)
               Value function loss: 17.5312
                    Surrogate loss: 0.0012
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 49.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 8.59s
                        Total time: 21310.49s
                               ETA: 959845.1s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.282s, learning 0.174s)
               Value function loss: 17.9366
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 9.98
               Mean episode length: 50.21
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 8.46s
                        Total time: 21318.94s
                               ETA: 959774.3s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.270s, learning 0.191s)
               Value function loss: 0.1420
                    Surrogate loss: -0.0201
             Mean action noise std: 0.70
                       Mean reward: 2.78
               Mean episode length: 50.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 8.46s
                        Total time: 21327.40s
                               ETA: 959703.8s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.227s, learning 0.172s)
               Value function loss: 64.0602
                    Surrogate loss: -0.0015
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 49.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 8.40s
                        Total time: 21335.80s
                               ETA: 959630.5s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.392s, learning 0.205s)
               Value function loss: 0.1529
                    Surrogate loss: -0.0255
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 50.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 8.60s
                        Total time: 21344.40s
                               ETA: 959566.2s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.028s, learning 0.229s)
               Value function loss: 212.7523
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 50.40
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 8.26s
                        Total time: 21352.66s
                               ETA: 959486.6s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.369s, learning 0.177s)
               Value function loss: 176.6984
                    Surrogate loss: -0.0015
             Mean action noise std: 0.70
                       Mean reward: 2.59
               Mean episode length: 50.92
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 8.55s
                        Total time: 21361.20s
                               ETA: 959420.1s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.404s, learning 0.174s)
               Value function loss: 0.5907
                    Surrogate loss: -0.0182
             Mean action noise std: 0.70
                       Mean reward: 2.77
               Mean episode length: 50.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 8.58s
                        Total time: 21369.78s
                               ETA: 959355.1s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.415s, learning 0.163s)
               Value function loss: 0.4630
                    Surrogate loss: -0.0180
             Mean action noise std: 0.70
                       Mean reward: 2.90
               Mean episode length: 51.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 8.58s
                        Total time: 21378.36s
                               ETA: 959290.2s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.253s, learning 0.173s)
               Value function loss: 0.2344
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 50.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 8.43s
                        Total time: 21386.79s
                               ETA: 959218.4s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.129s, learning 0.178s)
               Value function loss: 0.2704
                    Surrogate loss: -0.0173
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 50.48
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 8.31s
                        Total time: 21395.09s
                               ETA: 959141.4s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.250s, learning 0.214s)
               Value function loss: 0.2388
                    Surrogate loss: -0.0158
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 50.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 8.46s
                        Total time: 21403.56s
                               ETA: 959071.5s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.120s, learning 0.169s)
               Value function loss: 0.1755
                    Surrogate loss: -0.0191
             Mean action noise std: 0.70
                       Mean reward: 1.91
               Mean episode length: 48.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 8.29s
                        Total time: 21411.85s
                               ETA: 958993.8s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.103s, learning 0.165s)
               Value function loss: 0.2217
                    Surrogate loss: -0.0140
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 49.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 8.27s
                        Total time: 21420.11s
                               ETA: 958915.3s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.287s, learning 0.183s)
               Value function loss: 0.1414
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 49.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 8.47s
                        Total time: 21428.58s
                               ETA: 958845.8s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.352s, learning 0.181s)
               Value function loss: 0.1266
                    Surrogate loss: -0.0202
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 50.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 8.53s
                        Total time: 21437.12s
                               ETA: 958779.3s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.100s, learning 0.157s)
               Value function loss: 64.0361
                    Surrogate loss: -0.0005
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 50.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 8.26s
                        Total time: 21445.37s
                               ETA: 958700.4s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.016s, learning 0.173s)
               Value function loss: 0.1515
                    Surrogate loss: -0.0227
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 50.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 8.19s
                        Total time: 21453.56s
                               ETA: 958618.6s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.396s, learning 0.174s)
               Value function loss: 0.1219
                    Surrogate loss: -0.0198
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 49.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 8.57s
                        Total time: 21462.13s
                               ETA: 958553.8s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.285s, learning 0.184s)
               Value function loss: 0.1069
                    Surrogate loss: -0.0229
             Mean action noise std: 0.70
                       Mean reward: 2.53
               Mean episode length: 51.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 8.47s
                        Total time: 21470.60s
                               ETA: 958484.6s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.236s, learning 0.160s)
               Value function loss: 0.1132
                    Surrogate loss: -0.0153
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 49.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 8.40s
                        Total time: 21479.00s
                               ETA: 958412.1s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.300s, learning 0.169s)
               Value function loss: 0.1211
                    Surrogate loss: -0.0248
             Mean action noise std: 0.70
                       Mean reward: 2.70
               Mean episode length: 51.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 8.47s
                        Total time: 21487.47s
                               ETA: 958343.0s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.441s, learning 0.173s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0223
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 49.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 8.61s
                        Total time: 21496.08s
                               ETA: 958280.5s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.542s, learning 0.170s)
               Value function loss: 0.1142
                    Surrogate loss: -0.0234
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 49.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 8.71s
                        Total time: 21504.79s
                               ETA: 958222.3s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.088s, learning 0.166s)
               Value function loss: 191.3401
                    Surrogate loss: -0.0013
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 49.53
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 8.25s
                        Total time: 21513.05s
                               ETA: 958143.7s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.186s, learning 0.162s)
               Value function loss: 0.1694
                    Surrogate loss: -0.0232
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 49.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 8.35s
                        Total time: 21521.40s
                               ETA: 958069.4s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.230s, learning 0.169s)
               Value function loss: 0.1651
                    Surrogate loss: -0.0193
             Mean action noise std: 0.70
                       Mean reward: 13.08
               Mean episode length: 51.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 8.40s
                        Total time: 21529.79s
                               ETA: 957997.4s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.164s)
               Value function loss: 0.1368
                    Surrogate loss: -0.0135
             Mean action noise std: 0.70
                       Mean reward: 2.08
               Mean episode length: 48.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 8.17s
                        Total time: 21537.96s
                               ETA: 957915.4s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.305s, learning 0.161s)
               Value function loss: 0.1071
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 49.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 8.47s
                        Total time: 21546.43s
                               ETA: 957846.6s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.114s, learning 0.159s)
               Value function loss: 0.1073
                    Surrogate loss: -0.0236
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 51.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 8.27s
                        Total time: 21554.70s
                               ETA: 957769.2s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.095s, learning 0.220s)
               Value function loss: 120.4641
                    Surrogate loss: 0.0018
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 51.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 8.32s
                        Total time: 21563.02s
                               ETA: 957693.8s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.119s, learning 0.203s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0210
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 50.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 8.32s
                        Total time: 21571.34s
                               ETA: 957618.7s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.483s, learning 0.270s)
               Value function loss: 0.1173
                    Surrogate loss: -0.0145
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 51.28
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 8.75s
                        Total time: 21580.09s
                               ETA: 957562.8s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.437s, learning 0.172s)
               Value function loss: 263.6059
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 49.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 8.61s
                        Total time: 21588.70s
                               ETA: 957500.6s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.911s, learning 0.165s)
               Value function loss: 0.1899
                    Surrogate loss: -0.0094
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 50.08
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 8.08s
                        Total time: 21596.78s
                               ETA: 957414.7s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1997 steps/s (collection: 8.031s, learning 0.173s)
               Value function loss: 55.9707
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 49.97
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 8.20s
                        Total time: 21604.98s
                               ETA: 957334.7s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.303s, learning 0.166s)
               Value function loss: 0.2437
                    Surrogate loss: -0.0138
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 51.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 8.47s
                        Total time: 21613.45s
                               ETA: 957266.4s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.253s, learning 0.173s)
               Value function loss: 0.2272
                    Surrogate loss: -0.0185
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 49.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 8.43s
                        Total time: 21621.88s
                               ETA: 957196.3s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.858s, learning 0.211s)
               Value function loss: 46.4337
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 49.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 8.07s
                        Total time: 21629.95s
                               ETA: 957110.4s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.283s, learning 0.156s)
               Value function loss: 0.1696
                    Surrogate loss: -0.0185
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 49.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 8.44s
                        Total time: 21638.39s
                               ETA: 957041.0s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.478s, learning 0.161s)
               Value function loss: 0.1617
                    Surrogate loss: -0.0153
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 49.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 8.64s
                        Total time: 21647.02s
                               ETA: 956980.5s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.060s, learning 0.162s)
               Value function loss: 39.1636
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 51.53
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 8.22s
                        Total time: 21655.25s
                               ETA: 956901.6s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.145s, learning 0.178s)
               Value function loss: 0.1347
                    Surrogate loss: -0.0237
             Mean action noise std: 0.70
                       Mean reward: 7.47
               Mean episode length: 49.24
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 8.32s
                        Total time: 21663.57s
                               ETA: 956827.2s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.119s, learning 0.162s)
               Value function loss: 0.1275
                    Surrogate loss: -0.0155
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 50.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 8.28s
                        Total time: 21671.85s
                               ETA: 956751.1s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.350s, learning 0.160s)
               Value function loss: 0.1155
                    Surrogate loss: -0.0215
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 51.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 8.51s
                        Total time: 21680.36s
                               ETA: 956685.1s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.104s, learning 0.169s)
               Value function loss: 0.1386
                    Surrogate loss: -0.0244
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 50.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 8.27s
                        Total time: 21688.63s
                               ETA: 956608.7s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.165s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0230
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 49.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 8.42s
                        Total time: 21697.06s
                               ETA: 956539.0s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.232s, learning 0.265s)
               Value function loss: 0.1091
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 49.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 8.50s
                        Total time: 21705.55s
                               ETA: 956472.5s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.114s, learning 0.215s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0273
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 49.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 8.33s
                        Total time: 21713.88s
                               ETA: 956398.8s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.274s, learning 0.160s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0229
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 50.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 8.43s
                        Total time: 21722.32s
                               ETA: 956329.7s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.078s, learning 0.169s)
               Value function loss: 92.9861
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 50.15
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 8.25s
                        Total time: 21730.56s
                               ETA: 956252.4s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.167s)
               Value function loss: 0.0997
                    Surrogate loss: -0.0224
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 49.66
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 8.45s
                        Total time: 21739.02s
                               ETA: 956184.3s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.350s, learning 0.162s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0188
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 49.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 8.51s
                        Total time: 21747.53s
                               ETA: 956118.8s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.229s, learning 0.169s)
               Value function loss: 0.0713
                    Surrogate loss: -0.0232
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 49.79
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 8.40s
                        Total time: 21755.93s
                               ETA: 956048.3s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.300s, learning 0.174s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0227
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 50.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 8.47s
                        Total time: 21764.40s
                               ETA: 955981.3s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.275s, learning 0.170s)
               Value function loss: 0.0893
                    Surrogate loss: -0.0222
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 51.65
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 8.45s
                        Total time: 21772.85s
                               ETA: 955913.0s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.255s, learning 0.161s)
               Value function loss: 0.0852
                    Surrogate loss: -0.0204
             Mean action noise std: 0.70
                       Mean reward: 1.96
               Mean episode length: 50.59
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 8.42s
                        Total time: 21781.26s
                               ETA: 955843.5s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.327s, learning 0.166s)
               Value function loss: 0.0802
                    Surrogate loss: -0.0236
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 50.93
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 8.49s
                        Total time: 21789.76s
                               ETA: 955777.5s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.930s, learning 0.176s)
               Value function loss: 0.0702
                    Surrogate loss: -0.0274
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 52.00
                  Mean reward/step: 0.04
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 8.11s
                        Total time: 21797.86s
                               ETA: 955694.5s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.090s, learning 0.169s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0266
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 52.74
                  Mean reward/step: 0.04
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 8.26s
                        Total time: 21806.12s
                               ETA: 955618.3s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.227s, learning 0.213s)
               Value function loss: 0.0903
                    Surrogate loss: -0.0284
             Mean action noise std: 0.70
                       Mean reward: 1.97
               Mean episode length: 50.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 8.44s
                        Total time: 21814.56s
                               ETA: 955550.1s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.996s, learning 0.163s)
               Value function loss: 0.0687
                    Surrogate loss: -0.0273
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 50.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 8.16s
                        Total time: 21822.72s
                               ETA: 955469.6s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.390s, learning 0.166s)
               Value function loss: 0.0756
                    Surrogate loss: -0.0334
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 51.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 8.56s
                        Total time: 21831.28s
                               ETA: 955406.6s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.100s, learning 0.164s)
               Value function loss: 0.0790
                    Surrogate loss: -0.0257
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 51.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 8.26s
                        Total time: 21839.54s
                               ETA: 955330.9s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.631s, learning 0.165s)
               Value function loss: 0.0916
                    Surrogate loss: -0.0300
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 51.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 8.80s
                        Total time: 21848.34s
                               ETA: 955278.4s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.172s, learning 0.163s)
               Value function loss: 7.1044
                    Surrogate loss: 0.0015
             Mean action noise std: 0.70
                       Mean reward: 4.95
               Mean episode length: 49.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 8.33s
                        Total time: 21856.67s
                               ETA: 955205.9s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.217s, learning 0.162s)
               Value function loss: 163.3338
                    Surrogate loss: -0.0014
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 50.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 8.38s
                        Total time: 21865.05s
                               ETA: 955135.3s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.446s, learning 0.173s)
               Value function loss: 248.2578
                    Surrogate loss: -0.0023
             Mean action noise std: 0.70
                       Mean reward: 27.70
               Mean episode length: 51.53
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 8.62s
                        Total time: 21873.67s
                               ETA: 955075.3s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.157s, learning 0.160s)
               Value function loss: 0.6516
                    Surrogate loss: -0.0192
             Mean action noise std: 0.70
                       Mean reward: 2.66
               Mean episode length: 50.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 8.32s
                        Total time: 21881.99s
                               ETA: 955002.1s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.163s, learning 0.176s)
               Value function loss: 299.9676
                    Surrogate loss: -0.0020
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 50.39
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 8.34s
                        Total time: 21890.32s
                               ETA: 954930.0s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.213s, learning 0.160s)
               Value function loss: 17.5806
                    Surrogate loss: -0.0041
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 50.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 8.37s
                        Total time: 21898.70s
                               ETA: 954859.3s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.090s, learning 0.170s)
               Value function loss: 93.4752
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.77
               Mean episode length: 50.29
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 8.26s
                        Total time: 21906.96s
                               ETA: 954783.9s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.158s, learning 0.210s)
               Value function loss: 0.6827
                    Surrogate loss: -0.0185
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 50.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 8.37s
                        Total time: 21915.32s
                               ETA: 954713.1s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.260s, learning 0.215s)
               Value function loss: 0.4303
                    Surrogate loss: -0.0090
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 50.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 8.47s
                        Total time: 21923.80s
                               ETA: 954647.1s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.389s, learning 0.183s)
               Value function loss: 0.3703
                    Surrogate loss: -0.0073
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 49.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 8.57s
                        Total time: 21932.37s
                               ETA: 954585.4s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 2051 steps/s (collection: 7.820s, learning 0.166s)
               Value function loss: 0.3315
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 2.17
               Mean episode length: 48.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 7.99s
                        Total time: 21940.36s
                               ETA: 954498.2s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.107s, learning 0.178s)
               Value function loss: 129.3997
                    Surrogate loss: 0.0000
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 49.55
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 8.28s
                        Total time: 21948.64s
                               ETA: 954424.1s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.188s, learning 0.172s)
               Value function loss: 44.8563
                    Surrogate loss: -0.0020
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 49.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 8.36s
                        Total time: 21957.00s
                               ETA: 954353.4s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.035s, learning 0.162s)
               Value function loss: 0.5406
                    Surrogate loss: -0.0120
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 49.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 8.20s
                        Total time: 21965.20s
                               ETA: 954275.6s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.921s, learning 0.167s)
               Value function loss: 0.3864
                    Surrogate loss: -0.0127
             Mean action noise std: 0.70
                       Mean reward: 2.43
               Mean episode length: 49.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 8.09s
                        Total time: 21973.29s
                               ETA: 954193.1s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.171s, learning 0.160s)
               Value function loss: 0.3267
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 49.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 8.33s
                        Total time: 21981.62s
                               ETA: 954121.2s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.324s, learning 0.172s)
               Value function loss: 0.3029
                    Surrogate loss: -0.0118
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 49.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 8.50s
                        Total time: 21990.11s
                               ETA: 954056.6s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.047s, learning 0.164s)
               Value function loss: 0.2809
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 48.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 8.21s
                        Total time: 21998.32s
                               ETA: 953979.6s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 2036 steps/s (collection: 7.870s, learning 0.173s)
               Value function loss: 0.1973
                    Surrogate loss: -0.0152
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 49.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 8.04s
                        Total time: 22006.37s
                               ETA: 953895.5s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.007s, learning 0.164s)
               Value function loss: 0.1474
                    Surrogate loss: -0.0188
             Mean action noise std: 0.70
                       Mean reward: 2.84
               Mean episode length: 52.15
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 8.17s
                        Total time: 22014.54s
                               ETA: 953817.0s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 2047 steps/s (collection: 7.845s, learning 0.159s)
               Value function loss: 0.1158
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 50.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 8.00s
                        Total time: 22022.54s
                               ETA: 953731.2s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.165s, learning 0.167s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 49.52
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 8.33s
                        Total time: 22030.87s
                               ETA: 953659.7s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.211s, learning 0.166s)
               Value function loss: 0.0868
                    Surrogate loss: -0.0202
             Mean action noise std: 0.70
                       Mean reward: 2.53
               Mean episode length: 50.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 8.38s
                        Total time: 22039.25s
                               ETA: 953590.3s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.142s, learning 0.175s)
               Value function loss: 0.0931
                    Surrogate loss: -0.0206
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 48.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 8.32s
                        Total time: 22047.57s
                               ETA: 953518.3s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.069s, learning 0.163s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0278
             Mean action noise std: 0.70
                       Mean reward: 2.16
               Mean episode length: 49.99
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 8.23s
                        Total time: 22055.80s
                               ETA: 953442.7s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.436s, learning 0.161s)
               Value function loss: 161.9593
                    Surrogate loss: -0.0008
             Mean action noise std: 0.70
                       Mean reward: 14.65
               Mean episode length: 49.17
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 8.60s
                        Total time: 22064.40s
                               ETA: 953382.9s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.200s, learning 0.168s)
               Value function loss: 211.7721
                    Surrogate loss: -0.0027
             Mean action noise std: 0.70
                       Mean reward: 12.64
               Mean episode length: 51.21
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 8.37s
                        Total time: 22072.77s
                               ETA: 953313.3s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.213s, learning 0.162s)
               Value function loss: 0.6459
                    Surrogate loss: -0.0179
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 49.94
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 8.38s
                        Total time: 22081.14s
                               ETA: 953244.0s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.105s, learning 0.191s)
               Value function loss: 0.4431
                    Surrogate loss: -0.0061
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 49.78
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 8.30s
                        Total time: 22089.44s
                               ETA: 953171.4s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.118s, learning 0.163s)
               Value function loss: 0.3752
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 51.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 8.28s
                        Total time: 22097.72s
                               ETA: 953098.2s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.076s, learning 0.166s)
               Value function loss: 247.1052
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 48.99
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 8.24s
                        Total time: 22105.96s
                               ETA: 953023.3s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.083s, learning 0.173s)
               Value function loss: 0.4497
                    Surrogate loss: -0.0180
             Mean action noise std: 0.70
                       Mean reward: 17.41
               Mean episode length: 48.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 8.26s
                        Total time: 22114.22s
                               ETA: 952949.1s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.271s, learning 0.176s)
               Value function loss: 0.4211
                    Surrogate loss: -0.0141
             Mean action noise std: 0.70
                       Mean reward: 2.51
               Mean episode length: 49.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 8.45s
                        Total time: 22122.66s
                               ETA: 952883.2s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.119s, learning 0.164s)
               Value function loss: 0.3191
                    Surrogate loss: -0.0111
             Mean action noise std: 0.70
                       Mean reward: 2.57
               Mean episode length: 49.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 8.28s
                        Total time: 22130.94s
                               ETA: 952810.3s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1072 steps/s (collection: 15.108s, learning 0.171s)
               Value function loss: 65.1899
                    Surrogate loss: -0.0011
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 50.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 15.28s
                        Total time: 22146.22s
                               ETA: 953038.5s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.194s, learning 0.167s)
               Value function loss: 4.1219
                    Surrogate loss: -0.0050
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 50.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 16.36s
                        Total time: 22162.58s
                               ETA: 953313.0s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.219s, learning 0.168s)
               Value function loss: 0.3964
                    Surrogate loss: -0.0143
             Mean action noise std: 0.70
                       Mean reward: 2.64
               Mean episode length: 50.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 16.39s
                        Total time: 22178.97s
                               ETA: 953588.4s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.907s, learning 0.161s)
               Value function loss: 0.1925
                    Surrogate loss: -0.0058
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 50.42
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 16.07s
                        Total time: 22195.04s
                               ETA: 953849.8s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.057s, learning 0.176s)
               Value function loss: 0.2311
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 49.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 16.23s
                        Total time: 22211.27s
                               ETA: 954118.1s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.370s, learning 0.173s)
               Value function loss: 0.1807
                    Surrogate loss: -0.0176
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 49.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 16.54s
                        Total time: 22227.82s
                               ETA: 954399.5s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.779s, learning 0.163s)
               Value function loss: 0.1637
                    Surrogate loss: -0.0156
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 49.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 15.94s
                        Total time: 22243.76s
                               ETA: 954654.8s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.374s, learning 0.168s)
               Value function loss: 0.1510
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 50.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 16.54s
                        Total time: 22260.30s
                               ETA: 954935.6s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.201s, learning 0.180s)
               Value function loss: 0.1640
                    Surrogate loss: -0.0174
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 50.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 16.38s
                        Total time: 22276.68s
                               ETA: 955209.2s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.164s, learning 0.167s)
               Value function loss: 0.1354
                    Surrogate loss: -0.0209
             Mean action noise std: 0.70
                       Mean reward: 2.04
               Mean episode length: 49.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 16.33s
                        Total time: 22293.01s
                               ETA: 955480.4s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.185s, learning 0.181s)
               Value function loss: 67.5395
                    Surrogate loss: -0.0001
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 50.53
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 16.37s
                        Total time: 22309.38s
                               ETA: 955752.9s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.176s, learning 0.166s)
               Value function loss: 17.6704
                    Surrogate loss: -0.0018
             Mean action noise std: 0.70
                       Mean reward: 12.31
               Mean episode length: 49.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 16.34s
                        Total time: 22325.72s
                               ETA: 956024.1s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.889s, learning 0.163s)
               Value function loss: 0.2445
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 48.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 16.05s
                        Total time: 22341.77s
                               ETA: 956282.6s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.073s, learning 0.170s)
               Value function loss: 0.1989
                    Surrogate loss: -0.0110
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 50.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 16.24s
                        Total time: 22358.01s
                               ETA: 956549.1s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.937s, learning 0.181s)
               Value function loss: 0.1557
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 2.19
               Mean episode length: 49.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 16.12s
                        Total time: 22374.13s
                               ETA: 956809.9s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.777s, learning 0.165s)
               Value function loss: 0.1332
                    Surrogate loss: -0.0181
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 48.51
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 15.94s
                        Total time: 22390.07s
                               ETA: 957063.0s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.053s, learning 0.186s)
               Value function loss: 321.5645
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 48.29
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 16.24s
                        Total time: 22406.31s
                               ETA: 957328.5s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.395s, learning 0.212s)
               Value function loss: 0.2553
                    Surrogate loss: -0.0113
             Mean action noise std: 0.70
                       Mean reward: 2.37
               Mean episode length: 49.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 16.61s
                        Total time: 22422.92s
                               ETA: 957609.6s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.426s, learning 0.211s)
               Value function loss: 0.1840
                    Surrogate loss: -0.0176
             Mean action noise std: 0.70
                       Mean reward: 2.41
               Mean episode length: 49.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 16.64s
                        Total time: 22439.56s
                               ETA: 957891.7s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.846s, learning 0.166s)
               Value function loss: 0.1839
                    Surrogate loss: -0.0036
             Mean action noise std: 0.70
                       Mean reward: 2.03
               Mean episode length: 47.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 16.01s
                        Total time: 22455.57s
                               ETA: 958146.7s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.304s, learning 0.171s)
               Value function loss: 0.2156
                    Surrogate loss: -0.0115
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 48.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 16.48s
                        Total time: 22472.04s
                               ETA: 958421.4s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.021s, learning 0.162s)
               Value function loss: 0.2172
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 47.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 16.18s
                        Total time: 22488.23s
                               ETA: 958683.3s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.095s, learning 0.167s)
               Value function loss: 0.1912
                    Surrogate loss: -0.0139
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 49.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 16.26s
                        Total time: 22504.49s
                               ETA: 958948.4s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.390s, learning 0.164s)
               Value function loss: 0.1816
                    Surrogate loss: -0.0179
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 49.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 16.55s
                        Total time: 22521.04s
                               ETA: 959225.6s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.312s, learning 0.162s)
               Value function loss: 0.1900
                    Surrogate loss: -0.0188
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 50.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 16.47s
                        Total time: 22537.52s
                               ETA: 959499.2s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.014s, learning 0.185s)
               Value function loss: 0.1245
                    Surrogate loss: -0.0188
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 49.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 16.20s
                        Total time: 22553.72s
                               ETA: 959760.8s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.253s, learning 0.168s)
               Value function loss: 302.3258
                    Surrogate loss: 0.0014
             Mean action noise std: 0.70
                       Mean reward: 3.09
               Mean episode length: 51.19
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 16.42s
                        Total time: 22570.14s
                               ETA: 960031.6s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.813s, learning 0.164s)
               Value function loss: 245.0174
                    Surrogate loss: -0.0021
             Mean action noise std: 0.70
                       Mean reward: 30.69
               Mean episode length: 50.87
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 15.98s
                        Total time: 22586.11s
                               ETA: 960283.3s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.810s, learning 0.163s)
               Value function loss: 0.8330
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 49.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 15.97s
                        Total time: 22602.09s
                               ETA: 960534.6s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.956s, learning 0.164s)
               Value function loss: 0.5968
                    Surrogate loss: -0.0156
             Mean action noise std: 0.70
                       Mean reward: 2.79
               Mean episode length: 49.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 16.12s
                        Total time: 22618.21s
                               ETA: 960791.9s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.160s, learning 0.162s)
               Value function loss: 0.3729
                    Surrogate loss: -0.0152
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 48.89
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 16.32s
                        Total time: 22634.53s
                               ETA: 961057.5s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.170s, learning 0.172s)
               Value function loss: 0.3971
                    Surrogate loss: -0.0148
             Mean action noise std: 0.70
                       Mean reward: 2.18
               Mean episode length: 47.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 16.34s
                        Total time: 22650.87s
                               ETA: 961323.8s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.281s, learning 0.164s)
               Value function loss: 0.2276
                    Surrogate loss: -0.0004
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 48.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 16.45s
                        Total time: 22667.32s
                               ETA: 961594.2s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.930s, learning 0.165s)
               Value function loss: 0.2421
                    Surrogate loss: -0.0136
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 49.23
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 16.10s
                        Total time: 22683.41s
                               ETA: 961849.5s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.337s, learning 0.168s)
               Value function loss: 0.2034
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 2.74
               Mean episode length: 49.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 16.51s
                        Total time: 22699.92s
                               ETA: 962121.9s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.089s, learning 0.164s)
               Value function loss: 0.1810
                    Surrogate loss: -0.0229
             Mean action noise std: 0.70
                       Mean reward: 3.06
               Mean episode length: 50.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 16.25s
                        Total time: 22716.17s
                               ETA: 962383.4s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.251s, learning 0.165s)
               Value function loss: 0.1346
                    Surrogate loss: -0.0034
             Mean action noise std: 0.70
                       Mean reward: 2.68
               Mean episode length: 48.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 16.42s
                        Total time: 22732.58s
                               ETA: 962651.6s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1135 steps/s (collection: 14.260s, learning 0.171s)
               Value function loss: 0.1446
                    Surrogate loss: -0.0198
             Mean action noise std: 0.70
                       Mean reward: 2.55
               Mean episode length: 49.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 14.43s
                        Total time: 22747.02s
                               ETA: 962835.4s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.394s, learning 0.161s)
               Value function loss: 0.1243
                    Surrogate loss: -0.0172
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 48.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 8.56s
                        Total time: 22755.57s
                               ETA: 962770.6s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.280s, learning 0.166s)
               Value function loss: 0.1646
                    Surrogate loss: -0.0176
             Mean action noise std: 0.70
                       Mean reward: 2.75
               Mean episode length: 49.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 8.45s
                        Total time: 22764.02s
                               ETA: 962701.1s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.208s, learning 0.163s)
               Value function loss: 0.1382
                    Surrogate loss: -0.0108
             Mean action noise std: 0.70
                       Mean reward: 2.68
               Mean episode length: 49.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 8.37s
                        Total time: 22772.39s
                               ETA: 962628.6s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.418s, learning 0.160s)
               Value function loss: 0.1285
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.89
               Mean episode length: 48.90
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 8.58s
                        Total time: 22780.97s
                               ETA: 962564.8s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.214s, learning 0.168s)
               Value function loss: 0.1096
                    Surrogate loss: -0.0238
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 49.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 8.38s
                        Total time: 22789.35s
                               ETA: 962492.9s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.935s, learning 0.169s)
               Value function loss: 0.1297
                    Surrogate loss: -0.0251
             Mean action noise std: 0.70
                       Mean reward: 2.77
               Mean episode length: 49.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 8.10s
                        Total time: 22797.45s
                               ETA: 962409.2s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.248s, learning 0.165s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0256
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 48.62
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 8.41s
                        Total time: 22805.87s
                               ETA: 962338.6s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.170s)
               Value function loss: 0.0979
                    Surrogate loss: -0.0280
             Mean action noise std: 0.70
                       Mean reward: 2.62
               Mean episode length: 49.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 8.43s
                        Total time: 22814.30s
                               ETA: 962268.8s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.234s, learning 0.167s)
               Value function loss: 0.0861
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 2.85
               Mean episode length: 49.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 8.40s
                        Total time: 22822.70s
                               ETA: 962197.9s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.939s, learning 0.172s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0288
             Mean action noise std: 0.70
                       Mean reward: 2.82
               Mean episode length: 49.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 8.11s
                        Total time: 22830.81s
                               ETA: 962114.7s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.043s, learning 0.170s)
               Value function loss: 0.1078
                    Surrogate loss: -0.0233
             Mean action noise std: 0.70
                       Mean reward: 2.77
               Mean episode length: 49.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 8.21s
                        Total time: 22839.02s
                               ETA: 962035.9s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.325s, learning 0.165s)
               Value function loss: 0.0834
                    Surrogate loss: -0.0241
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 48.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 8.49s
                        Total time: 22847.51s
                               ETA: 961968.9s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.926s, learning 0.163s)
               Value function loss: 132.9061
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 2.11
               Mean episode length: 47.59
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 8.09s
                        Total time: 22855.60s
                               ETA: 961885.0s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.200s, learning 0.161s)
               Value function loss: 0.1126
                    Surrogate loss: -0.0181
             Mean action noise std: 0.70
                       Mean reward: 2.76
               Mean episode length: 49.26
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 8.36s
                        Total time: 22863.96s
                               ETA: 961812.6s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.166s, learning 0.167s)
               Value function loss: 132.8236
                    Surrogate loss: -0.0010
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 49.39
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 8.33s
                        Total time: 22872.29s
                               ETA: 961739.1s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.252s, learning 0.205s)
               Value function loss: 0.1659
                    Surrogate loss: -0.0106
             Mean action noise std: 0.70
                       Mean reward: 2.67
               Mean episode length: 49.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 8.46s
                        Total time: 22880.75s
                               ETA: 961670.9s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.161s, learning 0.241s)
               Value function loss: 0.1359
                    Surrogate loss: -0.0143
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 50.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 8.40s
                        Total time: 22889.15s
                               ETA: 961600.4s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.170s)
               Value function loss: 0.1296
                    Surrogate loss: -0.0102
             Mean action noise std: 0.70
                       Mean reward: 2.26
               Mean episode length: 49.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 8.48s
                        Total time: 22897.64s
                               ETA: 961533.3s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.162s)
               Value function loss: 0.1149
                    Surrogate loss: -0.0192
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 49.63
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 8.36s
                        Total time: 22905.99s
                               ETA: 961461.1s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.162s, learning 0.183s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 2.85
               Mean episode length: 51.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 8.35s
                        Total time: 22914.34s
                               ETA: 961388.4s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.340s, learning 0.169s)
               Value function loss: 0.0925
                    Surrogate loss: -0.0160
             Mean action noise std: 0.70
                       Mean reward: 2.73
               Mean episode length: 51.59
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 8.51s
                        Total time: 22922.85s
                               ETA: 961322.6s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.354s, learning 0.172s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0198
             Mean action noise std: 0.70
                       Mean reward: 2.49
               Mean episode length: 51.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 8.53s
                        Total time: 22931.37s
                               ETA: 961257.6s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.127s, learning 0.160s)
               Value function loss: 0.0913
                    Surrogate loss: -0.0177
             Mean action noise std: 0.70
                       Mean reward: 2.07
               Mean episode length: 50.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 8.29s
                        Total time: 22939.66s
                               ETA: 961182.6s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.074s, learning 0.174s)
               Value function loss: 29.2861
                    Surrogate loss: 0.0031
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 50.86
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 8.25s
                        Total time: 22947.91s
                               ETA: 961106.0s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.227s, learning 0.195s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.25
               Mean episode length: 48.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 8.42s
                        Total time: 22956.33s
                               ETA: 961036.8s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.171s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0182
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 50.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 8.48s
                        Total time: 22964.81s
                               ETA: 960970.3s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.019s, learning 0.167s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0158
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 50.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 8.19s
                        Total time: 22973.00s
                               ETA: 960891.3s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.187s, learning 0.167s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0176
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 49.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 8.35s
                        Total time: 22981.35s
                               ETA: 960819.3s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.139s, learning 0.167s)
               Value function loss: 0.1058
                    Surrogate loss: -0.0179
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 51.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 8.31s
                        Total time: 22989.66s
                               ETA: 960745.5s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.142s, learning 0.211s)
               Value function loss: 0.0963
                    Surrogate loss: -0.0277
             Mean action noise std: 0.70
                       Mean reward: 2.66
               Mean episode length: 50.70
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 8.35s
                        Total time: 22998.01s
                               ETA: 960673.7s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.201s, learning 0.220s)
               Value function loss: 46.9248
                    Surrogate loss: 0.0003
             Mean action noise std: 0.70
                       Mean reward: 2.28
               Mean episode length: 49.45
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 8.42s
                        Total time: 23006.44s
                               ETA: 960604.7s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.171s)
               Value function loss: 0.1060
                    Surrogate loss: -0.0236
             Mean action noise std: 0.70
                       Mean reward: 7.55
               Mean episode length: 50.74
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 8.46s
                        Total time: 23014.89s
                               ETA: 960537.3s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.340s, learning 0.172s)
               Value function loss: 0.0900
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 50.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 8.51s
                        Total time: 23023.40s
                               ETA: 960472.3s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.345s, learning 0.168s)
               Value function loss: 0.1242
                    Surrogate loss: -0.0208
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 48.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 8.51s
                        Total time: 23031.92s
                               ETA: 960407.3s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.165s, learning 0.215s)
               Value function loss: 29.5353
                    Surrogate loss: -0.0007
             Mean action noise std: 0.70
                       Mean reward: 2.66
               Mean episode length: 49.71
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 8.38s
                        Total time: 23040.30s
                               ETA: 960336.9s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.257s, learning 0.168s)
               Value function loss: 0.0882
                    Surrogate loss: -0.0283
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 47.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 8.42s
                        Total time: 23048.72s
                               ETA: 960268.3s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.400s, learning 0.165s)
               Value function loss: 0.0888
                    Surrogate loss: -0.0264
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 48.65
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 8.57s
                        Total time: 23057.29s
                               ETA: 960205.7s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.789s, learning 0.173s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0300
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 49.37
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 7.96s
                        Total time: 23065.25s
                               ETA: 960118.0s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.260s, learning 0.184s)
               Value function loss: 0.1148
                    Surrogate loss: -0.0178
             Mean action noise std: 0.70
                       Mean reward: 2.76
               Mean episode length: 49.05
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 8.44s
                        Total time: 23073.69s
                               ETA: 960050.4s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.142s, learning 0.162s)
               Value function loss: 0.0910
                    Surrogate loss: -0.0262
             Mean action noise std: 0.70
                       Mean reward: 2.54
               Mean episode length: 49.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 8.30s
                        Total time: 23082.00s
                               ETA: 959977.1s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.167s, learning 0.207s)
               Value function loss: 0.1026
                    Surrogate loss: -0.0272
             Mean action noise std: 0.70
                       Mean reward: 2.71
               Mean episode length: 49.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 8.37s
                        Total time: 23090.37s
                               ETA: 959906.7s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.240s, learning 0.184s)
               Value function loss: 0.1136
                    Surrogate loss: -0.0186
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 48.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 8.42s
                        Total time: 23098.79s
                               ETA: 959838.4s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.068s, learning 0.162s)
               Value function loss: 0.0914
                    Surrogate loss: -0.0245
             Mean action noise std: 0.70
                       Mean reward: 2.45
               Mean episode length: 48.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 8.23s
                        Total time: 23107.02s
                               ETA: 959762.2s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.437s, learning 0.177s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0200
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 48.67
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 8.61s
                        Total time: 23115.64s
                               ETA: 959701.9s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.134s, learning 0.180s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0273
             Mean action noise std: 0.70
                       Mean reward: 2.73
               Mean episode length: 49.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 8.31s
                        Total time: 23123.95s
                               ETA: 959629.3s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.171s, learning 0.190s)
               Value function loss: 0.0937
                    Surrogate loss: -0.0236
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 47.30
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 8.36s
                        Total time: 23132.31s
                               ETA: 959558.6s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.267s, learning 0.240s)
               Value function loss: 0.0831
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 2.74
               Mean episode length: 48.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 8.51s
                        Total time: 23140.82s
                               ETA: 959494.0s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.450s, learning 0.224s)
               Value function loss: 0.0847
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 2.48
               Mean episode length: 48.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 8.67s
                        Total time: 23149.49s
                               ETA: 959436.5s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.292s, learning 0.205s)
               Value function loss: 0.0728
                    Surrogate loss: -0.0235
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 47.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 8.50s
                        Total time: 23157.99s
                               ETA: 959371.6s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.437s, learning 0.208s)
               Value function loss: 0.0788
                    Surrogate loss: -0.0241
             Mean action noise std: 0.70
                       Mean reward: 2.42
               Mean episode length: 48.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 8.64s
                        Total time: 23166.64s
                               ETA: 959312.9s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.315s, learning 0.250s)
               Value function loss: 0.0838
                    Surrogate loss: -0.0260
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 47.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 8.56s
                        Total time: 23175.20s
                               ETA: 959250.9s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.677s, learning 0.166s)
               Value function loss: 0.0765
                    Surrogate loss: -0.0246
             Mean action noise std: 0.70
                       Mean reward: 2.40
               Mean episode length: 48.60
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 8.84s
                        Total time: 23184.04s
                               ETA: 959200.5s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.378s, learning 0.162s)
               Value function loss: 132.6013
                    Surrogate loss: -0.0000
             Mean action noise std: 0.70
                       Mean reward: 2.41
               Mean episode length: 47.64
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 8.54s
                        Total time: 23192.58s
                               ETA: 959137.6s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.171s)
               Value function loss: 489.0445
                    Surrogate loss: -0.0022
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 48.36
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 8.53s
                        Total time: 23201.11s
                               ETA: 959074.3s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.043s, learning 0.256s)
               Value function loss: 46.1535
                    Surrogate loss: -0.0041
             Mean action noise std: 0.70
                       Mean reward: 2.84
               Mean episode length: 49.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 8.30s
                        Total time: 23209.41s
                               ETA: 959001.6s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.408s, learning 0.207s)
               Value function loss: 2.9435
                    Surrogate loss: 0.0006
             Mean action noise std: 0.70
                       Mean reward: 2.46
               Mean episode length: 46.97
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 8.62s
                        Total time: 23218.03s
                               ETA: 958941.9s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.540s, learning 0.213s)
               Value function loss: 2.0080
                    Surrogate loss: -0.0099
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 47.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 8.75s
                        Total time: 23226.78s
                               ETA: 958887.9s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.469s, learning 0.211s)
               Value function loss: 0.8828
                    Surrogate loss: -0.0048
             Mean action noise std: 0.70
                       Mean reward: 2.83
               Mean episode length: 48.69
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 8.68s
                        Total time: 23235.46s
                               ETA: 958831.0s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.144s, learning 0.278s)
               Value function loss: 0.4425
                    Surrogate loss: -0.0026
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 48.36
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 8.42s
                        Total time: 23243.88s
                               ETA: 958763.5s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.241s, learning 0.220s)
               Value function loss: 0.4377
                    Surrogate loss: -0.0139
             Mean action noise std: 0.70
                       Mean reward: 2.38
               Mean episode length: 46.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 8.46s
                        Total time: 23252.34s
                               ETA: 958697.6s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.546s, learning 0.174s)
               Value function loss: 0.2995
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 2.66
               Mean episode length: 48.50
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 8.72s
                        Total time: 23261.06s
                               ETA: 958642.5s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.371s, learning 0.189s)
               Value function loss: 0.1856
                    Surrogate loss: -0.0090
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 47.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 8.56s
                        Total time: 23269.62s
                               ETA: 958580.9s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.166s, learning 0.170s)
               Value function loss: 0.1942
                    Surrogate loss: -0.0132
             Mean action noise std: 0.70
                       Mean reward: 2.55
               Mean episode length: 47.64
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 8.34s
                        Total time: 23277.96s
                               ETA: 958510.0s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.118s, learning 0.167s)
               Value function loss: 0.1655
                    Surrogate loss: -0.0098
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 49.39
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 8.29s
                        Total time: 23286.25s
                               ETA: 958437.2s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.369s, learning 0.221s)
               Value function loss: 0.1745
                    Surrogate loss: -0.0116
             Mean action noise std: 0.70
                       Mean reward: 2.56
               Mean episode length: 48.72
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 8.59s
                        Total time: 23294.84s
                               ETA: 958376.9s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.349s, learning 0.210s)
               Value function loss: 0.1879
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 2.54
               Mean episode length: 48.84
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 8.56s
                        Total time: 23303.40s
                               ETA: 958315.4s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.179s, learning 0.171s)
               Value function loss: 0.1354
                    Surrogate loss: -0.0149
             Mean action noise std: 0.70
                       Mean reward: 2.34
               Mean episode length: 49.19
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 8.35s
                        Total time: 23311.75s
                               ETA: 958245.3s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.226s, learning 0.173s)
               Value function loss: 0.1477
                    Surrogate loss: -0.0060
             Mean action noise std: 0.70
                       Mean reward: 2.79
               Mean episode length: 49.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 8.40s
                        Total time: 23320.15s
                               ETA: 958177.3s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.351s, learning 0.169s)
               Value function loss: 0.1364
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 2.29
               Mean episode length: 49.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 8.52s
                        Total time: 23328.67s
                               ETA: 958114.3s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.301s, learning 0.183s)
               Value function loss: 0.1315
                    Surrogate loss: -0.0068
             Mean action noise std: 0.70
                       Mean reward: 2.53
               Mean episode length: 48.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 8.48s
                        Total time: 23337.15s
                               ETA: 958049.8s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.131s, learning 0.228s)
               Value function loss: 0.1232
                    Surrogate loss: -0.0065
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 49.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 8.36s
                        Total time: 23345.51s
                               ETA: 957980.3s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.524s, learning 0.164s)
               Value function loss: 302.7359
                    Surrogate loss: 0.0002
             Mean action noise std: 0.70
                       Mean reward: 3.06
               Mean episode length: 52.02
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 8.69s
                        Total time: 23354.20s
                               ETA: 957924.4s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.270s, learning 0.207s)
               Value function loss: 4.0028
                    Surrogate loss: -0.0017
             Mean action noise std: 0.70
                       Mean reward: 17.58
               Mean episode length: 50.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 8.48s
                        Total time: 23362.67s
                               ETA: 957859.8s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.341s, learning 0.208s)
               Value function loss: 0.2064
                    Surrogate loss: -0.0111
             Mean action noise std: 0.70
                       Mean reward: 2.91
               Mean episode length: 51.31
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 8.55s
                        Total time: 23371.22s
                               ETA: 957798.2s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.244s, learning 0.224s)
               Value function loss: 0.1668
                    Surrogate loss: -0.0124
             Mean action noise std: 0.70
                       Mean reward: 2.74
               Mean episode length: 50.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 8.47s
                        Total time: 23379.69s
                               ETA: 957733.4s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.242s, learning 0.176s)
               Value function loss: 0.1445
                    Surrogate loss: 0.0567
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 48.49
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 8.42s
                        Total time: 23388.11s
                               ETA: 957666.5s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.407s, learning 0.214s)
               Value function loss: 0.1905
                    Surrogate loss: -0.0107
             Mean action noise std: 0.70
                       Mean reward: 2.63
               Mean episode length: 47.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 8.62s
                        Total time: 23396.73s
                               ETA: 957608.0s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.315s, learning 0.160s)
               Value function loss: 0.1263
                    Surrogate loss: -0.0117
             Mean action noise std: 0.70
                       Mean reward: 2.13
               Mean episode length: 48.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 8.47s
                        Total time: 23405.20s
                               ETA: 957543.6s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.183s, learning 0.171s)
               Value function loss: 13.9805
                    Surrogate loss: 0.0012
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 49.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 8.35s
                        Total time: 23413.56s
                               ETA: 957474.3s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.354s, learning 0.173s)
               Value function loss: 0.1148
                    Surrogate loss: -0.0173
             Mean action noise std: 0.70
                       Mean reward: 2.64
               Mean episode length: 49.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 8.53s
                        Total time: 23422.09s
                               ETA: 957412.1s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.891s, learning 0.162s)
               Value function loss: 0.1356
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 2.36
               Mean episode length: 48.61
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 8.05s
                        Total time: 23430.14s
                               ETA: 957330.5s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.264s, learning 0.207s)
               Value function loss: 0.1317
                    Surrogate loss: 0.0051
             Mean action noise std: 0.70
                       Mean reward: 3.01
               Mean episode length: 50.68
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 8.47s
                        Total time: 23438.61s
                               ETA: 957266.2s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.187s, learning 0.165s)
               Value function loss: 0.0968
                    Surrogate loss: -0.0086
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 49.80
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 8.35s
                        Total time: 23446.96s
                               ETA: 957197.0s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.251s, learning 0.171s)
               Value function loss: 0.1104
                    Surrogate loss: -0.0237
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 50.98
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 8.42s
                        Total time: 23455.38s
                               ETA: 957130.7s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.283s, learning 0.167s)
               Value function loss: 0.0905
                    Surrogate loss: -0.0221
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 50.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 8.45s
                        Total time: 23463.83s
                               ETA: 957065.6s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 2012 steps/s (collection: 7.979s, learning 0.164s)
               Value function loss: 0.0899
                    Surrogate loss: -0.0254
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 48.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 8.14s
                        Total time: 23471.98s
                               ETA: 956988.0s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.324s, learning 0.167s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0169
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 48.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 8.49s
                        Total time: 23480.47s
                               ETA: 956924.7s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.137s, learning 0.166s)
               Value function loss: 0.0767
                    Surrogate loss: -0.0217
             Mean action noise std: 0.70
                       Mean reward: 2.39
               Mean episode length: 50.13
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 8.30s
                        Total time: 23488.77s
                               ETA: 956853.8s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.257s, learning 0.160s)
               Value function loss: 0.1086
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 2.06
               Mean episode length: 47.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 8.42s
                        Total time: 23497.19s
                               ETA: 956787.5s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.936s, learning 0.187s)
               Value function loss: 0.0957
                    Surrogate loss: -0.0165
             Mean action noise std: 0.70
                       Mean reward: 2.32
               Mean episode length: 49.01
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 8.12s
                        Total time: 23505.31s
                               ETA: 956709.3s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.176s, learning 0.170s)
               Value function loss: 0.0987
                    Surrogate loss: -0.0156
             Mean action noise std: 0.70
                       Mean reward: 2.84
               Mean episode length: 50.82
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 8.35s
                        Total time: 23513.66s
                               ETA: 956640.3s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.299s, learning 0.172s)
               Value function loss: 0.0896
                    Surrogate loss: -0.0180
             Mean action noise std: 0.70
                       Mean reward: 2.73
               Mean episode length: 51.38
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 8.47s
                        Total time: 23522.13s
                               ETA: 956576.4s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.616s, learning 0.169s)
               Value function loss: 0.0820
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 2.27
               Mean episode length: 48.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 8.79s
                        Total time: 23530.91s
                               ETA: 956525.3s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.295s, learning 0.163s)
               Value function loss: 0.0791
                    Surrogate loss: -0.0208
             Mean action noise std: 0.70
                       Mean reward: 3.02
               Mean episode length: 51.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 8.46s
                        Total time: 23539.37s
                               ETA: 956460.9s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.291s, learning 0.170s)
               Value function loss: 0.0755
                    Surrogate loss: -0.0233
             Mean action noise std: 0.70
                       Mean reward: 2.57
               Mean episode length: 50.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 8.46s
                        Total time: 23547.83s
                               ETA: 956396.8s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.340s, learning 0.161s)
               Value function loss: 3.9186
                    Surrogate loss: 0.0025
             Mean action noise std: 0.70
                       Mean reward: 5.28
               Mean episode length: 50.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 8.50s
                        Total time: 23556.33s
                               ETA: 956334.3s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.367s, learning 0.195s)
               Value function loss: 0.0772
                    Surrogate loss: -0.0239
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 49.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 8.56s
                        Total time: 23564.90s
                               ETA: 956274.2s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.546s, learning 0.161s)
               Value function loss: 0.0835
                    Surrogate loss: -0.0286
             Mean action noise std: 0.70
                       Mean reward: 2.50
               Mean episode length: 49.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 8.71s
                        Total time: 23573.60s
                               ETA: 956220.2s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.230s, learning 0.178s)
               Value function loss: 0.0706
                    Surrogate loss: -0.0270
             Mean action noise std: 0.70
                       Mean reward: 2.51
               Mean episode length: 49.47
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 8.41s
                        Total time: 23582.01s
                               ETA: 956154.0s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.299s, learning 0.179s)
               Value function loss: 0.0600
                    Surrogate loss: -0.0244
             Mean action noise std: 0.70
                       Mean reward: 2.55
               Mean episode length: 49.17
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 8.48s
                        Total time: 23590.49s
                               ETA: 956090.8s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.161s, learning 0.175s)
               Value function loss: 0.0801
                    Surrogate loss: -0.0280
             Mean action noise std: 0.70
                       Mean reward: 1.99
               Mean episode length: 48.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 8.34s
                        Total time: 23598.83s
                               ETA: 956021.8s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.388s, learning 0.158s)
               Value function loss: 0.0650
                    Surrogate loss: -0.0229
             Mean action noise std: 0.70
                       Mean reward: 2.22
               Mean episode length: 48.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 8.55s
                        Total time: 23607.37s
                               ETA: 955961.4s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.594s, learning 0.164s)
               Value function loss: 0.0771
                    Surrogate loss: -0.0220
             Mean action noise std: 0.70
                       Mean reward: 2.30
               Mean episode length: 49.18
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 8.76s
                        Total time: 23616.13s
                               ETA: 955909.6s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.274s, learning 0.161s)
               Value function loss: 0.0683
                    Surrogate loss: -0.0282
             Mean action noise std: 0.70
                       Mean reward: 2.80
               Mean episode length: 50.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 8.44s
                        Total time: 23624.57s
                               ETA: 955844.8s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.028s, learning 0.178s)
               Value function loss: 0.0779
                    Surrogate loss: -0.0254
             Mean action noise std: 0.70
                       Mean reward: 2.24
               Mean episode length: 49.02
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 8.21s
                        Total time: 23632.77s
                               ETA: 955770.8s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.207s, learning 0.174s)
               Value function loss: 0.0625
                    Surrogate loss: -0.0285
             Mean action noise std: 0.70
                       Mean reward: 2.35
               Mean episode length: 49.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 8.38s
                        Total time: 23641.15s
                               ETA: 955703.9s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.179s, learning 0.163s)
               Value function loss: 0.0655
                    Surrogate loss: -0.0242
             Mean action noise std: 0.70
                       Mean reward: 2.33
               Mean episode length: 49.83
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 8.34s
                        Total time: 23649.49s
                               ETA: 955635.4s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.164s)
               Value function loss: 0.0624
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 49.53
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 8.57s
                        Total time: 23658.06s
                               ETA: 955576.2s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.020s, learning 0.172s)
               Value function loss: 0.0881
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.15
               Mean episode length: 48.07
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 8.19s
                        Total time: 23666.26s
                               ETA: 955501.8s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.403s, learning 0.172s)
               Value function loss: 0.0933
                    Surrogate loss: -0.0223
             Mean action noise std: 0.70
                       Mean reward: 2.23
               Mean episode length: 48.12
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 8.57s
                        Total time: 23674.83s
                               ETA: 955442.9s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.386s, learning 0.212s)
               Value function loss: 0.0921
                    Surrogate loss: -0.0219
             Mean action noise std: 0.70
                       Mean reward: 2.55
               Mean episode length: 49.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 8.60s
                        Total time: 23683.43s
                               ETA: 955385.0s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.459s, learning 0.162s)
               Value function loss: 0.0709
                    Surrogate loss: -0.0171
             Mean action noise std: 0.70
                       Mean reward: 2.31
               Mean episode length: 49.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 8.62s
                        Total time: 23692.05s
                               ETA: 955328.0s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.239s, learning 0.161s)
               Value function loss: 0.0995
                    Surrogate loss: -0.0221
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 51.32
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 8.40s
                        Total time: 23700.45s
                               ETA: 955262.1s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.006s, learning 0.169s)
               Value function loss: 0.1041
                    Surrogate loss: -0.0246
             Mean action noise std: 0.70
                       Mean reward: 2.79
               Mean episode length: 50.58
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 8.18s
                        Total time: 23708.62s
                               ETA: 955187.3s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.183s, learning 0.169s)
               Value function loss: 0.1022
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 2.58
               Mean episode length: 49.45
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 8.35s
                        Total time: 23716.97s
                               ETA: 955119.7s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.423s, learning 0.165s)
               Value function loss: 0.1037
                    Surrogate loss: -0.0265
             Mean action noise std: 0.70
                       Mean reward: 3.08
               Mean episode length: 49.44
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 8.59s
                        Total time: 23725.56s
                               ETA: 955061.6s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.269s, learning 0.213s)
               Value function loss: 0.0874
                    Surrogate loss: -0.0218
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 49.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 8.48s
                        Total time: 23734.05s
                               ETA: 954999.3s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.174s, learning 0.172s)
               Value function loss: 0.0906
                    Surrogate loss: -0.0180
             Mean action noise std: 0.70
                       Mean reward: 2.78
               Mean episode length: 50.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 8.35s
                        Total time: 23742.39s
                               ETA: 954931.5s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.328s, learning 0.168s)
               Value function loss: 0.0816
                    Surrogate loss: -0.0265
             Mean action noise std: 0.70
                       Mean reward: 2.94
               Mean episode length: 50.35
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 8.50s
                        Total time: 23750.89s
                               ETA: 954869.8s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.436s, learning 0.165s)
               Value function loss: 0.0792
                    Surrogate loss: -0.0162
             Mean action noise std: 0.70
                       Mean reward: 2.59
               Mean episode length: 50.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 8.60s
                        Total time: 23759.49s
                               ETA: 954812.4s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.440s, learning 0.214s)
               Value function loss: 0.0784
                    Surrogate loss: -0.0200
             Mean action noise std: 0.70
                       Mean reward: 2.79
               Mean episode length: 50.55
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 8.65s
                        Total time: 23768.14s
                               ETA: 954757.2s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.159s, learning 0.185s)
               Value function loss: 0.1076
                    Surrogate loss: -0.0233
             Mean action noise std: 0.70
                       Mean reward: 2.54
               Mean episode length: 49.28
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 8.34s
                        Total time: 23776.49s
                               ETA: 954689.5s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.095s, learning 0.159s)
               Value function loss: 0.0886
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 2.44
               Mean episode length: 49.16
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 8.25s
                        Total time: 23784.74s
                               ETA: 954618.4s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.171s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0224
             Mean action noise std: 0.70
                       Mean reward: 2.21
               Mean episode length: 48.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 8.48s
                        Total time: 23793.23s
                               ETA: 954556.4s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.367s, learning 0.161s)
               Value function loss: 0.0879
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 48.34
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 8.53s
                        Total time: 23801.75s
                               ETA: 954496.3s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.380s, learning 0.165s)
               Value function loss: 0.0849
                    Surrogate loss: -0.0242
             Mean action noise std: 0.70
                       Mean reward: 2.98
               Mean episode length: 50.33
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 8.54s
                        Total time: 23810.30s
                               ETA: 954436.9s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.162s, learning 0.164s)
               Value function loss: 0.0812
                    Surrogate loss: -0.0171
             Mean action noise std: 0.70
                       Mean reward: 2.92
               Mean episode length: 50.77
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 8.33s
                        Total time: 23818.62s
                               ETA: 954368.8s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.040s, learning 0.170s)
               Value function loss: 0.0671
                    Surrogate loss: -0.0247
             Mean action noise std: 0.70
                       Mean reward: 2.80
               Mean episode length: 49.56
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 8.21s
                        Total time: 23826.83s
                               ETA: 954296.0s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.346s, learning 0.170s)
               Value function loss: 0.0730
                    Surrogate loss: -0.0252
             Mean action noise std: 0.70
                       Mean reward: 2.82
               Mean episode length: 49.73
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 8.52s
                        Total time: 23835.35s
                               ETA: 954235.6s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1862 steps/s (collection: 8.619s, learning 0.176s)
               Value function loss: 0.0833
                    Surrogate loss: -0.0255
             Mean action noise std: 0.70
                       Mean reward: 2.76
               Mean episode length: 50.22
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 8.80s
                        Total time: 23844.14s
                               ETA: 954186.3s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.244s, learning 0.173s)
               Value function loss: 0.0867
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 2.85
               Mean episode length: 49.86
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 8.42s
                        Total time: 23852.56s
                               ETA: 954122.0s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.126s, learning 0.167s)
               Value function loss: 0.0962
                    Surrogate loss: -0.0153
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 49.06
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 8.29s
                        Total time: 23860.85s
                               ETA: 954052.8s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.167s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0255
             Mean action noise std: 0.70
                       Mean reward: 2.52
               Mean episode length: 49.79
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 8.29s
                        Total time: 23869.14s
                               ETA: 953983.5s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.442s, learning 0.209s)
               Value function loss: 0.0932
                    Surrogate loss: -0.0212
             Mean action noise std: 0.70
                       Mean reward: 3.25
               Mean episode length: 51.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 8.65s
                        Total time: 23877.80s
                               ETA: 953928.7s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.228s, learning 0.164s)
               Value function loss: 0.1008
                    Surrogate loss: -0.0238
             Mean action noise std: 0.70
                       Mean reward: 2.47
               Mean episode length: 48.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 8.39s
                        Total time: 23886.19s
                               ETA: 953863.6s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.288s, learning 0.164s)
               Value function loss: 0.0980
                    Surrogate loss: -0.0270
             Mean action noise std: 0.70
                       Mean reward: 3.29
               Mean episode length: 50.47
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 8.45s
                        Total time: 23894.64s
                               ETA: 953800.9s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.274s, learning 0.170s)
               Value function loss: 0.1005
                    Surrogate loss: -0.0225
             Mean action noise std: 0.70
                       Mean reward: 2.85
               Mean episode length: 49.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 8.44s
                        Total time: 23903.08s
                               ETA: 953737.9s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.396s, learning 0.166s)
               Value function loss: 0.1045
                    Surrogate loss: -0.0247
             Mean action noise std: 0.70
                       Mean reward: 2.64
               Mean episode length: 47.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 8.56s
                        Total time: 23911.65s
                               ETA: 953679.7s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.216s, learning 0.171s)
               Value function loss: 0.1041
                    Surrogate loss: -0.0142
             Mean action noise std: 0.70
                       Mean reward: 2.57
               Mean episode length: 47.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 8.39s
                        Total time: 23920.03s
                               ETA: 953614.6s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.104s, learning 0.175s)
               Value function loss: 0.0993
                    Surrogate loss: -0.0216
             Mean action noise std: 0.70
                       Mean reward: 2.81
               Mean episode length: 50.11
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 8.28s
                        Total time: 23928.31s
                               ETA: 953545.2s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.120s, learning 0.172s)
               Value function loss: 0.0977
                    Surrogate loss: -0.0219
             Mean action noise std: 0.70
                       Mean reward: 2.87
               Mean episode length: 50.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 8.29s
                        Total time: 23936.60s
                               ETA: 953476.4s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.253s, learning 0.232s)
               Value function loss: 0.0997
                    Surrogate loss: -0.0141
             Mean action noise std: 0.70
                       Mean reward: 2.81
               Mean episode length: 48.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 8.48s
                        Total time: 23945.09s
                               ETA: 953415.3s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.366s, learning 0.167s)
               Value function loss: 0.1181
                    Surrogate loss: -0.0016
             Mean action noise std: 0.70
                       Mean reward: 2.99
               Mean episode length: 49.27
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 8.53s
                        Total time: 23953.62s
                               ETA: 953356.1s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.362s, learning 0.177s)
               Value function loss: 0.1314
                    Surrogate loss: -0.0174
             Mean action noise std: 0.70
                       Mean reward: 2.77
               Mean episode length: 48.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 8.54s
                        Total time: 23962.16s
                               ETA: 953297.2s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.386s, learning 0.162s)
               Value function loss: 0.0978
                    Surrogate loss: -0.0131
             Mean action noise std: 0.70
                       Mean reward: 3.23
               Mean episode length: 50.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 8.55s
                        Total time: 23970.71s
                               ETA: 953238.7s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1986 steps/s (collection: 7.992s, learning 0.256s)
               Value function loss: 0.1113
                    Surrogate loss: -0.0144
             Mean action noise std: 0.70
                       Mean reward: 2.46
               Mean episode length: 48.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 8.25s
                        Total time: 23978.96s
                               ETA: 953168.4s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.172s)
               Value function loss: 0.1224
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 2.94
               Mean episode length: 49.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 8.51s
                        Total time: 23987.47s
                               ETA: 953108.6s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.106s, learning 0.164s)
               Value function loss: 0.1133
                    Surrogate loss: -0.0039
             Mean action noise std: 0.70
                       Mean reward: 2.64
               Mean episode length: 49.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 8.27s
                        Total time: 23995.74s
                               ETA: 953039.2s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.165s)
               Value function loss: 0.1092
                    Surrogate loss: -0.0170
             Mean action noise std: 0.70
                       Mean reward: 3.07
               Mean episode length: 50.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 8.29s
                        Total time: 24004.03s
                               ETA: 952970.6s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1051 steps/s (collection: 15.419s, learning 0.166s)
               Value function loss: 0.1112
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 2.90
               Mean episode length: 48.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 15.58s
                        Total time: 24019.61s
                               ETA: 953191.6s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.950s, learning 0.234s)
               Value function loss: 0.1194
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 3.33
               Mean episode length: 50.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 16.18s
                        Total time: 24035.79s
                               ETA: 953436.2s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.923s, learning 0.206s)
               Value function loss: 0.1315
                    Surrogate loss: -0.0199
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 49.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 16.13s
                        Total time: 24051.92s
                               ETA: 953678.3s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.043s, learning 0.162s)
               Value function loss: 0.0971
                    Surrogate loss: -0.0215
             Mean action noise std: 0.70
                       Mean reward: 2.86
               Mean episode length: 49.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 16.21s
                        Total time: 24068.13s
                               ETA: 953923.3s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.401s, learning 0.163s)
               Value function loss: 0.0933
                    Surrogate loss: -0.0196
             Mean action noise std: 0.70
                       Mean reward: 2.72
               Mean episode length: 49.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 16.56s
                        Total time: 24084.69s
                               ETA: 954182.3s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.384s, learning 0.169s)
               Value function loss: 0.0908
                    Surrogate loss: -0.0248
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 48.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 16.55s
                        Total time: 24101.25s
                               ETA: 954440.6s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.044s, learning 0.167s)
               Value function loss: 0.1030
                    Surrogate loss: -0.0247
             Mean action noise std: 0.70
                       Mean reward: 2.62
               Mean episode length: 48.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 16.21s
                        Total time: 24117.46s
                               ETA: 954685.2s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.322s, learning 0.163s)
               Value function loss: 0.1200
                    Surrogate loss: -0.0196
             Mean action noise std: 0.70
                       Mean reward: 2.67
               Mean episode length: 48.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 16.48s
                        Total time: 24133.94s
                               ETA: 954940.4s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.136s, learning 0.258s)
               Value function loss: 0.1160
                    Surrogate loss: -0.0195
             Mean action noise std: 0.70
                       Mean reward: 2.64
               Mean episode length: 48.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 16.39s
                        Total time: 24150.33s
                               ETA: 955191.8s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.319s, learning 0.165s)
               Value function loss: 0.1058
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 2.89
               Mean episode length: 48.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 16.48s
                        Total time: 24166.82s
                               ETA: 955446.5s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.120s, learning 0.195s)
               Value function loss: 0.0999
                    Surrogate loss: -0.0182
             Mean action noise std: 0.70
                       Mean reward: 2.73
               Mean episode length: 48.54
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 16.31s
                        Total time: 24183.13s
                               ETA: 955694.3s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.356s, learning 0.171s)
               Value function loss: 0.0854
                    Surrogate loss: -0.0165
             Mean action noise std: 0.70
                       Mean reward: 3.17
               Mean episode length: 51.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 16.53s
                        Total time: 24199.66s
                               ETA: 955950.3s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.029s, learning 0.164s)
               Value function loss: 0.0924
                    Surrogate loss: -0.0189
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 49.41
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 16.19s
                        Total time: 24215.85s
                               ETA: 956192.9s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.142s, learning 0.163s)
               Value function loss: 0.1119
                    Surrogate loss: -0.0201
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 49.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 16.30s
                        Total time: 24232.16s
                               ETA: 956439.7s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.164s, learning 0.203s)
               Value function loss: 0.1077
                    Surrogate loss: -0.0168
             Mean action noise std: 0.70
                       Mean reward: 3.14
               Mean episode length: 50.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 16.37s
                        Total time: 24248.53s
                               ETA: 956688.7s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.401s, learning 0.168s)
               Value function loss: 0.1033
                    Surrogate loss: -0.0176
             Mean action noise std: 0.70
                       Mean reward: 2.60
               Mean episode length: 47.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 16.57s
                        Total time: 24265.09s
                               ETA: 956945.4s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.266s, learning 0.160s)
               Value function loss: 0.1137
                    Surrogate loss: -0.0245
             Mean action noise std: 0.70
                       Mean reward: 2.65
               Mean episode length: 49.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 16.43s
                        Total time: 24281.52s
                               ETA: 957196.4s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.672s, learning 0.165s)
               Value function loss: 0.1088
                    Surrogate loss: -0.0240
             Mean action noise std: 0.70
                       Mean reward: 2.78
               Mean episode length: 49.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 16.84s
                        Total time: 24298.36s
                               ETA: 957463.3s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.354s, learning 0.165s)
               Value function loss: 0.0920
                    Surrogate loss: -0.0195
             Mean action noise std: 0.70
                       Mean reward: 2.82
               Mean episode length: 49.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 16.52s
                        Total time: 24314.88s
                               ETA: 957717.4s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.268s, learning 0.164s)
               Value function loss: 0.1013
                    Surrogate loss: -0.0219
             Mean action noise std: 0.70
                       Mean reward: 3.06
               Mean episode length: 50.40
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 16.43s
                        Total time: 24331.31s
                               ETA: 957967.9s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.406s, learning 0.164s)
               Value function loss: 0.0928
                    Surrogate loss: -0.0251
             Mean action noise std: 0.70
                       Mean reward: 3.37
               Mean episode length: 49.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 16.57s
                        Total time: 24347.88s
                               ETA: 958223.6s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.062s, learning 0.169s)
               Value function loss: 0.1152
                    Surrogate loss: -0.0200
             Mean action noise std: 0.70
                       Mean reward: 2.86
               Mean episode length: 48.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 16.23s
                        Total time: 24364.11s
                               ETA: 958465.8s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.057s, learning 0.159s)
               Value function loss: 0.1051
                    Surrogate loss: -0.0223
             Mean action noise std: 0.70
                       Mean reward: 2.61
               Mean episode length: 48.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 16.22s
                        Total time: 24380.33s
                               ETA: 958707.1s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.839s, learning 0.162s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0255
             Mean action noise std: 0.70
                       Mean reward: 3.06
               Mean episode length: 50.07
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 16.00s
                        Total time: 24396.33s
                               ETA: 958939.8s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.244s, learning 0.271s)
               Value function loss: 0.1146
                    Surrogate loss: -0.0164
             Mean action noise std: 0.70
                       Mean reward: 2.83
               Mean episode length: 49.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 16.52s
                        Total time: 24412.84s
                               ETA: 959192.6s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.064s, learning 0.162s)
               Value function loss: 0.1047
                    Surrogate loss: -0.0173
             Mean action noise std: 0.70
                       Mean reward: 3.40
               Mean episode length: 51.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 16.23s
                        Total time: 24429.07s
                               ETA: 959433.7s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.138s, learning 0.214s)
               Value function loss: 0.1083
                    Surrogate loss: -0.0143
             Mean action noise std: 0.70
                       Mean reward: 3.11
               Mean episode length: 50.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 16.35s
                        Total time: 24445.42s
                               ETA: 959679.6s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.039s, learning 0.165s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0250
             Mean action noise std: 0.70
                       Mean reward: 3.10
               Mean episode length: 50.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 16.20s
                        Total time: 24461.62s
                               ETA: 959919.4s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.532s, learning 0.161s)
               Value function loss: 0.1156
                    Surrogate loss: -0.0198
             Mean action noise std: 0.70
                       Mean reward: 3.16
               Mean episode length: 50.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 16.69s
                        Total time: 24478.32s
                               ETA: 960178.2s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.064s, learning 0.162s)
               Value function loss: 0.1234
                    Surrogate loss: -0.0190
             Mean action noise std: 0.70
                       Mean reward: 2.87
               Mean episode length: 50.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 16.23s
                        Total time: 24494.54s
                               ETA: 960418.5s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.388s, learning 0.174s)
               Value function loss: 0.1281
                    Surrogate loss: -0.0231
             Mean action noise std: 0.70
                       Mean reward: 2.89
               Mean episode length: 49.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 16.56s
                        Total time: 24511.11s
                               ETA: 960671.8s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.103s, learning 0.186s)
               Value function loss: 0.1072
                    Surrogate loss: -0.0182
             Mean action noise std: 0.70
                       Mean reward: 2.92
               Mean episode length: 50.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 16.29s
                        Total time: 24527.40s
                               ETA: 960914.2s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.454s, learning 0.162s)
               Value function loss: 0.1390
                    Surrogate loss: -0.0157
             Mean action noise std: 0.70
                       Mean reward: 3.40
               Mean episode length: 51.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 16.62s
                        Total time: 24544.01s
                               ETA: 961169.1s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.959s, learning 0.166s)
               Value function loss: 0.1240
                    Surrogate loss: -0.0186
             Mean action noise std: 0.70
                       Mean reward: 2.71
               Mean episode length: 49.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 16.13s
                        Total time: 24560.14s
                               ETA: 961404.6s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.984s, learning 0.161s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0201
             Mean action noise std: 0.70
                       Mean reward: 3.20
               Mean episode length: 52.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 16.15s
                        Total time: 24576.28s
                               ETA: 961640.7s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.136s, learning 0.170s)
               Value function loss: 0.1214
                    Surrogate loss: -0.0204
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 49.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 16.31s
                        Total time: 24592.59s
                               ETA: 961882.9s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.181s, learning 0.161s)
               Value function loss: 0.1099
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 3.10
               Mean episode length: 50.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 16.34s
                        Total time: 24608.93s
                               ETA: 962126.3s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1116 steps/s (collection: 14.482s, learning 0.187s)
               Value function loss: 0.1413
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 2.88
               Mean episode length: 50.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 14.67s
                        Total time: 24623.60s
                               ETA: 962304.0s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.240s, learning 0.208s)
               Value function loss: 0.1457
                    Surrogate loss: -0.0196
             Mean action noise std: 0.70
                       Mean reward: 3.37
               Mean episode length: 51.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 8.45s
                        Total time: 24632.05s
                               ETA: 962238.6s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.187s, learning 0.161s)
               Value function loss: 0.1153
                    Surrogate loss: -0.0178
             Mean action noise std: 0.70
                       Mean reward: 3.10
               Mean episode length: 51.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 8.35s
                        Total time: 24640.39s
                               ETA: 962169.4s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.414s, learning 0.175s)
               Value function loss: 0.1074
                    Surrogate loss: -0.0247
             Mean action noise std: 0.70
                       Mean reward: 3.11
               Mean episode length: 49.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 8.59s
                        Total time: 24648.98s
                               ETA: 962109.6s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.332s, learning 0.171s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0242
             Mean action noise std: 0.70
                       Mean reward: 3.65
               Mean episode length: 51.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 8.50s
                        Total time: 24657.49s
                               ETA: 962046.5s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.319s, learning 0.199s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0230
             Mean action noise std: 0.70
                       Mean reward: 3.23
               Mean episode length: 50.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 8.52s
                        Total time: 24666.00s
                               ETA: 961984.0s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.438s, learning 0.248s)
               Value function loss: 0.1125
                    Surrogate loss: -0.0161
             Mean action noise std: 0.70
                       Mean reward: 2.90
               Mean episode length: 51.72
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 8.69s
                        Total time: 24674.69s
                               ETA: 961928.1s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.469s, learning 0.162s)
               Value function loss: 0.1283
                    Surrogate loss: -0.0224
             Mean action noise std: 0.70
                       Mean reward: 3.28
               Mean episode length: 51.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 8.63s
                        Total time: 24683.32s
                               ETA: 961870.1s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.236s, learning 0.160s)
               Value function loss: 0.1128
                    Surrogate loss: -0.0256
             Mean action noise std: 0.70
                       Mean reward: 2.75
               Mean episode length: 49.75
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 8.40s
                        Total time: 24691.72s
                               ETA: 961803.1s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.165s)
               Value function loss: 0.1048
                    Surrogate loss: -0.0219
             Mean action noise std: 0.70
                       Mean reward: 3.18
               Mean episode length: 51.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 8.45s
                        Total time: 24700.17s
                               ETA: 961738.1s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.078s, learning 0.161s)
               Value function loss: 0.1089
                    Surrogate loss: -0.0259
             Mean action noise std: 0.70
                       Mean reward: 3.21
               Mean episode length: 51.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 8.24s
                        Total time: 24708.41s
                               ETA: 961665.0s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.175s, learning 0.161s)
               Value function loss: 0.1198
                    Surrogate loss: -0.0237
             Mean action noise std: 0.70
                       Mean reward: 3.33
               Mean episode length: 51.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 8.34s
                        Total time: 24716.74s
                               ETA: 961595.7s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.340s, learning 0.161s)
               Value function loss: 0.1195
                    Surrogate loss: -0.0233
             Mean action noise std: 0.70
                       Mean reward: 2.84
               Mean episode length: 50.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 8.50s
                        Total time: 24725.24s
                               ETA: 961532.9s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.151s, learning 0.162s)
               Value function loss: 0.1254
                    Surrogate loss: -0.0223
             Mean action noise std: 0.70
                       Mean reward: 3.00
               Mean episode length: 50.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 8.31s
                        Total time: 24733.56s
                               ETA: 961462.8s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.163s)
               Value function loss: 0.1272
                    Surrogate loss: -0.0229
             Mean action noise std: 0.70
                       Mean reward: 3.07
               Mean episode length: 50.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 8.25s
                        Total time: 24741.80s
                               ETA: 961390.2s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.219s, learning 0.161s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0202
             Mean action noise std: 0.70
                       Mean reward: 2.91
               Mean episode length: 50.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 8.38s
                        Total time: 24750.18s
                               ETA: 961322.8s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.160s)
               Value function loss: 0.1137
                    Surrogate loss: -0.0248
             Mean action noise std: 0.70
                       Mean reward: 2.84
               Mean episode length: 49.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 8.47s
                        Total time: 24758.65s
                               ETA: 961258.9s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.267s, learning 0.205s)
               Value function loss: 0.1277
                    Surrogate loss: -0.0197
             Mean action noise std: 0.70
                       Mean reward: 2.99
               Mean episode length: 50.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 8.47s
                        Total time: 24767.13s
                               ETA: 961195.2s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.242s, learning 0.210s)
               Value function loss: 0.1368
                    Surrogate loss: -0.0164
             Mean action noise std: 0.70
                       Mean reward: 3.13
               Mean episode length: 50.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 8.45s
                        Total time: 24775.58s
                               ETA: 961130.7s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.249s, learning 0.162s)
               Value function loss: 0.1150
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 3.30
               Mean episode length: 50.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 8.41s
                        Total time: 24783.99s
                               ETA: 961064.7s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.272s, learning 0.161s)
               Value function loss: 0.1268
                    Surrogate loss: -0.0121
             Mean action noise std: 0.70
                       Mean reward: 3.37
               Mean episode length: 50.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 8.43s
                        Total time: 24792.42s
                               ETA: 960999.5s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.372s, learning 0.170s)
               Value function loss: 0.1047
                    Surrogate loss: -0.0256
             Mean action noise std: 0.70
                       Mean reward: 3.32
               Mean episode length: 51.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 8.54s
                        Total time: 24800.96s
                               ETA: 960938.7s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.376s, learning 0.165s)
               Value function loss: 0.1170
                    Surrogate loss: -0.0180
             Mean action noise std: 0.70
                       Mean reward: 3.57
               Mean episode length: 52.21
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 8.54s
                        Total time: 24809.50s
                               ETA: 960877.8s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.157s, learning 0.170s)
               Value function loss: 0.1080
                    Surrogate loss: -0.0231
             Mean action noise std: 0.70
                       Mean reward: 3.39
               Mean episode length: 51.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 8.33s
                        Total time: 24817.83s
                               ETA: 960808.7s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.345s, learning 0.163s)
               Value function loss: 0.1247
                    Surrogate loss: -0.0233
             Mean action noise std: 0.70
                       Mean reward: 2.91
               Mean episode length: 50.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 8.51s
                        Total time: 24826.34s
                               ETA: 960746.7s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.224s, learning 0.165s)
               Value function loss: 0.1346
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 3.28
               Mean episode length: 50.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 8.39s
                        Total time: 24834.73s
                               ETA: 960680.1s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.182s, learning 0.172s)
               Value function loss: 0.1376
                    Surrogate loss: -0.0184
             Mean action noise std: 0.70
                       Mean reward: 2.83
               Mean episode length: 50.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 8.35s
                        Total time: 24843.08s
                               ETA: 960612.2s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.374s, learning 0.160s)
               Value function loss: 0.1145
                    Surrogate loss: -0.0214
             Mean action noise std: 0.70
                       Mean reward: 3.31
               Mean episode length: 50.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 8.53s
                        Total time: 24851.61s
                               ETA: 960551.3s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.377s, learning 0.168s)
               Value function loss: 0.1417
                    Surrogate loss: -0.0227
             Mean action noise std: 0.70
                       Mean reward: 3.06
               Mean episode length: 50.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 8.55s
                        Total time: 24860.16s
                               ETA: 960490.9s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.305s, learning 0.191s)
               Value function loss: 0.1317
                    Surrogate loss: -0.0157
             Mean action noise std: 0.70
                       Mean reward: 3.14
               Mean episode length: 50.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 8.50s
                        Total time: 24868.66s
                               ETA: 960428.6s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.082s, learning 0.195s)
               Value function loss: 0.1231
                    Surrogate loss: -0.0222
             Mean action noise std: 0.70
                       Mean reward: 3.79
               Mean episode length: 52.09
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 8.28s
                        Total time: 24876.93s
                               ETA: 960358.0s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.330s, learning 0.164s)
               Value function loss: 0.1437
                    Surrogate loss: -0.0211
             Mean action noise std: 0.70
                       Mean reward: 3.69
               Mean episode length: 52.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 8.49s
                        Total time: 24885.43s
                               ETA: 960295.7s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.308s, learning 0.174s)
               Value function loss: 0.1178
                    Surrogate loss: -0.0248
             Mean action noise std: 0.70
                       Mean reward: 3.34
               Mean episode length: 51.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 8.48s
                        Total time: 24893.91s
                               ETA: 960233.0s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.492s, learning 0.265s)
               Value function loss: 0.1220
                    Surrogate loss: -0.0254
             Mean action noise std: 0.70
                       Mean reward: 3.43
               Mean episode length: 51.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 8.76s
                        Total time: 24902.67s
                               ETA: 960181.0s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.402s, learning 0.171s)
               Value function loss: 0.1311
                    Surrogate loss: -0.0054
             Mean action noise std: 0.70
                       Mean reward: 3.33
               Mean episode length: 51.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 8.57s
                        Total time: 24911.24s
                               ETA: 960121.9s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.194s, learning 0.178s)
               Value function loss: 0.1228
                    Surrogate loss: -0.0212
             Mean action noise std: 0.70
                       Mean reward: 3.31
               Mean episode length: 50.91
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 8.37s
                        Total time: 24919.61s
                               ETA: 960055.1s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.018s, learning 0.161s)
               Value function loss: 0.1201
                    Surrogate loss: -0.0123
             Mean action noise std: 0.70
                       Mean reward: 4.18
               Mean episode length: 52.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 8.18s
                        Total time: 24927.79s
                               ETA: 959980.9s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.162s)
               Value function loss: 0.1151
                    Surrogate loss: -0.0241
             Mean action noise std: 0.70
                       Mean reward: 3.39
               Mean episode length: 51.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 8.47s
                        Total time: 24936.26s
                               ETA: 959917.8s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.168s)
               Value function loss: 0.1315
                    Surrogate loss: -0.0179
             Mean action noise std: 0.70
                       Mean reward: 3.43
               Mean episode length: 51.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 8.41s
                        Total time: 24944.66s
                               ETA: 959852.5s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.590s, learning 0.168s)
               Value function loss: 0.1180
                    Surrogate loss: -0.0179
             Mean action noise std: 0.70
                       Mean reward: 2.98
               Mean episode length: 50.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 8.76s
                        Total time: 24953.42s
                               ETA: 959800.8s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.101s, learning 0.165s)
               Value function loss: 0.1415
                    Surrogate loss: -0.0223
             Mean action noise std: 0.70
                       Mean reward: 3.28
               Mean episode length: 51.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 8.27s
                        Total time: 24961.69s
                               ETA: 959730.2s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.319s, learning 0.171s)
               Value function loss: 0.1331
                    Surrogate loss: -0.0236
             Mean action noise std: 0.70
                       Mean reward: 3.38
               Mean episode length: 51.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 8.49s
                        Total time: 24970.18s
                               ETA: 959668.1s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.441s, learning 0.198s)
               Value function loss: 0.1471
                    Surrogate loss: -0.0238
             Mean action noise std: 0.70
                       Mean reward: 2.95
               Mean episode length: 50.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 8.64s
                        Total time: 24978.82s
                               ETA: 959611.9s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.286s, learning 0.169s)
               Value function loss: 0.1423
                    Surrogate loss: -0.0082
             Mean action noise std: 0.70
                       Mean reward: 3.76
               Mean episode length: 51.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 8.46s
                        Total time: 24987.27s
                               ETA: 959548.7s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.163s)
               Value function loss: 0.1452
                    Surrogate loss: -0.0218
             Mean action noise std: 0.70
                       Mean reward: 3.73
               Mean episode length: 51.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 8.53s
                        Total time: 24995.81s
                               ETA: 959488.5s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.429s, learning 0.162s)
               Value function loss: 0.1357
                    Surrogate loss: -0.0210
             Mean action noise std: 0.70
                       Mean reward: 3.49
               Mean episode length: 51.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 8.59s
                        Total time: 25004.40s
                               ETA: 959430.5s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.135s, learning 0.173s)
               Value function loss: 0.1491
                    Surrogate loss: -0.0176
             Mean action noise std: 0.70
                       Mean reward: 3.15
               Mean episode length: 50.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 8.31s
                        Total time: 25012.71s
                               ETA: 959361.8s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.283s, learning 0.160s)
               Value function loss: 0.1310
                    Surrogate loss: -0.0261
             Mean action noise std: 0.70
                       Mean reward: 3.38
               Mean episode length: 50.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 8.44s
                        Total time: 25021.15s
                               ETA: 959298.2s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.455s, learning 0.172s)
               Value function loss: 0.1334
                    Surrogate loss: -0.0213
             Mean action noise std: 0.70
                       Mean reward: 3.45
               Mean episode length: 51.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 8.63s
                        Total time: 25029.78s
                               ETA: 959241.8s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.438s, learning 0.160s)
               Value function loss: 0.1431
                    Surrogate loss: -0.0131
             Mean action noise std: 0.70
                       Mean reward: 3.52
               Mean episode length: 52.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 8.60s
                        Total time: 25038.37s
                               ETA: 959184.3s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.473s, learning 0.173s)
               Value function loss: 0.1265
                    Surrogate loss: -0.0230
             Mean action noise std: 0.70
                       Mean reward: 4.04
               Mean episode length: 52.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 8.65s
                        Total time: 25047.02s
                               ETA: 959128.6s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.343s, learning 0.171s)
               Value function loss: 0.1224
                    Surrogate loss: -0.0222
             Mean action noise std: 0.70
                       Mean reward: 3.39
               Mean episode length: 50.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 8.51s
                        Total time: 25055.53s
                               ETA: 959068.0s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.542s, learning 0.170s)
               Value function loss: 0.1212
                    Surrogate loss: -0.0227
             Mean action noise std: 0.70
                       Mean reward: 3.76
               Mean episode length: 52.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 8.71s
                        Total time: 25064.25s
                               ETA: 959015.0s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.307s, learning 0.195s)
               Value function loss: 0.1246
                    Surrogate loss: -0.0228
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 51.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 8.50s
                        Total time: 25072.75s
                               ETA: 958953.9s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.501s, learning 0.201s)
               Value function loss: 0.1419
                    Surrogate loss: -0.0210
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 51.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 8.70s
                        Total time: 25081.45s
                               ETA: 958900.5s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.284s, learning 0.207s)
               Value function loss: 0.1428
                    Surrogate loss: -0.0173
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 51.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 8.49s
                        Total time: 25089.94s
                               ETA: 958839.2s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1869 steps/s (collection: 8.591s, learning 0.173s)
               Value function loss: 0.1412
                    Surrogate loss: -0.0239
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 52.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 8.76s
                        Total time: 25098.71s
                               ETA: 958788.3s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.028s, learning 0.157s)
               Value function loss: 0.1269
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 50.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 8.19s
                        Total time: 25106.89s
                               ETA: 958715.3s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.271s, learning 0.191s)
               Value function loss: 0.1230
                    Surrogate loss: -0.0244
             Mean action noise std: 0.69
                       Mean reward: 3.46
               Mean episode length: 51.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 8.46s
                        Total time: 25115.35s
                               ETA: 958653.0s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.354s, learning 0.212s)
               Value function loss: 0.1177
                    Surrogate loss: -0.0201
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 51.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 8.57s
                        Total time: 25123.92s
                               ETA: 958594.6s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.352s, learning 0.241s)
               Value function loss: 0.1271
                    Surrogate loss: -0.0253
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 51.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 8.59s
                        Total time: 25132.51s
                               ETA: 958537.3s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.280s, learning 0.172s)
               Value function loss: 0.1293
                    Surrogate loss: -0.0201
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 8.45s
                        Total time: 25140.97s
                               ETA: 958474.7s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.293s, learning 0.172s)
               Value function loss: 0.1500
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 51.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 8.47s
                        Total time: 25149.43s
                               ETA: 958412.6s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.218s, learning 0.214s)
               Value function loss: 17.5221
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 50.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 8.43s
                        Total time: 25157.86s
                               ETA: 958349.3s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.358s, learning 0.240s)
               Value function loss: 0.1873
                    Surrogate loss: -0.0232
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 51.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 8.60s
                        Total time: 25166.46s
                               ETA: 958292.4s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.168s)
               Value function loss: 0.1794
                    Surrogate loss: -0.0206
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 50.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 8.49s
                        Total time: 25174.95s
                               ETA: 958231.3s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.289s, learning 0.173s)
               Value function loss: 0.1733
                    Surrogate loss: -0.0199
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 51.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 8.46s
                        Total time: 25183.41s
                               ETA: 958169.3s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.450s, learning 0.163s)
               Value function loss: 0.1606
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 51.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 8.61s
                        Total time: 25192.02s
                               ETA: 958113.0s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.271s, learning 0.163s)
               Value function loss: 0.1726
                    Surrogate loss: -0.0178
             Mean action noise std: 0.69
                       Mean reward: 4.00
               Mean episode length: 53.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 8.43s
                        Total time: 25200.46s
                               ETA: 958050.0s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.494s, learning 0.164s)
               Value function loss: 0.1505
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 51.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 8.66s
                        Total time: 25209.12s
                               ETA: 957995.5s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.342s, learning 0.162s)
               Value function loss: 0.1407
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 52.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 8.50s
                        Total time: 25217.62s
                               ETA: 957935.3s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.562s, learning 0.214s)
               Value function loss: 0.1334
                    Surrogate loss: -0.0179
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 52.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 8.78s
                        Total time: 25226.39s
                               ETA: 957885.3s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.326s, learning 0.246s)
               Value function loss: 0.1339
                    Surrogate loss: -0.0128
             Mean action noise std: 0.69
                       Mean reward: 3.29
               Mean episode length: 50.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 8.57s
                        Total time: 25234.97s
                               ETA: 957827.7s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.555s, learning 0.174s)
               Value function loss: 0.1461
                    Surrogate loss: -0.0199
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 52.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 8.73s
                        Total time: 25243.70s
                               ETA: 957776.1s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.455s, learning 0.164s)
               Value function loss: 0.1393
                    Surrogate loss: -0.0246
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 50.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 8.62s
                        Total time: 25252.32s
                               ETA: 957720.4s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.167s, learning 0.172s)
               Value function loss: 0.1491
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 51.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 8.34s
                        Total time: 25260.65s
                               ETA: 957654.0s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.282s, learning 0.169s)
               Value function loss: 0.1882
                    Surrogate loss: -0.0237
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 51.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 8.45s
                        Total time: 25269.11s
                               ETA: 957592.0s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.537s, learning 0.216s)
               Value function loss: 0.1608
                    Surrogate loss: -0.0180
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 51.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 8.75s
                        Total time: 25277.86s
                               ETA: 957541.4s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.291s, learning 0.204s)
               Value function loss: 0.1512
                    Surrogate loss: -0.0190
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 50.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 8.50s
                        Total time: 25286.35s
                               ETA: 957481.1s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.309s, learning 0.173s)
               Value function loss: 0.1512
                    Surrogate loss: -0.0187
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 51.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 8.48s
                        Total time: 25294.84s
                               ETA: 957420.4s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.308s, learning 0.217s)
               Value function loss: 0.1383
                    Surrogate loss: -0.0236
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 50.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 8.53s
                        Total time: 25303.36s
                               ETA: 957361.3s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.152s, learning 0.166s)
               Value function loss: 0.1399
                    Surrogate loss: -0.0185
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 8.32s
                        Total time: 25311.68s
                               ETA: 957294.4s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.150s, learning 0.167s)
               Value function loss: 0.1252
                    Surrogate loss: -0.0245
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 50.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 8.32s
                        Total time: 25320.00s
                               ETA: 957227.6s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.400s, learning 0.162s)
               Value function loss: 0.1317
                    Surrogate loss: -0.0199
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 49.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 8.56s
                        Total time: 25328.56s
                               ETA: 957170.0s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.037s, learning 0.160s)
               Value function loss: 0.1413
                    Surrogate loss: -0.0208
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 51.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 8.20s
                        Total time: 25336.76s
                               ETA: 957098.7s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.240s, learning 0.170s)
               Value function loss: 0.1267
                    Surrogate loss: -0.0226
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 51.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 8.41s
                        Total time: 25345.17s
                               ETA: 957035.4s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.279s, learning 0.169s)
               Value function loss: 0.1230
                    Surrogate loss: -0.0230
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 50.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 8.45s
                        Total time: 25353.61s
                               ETA: 956973.7s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.045s, learning 0.265s)
               Value function loss: 0.1625
                    Surrogate loss: -0.0211
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 50.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 8.31s
                        Total time: 25361.92s
                               ETA: 956906.8s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.143s, learning 0.169s)
               Value function loss: 0.1419
                    Surrogate loss: -0.0246
             Mean action noise std: 0.69
                       Mean reward: 3.78
               Mean episode length: 51.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 8.31s
                        Total time: 25370.24s
                               ETA: 956840.0s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.319s, learning 0.217s)
               Value function loss: 0.1549
                    Surrogate loss: -0.0191
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 51.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 8.54s
                        Total time: 25378.77s
                               ETA: 956781.7s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.392s, learning 0.210s)
               Value function loss: 0.1432
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 50.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 8.60s
                        Total time: 25387.37s
                               ETA: 956725.9s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.211s, learning 0.205s)
               Value function loss: 0.1359
                    Surrogate loss: -0.0218
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 50.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 8.42s
                        Total time: 25395.79s
                               ETA: 956663.1s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.252s, learning 0.208s)
               Value function loss: 0.1404
                    Surrogate loss: -0.0244
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 51.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 8.46s
                        Total time: 25404.25s
                               ETA: 956602.1s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.310s, learning 0.211s)
               Value function loss: 0.1329
                    Surrogate loss: -0.0232
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 50.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 8.52s
                        Total time: 25412.77s
                               ETA: 956543.4s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.120s, learning 0.163s)
               Value function loss: 0.1312
                    Surrogate loss: -0.0255
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 50.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 8.28s
                        Total time: 25421.05s
                               ETA: 956475.8s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1880 steps/s (collection: 8.461s, learning 0.252s)
               Value function loss: 0.1284
                    Surrogate loss: -0.0177
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 50.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 8.71s
                        Total time: 25429.77s
                               ETA: 956424.3s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.170s)
               Value function loss: 0.1373
                    Surrogate loss: -0.0147
             Mean action noise std: 0.69
                       Mean reward: 2.95
               Mean episode length: 49.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 8.48s
                        Total time: 25438.25s
                               ETA: 956364.3s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.372s, learning 0.174s)
               Value function loss: 0.1517
                    Surrogate loss: -0.0143
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 51.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 8.55s
                        Total time: 25446.80s
                               ETA: 956306.7s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.066s, learning 0.162s)
               Value function loss: 0.1428
                    Surrogate loss: -0.0179
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 49.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 8.23s
                        Total time: 25455.02s
                               ETA: 956237.2s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.359s, learning 0.171s)
               Value function loss: 0.1490
                    Surrogate loss: -0.0233
             Mean action noise std: 0.69
                       Mean reward: 3.29
               Mean episode length: 50.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 8.53s
                        Total time: 25463.55s
                               ETA: 956179.0s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.018s, learning 0.171s)
               Value function loss: 0.1496
                    Surrogate loss: -0.0193
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 49.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 8.19s
                        Total time: 25471.74s
                               ETA: 956108.2s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.269s, learning 0.162s)
               Value function loss: 0.1608
                    Surrogate loss: -0.0147
             Mean action noise std: 0.69
                       Mean reward: 4.16
               Mean episode length: 52.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 8.43s
                        Total time: 25480.17s
                               ETA: 956046.4s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.209s, learning 0.168s)
               Value function loss: 0.1726
                    Surrogate loss: -0.0094
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 49.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 8.38s
                        Total time: 25488.55s
                               ETA: 955982.6s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.268s, learning 0.165s)
               Value function loss: 0.1616
                    Surrogate loss: -0.0192
             Mean action noise std: 0.69
                       Mean reward: 3.95
               Mean episode length: 51.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 8.43s
                        Total time: 25496.99s
                               ETA: 955921.0s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.984s, learning 0.163s)
               Value function loss: 0.1662
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 49.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 8.15s
                        Total time: 25505.13s
                               ETA: 955848.8s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.163s)
               Value function loss: 131.9941
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 4.25
               Mean episode length: 53.63
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 8.42s
                        Total time: 25513.55s
                               ETA: 955786.8s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.296s, learning 0.161s)
               Value function loss: 493.4570
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 51.54
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 8.46s
                        Total time: 25522.01s
                               ETA: 955726.2s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.144s, learning 0.159s)
               Value function loss: 6.2795
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 50.26
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 8.30s
                        Total time: 25530.31s
                               ETA: 955659.9s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.389s, learning 0.160s)
               Value function loss: 1.5104
                    Surrogate loss: -0.0160
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 50.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 8.55s
                        Total time: 25538.86s
                               ETA: 955602.8s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.221s, learning 0.165s)
               Value function loss: 0.8592
                    Surrogate loss: 0.0041
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 52.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 8.39s
                        Total time: 25547.25s
                               ETA: 955539.7s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.232s, learning 0.159s)
               Value function loss: 0.4532
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 52.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 8.39s
                        Total time: 25555.64s
                               ETA: 955476.8s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.158s, learning 0.162s)
               Value function loss: 0.3920
                    Surrogate loss: -0.0116
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 50.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 8.32s
                        Total time: 25563.96s
                               ETA: 955411.3s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.319s, learning 0.167s)
               Value function loss: 0.3174
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 51.59
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 8.49s
                        Total time: 25572.45s
                               ETA: 955352.0s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.162s)
               Value function loss: 0.3211
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 51.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 8.36s
                        Total time: 25580.81s
                               ETA: 955288.2s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.307s, learning 0.161s)
               Value function loss: 0.2989
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 50.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 8.47s
                        Total time: 25589.28s
                               ETA: 955228.4s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.280s, learning 0.207s)
               Value function loss: 0.2544
                    Surrogate loss: -0.0118
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 51.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 8.49s
                        Total time: 25597.76s
                               ETA: 955169.3s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.086s, learning 0.169s)
               Value function loss: 0.2353
                    Surrogate loss: -0.0202
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 52.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 8.26s
                        Total time: 25606.02s
                               ETA: 955101.6s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.186s, learning 0.174s)
               Value function loss: 0.1805
                    Surrogate loss: -0.0145
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 51.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 8.36s
                        Total time: 25614.38s
                               ETA: 955037.8s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.147s, learning 0.169s)
               Value function loss: 0.2127
                    Surrogate loss: -0.0185
             Mean action noise std: 0.69
                       Mean reward: 4.00
               Mean episode length: 53.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 8.32s
                        Total time: 25622.69s
                               ETA: 954972.4s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.312s, learning 0.172s)
               Value function loss: 0.1688
                    Surrogate loss: -0.0232
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 50.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 8.48s
                        Total time: 25631.18s
                               ETA: 954913.4s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.032s, learning 0.173s)
               Value function loss: 0.2036
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 3.74
               Mean episode length: 51.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 8.21s
                        Total time: 25639.38s
                               ETA: 954844.0s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.385s, learning 0.206s)
               Value function loss: 0.1806
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 52.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 8.59s
                        Total time: 25647.97s
                               ETA: 954789.0s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.029s, learning 0.162s)
               Value function loss: 0.1684
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 51.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 8.19s
                        Total time: 25656.17s
                               ETA: 954719.1s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.086s, learning 0.167s)
               Value function loss: 0.1945
                    Surrogate loss: -0.0194
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 51.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 8.25s
                        Total time: 25664.42s
                               ETA: 954651.6s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.308s, learning 0.165s)
               Value function loss: 0.1913
                    Surrogate loss: -0.0266
             Mean action noise std: 0.69
                       Mean reward: 3.66
               Mean episode length: 52.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 8.47s
                        Total time: 25672.89s
                               ETA: 954592.4s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.163s)
               Value function loss: 0.1632
                    Surrogate loss: -0.0167
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 52.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 8.42s
                        Total time: 25681.31s
                               ETA: 954531.2s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.106s, learning 0.201s)
               Value function loss: 0.1552
                    Surrogate loss: -0.0244
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 51.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 8.31s
                        Total time: 25689.62s
                               ETA: 954465.9s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.330s, learning 0.166s)
               Value function loss: 0.1488
                    Surrogate loss: -0.0164
             Mean action noise std: 0.69
                       Mean reward: 4.25
               Mean episode length: 51.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 8.50s
                        Total time: 25698.11s
                               ETA: 954407.6s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.067s, learning 0.167s)
               Value function loss: 0.1716
                    Surrogate loss: -0.0197
             Mean action noise std: 0.69
                       Mean reward: 4.13
               Mean episode length: 52.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 8.23s
                        Total time: 25706.35s
                               ETA: 954339.7s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.164s)
               Value function loss: 0.1633
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 52.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 8.37s
                        Total time: 25714.72s
                               ETA: 954276.8s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.312s, learning 0.163s)
               Value function loss: 0.1626
                    Surrogate loss: -0.0237
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 8.47s
                        Total time: 25723.20s
                               ETA: 954217.8s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.140s, learning 0.167s)
               Value function loss: 0.1815
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 50.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 8.31s
                        Total time: 25731.50s
                               ETA: 954152.7s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.129s, learning 0.166s)
               Value function loss: 0.1768
                    Surrogate loss: -0.0200
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 51.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 8.30s
                        Total time: 25739.80s
                               ETA: 954087.2s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.386s, learning 0.164s)
               Value function loss: 0.1747
                    Surrogate loss: -0.0228
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 52.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 8.55s
                        Total time: 25748.35s
                               ETA: 954031.2s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.299s, learning 0.170s)
               Value function loss: 0.1777
                    Surrogate loss: -0.0184
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 52.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 8.47s
                        Total time: 25756.82s
                               ETA: 953972.2s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.302s, learning 0.157s)
               Value function loss: 0.1699
                    Surrogate loss: -0.0222
             Mean action noise std: 0.69
                       Mean reward: 4.02
               Mean episode length: 52.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 8.46s
                        Total time: 25765.28s
                               ETA: 953912.8s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.094s, learning 0.159s)
               Value function loss: 0.1663
                    Surrogate loss: -0.0211
             Mean action noise std: 0.69
                       Mean reward: 2.86
               Mean episode length: 49.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 8.25s
                        Total time: 25773.53s
                               ETA: 953845.9s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.180s, learning 0.177s)
               Value function loss: 70.7092
                    Surrogate loss: 0.0005
             Mean action noise std: 0.69
                       Mean reward: 11.60
               Mean episode length: 53.51
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 8.36s
                        Total time: 25781.89s
                               ETA: 953782.9s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.263s, learning 0.160s)
               Value function loss: 0.2018
                    Surrogate loss: -0.0197
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 50.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 8.42s
                        Total time: 25790.31s
                               ETA: 953722.3s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.380s, learning 0.182s)
               Value function loss: 0.1829
                    Surrogate loss: -0.0137
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 51.10
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 8.56s
                        Total time: 25798.87s
                               ETA: 953666.9s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.289s, learning 0.172s)
               Value function loss: 0.1853
                    Surrogate loss: -0.0113
             Mean action noise std: 0.69
                       Mean reward: 3.86
               Mean episode length: 52.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 8.46s
                        Total time: 25807.33s
                               ETA: 953607.9s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.991s, learning 0.175s)
               Value function loss: 46.4530
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 52.45
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 8.17s
                        Total time: 25815.50s
                               ETA: 953537.9s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.090s, learning 0.161s)
               Value function loss: 0.2094
                    Surrogate loss: -0.0224
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 8.25s
                        Total time: 25823.75s
                               ETA: 953471.2s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.194s, learning 0.211s)
               Value function loss: 0.2080
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 8.40s
                        Total time: 25832.15s
                               ETA: 953410.2s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1884 steps/s (collection: 8.405s, learning 0.289s)
               Value function loss: 0.1830
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 3.77
               Mean episode length: 53.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 8.69s
                        Total time: 25840.85s
                               ETA: 953359.9s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.304s, learning 0.163s)
               Value function loss: 0.1658
                    Surrogate loss: -0.0248
             Mean action noise std: 0.69
                       Mean reward: 4.12
               Mean episode length: 52.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 8.47s
                        Total time: 25849.32s
                               ETA: 953301.2s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.252s, learning 0.195s)
               Value function loss: 0.1722
                    Surrogate loss: -0.0120
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 53.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 8.45s
                        Total time: 25857.76s
                               ETA: 953241.9s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.263s, learning 0.166s)
               Value function loss: 0.1711
                    Surrogate loss: -0.0179
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 53.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 8.43s
                        Total time: 25866.19s
                               ETA: 953181.9s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.286s, learning 0.165s)
               Value function loss: 0.1421
                    Surrogate loss: -0.0243
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 8.45s
                        Total time: 25874.64s
                               ETA: 953122.8s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.142s, learning 0.163s)
               Value function loss: 0.1621
                    Surrogate loss: -0.0188
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 53.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 8.31s
                        Total time: 25882.95s
                               ETA: 953058.3s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.242s, learning 0.165s)
               Value function loss: 0.1525
                    Surrogate loss: -0.0120
             Mean action noise std: 0.69
                       Mean reward: 3.50
               Mean episode length: 51.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 8.41s
                        Total time: 25891.35s
                               ETA: 952997.6s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.223s, learning 0.165s)
               Value function loss: 0.1551
                    Surrogate loss: -0.0221
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 54.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 8.39s
                        Total time: 25899.74s
                               ETA: 952936.3s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1131 steps/s (collection: 14.317s, learning 0.163s)
               Value function loss: 0.1510
                    Surrogate loss: -0.0160
             Mean action noise std: 0.69
                       Mean reward: 3.58
               Mean episode length: 52.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 14.48s
                        Total time: 25914.22s
                               ETA: 953099.1s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.344s, learning 0.214s)
               Value function loss: 0.1654
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.95
               Mean episode length: 52.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 16.56s
                        Total time: 25930.78s
                               ETA: 953338.1s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.472s, learning 0.175s)
               Value function loss: 0.1434
                    Surrogate loss: -0.0191
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 16.65s
                        Total time: 25947.43s
                               ETA: 953580.2s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.253s, learning 0.215s)
               Value function loss: 0.1489
                    Surrogate loss: -0.0167
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 51.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 16.47s
                        Total time: 25963.90s
                               ETA: 953815.5s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.517s, learning 0.179s)
               Value function loss: 0.1566
                    Surrogate loss: -0.0240
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 52.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 16.70s
                        Total time: 25980.59s
                               ETA: 954059.1s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.221s, learning 0.164s)
               Value function loss: 0.1622
                    Surrogate loss: -0.0174
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 51.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 16.39s
                        Total time: 25996.98s
                               ETA: 954291.0s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.412s, learning 0.224s)
               Value function loss: 0.1757
                    Surrogate loss: -0.0254
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 50.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 16.64s
                        Total time: 26013.61s
                               ETA: 954531.9s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.346s, learning 0.169s)
               Value function loss: 0.1518
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 52.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 16.51s
                        Total time: 26030.13s
                               ETA: 954768.2s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.529s, learning 0.169s)
               Value function loss: 0.1683
                    Surrogate loss: -0.0192
             Mean action noise std: 0.69
                       Mean reward: 3.91
               Mean episode length: 52.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 16.70s
                        Total time: 26046.83s
                               ETA: 955011.0s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.097s, learning 0.162s)
               Value function loss: 0.1574
                    Surrogate loss: -0.0173
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 53.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 16.26s
                        Total time: 26063.08s
                               ETA: 955237.6s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.183s, learning 0.168s)
               Value function loss: 0.1501
                    Surrogate loss: -0.0224
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 52.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 16.35s
                        Total time: 26079.43s
                               ETA: 955467.3s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.870s, learning 0.165s)
               Value function loss: 0.1633
                    Surrogate loss: -0.0229
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 16.03s
                        Total time: 26095.47s
                               ETA: 955685.2s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.182s, learning 0.162s)
               Value function loss: 0.1483
                    Surrogate loss: -0.0195
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 51.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 16.34s
                        Total time: 26111.81s
                               ETA: 955914.3s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.399s, learning 0.162s)
               Value function loss: 0.1539
                    Surrogate loss: -0.0188
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 51.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 16.56s
                        Total time: 26128.37s
                               ETA: 956151.2s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.195s, learning 0.167s)
               Value function loss: 0.1559
                    Surrogate loss: -0.0229
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 50.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 16.36s
                        Total time: 26144.74s
                               ETA: 956380.6s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.391s, learning 0.222s)
               Value function loss: 0.1458
                    Surrogate loss: -0.0174
             Mean action noise std: 0.69
                       Mean reward: 4.13
               Mean episode length: 53.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 16.61s
                        Total time: 26161.35s
                               ETA: 956619.0s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.399s, learning 0.162s)
               Value function loss: 0.1359
                    Surrogate loss: -0.0210
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 53.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 16.56s
                        Total time: 26177.91s
                               ETA: 956855.2s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1013 steps/s (collection: 16.001s, learning 0.168s)
               Value function loss: 0.1344
                    Surrogate loss: -0.0191
             Mean action noise std: 0.69
                       Mean reward: 3.93
               Mean episode length: 53.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 16.17s
                        Total time: 26194.08s
                               ETA: 957077.0s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.100s, learning 0.163s)
               Value function loss: 0.1885
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 51.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 16.26s
                        Total time: 26210.34s
                               ETA: 957302.1s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.978s, learning 0.165s)
               Value function loss: 0.1578
                    Surrogate loss: -0.0175
             Mean action noise std: 0.69
                       Mean reward: 3.65
               Mean episode length: 51.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 16.14s
                        Total time: 26226.49s
                               ETA: 957522.5s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.433s, learning 0.165s)
               Value function loss: 0.1785
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 16.60s
                        Total time: 26243.08s
                               ETA: 957759.4s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.307s, learning 0.221s)
               Value function loss: 0.1658
                    Surrogate loss: -0.0226
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 53.01
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0001
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 16.53s
                        Total time: 26259.61s
                               ETA: 957993.5s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.147s, learning 0.167s)
               Value function loss: 3.9351
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 4.07
               Mean episode length: 53.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 16.31s
                        Total time: 26275.93s
                               ETA: 958219.7s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.373s, learning 0.163s)
               Value function loss: 0.1870
                    Surrogate loss: -0.0270
             Mean action noise std: 0.69
                       Mean reward: 4.03
               Mean episode length: 53.33
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 16.54s
                        Total time: 26292.46s
                               ETA: 958453.8s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.028s, learning 0.209s)
               Value function loss: 0.1606
                    Surrogate loss: -0.0185
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 53.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 16.24s
                        Total time: 26308.70s
                               ETA: 958676.8s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.062s, learning 0.179s)
               Value function loss: 0.1711
                    Surrogate loss: -0.0247
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 50.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 16.24s
                        Total time: 26324.94s
                               ETA: 958899.8s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.132s, learning 0.168s)
               Value function loss: 132.7288
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 16.30s
                        Total time: 26341.24s
                               ETA: 959124.7s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.351s, learning 0.164s)
               Value function loss: 3.9891
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3.76
               Mean episode length: 51.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 16.52s
                        Total time: 26357.76s
                               ETA: 959357.3s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.396s, learning 0.164s)
               Value function loss: 0.2758
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 51.73
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 16.56s
                        Total time: 26374.32s
                               ETA: 959591.3s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.004s, learning 0.170s)
               Value function loss: 0.2058
                    Surrogate loss: -0.0195
             Mean action noise std: 0.69
                       Mean reward: 4.08
               Mean episode length: 53.37
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 16.17s
                        Total time: 26390.49s
                               ETA: 959811.1s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.297s, learning 0.165s)
               Value function loss: 4.0019
                    Surrogate loss: 0.0062
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 51.92
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 16.46s
                        Total time: 26406.95s
                               ETA: 960041.2s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.039s, learning 0.172s)
               Value function loss: 0.1743
                    Surrogate loss: -0.0189
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 50.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 16.21s
                        Total time: 26423.16s
                               ETA: 960262.0s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.134s, learning 0.209s)
               Value function loss: 0.1777
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.88
               Mean episode length: 51.94
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 16.34s
                        Total time: 26439.51s
                               ETA: 960487.4s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.357s, learning 0.161s)
               Value function loss: 0.2093
                    Surrogate loss: -0.0184
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 50.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 16.52s
                        Total time: 26456.03s
                               ETA: 960719.0s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.053s, learning 0.190s)
               Value function loss: 0.1729
                    Surrogate loss: -0.0132
             Mean action noise std: 0.69
                       Mean reward: 4.20
               Mean episode length: 53.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 16.24s
                        Total time: 26472.27s
                               ETA: 960940.4s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.287s, learning 0.213s)
               Value function loss: 0.1770
                    Surrogate loss: -0.0214
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 50.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 16.50s
                        Total time: 26488.77s
                               ETA: 961170.9s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.445s, learning 0.169s)
               Value function loss: 0.1667
                    Surrogate loss: -0.0173
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 53.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 16.61s
                        Total time: 26505.38s
                               ETA: 961405.4s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1080 steps/s (collection: 14.997s, learning 0.163s)
               Value function loss: 29.5384
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 50.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 15.16s
                        Total time: 26520.54s
                               ETA: 961587.0s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.281s, learning 0.167s)
               Value function loss: 0.2196
                    Surrogate loss: -0.0257
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 8.45s
                        Total time: 26528.99s
                               ETA: 961525.2s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.404s, learning 0.161s)
               Value function loss: 0.1977
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 51.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 8.56s
                        Total time: 26537.56s
                               ETA: 961467.7s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.001s, learning 0.174s)
               Value function loss: 0.1718
                    Surrogate loss: -0.0227
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 51.31
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 8.17s
                        Total time: 26545.73s
                               ETA: 961396.1s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.417s, learning 0.177s)
               Value function loss: 0.1853
                    Surrogate loss: -0.0174
             Mean action noise std: 0.69
                       Mean reward: 3.88
               Mean episode length: 53.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0009
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 8.59s
                        Total time: 26554.32s
                               ETA: 961339.7s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.948s, learning 0.169s)
               Value function loss: 0.2005
                    Surrogate loss: -0.0145
             Mean action noise std: 0.69
                       Mean reward: 3.46
               Mean episode length: 51.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 8.12s
                        Total time: 26562.44s
                               ETA: 961266.0s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.241s, learning 0.185s)
               Value function loss: 0.1805
                    Surrogate loss: -0.0222
             Mean action noise std: 0.69
                       Mean reward: 4.14
               Mean episode length: 54.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0008
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 8.43s
                        Total time: 26570.87s
                               ETA: 961203.6s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.229s, learning 0.168s)
               Value function loss: 0.1753
                    Surrogate loss: -0.0209
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 51.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0007
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 8.40s
                        Total time: 26579.26s
                               ETA: 961140.2s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.133s, learning 0.178s)
               Value function loss: 0.1729
                    Surrogate loss: -0.0115
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 52.88
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 8.31s
                        Total time: 26587.57s
                               ETA: 961073.7s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.260s, learning 0.170s)
               Value function loss: 0.1812
                    Surrogate loss: -0.0176
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 53.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0006
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 8.43s
                        Total time: 26596.00s
                               ETA: 961011.5s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.230s, learning 0.173s)
               Value function loss: 0.1928
                    Surrogate loss: -0.0113
             Mean action noise std: 0.69
                       Mean reward: 4.60
               Mean episode length: 54.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 8.40s
                        Total time: 26604.41s
                               ETA: 960948.4s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.269s, learning 0.161s)
               Value function loss: 0.1755
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 3.77
               Mean episode length: 52.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 8.43s
                        Total time: 26612.84s
                               ETA: 960886.4s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.272s, learning 0.279s)
               Value function loss: 0.1830
                    Surrogate loss: -0.0141
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0005
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 8.55s
                        Total time: 26621.39s
                               ETA: 960828.7s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.409s, learning 0.209s)
               Value function loss: 0.1761
                    Surrogate loss: -0.0133
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 52.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 8.62s
                        Total time: 26630.01s
                               ETA: 960773.5s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 2075 steps/s (collection: 7.723s, learning 0.170s)
               Value function loss: 0.1767
                    Surrogate loss: -0.0184
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 51.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 7.89s
                        Total time: 26637.90s
                               ETA: 960692.2s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.129s, learning 0.162s)
               Value function loss: 0.1776
                    Surrogate loss: -0.0152
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 51.85
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0004
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 8.29s
                        Total time: 26646.19s
                               ETA: 960625.3s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.381s, learning 0.163s)
               Value function loss: 0.1526
                    Surrogate loss: -0.0220
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 51.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 8.54s
                        Total time: 26654.74s
                               ETA: 960567.6s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.363s, learning 0.173s)
               Value function loss: 0.1316
                    Surrogate loss: -0.0261
             Mean action noise std: 0.69
                       Mean reward: 4.36
               Mean episode length: 54.48
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 8.54s
                        Total time: 26663.27s
                               ETA: 960509.5s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.068s, learning 0.179s)
               Value function loss: 0.1442
                    Surrogate loss: -0.0143
             Mean action noise std: 0.69
                       Mean reward: 4.31
               Mean episode length: 53.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 8.25s
                        Total time: 26671.52s
                               ETA: 960441.2s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.281s, learning 0.158s)
               Value function loss: 0.1425
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 52.05
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0003
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 8.44s
                        Total time: 26679.96s
                               ETA: 960379.8s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.088s, learning 0.161s)
               Value function loss: 324.4084
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 4.48
               Mean episode length: 54.48
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 8.25s
                        Total time: 26688.21s
                               ETA: 960311.6s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.275s, learning 0.165s)
               Value function loss: 18.1438
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 53.01
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0002
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 8.44s
                        Total time: 26696.65s
                               ETA: 960250.2s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.351s, learning 0.165s)
               Value function loss: 0.3016
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 8.52s
                        Total time: 26705.16s
                               ETA: 960191.7s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.168s)
               Value function loss: 0.2293
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 52.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 8.41s
                        Total time: 26713.57s
                               ETA: 960129.5s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.380s, learning 0.175s)
               Value function loss: 73.0438
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 6.20
               Mean episode length: 53.53
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 8.56s
                        Total time: 26722.13s
                               ETA: 960072.4s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.321s, learning 0.171s)
               Value function loss: 0.2987
                    Surrogate loss: -0.0194
             Mean action noise std: 0.69
                       Mean reward: 8.79
               Mean episode length: 53.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 8.49s
                        Total time: 26730.62s
                               ETA: 960013.2s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.186s, learning 0.162s)
               Value function loss: 0.2500
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 53.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 8.35s
                        Total time: 26738.97s
                               ETA: 959948.7s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.257s, learning 0.164s)
               Value function loss: 0.2429
                    Surrogate loss: -0.0132
             Mean action noise std: 0.69
                       Mean reward: 3.97
               Mean episode length: 53.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 8.42s
                        Total time: 26747.39s
                               ETA: 959887.0s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.373s, learning 0.166s)
               Value function loss: 46.2406
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 3.72
               Mean episode length: 52.34
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 8.54s
                        Total time: 26755.93s
                               ETA: 959829.5s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.283s, learning 0.174s)
               Value function loss: 0.2761
                    Surrogate loss: -0.0256
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 53.22
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 8.46s
                        Total time: 26764.39s
                               ETA: 959769.1s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.105s, learning 0.168s)
               Value function loss: 0.2274
                    Surrogate loss: -0.0201
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 8.27s
                        Total time: 26772.66s
                               ETA: 959702.1s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 2008 steps/s (collection: 7.997s, learning 0.162s)
               Value function loss: 0.2547
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 4.61
               Mean episode length: 54.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 8.16s
                        Total time: 26780.82s
                               ETA: 959631.1s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.200s, learning 0.170s)
               Value function loss: 13.9445
                    Surrogate loss: 0.0030
             Mean action noise std: 0.69
                       Mean reward: 4.84
               Mean episode length: 57.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 8.37s
                        Total time: 26789.19s
                               ETA: 959567.7s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.954s, learning 0.166s)
               Value function loss: 0.2360
                    Surrogate loss: -0.0191
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 52.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 8.12s
                        Total time: 26797.31s
                               ETA: 959495.5s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.442s, learning 0.162s)
               Value function loss: 0.1826
                    Surrogate loss: -0.0106
             Mean action noise std: 0.69
                       Mean reward: 3.87
               Mean episode length: 54.77
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 8.60s
                        Total time: 26805.91s
                               ETA: 959440.5s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.322s, learning 0.163s)
               Value function loss: 0.2088
                    Surrogate loss: -0.0138
             Mean action noise std: 0.69
                       Mean reward: 3.87
               Mean episode length: 53.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 8.48s
                        Total time: 26814.40s
                               ETA: 959381.4s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.165s)
               Value function loss: 0.1963
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 54.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 8.39s
                        Total time: 26822.79s
                               ETA: 959318.9s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.285s, learning 0.166s)
               Value function loss: 0.1559
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 52.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 8.45s
                        Total time: 26831.24s
                               ETA: 959258.6s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.247s, learning 0.179s)
               Value function loss: 0.1794
                    Surrogate loss: -0.0135
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.83
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 8.43s
                        Total time: 26839.66s
                               ETA: 959197.5s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.159s, learning 0.164s)
               Value function loss: 0.1451
                    Surrogate loss: -0.0201
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 52.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 8.32s
                        Total time: 26847.99s
                               ETA: 959132.7s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.064s, learning 0.169s)
               Value function loss: 0.1567
                    Surrogate loss: -0.0186
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 52.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 8.23s
                        Total time: 26856.22s
                               ETA: 959064.8s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.244s, learning 0.175s)
               Value function loss: 0.1956
                    Surrogate loss: -0.0175
             Mean action noise std: 0.69
                       Mean reward: 4.67
               Mean episode length: 55.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0011
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 8.42s
                        Total time: 26864.64s
                               ETA: 959003.5s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.133s, learning 0.169s)
               Value function loss: 16.5385
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 53.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0010
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 8.30s
                        Total time: 26872.94s
                               ETA: 958938.1s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.364s, learning 0.172s)
               Value function loss: 260.4228
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 5.83
               Mean episode length: 52.61
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 8.54s
                        Total time: 26881.48s
                               ETA: 958881.1s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.145s, learning 0.165s)
               Value function loss: 63.1381
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 52.33
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 8.31s
                        Total time: 26889.79s
                               ETA: 958816.0s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.361s, learning 0.197s)
               Value function loss: 0.9551
                    Surrogate loss: -0.0152
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 53.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 8.56s
                        Total time: 26898.34s
                               ETA: 958759.9s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.384s, learning 0.161s)
               Value function loss: 64.4498
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 52.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 8.54s
                        Total time: 26906.89s
                               ETA: 958703.3s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.161s)
               Value function loss: 4.2757
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 52.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 8.51s
                        Total time: 26915.40s
                               ETA: 958645.4s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.298s, learning 0.169s)
               Value function loss: 0.5852
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.65
               Mean episode length: 52.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 8.47s
                        Total time: 26923.87s
                               ETA: 958586.2s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.162s)
               Value function loss: 107.0939
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 4.53
               Mean episode length: 54.37
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 8.46s
                        Total time: 26932.33s
                               ETA: 958526.9s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.325s, learning 0.162s)
               Value function loss: 0.5633
                    Surrogate loss: -0.0166
             Mean action noise std: 0.69
                       Mean reward: 11.27
               Mean episode length: 53.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 8.49s
                        Total time: 26940.82s
                               ETA: 958468.3s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.367s, learning 0.211s)
               Value function loss: 0.4613
                    Surrogate loss: -0.0220
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 52.15
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 8.58s
                        Total time: 26949.39s
                               ETA: 958413.1s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.432s, learning 0.213s)
               Value function loss: 0.4413
                    Surrogate loss: -0.0141
             Mean action noise std: 0.69
                       Mean reward: 4.00
               Mean episode length: 53.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 8.65s
                        Total time: 26958.04s
                               ETA: 958360.3s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.332s, learning 0.159s)
               Value function loss: 119.1175
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 11.22
               Mean episode length: 52.34
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 8.49s
                        Total time: 26966.53s
                               ETA: 958302.0s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.250s, learning 0.167s)
               Value function loss: 0.5193
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 52.17
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 8.42s
                        Total time: 26974.95s
                               ETA: 958241.2s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.217s, learning 0.209s)
               Value function loss: 0.4100
                    Surrogate loss: -0.0189
             Mean action noise std: 0.69
                       Mean reward: 4.04
               Mean episode length: 54.02
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 8.43s
                        Total time: 26983.37s
                               ETA: 958180.7s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.164s)
               Value function loss: 0.3452
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 4.42
               Mean episode length: 53.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 8.53s
                        Total time: 26991.91s
                               ETA: 958124.1s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.301s, learning 0.171s)
               Value function loss: 0.3191
                    Surrogate loss: -0.0126
             Mean action noise std: 0.69
                       Mean reward: 4.39
               Mean episode length: 54.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 8.47s
                        Total time: 27000.38s
                               ETA: 958065.3s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.429s, learning 0.163s)
               Value function loss: 0.3008
                    Surrogate loss: -0.0155
             Mean action noise std: 0.69
                       Mean reward: 4.42
               Mean episode length: 55.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 8.59s
                        Total time: 27008.97s
                               ETA: 958010.8s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.531s, learning 0.170s)
               Value function loss: 0.3040
                    Surrogate loss: 0.0113
             Mean action noise std: 0.69
                       Mean reward: 3.46
               Mean episode length: 52.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 8.70s
                        Total time: 27017.67s
                               ETA: 957960.2s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.329s, learning 0.159s)
               Value function loss: 0.2697
                    Surrogate loss: -0.0148
             Mean action noise std: 0.69
                       Mean reward: 4.73
               Mean episode length: 55.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 8.49s
                        Total time: 27026.16s
                               ETA: 957902.1s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.337s, learning 0.161s)
               Value function loss: 0.2573
                    Surrogate loss: -0.0165
             Mean action noise std: 0.69
                       Mean reward: 3.88
               Mean episode length: 52.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 8.50s
                        Total time: 27034.66s
                               ETA: 957844.4s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.255s, learning 0.172s)
               Value function loss: 0.2485
                    Surrogate loss: -0.0220
             Mean action noise std: 0.69
                       Mean reward: 4.61
               Mean episode length: 54.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 8.43s
                        Total time: 27043.09s
                               ETA: 957784.1s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.231s, learning 0.163s)
               Value function loss: 0.1841
                    Surrogate loss: -0.0070
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 53.00
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 8.39s
                        Total time: 27051.48s
                               ETA: 957722.8s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.178s, learning 0.163s)
               Value function loss: 0.1813
                    Surrogate loss: -0.0162
             Mean action noise std: 0.69
                       Mean reward: 4.01
               Mean episode length: 53.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 8.34s
                        Total time: 27059.82s
                               ETA: 957659.6s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.117s, learning 0.165s)
               Value function loss: 493.9091
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.79
               Mean episode length: 53.06
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 8.28s
                        Total time: 27068.10s
                               ETA: 957594.4s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.363s, learning 0.174s)
               Value function loss: 4.2168
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 8.54s
                        Total time: 27076.64s
                               ETA: 957538.3s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.258s, learning 0.167s)
               Value function loss: 0.4760
                    Surrogate loss: 0.0613
             Mean action noise std: 0.69
                       Mean reward: 3.77
               Mean episode length: 52.56
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 8.43s
                        Total time: 27085.06s
                               ETA: 957478.2s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.257s, learning 0.168s)
               Value function loss: 29.3078
                    Surrogate loss: 0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 51.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 8.42s
                        Total time: 27093.49s
                               ETA: 957418.1s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.170s, learning 0.175s)
               Value function loss: 0.2801
                    Surrogate loss: -0.0151
             Mean action noise std: 0.69
                       Mean reward: 3.46
               Mean episode length: 52.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 8.35s
                        Total time: 27101.83s
                               ETA: 957355.3s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.953s, learning 0.178s)
               Value function loss: 13.5780
                    Surrogate loss: 0.0064
             Mean action noise std: 0.69
                       Mean reward: 4.30
               Mean episode length: 54.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 8.13s
                        Total time: 27109.97s
                               ETA: 957285.0s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.133s, learning 0.161s)
               Value function loss: 64.4632
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 3.91
               Mean episode length: 54.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 8.29s
                        Total time: 27118.26s
                               ETA: 957220.4s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.284s, learning 0.165s)
               Value function loss: 356.6043
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 26.20
               Mean episode length: 53.06
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 8.45s
                        Total time: 27126.71s
                               ETA: 957161.4s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 2062 steps/s (collection: 7.781s, learning 0.164s)
               Value function loss: 1.1527
                    Surrogate loss: -0.0123
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 52.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 7.95s
                        Total time: 27134.65s
                               ETA: 957084.6s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.075s, learning 0.267s)
               Value function loss: 0.7781
                    Surrogate loss: -0.0169
             Mean action noise std: 0.69
                       Mean reward: 4.08
               Mean episode length: 53.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 8.34s
                        Total time: 27143.00s
                               ETA: 957021.9s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.198s, learning 0.162s)
               Value function loss: 382.8303
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 50.56
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 8.36s
                        Total time: 27151.36s
                               ETA: 956959.9s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.193s, learning 0.159s)
               Value function loss: 91.6817
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 51.38
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 8.35s
                        Total time: 27159.71s
                               ETA: 956897.6s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.280s, learning 0.161s)
               Value function loss: 211.2129
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.86
               Mean episode length: 51.90
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 8.44s
                        Total time: 27168.15s
                               ETA: 956838.4s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.244s, learning 0.167s)
               Value function loss: 19.0612
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 19.77
               Mean episode length: 54.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 8.41s
                        Total time: 27176.56s
                               ETA: 956778.3s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.291s, learning 0.163s)
               Value function loss: 12.8919
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 6.21
               Mean episode length: 52.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 8.45s
                        Total time: 27185.01s
                               ETA: 956719.7s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.112s, learning 0.168s)
               Value function loss: 1.1352
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 3.66
               Mean episode length: 52.12
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 8.28s
                        Total time: 27193.29s
                               ETA: 956655.0s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.431s, learning 0.211s)
               Value function loss: 0.7826
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.77
               Mean episode length: 51.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 8.64s
                        Total time: 27201.94s
                               ETA: 956603.1s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.245s, learning 0.176s)
               Value function loss: 12.9076
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.65
               Mean episode length: 52.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 8.42s
                        Total time: 27210.36s
                               ETA: 956543.4s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.333s, learning 0.209s)
               Value function loss: 0.7467
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.90
               Mean episode length: 51.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 8.54s
                        Total time: 27218.90s
                               ETA: 956488.0s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.408s, learning 0.160s)
               Value function loss: 0.6714
                    Surrogate loss: -0.0192
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 52.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 8.57s
                        Total time: 27227.47s
                               ETA: 956433.6s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.107s, learning 0.168s)
               Value function loss: 64.4537
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 51.72
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 8.27s
                        Total time: 27235.74s
                               ETA: 956369.0s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.121s, learning 0.177s)
               Value function loss: 75.4898
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 52.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 8.30s
                        Total time: 27244.04s
                               ETA: 956305.1s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.204s, learning 0.164s)
               Value function loss: 4.6832
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 19.53
               Mean episode length: 53.34
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 8.37s
                        Total time: 27252.41s
                               ETA: 956243.8s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.164s)
               Value function loss: 162.5679
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 53.08
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 8.38s
                        Total time: 27260.79s
                               ETA: 956183.0s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.200s, learning 0.169s)
               Value function loss: 1.0003
                    Surrogate loss: -0.0184
             Mean action noise std: 0.69
                       Mean reward: 14.16
               Mean episode length: 53.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 8.37s
                        Total time: 27269.16s
                               ETA: 956121.7s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.323s, learning 0.162s)
               Value function loss: 0.8365
                    Surrogate loss: -0.0130
             Mean action noise std: 0.69
                       Mean reward: 4.38
               Mean episode length: 53.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 8.49s
                        Total time: 27277.64s
                               ETA: 956064.6s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.206s, learning 0.161s)
               Value function loss: 0.5522
                    Surrogate loss: -0.0143
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 50.46
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 8.37s
                        Total time: 27286.01s
                               ETA: 956003.4s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.096s, learning 0.213s)
               Value function loss: 0.5097
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 52.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 8.31s
                        Total time: 27294.32s
                               ETA: 955940.2s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.216s, learning 0.173s)
               Value function loss: 29.6449
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 9.12
               Mean episode length: 52.58
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 8.39s
                        Total time: 27302.71s
                               ETA: 955879.8s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.407s, learning 0.162s)
               Value function loss: 47.1028
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.91
               Mean episode length: 53.39
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 8.57s
                        Total time: 27311.28s
                               ETA: 955825.8s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.264s, learning 0.160s)
               Value function loss: 133.1080
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 16.52
               Mean episode length: 53.91
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 8.42s
                        Total time: 27319.70s
                               ETA: 955766.8s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.228s, learning 0.232s)
               Value function loss: 0.7836
                    Surrogate loss: -0.0070
             Mean action noise std: 0.69
                       Mean reward: 3.87
               Mean episode length: 53.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 8.46s
                        Total time: 27328.16s
                               ETA: 955709.0s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.060s, learning 0.169s)
               Value function loss: 0.9520
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 52.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 8.23s
                        Total time: 27336.39s
                               ETA: 955643.2s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.097s, learning 0.168s)
               Value function loss: 29.5400
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 4.19
               Mean episode length: 53.84
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 8.26s
                        Total time: 27344.65s
                               ETA: 955578.6s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.241s, learning 0.175s)
               Value function loss: 0.5607
                    Surrogate loss: -0.0134
             Mean action noise std: 0.69
                       Mean reward: 8.81
               Mean episode length: 53.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 8.42s
                        Total time: 27353.07s
                               ETA: 955519.4s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.184s, learning 0.160s)
               Value function loss: 0.3997
                    Surrogate loss: -0.0119
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 51.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 8.34s
                        Total time: 27361.41s
                               ETA: 955457.7s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.131s, learning 0.167s)
               Value function loss: 0.3292
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 4.10
               Mean episode length: 53.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 8.30s
                        Total time: 27369.71s
                               ETA: 955394.5s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.179s, learning 0.171s)
               Value function loss: 0.3460
                    Surrogate loss: -0.0155
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 51.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 8.35s
                        Total time: 27378.06s
                               ETA: 955333.1s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.092s, learning 0.157s)
               Value function loss: 129.3058
                    Surrogate loss: 0.0027
             Mean action noise std: 0.69
                       Mean reward: 11.09
               Mean episode length: 52.20
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 8.25s
                        Total time: 27386.31s
                               ETA: 955268.2s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.165s)
               Value function loss: 131.0146
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 4.15
               Mean episode length: 54.12
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 8.43s
                        Total time: 27394.74s
                               ETA: 955209.8s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.384s, learning 0.164s)
               Value function loss: 498.7256
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 51.38
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 8.55s
                        Total time: 27403.29s
                               ETA: 955155.5s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.333s, learning 0.165s)
               Value function loss: 89.0942
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 6.36
               Mean episode length: 51.57
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 8.50s
                        Total time: 27411.79s
                               ETA: 955099.4s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.543s, learning 0.175s)
               Value function loss: 3.1753
                    Surrogate loss: -0.0079
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 51.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 8.72s
                        Total time: 27420.50s
                               ETA: 955051.0s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.964s, learning 0.162s)
               Value function loss: 1.8387
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 3.64
               Mean episode length: 51.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 8.13s
                        Total time: 27428.63s
                               ETA: 954982.0s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.066s, learning 0.168s)
               Value function loss: 1.5690
                    Surrogate loss: -0.0138
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 51.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 8.23s
                        Total time: 27436.86s
                               ETA: 954916.8s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.902s, learning 0.214s)
               Value function loss: 0.8757
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 50.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 8.12s
                        Total time: 27444.98s
                               ETA: 954847.6s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.255s, learning 0.257s)
               Value function loss: 0.7776
                    Surrogate loss: -0.0224
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 49.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 8.51s
                        Total time: 27453.49s
                               ETA: 954792.1s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.162s)
               Value function loss: 0.5413
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 4.15
               Mean episode length: 52.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 8.41s
                        Total time: 27461.91s
                               ETA: 954733.4s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.030s, learning 0.165s)
               Value function loss: 0.4972
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 50.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 8.19s
                        Total time: 27470.10s
                               ETA: 954667.0s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.468s, learning 0.164s)
               Value function loss: 0.4134
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 3.06
               Mean episode length: 49.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 8.63s
                        Total time: 27478.73s
                               ETA: 954615.9s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.167s)
               Value function loss: 0.3597
                    Surrogate loss: -0.0223
             Mean action noise std: 0.69
                       Mean reward: 4.88
               Mean episode length: 53.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 8.57s
                        Total time: 27487.30s
                               ETA: 954562.7s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.325s, learning 0.172s)
               Value function loss: 0.3123
                    Surrogate loss: -0.0128
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 50.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 8.50s
                        Total time: 27495.80s
                               ETA: 954506.9s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.362s, learning 0.205s)
               Value function loss: 63.8387
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 49.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 8.57s
                        Total time: 27504.37s
                               ETA: 954453.6s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.009s, learning 0.209s)
               Value function loss: 4.2416
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 52.24
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 8.22s
                        Total time: 27512.59s
                               ETA: 954388.3s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.386s, learning 0.175s)
               Value function loss: 0.3227
                    Surrogate loss: -0.0153
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 50.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 8.56s
                        Total time: 27521.15s
                               ETA: 954334.8s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.179s, learning 0.177s)
               Value function loss: 81.3269
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 50.67
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 8.36s
                        Total time: 27529.50s
                               ETA: 954274.3s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.126s, learning 0.164s)
               Value function loss: 4.2829
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 50.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 8.29s
                        Total time: 27537.79s
                               ETA: 954211.5s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.335s, learning 0.168s)
               Value function loss: 18.1612
                    Surrogate loss: 0.0058
             Mean action noise std: 0.69
                       Mean reward: 4.00
               Mean episode length: 52.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 8.50s
                        Total time: 27546.30s
                               ETA: 954156.2s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.455s, learning 0.209s)
               Value function loss: 16.9319
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 50.94
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 8.66s
                        Total time: 27554.96s
                               ETA: 954106.5s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.199s, learning 0.161s)
               Value function loss: 0.4774
                    Surrogate loss: -0.0205
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 52.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 8.36s
                        Total time: 27563.32s
                               ETA: 954046.2s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.438s, learning 0.161s)
               Value function loss: 0.4494
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 3.46
               Mean episode length: 50.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 8.60s
                        Total time: 27571.92s
                               ETA: 953994.3s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.298s, learning 0.173s)
               Value function loss: 0.4027
                    Surrogate loss: -0.0063
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 51.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 8.47s
                        Total time: 27580.39s
                               ETA: 953938.0s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.201s, learning 0.263s)
               Value function loss: 0.3613
                    Surrogate loss: -0.0144
             Mean action noise std: 0.69
                       Mean reward: 3.57
               Mean episode length: 50.95
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 8.46s
                        Total time: 27588.85s
                               ETA: 953881.5s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.163s)
               Value function loss: 11.8692
                    Surrogate loss: 0.0038
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 51.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 8.43s
                        Total time: 27597.28s
                               ETA: 953823.8s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.096s, learning 0.213s)
               Value function loss: 0.3185
                    Surrogate loss: -0.0201
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 51.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 8.31s
                        Total time: 27605.59s
                               ETA: 953762.0s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.225s, learning 0.217s)
               Value function loss: 0.2419
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 51.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 8.44s
                        Total time: 27614.04s
                               ETA: 953704.8s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.163s)
               Value function loss: 0.2904
                    Surrogate loss: -0.0190
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 52.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 8.33s
                        Total time: 27622.37s
                               ETA: 953643.9s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.163s, learning 0.162s)
               Value function loss: 0.2601
                    Surrogate loss: -0.0170
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 8.32s
                        Total time: 27630.69s
                               ETA: 953582.7s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.499s, learning 0.170s)
               Value function loss: 0.2533
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 49.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 8.67s
                        Total time: 27639.36s
                               ETA: 953533.5s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.309s, learning 0.160s)
               Value function loss: 0.2324
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 51.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 8.47s
                        Total time: 27647.83s
                               ETA: 953477.4s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.183s, learning 0.169s)
               Value function loss: 0.2446
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 4.45
               Mean episode length: 53.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 8.35s
                        Total time: 27656.18s
                               ETA: 953417.2s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.287s, learning 0.170s)
               Value function loss: 0.2094
                    Surrogate loss: -0.0152
             Mean action noise std: 0.69
                       Mean reward: 3.76
               Mean episode length: 53.24
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 8.46s
                        Total time: 27664.64s
                               ETA: 953360.8s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.253s, learning 0.162s)
               Value function loss: 0.2648
                    Surrogate loss: -0.0145
             Mean action noise std: 0.69
                       Mean reward: 3.44
               Mean episode length: 50.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 8.41s
                        Total time: 27673.05s
                               ETA: 953302.9s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.050s, learning 0.163s)
               Value function loss: 213.5248
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.64
               Mean episode length: 52.26
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 8.21s
                        Total time: 27681.27s
                               ETA: 953238.1s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.210s, learning 0.165s)
               Value function loss: 73.8225
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.80
               Mean episode length: 51.83
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 8.37s
                        Total time: 27689.64s
                               ETA: 953178.9s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.163s)
               Value function loss: 4.6388
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 11.02
               Mean episode length: 52.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 8.62s
                        Total time: 27698.26s
                               ETA: 953128.3s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.422s, learning 0.164s)
               Value function loss: 0.5976
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 52.69
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 8.59s
                        Total time: 27706.85s
                               ETA: 953076.4s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.257s, learning 0.172s)
               Value function loss: 193.2341
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 52.23
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 8.43s
                        Total time: 27715.28s
                               ETA: 953019.2s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.067s, learning 0.214s)
               Value function loss: 306.8699
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 51.21
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 8.28s
                        Total time: 27723.56s
                               ETA: 952956.9s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.517s, learning 0.175s)
               Value function loss: 6.4085
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 4.02
               Mean episode length: 53.84
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 8.69s
                        Total time: 27732.25s
                               ETA: 952908.8s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.229s, learning 0.164s)
               Value function loss: 2.2700
                    Surrogate loss: -0.0128
             Mean action noise std: 0.69
                       Mean reward: 3.57
               Mean episode length: 51.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 8.39s
                        Total time: 27740.65s
                               ETA: 952850.5s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.172s)
               Value function loss: 1.8662
                    Surrogate loss: -0.0173
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 8.38s
                        Total time: 27749.03s
                               ETA: 952791.9s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.383s, learning 0.166s)
               Value function loss: 1.2913
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 51.47
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 8.55s
                        Total time: 27757.58s
                               ETA: 952738.9s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.120s, learning 0.169s)
               Value function loss: 105.2875
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 52.82
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 8.29s
                        Total time: 27765.87s
                               ETA: 952677.1s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.163s, learning 0.174s)
               Value function loss: 211.7728
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.61
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 8.34s
                        Total time: 27774.20s
                               ETA: 952617.0s
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:44: DeprecationWarning: [33mWARN: `env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["render.modes"] is marked as deprecated and will be replaced with `env.metadata["render_modes"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:116: DeprecationWarning: [33mWARN: `env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` see https://github.com/openai/gym/pull/2654 for more details[0m
  '`env.metadata["video.frames_per_second"] is marked as deprecated and will be replaced with `env.metadata["render_fps"]` '
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/gym/wrappers/monitoring/video_recorder.py:422: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  np.__version__

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.167s)
               Value function loss: 18.4935
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 26.59
               Mean episode length: 53.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 8.44s
                        Total time: 27782.64s
                               ETA: 952560.3s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.327s, learning 0.164s)
               Value function loss: 18.8735
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 51.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 8.49s
                        Total time: 27791.13s
                               ETA: 952505.5s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.310s, learning 0.161s)
               Value function loss: 238.8400
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 21.29
               Mean episode length: 53.08
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 8.47s
                        Total time: 27799.60s
                               ETA: 952450.1s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1055 steps/s (collection: 15.350s, learning 0.179s)
               Value function loss: 2.0313
                    Surrogate loss: -0.0142
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.84
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 15.53s
                        Total time: 27815.13s
                               ETA: 952636.4s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.353s, learning 0.173s)
               Value function loss: 1.2251
                    Surrogate loss: -0.0118
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 53.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 16.53s
                        Total time: 27831.66s
                               ETA: 952856.7s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.300s, learning 0.175s)
               Value function loss: 0.9312
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 50.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 16.47s
                        Total time: 27848.13s
                               ETA: 953075.1s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.064s, learning 0.173s)
               Value function loss: 0.9536
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 52.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 16.24s
                        Total time: 27864.37s
                               ETA: 953285.2s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.827s, learning 0.189s)
               Value function loss: 58.9830
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 53.68
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 16.02s
                        Total time: 27880.39s
                               ETA: 953487.6s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.850s, learning 0.179s)
               Value function loss: 1.0556
                    Surrogate loss: -0.0128
             Mean action noise std: 0.69
                       Mean reward: 8.87
               Mean episode length: 53.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 16.03s
                        Total time: 27896.41s
                               ETA: 953690.3s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.909s, learning 0.182s)
               Value function loss: 0.7684
                    Surrogate loss: -0.0144
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 51.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 16.09s
                        Total time: 27912.51s
                               ETA: 953894.9s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.100s, learning 0.171s)
               Value function loss: 337.9297
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 4.16
               Mean episode length: 54.47
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 16.27s
                        Total time: 27928.78s
                               ETA: 954105.5s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.086s, learning 0.200s)
               Value function loss: 550.1816
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 52.35
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 16.29s
                        Total time: 27945.06s
                               ETA: 954316.5s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.278s, learning 0.174s)
               Value function loss: 187.0322
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 52.72
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 16.45s
                        Total time: 27961.51s
                               ETA: 954533.0s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.308s, learning 0.184s)
               Value function loss: 277.4321
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 51.42
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 16.49s
                        Total time: 27978.01s
                               ETA: 954750.7s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.000s, learning 0.174s)
               Value function loss: 211.0702
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 16.25
               Mean episode length: 52.77
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 16.17s
                        Total time: 27994.18s
                               ETA: 954957.4s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.238s, learning 0.182s)
               Value function loss: 332.4990
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.81
               Mean episode length: 53.59
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 16.42s
                        Total time: 28010.60s
                               ETA: 955172.3s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1021 steps/s (collection: 15.868s, learning 0.177s)
               Value function loss: 111.5753
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.88
               Mean episode length: 53.66
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 16.04s
                        Total time: 28026.65s
                               ETA: 955374.3s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.007s, learning 0.172s)
               Value function loss: 27.8470
                    Surrogate loss: -0.0050
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 53.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 16.18s
                        Total time: 28042.82s
                               ETA: 955580.6s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.165s, learning 0.219s)
               Value function loss: 294.7017
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.80
               Mean episode length: 52.69
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 16.38s
                        Total time: 28059.21s
                               ETA: 955793.8s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.212s, learning 0.173s)
               Value function loss: 10.1459
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 16.27
               Mean episode length: 52.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 16.39s
                        Total time: 28075.59s
                               ETA: 956006.9s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.829s, learning 0.175s)
               Value function loss: 5.9612
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 52.65
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 16.00s
                        Total time: 28091.60s
                               ETA: 956206.9s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.107s, learning 0.170s)
               Value function loss: 5.0964
                    Surrogate loss: -0.0143
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 51.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 16.28s
                        Total time: 28107.88s
                               ETA: 956416.0s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.024s, learning 0.160s)
               Value function loss: 705.9254
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.79
               Mean episode length: 53.19
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 16.18s
                        Total time: 28124.06s
                               ETA: 956621.8s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.108s, learning 0.178s)
               Value function loss: 642.8893
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 30.88
               Mean episode length: 51.98
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 16.29s
                        Total time: 28140.35s
                               ETA: 956830.9s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.104s, learning 0.168s)
               Value function loss: 62.9241
                    Surrogate loss: -0.0105
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 52.18
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 16.27s
                        Total time: 28156.62s
                               ETA: 957039.3s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.129s, learning 0.174s)
               Value function loss: 99.2021
                    Surrogate loss: 0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 51.46
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 16.30s
                        Total time: 28172.92s
                               ETA: 957248.7s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.217s, learning 0.168s)
               Value function loss: 7.2725
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 53.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 16.38s
                        Total time: 28189.31s
                               ETA: 957460.6s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.065s, learning 0.167s)
               Value function loss: 4.9698
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 52.27
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 16.23s
                        Total time: 28205.54s
                               ETA: 957667.2s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.424s, learning 0.219s)
               Value function loss: 19.1946
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 50.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 16.64s
                        Total time: 28222.18s
                               ETA: 957887.6s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.146s, learning 0.172s)
               Value function loss: 155.6078
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 4.03
               Mean episode length: 54.14
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 16.32s
                        Total time: 28238.50s
                               ETA: 958096.8s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.213s, learning 0.164s)
               Value function loss: 5.0628
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 52.78
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 16.38s
                        Total time: 28254.87s
                               ETA: 958307.9s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.207s, learning 0.171s)
               Value function loss: 3.3797
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 51.89
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 16.38s
                        Total time: 28271.25s
                               ETA: 958518.8s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.077s, learning 0.166s)
               Value function loss: 17.7423
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.87
               Mean episode length: 53.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 16.24s
                        Total time: 28287.50s
                               ETA: 958725.0s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1012 steps/s (collection: 15.911s, learning 0.265s)
               Value function loss: 1.3020
                    Surrogate loss: -0.0142
             Mean action noise std: 0.69
                       Mean reward: 6.92
               Mean episode length: 55.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 16.18s
                        Total time: 28303.67s
                               ETA: 958928.8s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.754s, learning 0.163s)
               Value function loss: 0.8091
                    Surrogate loss: -0.0147
             Mean action noise std: 0.69
                       Mean reward: 3.58
               Mean episode length: 52.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 15.92s
                        Total time: 28319.59s
                               ETA: 959123.7s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.979s, learning 0.166s)
               Value function loss: 0.8268
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 51.96
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 16.14s
                        Total time: 28335.73s
                               ETA: 959326.1s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.087s, learning 0.162s)
               Value function loss: 0.6756
                    Surrogate loss: -0.0153
             Mean action noise std: 0.69
                       Mean reward: 4.11
               Mean episode length: 54.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 16.25s
                        Total time: 28351.98s
                               ETA: 959531.9s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.385s, learning 0.192s)
               Value function loss: 59.6330
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.51
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 16.58s
                        Total time: 28368.56s
                               ETA: 959748.6s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.000s, learning 0.204s)
               Value function loss: 63.4301
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.33
               Mean episode length: 52.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 16.20s
                        Total time: 28384.76s
                               ETA: 959952.6s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.121s, learning 0.162s)
               Value function loss: 132.4762
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 8.74
               Mean episode length: 51.93
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 16.28s
                        Total time: 28401.05s
                               ETA: 960159.0s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1154 steps/s (collection: 14.026s, learning 0.160s)
               Value function loss: 35.7029
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.82
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 14.19s
                        Total time: 28415.23s
                               ETA: 960294.5s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.061s, learning 0.212s)
               Value function loss: 1.4747
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 52.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 8.27s
                        Total time: 28423.51s
                               ETA: 960230.1s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.405s, learning 0.188s)
               Value function loss: 1.2284
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 52.12
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 8.59s
                        Total time: 28432.10s
                               ETA: 960176.5s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.396s, learning 0.241s)
               Value function loss: 246.5801
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 18.70
               Mean episode length: 51.88
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 8.64s
                        Total time: 28440.74s
                               ETA: 960124.5s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.110s, learning 0.162s)
               Value function loss: 19.1291
                    Surrogate loss: 0.0098
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 52.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 8.27s
                        Total time: 28449.01s
                               ETA: 960060.1s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.166s, learning 0.166s)
               Value function loss: 131.6715
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 4.06
               Mean episode length: 53.62
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 8.33s
                        Total time: 28457.34s
                               ETA: 959997.9s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.169s, learning 0.169s)
               Value function loss: 45.5446
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 6.66
               Mean episode length: 54.12
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 8.34s
                        Total time: 28465.68s
                               ETA: 959935.8s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.412s, learning 0.163s)
               Value function loss: 88.8938
                    Surrogate loss: 0.0037
             Mean action noise std: 0.69
                       Mean reward: 4.39
               Mean episode length: 54.34
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 8.57s
                        Total time: 28474.25s
                               ETA: 959881.8s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.556s, learning 0.166s)
               Value function loss: 338.7158
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.24
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 8.72s
                        Total time: 28482.98s
                               ETA: 959832.8s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.407s, learning 0.165s)
               Value function loss: 6.8736
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 3.72
               Mean episode length: 52.71
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 8.57s
                        Total time: 28491.55s
                               ETA: 959778.7s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.121s, learning 0.213s)
               Value function loss: 2.9125
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3.90
               Mean episode length: 52.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 8.33s
                        Total time: 28499.88s
                               ETA: 959716.7s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.329s, learning 0.163s)
               Value function loss: 2.2494
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.88
               Mean episode length: 52.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 8.49s
                        Total time: 28508.37s
                               ETA: 959660.0s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.347s, learning 0.163s)
               Value function loss: 40.7987
                    Surrogate loss: 0.0067
             Mean action noise std: 0.69
                       Mean reward: 3.78
               Mean episode length: 52.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 8.51s
                        Total time: 28516.88s
                               ETA: 959603.9s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.203s, learning 0.163s)
               Value function loss: 304.6478
                    Surrogate loss: 0.0022
             Mean action noise std: 0.69
                       Mean reward: 6.60
               Mean episode length: 52.23
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 8.37s
                        Total time: 28525.25s
                               ETA: 959543.1s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.152s, learning 0.165s)
               Value function loss: 19.5835
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 51.25
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 8.32s
                        Total time: 28533.56s
                               ETA: 959480.6s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.150s, learning 0.165s)
               Value function loss: 19.5610
                    Surrogate loss: 0.0196
             Mean action noise std: 0.69
                       Mean reward: 21.30
               Mean episode length: 52.19
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 8.32s
                        Total time: 28541.88s
                               ETA: 959418.1s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.304s, learning 0.167s)
               Value function loss: 460.1950
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 6.48
               Mean episode length: 53.35
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 8.47s
                        Total time: 28550.35s
                               ETA: 959360.9s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.123s, learning 0.219s)
               Value function loss: 4.1168
                    Surrogate loss: -0.0079
             Mean action noise std: 0.69
                       Mean reward: 4.05
               Mean episode length: 52.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 8.34s
                        Total time: 28558.69s
                               ETA: 959299.4s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.398s, learning 0.207s)
               Value function loss: 205.1944
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.23
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 8.60s
                        Total time: 28567.30s
                               ETA: 959246.8s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.377s, learning 0.266s)
               Value function loss: 132.0030
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.96
               Mean episode length: 54.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 8.64s
                        Total time: 28575.94s
                               ETA: 959195.4s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1857 steps/s (collection: 8.662s, learning 0.160s)
               Value function loss: 300.8656
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 52.56
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 8.82s
                        Total time: 28584.76s
                               ETA: 959150.1s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.273s, learning 0.159s)
               Value function loss: 278.2798
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 13.72
               Mean episode length: 52.08
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 8.43s
                        Total time: 28593.19s
                               ETA: 959091.8s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.447s, learning 0.161s)
               Value function loss: 95.0971
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 4.43
               Mean episode length: 54.33
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 8.61s
                        Total time: 28601.80s
                               ETA: 959039.3s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.208s, learning 0.173s)
               Value function loss: 38.1118
                    Surrogate loss: 0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.76
               Mean episode length: 52.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 8.38s
                        Total time: 28610.18s
                               ETA: 958979.3s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.314s, learning 0.167s)
               Value function loss: 52.6680
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 50.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 8.48s
                        Total time: 28618.66s
                               ETA: 958922.7s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.164s)
               Value function loss: 320.6004
                    Surrogate loss: 0.0023
             Mean action noise std: 0.69
                       Mean reward: 13.79
               Mean episode length: 51.47
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 8.46s
                        Total time: 28627.12s
                               ETA: 958865.5s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.295s, learning 0.170s)
               Value function loss: 200.8434
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 4.10
               Mean episode length: 53.58
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 8.47s
                        Total time: 28635.59s
                               ETA: 958808.4s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.281s, learning 0.165s)
               Value function loss: 654.2627
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 29.36
               Mean episode length: 53.25
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 8.45s
                        Total time: 28644.04s
                               ETA: 958750.8s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.245s, learning 0.167s)
               Value function loss: 920.0815
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 36.01
               Mean episode length: 50.80
                  Mean reward/step: 0.47
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 8.41s
                        Total time: 28652.45s
                               ETA: 958692.0s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.480s, learning 0.167s)
               Value function loss: 444.8374
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.72
               Mean episode length: 51.89
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 8.65s
                        Total time: 28661.10s
                               ETA: 958641.1s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.357s, learning 0.161s)
               Value function loss: 144.3027
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 50.14
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0241
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 8.52s
                        Total time: 28669.61s
                               ETA: 958585.9s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.358s, learning 0.173s)
               Value function loss: 511.6752
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 3.86
               Mean episode length: 51.39
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 8.53s
                        Total time: 28678.14s
                               ETA: 958531.2s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.229s, learning 0.171s)
               Value function loss: 263.5601
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 28.82
               Mean episode length: 52.77
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0250
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 8.40s
                        Total time: 28686.54s
                               ETA: 958472.1s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.273s, learning 0.165s)
               Value function loss: 97.3453
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 51.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0231
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 8.44s
                        Total time: 28694.98s
                               ETA: 958414.4s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.072s, learning 0.213s)
               Value function loss: 40.8009
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 51.29
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0213
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 8.28s
                        Total time: 28703.27s
                               ETA: 958351.6s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.356s, learning 0.170s)
               Value function loss: 168.5325
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 50.71
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 8.53s
                        Total time: 28711.79s
                               ETA: 958296.8s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.161s)
               Value function loss: 69.4771
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 21.49
               Mean episode length: 51.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0197
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 8.36s
                        Total time: 28720.15s
                               ETA: 958236.5s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.422s, learning 0.167s)
               Value function loss: 17.6705
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 50.93
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 8.59s
                        Total time: 28728.74s
                               ETA: 958183.9s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.428s, learning 0.170s)
               Value function loss: 265.5797
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 51.82
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 8.60s
                        Total time: 28737.34s
                               ETA: 958131.7s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.039s, learning 0.165s)
               Value function loss: 102.8522
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 23.91
               Mean episode length: 51.74
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 8.20s
                        Total time: 28745.54s
                               ETA: 958066.3s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.186s, learning 0.170s)
               Value function loss: 7.9820
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 52.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 8.36s
                        Total time: 28753.90s
                               ETA: 958006.1s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.300s, learning 0.167s)
               Value function loss: 2.4747
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 8.78
               Mean episode length: 52.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 8.47s
                        Total time: 28762.37s
                               ETA: 957949.6s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.482s, learning 0.160s)
               Value function loss: 303.8017
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 51.75
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 8.64s
                        Total time: 28771.01s
                               ETA: 957898.9s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.307s, learning 0.164s)
               Value function loss: 471.8898
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.86
               Mean episode length: 52.02
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 8.47s
                        Total time: 28779.48s
                               ETA: 957842.6s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.440s, learning 0.162s)
               Value function loss: 45.6645
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 50.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 8.60s
                        Total time: 28788.08s
                               ETA: 957790.7s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.237s, learning 0.161s)
               Value function loss: 28.1582
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 3.92
               Mean episode length: 52.15
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 8.40s
                        Total time: 28796.48s
                               ETA: 957732.0s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.073s, learning 0.258s)
               Value function loss: 7.0218
                    Surrogate loss: -0.0151
             Mean action noise std: 0.69
                       Mean reward: 4.05
               Mean episode length: 52.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 8.33s
                        Total time: 28804.81s
                               ETA: 957671.1s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.190s, learning 0.160s)
               Value function loss: 174.9568
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 52.20
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 8.35s
                        Total time: 28813.16s
                               ETA: 957611.0s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.441s, learning 0.162s)
               Value function loss: 495.8248
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 6.66
               Mean episode length: 54.42
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 8.60s
                        Total time: 28821.76s
                               ETA: 957559.2s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.367s, learning 0.172s)
               Value function loss: 889.2539
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 19.28
               Mean episode length: 53.11
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 8.54s
                        Total time: 28830.30s
                               ETA: 957505.3s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.239s, learning 0.164s)
               Value function loss: 513.6262
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 52.30
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 8.40s
                        Total time: 28838.70s
                               ETA: 957447.0s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.451s, learning 0.170s)
               Value function loss: 99.2676
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 11.79
               Mean episode length: 53.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0187
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 8.62s
                        Total time: 28847.32s
                               ETA: 957395.9s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.161s)
               Value function loss: 38.2883
                    Surrogate loss: -0.0169
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 53.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0189
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 8.57s
                        Total time: 28855.90s
                               ETA: 957343.3s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.594s, learning 0.176s)
               Value function loss: 567.9779
                    Surrogate loss: 0.0051
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 51.94
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 8.77s
                        Total time: 28864.67s
                               ETA: 957297.2s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.333s, learning 0.166s)
               Value function loss: 706.1144
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 23.99
               Mean episode length: 54.05
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 8.50s
                        Total time: 28873.17s
                               ETA: 957242.2s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.410s, learning 0.164s)
               Value function loss: 399.2024
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 8.43
               Mean episode length: 52.19
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 8.57s
                        Total time: 28881.74s
                               ETA: 957189.7s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.275s, learning 0.175s)
               Value function loss: 124.8470
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 4.49
               Mean episode length: 53.82
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 8.45s
                        Total time: 28890.19s
                               ETA: 957133.1s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.257s, learning 0.165s)
               Value function loss: 297.9425
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 52.22
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 8.42s
                        Total time: 28898.62s
                               ETA: 957075.6s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.104s, learning 0.168s)
               Value function loss: 580.3603
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 3.57
               Mean episode length: 52.43
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 8.27s
                        Total time: 28906.89s
                               ETA: 957013.2s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1852 steps/s (collection: 8.674s, learning 0.169s)
               Value function loss: 305.6264
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 53.05
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 8.84s
                        Total time: 28915.73s
                               ETA: 956969.7s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.396s, learning 0.166s)
               Value function loss: 115.8169
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 21.51
               Mean episode length: 52.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 8.56s
                        Total time: 28924.29s
                               ETA: 956917.0s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.273s, learning 0.163s)
               Value function loss: 28.7173
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0246
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 8.44s
                        Total time: 28932.73s
                               ETA: 956860.1s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.159s)
               Value function loss: 35.1174
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.41
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0227
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 8.36s
                        Total time: 28941.09s
                               ETA: 956800.6s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.522s, learning 0.165s)
               Value function loss: 98.9846
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 16.20
               Mean episode length: 52.98
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 8.69s
                        Total time: 28949.77s
                               ETA: 956752.0s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.984s, learning 0.164s)
               Value function loss: 135.7810
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.79
               Mean episode length: 52.82
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 8.15s
                        Total time: 28957.92s
                               ETA: 956685.7s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.454s, learning 0.216s)
               Value function loss: 102.0727
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 13.46
               Mean episode length: 52.73
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0209
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 8.67s
                        Total time: 28966.59s
                               ETA: 956636.7s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.370s, learning 0.166s)
               Value function loss: 46.9272
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.92
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0203
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 8.54s
                        Total time: 28975.13s
                               ETA: 956583.3s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.326s, learning 0.166s)
               Value function loss: 171.0545
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 4.03
               Mean episode length: 54.53
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0193
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 8.49s
                        Total time: 28983.62s
                               ETA: 956528.5s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.166s, learning 0.161s)
               Value function loss: 269.4163
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.95
               Mean episode length: 54.12
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 8.33s
                        Total time: 28991.95s
                               ETA: 956468.2s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.201s, learning 0.165s)
               Value function loss: 37.7812
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 52.01
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 8.37s
                        Total time: 29000.31s
                               ETA: 956409.2s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.334s, learning 0.168s)
               Value function loss: 6.1000
                    Surrogate loss: 0.0051
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 53.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 8.50s
                        Total time: 29008.82s
                               ETA: 956354.8s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.101s, learning 0.211s)
               Value function loss: 6.7464
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 53.63
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 8.31s
                        Total time: 29017.13s
                               ETA: 956294.2s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.397s, learning 0.187s)
               Value function loss: 188.1791
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 52.30
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 8.58s
                        Total time: 29025.71s
                               ETA: 956242.5s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.167s)
               Value function loss: 6.9940
                    Surrogate loss: -0.0088
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 52.76
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 8.28s
                        Total time: 29033.99s
                               ETA: 956180.9s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.329s, learning 0.176s)
               Value function loss: 152.2652
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 3.33
               Mean episode length: 51.99
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 8.50s
                        Total time: 29042.50s
                               ETA: 956126.7s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.242s, learning 0.162s)
               Value function loss: 358.3049
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 16.22
               Mean episode length: 53.60
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 8.40s
                        Total time: 29050.90s
                               ETA: 956069.2s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.239s, learning 0.162s)
               Value function loss: 223.1622
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 53.17
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 8.40s
                        Total time: 29059.30s
                               ETA: 956011.6s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.248s, learning 0.170s)
               Value function loss: 25.0255
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 6.27
               Mean episode length: 53.42
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 8.42s
                        Total time: 29067.72s
                               ETA: 955954.6s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.301s, learning 0.173s)
               Value function loss: 109.4627
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 26.62
               Mean episode length: 53.70
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 8.47s
                        Total time: 29076.19s
                               ETA: 955899.6s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.407s, learning 0.164s)
               Value function loss: 17.8448
                    Surrogate loss: -0.0060
             Mean action noise std: 0.69
                       Mean reward: 10.61
               Mean episode length: 52.50
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 8.57s
                        Total time: 29084.76s
                               ETA: 955847.7s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.469s, learning 0.170s)
               Value function loss: 4.9634
                    Surrogate loss: -0.0142
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 52.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 8.64s
                        Total time: 29093.40s
                               ETA: 955798.1s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.189s, learning 0.168s)
               Value function loss: 61.3982
                    Surrogate loss: 0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 51.85
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 8.36s
                        Total time: 29101.76s
                               ETA: 955739.3s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.257s, learning 0.200s)
               Value function loss: 92.4345
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.44
               Mean episode length: 52.98
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 8.46s
                        Total time: 29110.22s
                               ETA: 955683.7s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.383s, learning 0.163s)
               Value function loss: 5.4331
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 8.10
               Mean episode length: 52.39
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 8.55s
                        Total time: 29118.76s
                               ETA: 955631.1s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.290s, learning 0.178s)
               Value function loss: 318.2358
                    Surrogate loss: 0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 53.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 8.47s
                        Total time: 29127.23s
                               ETA: 955576.1s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.475s, learning 0.175s)
               Value function loss: 12.7759
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 53.61
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 8.65s
                        Total time: 29135.88s
                               ETA: 955527.0s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.213s, learning 0.221s)
               Value function loss: 261.5072
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 10.65
               Mean episode length: 52.19
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 8.43s
                        Total time: 29144.32s
                               ETA: 955470.8s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.249s, learning 0.171s)
               Value function loss: 186.7155
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 54.67
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 8.42s
                        Total time: 29152.74s
                               ETA: 955414.3s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.350s, learning 0.209s)
               Value function loss: 11.2076
                    Surrogate loss: -0.0113
             Mean action noise std: 0.69
                       Mean reward: 13.74
               Mean episode length: 53.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 8.56s
                        Total time: 29161.30s
                               ETA: 955362.3s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.479s, learning 0.162s)
               Value function loss: 4.7497
                    Surrogate loss: -0.0110
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 52.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 8.64s
                        Total time: 29169.94s
                               ETA: 955313.0s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.283s, learning 0.210s)
               Value function loss: 367.9199
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 51.96
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 8.49s
                        Total time: 29178.43s
                               ETA: 955258.9s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.231s, learning 0.166s)
               Value function loss: 7.3327
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 21.17
               Mean episode length: 53.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 8.40s
                        Total time: 29186.83s
                               ETA: 955201.7s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.091s, learning 0.164s)
               Value function loss: 2.2955
                    Surrogate loss: -0.0132
             Mean action noise std: 0.69
                       Mean reward: 4.11
               Mean episode length: 54.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 8.25s
                        Total time: 29195.08s
                               ETA: 955139.8s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.065s, learning 0.163s)
               Value function loss: 1.1122
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 2.75
               Mean episode length: 51.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 8.23s
                        Total time: 29203.31s
                               ETA: 955077.2s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.960s, learning 0.172s)
               Value function loss: 132.2976
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 52.62
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 8.13s
                        Total time: 29211.44s
                               ETA: 955011.4s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.325s, learning 0.164s)
               Value function loss: 314.6904
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.87
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 8.49s
                        Total time: 29219.93s
                               ETA: 954957.4s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.385s, learning 0.169s)
               Value function loss: 18.2385
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 15.88
               Mean episode length: 52.47
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 8.55s
                        Total time: 29228.48s
                               ETA: 954905.4s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.377s, learning 0.163s)
               Value function loss: 4.5784
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 15.65
               Mean episode length: 53.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 8.54s
                        Total time: 29237.02s
                               ETA: 954853.1s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.371s, learning 0.164s)
               Value function loss: 220.3236
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 50.36
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 8.54s
                        Total time: 29245.56s
                               ETA: 954800.6s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.425s, learning 0.165s)
               Value function loss: 185.1141
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 52.69
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 8.59s
                        Total time: 29254.15s
                               ETA: 954750.0s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.432s, learning 0.172s)
               Value function loss: 253.9616
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 52.38
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 8.60s
                        Total time: 29262.75s
                               ETA: 954699.8s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.414s, learning 0.163s)
               Value function loss: 208.4876
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 51.75
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 8.58s
                        Total time: 29271.33s
                               ETA: 954648.8s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.434s, learning 0.173s)
               Value function loss: 42.8873
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 8.61s
                        Total time: 29279.94s
                               ETA: 954598.8s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.149s, learning 0.163s)
               Value function loss: 35.5385
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 52.56
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 8.31s
                        Total time: 29288.25s
                               ETA: 954539.2s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.238s, learning 0.227s)
               Value function loss: 11.5035
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 52.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 8.47s
                        Total time: 29296.72s
                               ETA: 954484.6s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.314s, learning 0.208s)
               Value function loss: 29.1403
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 52.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 8.52s
                        Total time: 29305.24s
                               ETA: 954431.9s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.489s, learning 0.209s)
               Value function loss: 14.7853
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 51.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 8.70s
                        Total time: 29313.93s
                               ETA: 954385.0s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.313s, learning 0.169s)
               Value function loss: 4.4276
                    Surrogate loss: -0.0126
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 50.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 8.48s
                        Total time: 29322.42s
                               ETA: 954331.1s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.579s, learning 0.158s)
               Value function loss: 7.5109
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 52.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 8.74s
                        Total time: 29331.15s
                               ETA: 954285.5s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.367s, learning 0.182s)
               Value function loss: 145.1843
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 3.64
               Mean episode length: 53.15
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 8.55s
                        Total time: 29339.70s
                               ETA: 954233.8s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.207s, learning 0.166s)
               Value function loss: 22.5940
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 53.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 8.37s
                        Total time: 29348.08s
                               ETA: 954176.4s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.165s, learning 0.175s)
               Value function loss: 2.4953
                    Surrogate loss: -0.0148
             Mean action noise std: 0.69
                       Mean reward: 3.84
               Mean episode length: 53.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 8.34s
                        Total time: 29356.42s
                               ETA: 954117.9s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.257s, learning 0.173s)
               Value function loss: 216.5099
                    Surrogate loss: 0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 53.27
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 8.43s
                        Total time: 29364.85s
                               ETA: 954062.5s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.197s, learning 0.163s)
               Value function loss: 368.7382
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 53.16
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 8.36s
                        Total time: 29373.21s
                               ETA: 954004.7s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.269s, learning 0.168s)
               Value function loss: 49.2696
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 51.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 8.44s
                        Total time: 29381.64s
                               ETA: 953949.6s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.412s, learning 0.167s)
               Value function loss: 68.0113
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 36.44
               Mean episode length: 53.06
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 8.58s
                        Total time: 29390.22s
                               ETA: 953899.0s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.245s, learning 0.175s)
               Value function loss: 159.2626
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 50.19
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 8.42s
                        Total time: 29398.64s
                               ETA: 953843.4s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.342s, learning 0.165s)
               Value function loss: 59.9488
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.39
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 8.51s
                        Total time: 29407.15s
                               ETA: 953790.5s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.345s, learning 0.166s)
               Value function loss: 117.1878
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 51.87
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 8.51s
                        Total time: 29415.66s
                               ETA: 953737.9s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1894 steps/s (collection: 8.478s, learning 0.169s)
               Value function loss: 63.1398
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 8.78
               Mean episode length: 53.45
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 8.65s
                        Total time: 29424.31s
                               ETA: 953689.6s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.112s, learning 0.170s)
               Value function loss: 42.4695
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 13.91
               Mean episode length: 53.29
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 8.28s
                        Total time: 29432.59s
                               ETA: 953629.6s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.128s, learning 0.160s)
               Value function loss: 463.7013
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.89
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 8.29s
                        Total time: 29440.88s
                               ETA: 953569.8s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.252s, learning 0.167s)
               Value function loss: 91.7779
                    Surrogate loss: -0.0088
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 51.61
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 8.42s
                        Total time: 29449.29s
                               ETA: 953514.3s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.540s, learning 0.163s)
               Value function loss: 38.0634
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 20.98
               Mean episode length: 53.63
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 8.70s
                        Total time: 29458.00s
                               ETA: 953468.0s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1877 steps/s (collection: 8.554s, learning 0.172s)
               Value function loss: 231.9243
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 13.83
               Mean episode length: 52.29
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 8.73s
                        Total time: 29466.72s
                               ETA: 953422.5s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.039s, learning 0.166s)
               Value function loss: 208.1773
                    Surrogate loss: 0.0054
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.72
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 8.21s
                        Total time: 29474.93s
                               ETA: 953360.1s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.308s, learning 0.167s)
               Value function loss: 428.9837
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 52.09
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 8.48s
                        Total time: 29483.40s
                               ETA: 953306.6s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.256s, learning 0.167s)
               Value function loss: 107.8791
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 11.14
               Mean episode length: 52.75
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 8.42s
                        Total time: 29491.83s
                               ETA: 953251.3s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.193s, learning 0.161s)
               Value function loss: 57.8635
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 9.20
               Mean episode length: 54.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 8.35s
                        Total time: 29500.18s
                               ETA: 953193.9s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.154s, learning 0.170s)
               Value function loss: 68.6989
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 52.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 8.32s
                        Total time: 29508.50s
                               ETA: 953135.5s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.165s)
               Value function loss: 210.9787
                    Surrogate loss: 0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 51.38
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 8.38s
                        Total time: 29516.89s
                               ETA: 953079.1s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.055s, learning 0.161s)
               Value function loss: 28.8473
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 3.44
               Mean episode length: 52.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 8.22s
                        Total time: 29525.10s
                               ETA: 953017.3s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.268s, learning 0.162s)
               Value function loss: 9.2626
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 52.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 8.43s
                        Total time: 29533.53s
                               ETA: 952962.4s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.115s, learning 0.167s)
               Value function loss: 4.4868
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 3.36
               Mean episode length: 51.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 8.28s
                        Total time: 29541.82s
                               ETA: 952902.9s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.179s, learning 0.170s)
               Value function loss: 80.3792
                    Surrogate loss: 0.0000
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 51.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 8.35s
                        Total time: 29550.16s
                               ETA: 952845.5s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.171s)
               Value function loss: 64.4000
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.06
               Mean episode length: 51.43
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 8.63s
                        Total time: 29558.80s
                               ETA: 952797.2s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1866 steps/s (collection: 8.614s, learning 0.162s)
               Value function loss: 18.5277
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 53.23
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 8.78s
                        Total time: 29567.57s
                               ETA: 952753.6s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.055s, learning 0.168s)
               Value function loss: 8.6072
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 52.29
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 8.22s
                        Total time: 29575.80s
                               ETA: 952692.3s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.366s, learning 0.166s)
               Value function loss: 3.8450
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 3.06
               Mean episode length: 52.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 8.53s
                        Total time: 29584.33s
                               ETA: 952640.9s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.159s, learning 0.160s)
               Value function loss: 4.7104
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 3.36
               Mean episode length: 51.80
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 8.32s
                        Total time: 29592.65s
                               ETA: 952582.6s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.304s, learning 0.161s)
               Value function loss: 106.6543
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 51.36
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 8.47s
                        Total time: 29601.11s
                               ETA: 952529.2s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.062s, learning 0.164s)
               Value function loss: 4.2044
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 52.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 8.23s
                        Total time: 29609.34s
                               ETA: 952468.1s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.417s, learning 0.167s)
               Value function loss: 1.7190
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 10.80
               Mean episode length: 52.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 8.58s
                        Total time: 29617.92s
                               ETA: 952418.5s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.377s, learning 0.165s)
               Value function loss: 1.9771
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 52.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 8.54s
                        Total time: 29626.46s
                               ETA: 952367.6s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.106s, learning 0.167s)
               Value function loss: 127.9310
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 53.49
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 8.27s
                        Total time: 29634.74s
                               ETA: 952308.0s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.059s, learning 0.181s)
               Value function loss: 1.8991
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 51.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 8.24s
                        Total time: 29642.98s
                               ETA: 952247.5s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.141s, learning 0.161s)
               Value function loss: 90.9471
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 51.67
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 8.30s
                        Total time: 29651.28s
                               ETA: 952189.0s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.491s, learning 0.177s)
               Value function loss: 60.7985
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 52.63
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 8.67s
                        Total time: 29659.95s
                               ETA: 952142.2s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.223s, learning 0.165s)
               Value function loss: 146.3491
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 52.84
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 8.39s
                        Total time: 29668.33s
                               ETA: 952086.5s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.257s, learning 0.165s)
               Value function loss: 220.7179
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 52.46
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 8.42s
                        Total time: 29676.76s
                               ETA: 952031.9s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.118s, learning 0.162s)
               Value function loss: 12.6563
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.72
               Mean episode length: 50.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 8.28s
                        Total time: 29685.04s
                               ETA: 951972.8s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.209s, learning 0.167s)
               Value function loss: 2.9668
                    Surrogate loss: -0.0125
             Mean action noise std: 0.69
                       Mean reward: 5.55
               Mean episode length: 51.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 8.38s
                        Total time: 29693.41s
                               ETA: 951916.8s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.193s, learning 0.163s)
               Value function loss: 57.2526
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 52.79
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 8.36s
                        Total time: 29701.77s
                               ETA: 951860.2s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.160s, learning 0.166s)
               Value function loss: 19.6211
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 52.55
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 8.33s
                        Total time: 29710.10s
                               ETA: 951802.7s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.269s, learning 0.222s)
               Value function loss: 21.7772
                    Surrogate loss: 0.0036
             Mean action noise std: 0.69
                       Mean reward: 13.11
               Mean episode length: 52.18
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 8.49s
                        Total time: 29718.59s
                               ETA: 951750.5s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.291s, learning 0.161s)
               Value function loss: 243.1997
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 51.93
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 8.45s
                        Total time: 29727.04s
                               ETA: 951697.1s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.007s, learning 0.173s)
               Value function loss: 379.6582
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 51.86
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 8.18s
                        Total time: 29735.22s
                               ETA: 951635.0s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.218s, learning 0.159s)
               Value function loss: 98.2848
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 18.14
               Mean episode length: 52.42
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 8.38s
                        Total time: 29743.60s
                               ETA: 951579.2s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1209 steps/s (collection: 13.333s, learning 0.210s)
               Value function loss: 9.4962
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 52.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 13.54s
                        Total time: 29757.14s
                               ETA: 951688.6s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.105s, learning 0.175s)
               Value function loss: 145.8725
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 52.36
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 16.28s
                        Total time: 29773.42s
                               ETA: 951885.5s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.852s, learning 0.176s)
               Value function loss: 481.4402
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 52.31
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 16.03s
                        Total time: 29789.44s
                               ETA: 952074.2s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.032s, learning 0.170s)
               Value function loss: 178.2746
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 51.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 16.20s
                        Total time: 29805.65s
                               ETA: 952268.3s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.066s, learning 0.264s)
               Value function loss: 37.9318
                    Surrogate loss: -0.0101
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 51.55
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 16.33s
                        Total time: 29821.98s
                               ETA: 952466.4s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.078s, learning 0.174s)
               Value function loss: 20.1057
                    Surrogate loss: -0.0119
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 51.22
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 16.25s
                        Total time: 29838.23s
                               ETA: 952661.8s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.012s, learning 0.178s)
               Value function loss: 290.4302
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 52.04
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 16.19s
                        Total time: 29854.42s
                               ETA: 952855.2s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.680s, learning 0.168s)
               Value function loss: 173.9705
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 53.63
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 15.85s
                        Total time: 29870.27s
                               ETA: 953037.5s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1029 steps/s (collection: 15.729s, learning 0.186s)
               Value function loss: 111.8400
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 52.89
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 15.91s
                        Total time: 29886.18s
                               ETA: 953221.7s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.181s, learning 0.172s)
               Value function loss: 102.3580
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 53.01
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 16.35s
                        Total time: 29902.53s
                               ETA: 953419.9s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 997 steps/s (collection: 16.257s, learning 0.162s)
               Value function loss: 204.8095
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 8.46
               Mean episode length: 52.86
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 16.42s
                        Total time: 29918.95s
                               ETA: 953620.0s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.028s, learning 0.173s)
               Value function loss: 301.1316
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 2.80
               Mean episode length: 51.65
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 16.20s
                        Total time: 29935.16s
                               ETA: 953813.0s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.224s, learning 0.168s)
               Value function loss: 201.2237
                    Surrogate loss: -0.0060
             Mean action noise std: 0.69
                       Mean reward: 3.88
               Mean episode length: 53.23
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 16.39s
                        Total time: 29951.55s
                               ETA: 954011.9s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.852s, learning 0.171s)
               Value function loss: 166.8731
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 52.57
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 16.02s
                        Total time: 29967.57s
                               ETA: 954199.0s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.099s, learning 0.170s)
               Value function loss: 15.7377
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 53.10
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 16.27s
                        Total time: 29983.84s
                               ETA: 954393.7s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.294s, learning 0.162s)
               Value function loss: 374.3186
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 15.96
               Mean episode length: 52.84
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 16.46s
                        Total time: 30000.30s
                               ETA: 954594.3s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.046s, learning 0.168s)
               Value function loss: 14.5203
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 51.46
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 16.21s
                        Total time: 30016.51s
                               ETA: 954787.0s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.353s, learning 0.170s)
               Value function loss: 240.3390
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 51.49
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 16.52s
                        Total time: 30033.03s
                               ETA: 954989.4s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.094s, learning 0.166s)
               Value function loss: 9.6507
                    Surrogate loss: -0.0106
             Mean action noise std: 0.69
                       Mean reward: 3.76
               Mean episode length: 53.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 16.26s
                        Total time: 30049.29s
                               ETA: 955183.3s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.007s, learning 0.168s)
               Value function loss: 51.5832
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 18.70
               Mean episode length: 52.60
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0130
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 16.17s
                        Total time: 30065.47s
                               ETA: 955374.4s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.112s, learning 0.164s)
               Value function loss: 148.0571
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 8.51
               Mean episode length: 52.24
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 16.28s
                        Total time: 30081.75s
                               ETA: 955568.5s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.297s, learning 0.163s)
               Value function loss: 500.0120
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 50.90
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 16.46s
                        Total time: 30098.21s
                               ETA: 955768.4s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.088s, learning 0.218s)
               Value function loss: 117.4636
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 10.88
               Mean episode length: 52.03
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 16.31s
                        Total time: 30114.51s
                               ETA: 955963.2s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.039s, learning 0.162s)
               Value function loss: 302.4145
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 52.92
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 16.20s
                        Total time: 30130.71s
                               ETA: 956154.5s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.229s, learning 0.213s)
               Value function loss: 141.1521
                    Surrogate loss: 0.0184
             Mean action noise std: 0.69
                       Mean reward: 8.75
               Mean episode length: 53.78
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 16.44s
                        Total time: 30147.15s
                               ETA: 956353.4s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.180s, learning 0.169s)
               Value function loss: 62.0644
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 3.50
               Mean episode length: 53.45
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 16.35s
                        Total time: 30163.50s
                               ETA: 956549.1s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.018s, learning 0.178s)
               Value function loss: 50.3856
                    Surrogate loss: 0.0071
             Mean action noise std: 0.69
                       Mean reward: 6.19
               Mean episode length: 52.90
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 16.20s
                        Total time: 30179.70s
                               ETA: 956739.9s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.079s, learning 0.247s)
               Value function loss: 315.0801
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.23
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 16.33s
                        Total time: 30196.03s
                               ETA: 956934.7s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.180s, learning 0.162s)
               Value function loss: 318.3979
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.02
               Mean episode length: 51.19
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 16.34s
                        Total time: 30212.37s
                               ETA: 957129.8s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.916s, learning 0.211s)
               Value function loss: 524.7105
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 52.10
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 16.13s
                        Total time: 30228.49s
                               ETA: 957318.0s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.030s, learning 0.165s)
               Value function loss: 129.3939
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 26.54
               Mean episode length: 53.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 16.20s
                        Total time: 30244.69s
                               ETA: 957508.2s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.077s, learning 0.173s)
               Value function loss: 171.8278
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 50.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0196
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 16.25s
                        Total time: 30260.94s
                               ETA: 957700.0s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.386s, learning 0.164s)
               Value function loss: 174.9524
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 3.36
               Mean episode length: 52.47
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 16.55s
                        Total time: 30277.49s
                               ETA: 957901.1s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.125s, learning 0.167s)
               Value function loss: 240.2119
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 54.06
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 16.29s
                        Total time: 30293.78s
                               ETA: 958094.0s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.123s, learning 0.167s)
               Value function loss: 49.5613
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 53.20
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 16.29s
                        Total time: 30310.07s
                               ETA: 958286.7s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1044 steps/s (collection: 15.523s, learning 0.166s)
               Value function loss: 221.4383
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 51.18
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 15.69s
                        Total time: 30325.76s
                               ETA: 958460.2s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.920s, learning 0.160s)
               Value function loss: 325.1157
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.13
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 16.08s
                        Total time: 30341.84s
                               ETA: 958646.0s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.147s, learning 0.161s)
               Value function loss: 206.1924
                    Surrogate loss: -0.0088
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.30
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 16.31s
                        Total time: 30358.15s
                               ETA: 958838.8s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.226s, learning 0.163s)
               Value function loss: 360.1708
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 23.42
               Mean episode length: 51.93
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 8.39s
                        Total time: 30366.54s
                               ETA: 958781.5s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.137s, learning 0.210s)
               Value function loss: 445.3284
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 52.96
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 8.35s
                        Total time: 30374.89s
                               ETA: 958722.9s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.877s, learning 0.214s)
               Value function loss: 193.9001
                    Surrogate loss: -0.0087
             Mean action noise std: 0.69
                       Mean reward: 12.98
               Mean episode length: 50.71
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0211
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 8.09s
                        Total time: 30382.98s
                               ETA: 958656.2s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.165s)
               Value function loss: 45.0925
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 30.78
               Mean episode length: 51.37
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 8.34s
                        Total time: 30391.32s
                               ETA: 958597.5s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.097s, learning 0.170s)
               Value function loss: 65.9778
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 11.04
               Mean episode length: 52.12
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 8.27s
                        Total time: 30399.59s
                               ETA: 958536.4s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.288s, learning 0.165s)
               Value function loss: 62.3974
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 51.47
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 8.45s
                        Total time: 30408.04s
                               ETA: 958481.3s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.046s, learning 0.161s)
               Value function loss: 83.0512
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 51.61
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 8.21s
                        Total time: 30416.25s
                               ETA: 958418.4s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.221s, learning 0.161s)
               Value function loss: 10.2374
                    Surrogate loss: -0.0113
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 53.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 8.38s
                        Total time: 30424.63s
                               ETA: 958361.0s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.096s, learning 0.233s)
               Value function loss: 5.8392
                    Surrogate loss: -0.0123
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 51.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 8.33s
                        Total time: 30432.96s
                               ETA: 958302.1s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.230s, learning 0.166s)
               Value function loss: 167.6601
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 51.97
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 8.40s
                        Total time: 30441.35s
                               ETA: 958245.2s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.234s, learning 0.209s)
               Value function loss: 447.6835
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.49
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 8.44s
                        Total time: 30449.80s
                               ETA: 958189.9s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.406s, learning 0.212s)
               Value function loss: 18.3223
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 52.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 8.62s
                        Total time: 30458.42s
                               ETA: 958140.1s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.390s, learning 0.219s)
               Value function loss: 9.7269
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 20.83
               Mean episode length: 51.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 8.61s
                        Total time: 30467.03s
                               ETA: 958090.1s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.944s, learning 0.166s)
               Value function loss: 3.9835
                    Surrogate loss: -0.0151
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 53.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 8.11s
                        Total time: 30475.14s
                               ETA: 958024.4s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.241s, learning 0.173s)
               Value function loss: 204.2504
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 52.96
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 8.41s
                        Total time: 30483.55s
                               ETA: 957968.3s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.492s, learning 0.166s)
               Value function loss: 173.6211
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 50.43
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 8.66s
                        Total time: 30492.21s
                               ETA: 957919.9s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.157s, learning 0.175s)
               Value function loss: 368.5543
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 13.24
               Mean episode length: 51.45
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 8.33s
                        Total time: 30500.54s
                               ETA: 957861.2s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 2025 steps/s (collection: 7.928s, learning 0.161s)
               Value function loss: 402.8560
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.38
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 8.09s
                        Total time: 30508.63s
                               ETA: 957795.0s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.261s, learning 0.159s)
               Value function loss: 606.5472
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.64
               Mean episode length: 52.69
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 8.42s
                        Total time: 30517.05s
                               ETA: 957739.2s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.106s, learning 0.160s)
               Value function loss: 71.3695
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 13.21
               Mean episode length: 51.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 8.27s
                        Total time: 30525.31s
                               ETA: 957678.6s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.011s, learning 0.204s)
               Value function loss: 19.2688
                    Surrogate loss: -0.0063
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 51.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 8.21s
                        Total time: 30533.53s
                               ETA: 957616.5s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.067s, learning 0.166s)
               Value function loss: 18.1200
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 15.69
               Mean episode length: 51.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 8.23s
                        Total time: 30541.76s
                               ETA: 957554.9s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.150s, learning 0.161s)
               Value function loss: 148.4393
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 53.30
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 8.31s
                        Total time: 30550.07s
                               ETA: 957495.8s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.291s, learning 0.175s)
               Value function loss: 12.7635
                    Surrogate loss: -0.0132
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 51.92
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 8.47s
                        Total time: 30558.54s
                               ETA: 957441.6s

################################################################################
                    [1m Learning iteration 3093/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.328s, learning 0.167s)
               Value function loss: 11.5254
                    Surrogate loss: -0.0067
             Mean action noise std: 0.69
                       Mean reward: 13.24
               Mean episode length: 53.14
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 50692096
                    Iteration time: 8.49s
                        Total time: 30567.03s
                               ETA: 957388.3s

################################################################################
                    [1m Learning iteration 3094/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.153s, learning 0.224s)
               Value function loss: 195.7530
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 5.62
               Mean episode length: 50.40
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 50708480
                    Iteration time: 8.38s
                        Total time: 30575.41s
                               ETA: 957331.4s

################################################################################
                    [1m Learning iteration 3095/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.163s)
               Value function loss: 3.4045
                    Surrogate loss: -0.0160
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 50724864
                    Iteration time: 8.33s
                        Total time: 30583.73s
                               ETA: 957272.9s

################################################################################
                    [1m Learning iteration 3096/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.000s, learning 0.165s)
               Value function loss: 4.1133
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 13.24
               Mean episode length: 52.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 50741248
                    Iteration time: 8.16s
                        Total time: 30591.90s
                               ETA: 957209.4s

################################################################################
                    [1m Learning iteration 3097/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.159s, learning 0.176s)
               Value function loss: 2.9120
                    Surrogate loss: -0.0147
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 50.62
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 50757632
                    Iteration time: 8.33s
                        Total time: 30600.23s
                               ETA: 957151.2s

################################################################################
                    [1m Learning iteration 3098/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.030s, learning 0.160s)
               Value function loss: 163.7298
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 51.25
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 50774016
                    Iteration time: 8.19s
                        Total time: 30608.42s
                               ETA: 957088.6s

################################################################################
                    [1m Learning iteration 3099/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.275s, learning 0.165s)
               Value function loss: 384.4199
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.88
               Mean episode length: 50.52
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 50790400
                    Iteration time: 8.44s
                        Total time: 30616.86s
                               ETA: 957033.8s

################################################################################
                    [1m Learning iteration 3100/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.030s, learning 0.214s)
               Value function loss: 2.1454
                    Surrogate loss: -0.0178
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 51.49
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 50806784
                    Iteration time: 8.24s
                        Total time: 30625.11s
                               ETA: 956972.9s

################################################################################
                    [1m Learning iteration 3101/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.150s, learning 0.171s)
               Value function loss: 712.7014
                    Surrogate loss: 0.0086
             Mean action noise std: 0.69
                       Mean reward: 18.40
               Mean episode length: 52.67
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 50823168
                    Iteration time: 8.32s
                        Total time: 30633.43s
                               ETA: 956914.5s

################################################################################
                    [1m Learning iteration 3102/100000 [0m                    

                       Computation: 2004 steps/s (collection: 8.011s, learning 0.162s)
               Value function loss: 32.5181
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 51.81
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 50839552
                    Iteration time: 8.17s
                        Total time: 30641.60s
                               ETA: 956851.4s

################################################################################
                    [1m Learning iteration 3103/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.300s, learning 0.250s)
               Value function loss: 236.9150
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 52.18
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 50855936
                    Iteration time: 8.55s
                        Total time: 30650.15s
                               ETA: 956800.2s

################################################################################
                    [1m Learning iteration 3104/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.261s, learning 0.168s)
               Value function loss: 119.5699
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 13.26
               Mean episode length: 50.65
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 50872320
                    Iteration time: 8.43s
                        Total time: 30658.58s
                               ETA: 956745.2s

################################################################################
                    [1m Learning iteration 3105/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.172s, learning 0.190s)
               Value function loss: 5.9488
                    Surrogate loss: -0.0119
             Mean action noise std: 0.69
                       Mean reward: 13.04
               Mean episode length: 51.08
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 50888704
                    Iteration time: 8.36s
                        Total time: 30666.94s
                               ETA: 956688.2s

################################################################################
                    [1m Learning iteration 3106/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.419s, learning 0.166s)
               Value function loss: 2.2522
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 50.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 50905088
                    Iteration time: 8.59s
                        Total time: 30675.53s
                               ETA: 956638.1s

################################################################################
                    [1m Learning iteration 3107/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.177s, learning 0.161s)
               Value function loss: 447.3181
                    Surrogate loss: 0.0008
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 50.72
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 50921472
                    Iteration time: 8.34s
                        Total time: 30683.87s
                               ETA: 956580.4s

################################################################################
                    [1m Learning iteration 3108/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.498s, learning 0.172s)
               Value function loss: 48.1287
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 52.82
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 50937856
                    Iteration time: 8.67s
                        Total time: 30692.54s
                               ETA: 956533.0s

################################################################################
                    [1m Learning iteration 3109/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.259s, learning 0.159s)
               Value function loss: 3.7445
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 52.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 50954240
                    Iteration time: 8.42s
                        Total time: 30700.95s
                               ETA: 956477.9s

################################################################################
                    [1m Learning iteration 3110/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.287s, learning 0.202s)
               Value function loss: 2.7242
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 53.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 50970624
                    Iteration time: 8.49s
                        Total time: 30709.44s
                               ETA: 956424.9s

################################################################################
                    [1m Learning iteration 3111/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.286s, learning 0.261s)
               Value function loss: 234.5663
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 51.50
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 50987008
                    Iteration time: 8.55s
                        Total time: 30717.99s
                               ETA: 956373.9s

################################################################################
                    [1m Learning iteration 3112/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.290s, learning 0.207s)
               Value function loss: 125.3168
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 16.03
               Mean episode length: 52.80
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 51003392
                    Iteration time: 8.50s
                        Total time: 30726.49s
                               ETA: 956321.2s

################################################################################
                    [1m Learning iteration 3113/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.181s, learning 0.214s)
               Value function loss: 91.7734
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 51.10
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 51019776
                    Iteration time: 8.40s
                        Total time: 30734.88s
                               ETA: 956265.5s

################################################################################
                    [1m Learning iteration 3114/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.452s, learning 0.207s)
               Value function loss: 228.2307
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 50.97
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 51036160
                    Iteration time: 8.66s
                        Total time: 30743.54s
                               ETA: 956218.0s

################################################################################
                    [1m Learning iteration 3115/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.222s, learning 0.258s)
               Value function loss: 18.9710
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.50
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 51052544
                    Iteration time: 8.48s
                        Total time: 30752.02s
                               ETA: 956164.9s

################################################################################
                    [1m Learning iteration 3116/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.290s, learning 0.165s)
               Value function loss: 2.7721
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 51.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 51068928
                    Iteration time: 8.45s
                        Total time: 30760.48s
                               ETA: 956111.1s

################################################################################
                    [1m Learning iteration 3117/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.164s)
               Value function loss: 2.1361
                    Surrogate loss: -0.0145
             Mean action noise std: 0.69
                       Mean reward: 5.53
               Mean episode length: 51.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 51085312
                    Iteration time: 8.34s
                        Total time: 30768.82s
                               ETA: 956053.8s

################################################################################
                    [1m Learning iteration 3118/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.150s, learning 0.162s)
               Value function loss: 381.8140
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 52.44
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 51101696
                    Iteration time: 8.31s
                        Total time: 30777.13s
                               ETA: 955995.6s

################################################################################
                    [1m Learning iteration 3119/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.374s, learning 0.269s)
               Value function loss: 151.4581
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 51.73
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 51118080
                    Iteration time: 8.64s
                        Total time: 30785.78s
                               ETA: 955947.7s

################################################################################
                    [1m Learning iteration 3120/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.077s, learning 0.213s)
               Value function loss: 4.6076
                    Surrogate loss: -0.0132
             Mean action noise std: 0.69
                       Mean reward: 3.74
               Mean episode length: 53.75
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 51134464
                    Iteration time: 8.29s
                        Total time: 30794.07s
                               ETA: 955888.8s

################################################################################
                    [1m Learning iteration 3121/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.164s)
               Value function loss: 137.9805
                    Surrogate loss: 0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 52.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 51150848
                    Iteration time: 8.53s
                        Total time: 30802.60s
                               ETA: 955837.5s

################################################################################
                    [1m Learning iteration 3122/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.173s, learning 0.172s)
               Value function loss: 150.0287
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 52.54
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 51167232
                    Iteration time: 8.34s
                        Total time: 30810.94s
                               ETA: 955780.4s

################################################################################
                    [1m Learning iteration 3123/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.161s)
               Value function loss: 3.0476
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 52.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0096
--------------------------------------------------------------------------------
                   Total timesteps: 51183616
                    Iteration time: 8.37s
                        Total time: 30819.31s
                               ETA: 955724.1s

################################################################################
                    [1m Learning iteration 3124/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.004s, learning 0.165s)
               Value function loss: 58.4717
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 51.96
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 51200000
                    Iteration time: 8.17s
                        Total time: 30827.48s
                               ETA: 955661.7s

################################################################################
                    [1m Learning iteration 3125/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.169s, learning 0.163s)
               Value function loss: 2.5585
                    Surrogate loss: -0.0176
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 50.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 51216384
                    Iteration time: 8.33s
                        Total time: 30835.81s
                               ETA: 955604.3s

################################################################################
                    [1m Learning iteration 3126/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.212s, learning 0.176s)
               Value function loss: 60.8694
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 50.48
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 51232768
                    Iteration time: 8.39s
                        Total time: 30844.20s
                               ETA: 955548.7s

################################################################################
                    [1m Learning iteration 3127/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.174s, learning 0.162s)
               Value function loss: 403.5107
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 8.27
               Mean episode length: 52.32
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 51249152
                    Iteration time: 8.34s
                        Total time: 30852.53s
                               ETA: 955491.5s

################################################################################
                    [1m Learning iteration 3128/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.168s, learning 0.173s)
               Value function loss: 124.2890
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 51.13
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0083
--------------------------------------------------------------------------------
                   Total timesteps: 51265536
                    Iteration time: 8.34s
                        Total time: 30860.87s
                               ETA: 955434.5s

################################################################################
                    [1m Learning iteration 3129/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.053s, learning 0.167s)
               Value function loss: 64.1586
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 51.61
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 51281920
                    Iteration time: 8.22s
                        Total time: 30869.09s
                               ETA: 955373.8s

################################################################################
                    [1m Learning iteration 3130/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.200s, learning 0.173s)
               Value function loss: 115.2635
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 8.35
               Mean episode length: 51.04
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 51298304
                    Iteration time: 8.37s
                        Total time: 30877.47s
                               ETA: 955317.8s

################################################################################
                    [1m Learning iteration 3131/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.063s, learning 0.177s)
               Value function loss: 5.4999
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 50.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 51314688
                    Iteration time: 8.24s
                        Total time: 30885.71s
                               ETA: 955257.8s

################################################################################
                    [1m Learning iteration 3132/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.500s, learning 0.170s)
               Value function loss: 5.7379
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 52.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 51331072
                    Iteration time: 8.67s
                        Total time: 30894.38s
                               ETA: 955211.1s

################################################################################
                    [1m Learning iteration 3133/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.269s, learning 0.167s)
               Value function loss: 7.0339
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 2.54
               Mean episode length: 49.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 51347456
                    Iteration time: 8.44s
                        Total time: 30902.81s
                               ETA: 955157.2s

################################################################################
                    [1m Learning iteration 3134/100000 [0m                    

                       Computation: 1868 steps/s (collection: 8.518s, learning 0.249s)
               Value function loss: 242.6148
                    Surrogate loss: 0.0000
             Mean action noise std: 0.69
                       Mean reward: 2.91
               Mean episode length: 50.73
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 51363840
                    Iteration time: 8.77s
                        Total time: 30911.58s
                               ETA: 955113.6s

################################################################################
                    [1m Learning iteration 3135/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.119s, learning 0.166s)
               Value function loss: 372.8722
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 50.87
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 51380224
                    Iteration time: 8.28s
                        Total time: 30919.86s
                               ETA: 955055.0s

################################################################################
                    [1m Learning iteration 3136/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.335s, learning 0.161s)
               Value function loss: 55.0339
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 51.11
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 51396608
                    Iteration time: 8.50s
                        Total time: 30928.36s
                               ETA: 955003.1s

################################################################################
                    [1m Learning iteration 3137/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.246s, learning 0.178s)
               Value function loss: 4.6761
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 38.32
               Mean episode length: 51.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 51412992
                    Iteration time: 8.42s
                        Total time: 30936.79s
                               ETA: 954949.0s

################################################################################
                    [1m Learning iteration 3138/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.073s, learning 0.165s)
               Value function loss: 805.4066
                    Surrogate loss: 0.0075
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 51.50
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 51429376
                    Iteration time: 8.24s
                        Total time: 30945.02s
                               ETA: 954889.1s

################################################################################
                    [1m Learning iteration 3139/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.106s, learning 0.188s)
               Value function loss: 8.4369
                    Surrogate loss: -0.0111
             Mean action noise std: 0.69
                       Mean reward: 2.69
               Mean episode length: 50.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 51445760
                    Iteration time: 8.29s
                        Total time: 30953.32s
                               ETA: 954831.0s

################################################################################
                    [1m Learning iteration 3140/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.342s, learning 0.173s)
               Value function loss: 3.0971
                    Surrogate loss: -0.0118
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 51.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 51462144
                    Iteration time: 8.52s
                        Total time: 30961.83s
                               ETA: 954779.8s

################################################################################
                    [1m Learning iteration 3141/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.265s, learning 0.171s)
               Value function loss: 2.7303
                    Surrogate loss: -0.0169
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 50.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 51478528
                    Iteration time: 8.44s
                        Total time: 30970.27s
                               ETA: 954726.1s

################################################################################
                    [1m Learning iteration 3142/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.436s, learning 0.173s)
               Value function loss: 1.2309
                    Surrogate loss: -0.0180
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 51.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 51494912
                    Iteration time: 8.61s
                        Total time: 30978.88s
                               ETA: 954677.8s

################################################################################
                    [1m Learning iteration 3143/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.297s, learning 0.161s)
               Value function loss: 334.4322
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 51.01
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 51511296
                    Iteration time: 8.46s
                        Total time: 30987.34s
                               ETA: 954624.8s

################################################################################
                    [1m Learning iteration 3144/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.439s, learning 0.175s)
               Value function loss: 1.4028
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 51.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 51527680
                    Iteration time: 8.61s
                        Total time: 30995.95s
                               ETA: 954576.7s

################################################################################
                    [1m Learning iteration 3145/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.178s, learning 0.169s)
               Value function loss: 1.0698
                    Surrogate loss: -0.0185
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 50.98
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 51544064
                    Iteration time: 8.35s
                        Total time: 31004.30s
                               ETA: 954520.4s

################################################################################
                    [1m Learning iteration 3146/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.389s, learning 0.163s)
               Value function loss: 1.0274
                    Surrogate loss: -0.0180
             Mean action noise std: 0.69
                       Mean reward: 2.86
               Mean episode length: 51.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 51560448
                    Iteration time: 8.55s
                        Total time: 31012.85s
                               ETA: 954470.4s

################################################################################
                    [1m Learning iteration 3147/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.263s, learning 0.179s)
               Value function loss: 1.0496
                    Surrogate loss: -0.0138
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 51.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 51576832
                    Iteration time: 8.44s
                        Total time: 31021.29s
                               ETA: 954417.1s

################################################################################
                    [1m Learning iteration 3148/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.161s)
               Value function loss: 0.4372
                    Surrogate loss: -0.0212
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 50.92
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 51593216
                    Iteration time: 8.39s
                        Total time: 31029.69s
                               ETA: 954362.4s

################################################################################
                    [1m Learning iteration 3149/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.176s, learning 0.206s)
               Value function loss: 0.4671
                    Surrogate loss: -0.0137
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 52.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 51609600
                    Iteration time: 8.38s
                        Total time: 31038.07s
                               ETA: 954307.2s

################################################################################
                    [1m Learning iteration 3150/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.451s, learning 0.177s)
               Value function loss: 0.4980
                    Surrogate loss: -0.0234
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 50.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 51625984
                    Iteration time: 8.63s
                        Total time: 31046.69s
                               ETA: 954259.7s

################################################################################
                    [1m Learning iteration 3151/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.223s, learning 0.186s)
               Value function loss: 0.4992
                    Surrogate loss: -0.0168
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 52.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 51642368
                    Iteration time: 8.41s
                        Total time: 31055.10s
                               ETA: 954205.5s

################################################################################
                    [1m Learning iteration 3152/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.175s, learning 0.170s)
               Value function loss: 232.4752
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 51.81
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 51658752
                    Iteration time: 8.34s
                        Total time: 31063.45s
                               ETA: 954149.3s

################################################################################
                    [1m Learning iteration 3153/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.190s, learning 0.160s)
               Value function loss: 4.3103
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.95
               Mean episode length: 51.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 51675136
                    Iteration time: 8.35s
                        Total time: 31071.80s
                               ETA: 954093.4s

################################################################################
                    [1m Learning iteration 3154/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.167s)
               Value function loss: 0.4400
                    Surrogate loss: -0.0235
             Mean action noise std: 0.69
                       Mean reward: 15.63
               Mean episode length: 51.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 51691520
                    Iteration time: 8.45s
                        Total time: 31080.25s
                               ETA: 954040.6s

################################################################################
                    [1m Learning iteration 3155/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.359s, learning 0.173s)
               Value function loss: 0.3535
                    Surrogate loss: -0.0241
             Mean action noise std: 0.69
                       Mean reward: 7.94
               Mean episode length: 51.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 51707904
                    Iteration time: 8.53s
                        Total time: 31088.78s
                               ETA: 953990.2s

################################################################################
                    [1m Learning iteration 3156/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.284s, learning 0.169s)
               Value function loss: 0.3731
                    Surrogate loss: -0.0153
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 51.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 51724288
                    Iteration time: 8.45s
                        Total time: 31097.24s
                               ETA: 953937.5s

################################################################################
                    [1m Learning iteration 3157/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.334s, learning 0.166s)
               Value function loss: 0.4172
                    Surrogate loss: -0.0153
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 51.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 51740672
                    Iteration time: 8.50s
                        Total time: 31105.73s
                               ETA: 953886.2s

################################################################################
                    [1m Learning iteration 3158/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.325s, learning 0.167s)
               Value function loss: 0.4826
                    Surrogate loss: -0.0171
             Mean action noise std: 0.69
                       Mean reward: 2.81
               Mean episode length: 50.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 51757056
                    Iteration time: 8.49s
                        Total time: 31114.23s
                               ETA: 953834.7s

################################################################################
                    [1m Learning iteration 3159/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.382s, learning 0.167s)
               Value function loss: 0.3949
                    Surrogate loss: -0.0222
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.67
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 51773440
                    Iteration time: 8.55s
                        Total time: 31122.78s
                               ETA: 953785.0s

################################################################################
                    [1m Learning iteration 3160/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.154s, learning 0.166s)
               Value function loss: 0.3958
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 53.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 51789824
                    Iteration time: 8.32s
                        Total time: 31131.10s
                               ETA: 953728.3s

################################################################################
                    [1m Learning iteration 3161/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.426s, learning 0.163s)
               Value function loss: 446.4441
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 51.16
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 51806208
                    Iteration time: 8.59s
                        Total time: 31139.68s
                               ETA: 953679.9s

################################################################################
                    [1m Learning iteration 3162/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.253s, learning 0.167s)
               Value function loss: 632.7990
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 53.44
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 51822592
                    Iteration time: 8.42s
                        Total time: 31148.10s
                               ETA: 953626.3s

################################################################################
                    [1m Learning iteration 3163/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.172s, learning 0.165s)
               Value function loss: 109.4511
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.96
               Mean episode length: 53.68
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 51838976
                    Iteration time: 8.34s
                        Total time: 31156.44s
                               ETA: 953570.3s

################################################################################
                    [1m Learning iteration 3164/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.222s, learning 0.159s)
               Value function loss: 9.4795
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 53.43
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 51855360
                    Iteration time: 8.38s
                        Total time: 31164.82s
                               ETA: 953515.6s

################################################################################
                    [1m Learning iteration 3165/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.156s, learning 0.167s)
               Value function loss: 2.7304
                    Surrogate loss: -0.0136
             Mean action noise std: 0.69
                       Mean reward: 23.94
               Mean episode length: 53.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 51871744
                    Iteration time: 8.32s
                        Total time: 31173.15s
                               ETA: 953459.1s

################################################################################
                    [1m Learning iteration 3166/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.161s)
               Value function loss: 2.0296
                    Surrogate loss: -0.0140
             Mean action noise std: 0.69
                       Mean reward: 6.07
               Mean episode length: 52.57
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 51888128
                    Iteration time: 8.46s
                        Total time: 31181.61s
                               ETA: 953407.0s

################################################################################
                    [1m Learning iteration 3167/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.105s, learning 0.189s)
               Value function loss: 1.0126
                    Surrogate loss: -0.0176
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 51.86
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0084
--------------------------------------------------------------------------------
                   Total timesteps: 51904512
                    Iteration time: 8.29s
                        Total time: 31189.90s
                               ETA: 953349.7s

################################################################################
                    [1m Learning iteration 3168/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.185s, learning 0.161s)
               Value function loss: 48.4222
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 52.11
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 51920896
                    Iteration time: 8.35s
                        Total time: 31198.25s
                               ETA: 953294.0s

################################################################################
                    [1m Learning iteration 3169/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.178s, learning 0.220s)
               Value function loss: 48.1418
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 4.01
               Mean episode length: 54.02
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 51937280
                    Iteration time: 8.40s
                        Total time: 31206.65s
                               ETA: 953240.0s

################################################################################
                    [1m Learning iteration 3170/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.278s, learning 0.210s)
               Value function loss: 0.5964
                    Surrogate loss: -0.0203
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 51.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 51953664
                    Iteration time: 8.49s
                        Total time: 31215.13s
                               ETA: 953188.7s

################################################################################
                    [1m Learning iteration 3171/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.166s)
               Value function loss: 0.4304
                    Surrogate loss: -0.0134
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 51970048
                    Iteration time: 8.45s
                        Total time: 31223.58s
                               ETA: 953136.3s

################################################################################
                    [1m Learning iteration 3172/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.414s, learning 0.170s)
               Value function loss: 0.3813
                    Surrogate loss: -0.0164
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 54.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 51986432
                    Iteration time: 8.58s
                        Total time: 31232.17s
                               ETA: 953088.0s

################################################################################
                    [1m Learning iteration 3173/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.158s, learning 0.172s)
               Value function loss: 0.3645
                    Surrogate loss: -0.0181
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 52.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 52002816
                    Iteration time: 8.33s
                        Total time: 31240.50s
                               ETA: 953032.0s

################################################################################
                    [1m Learning iteration 3174/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.225s, learning 0.174s)
               Value function loss: 0.3639
                    Surrogate loss: -0.0150
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 52.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 52019200
                    Iteration time: 8.40s
                        Total time: 31248.90s
                               ETA: 952978.1s

################################################################################
                    [1m Learning iteration 3175/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.070s, learning 0.204s)
               Value function loss: 0.2970
                    Surrogate loss: -0.0199
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 50.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 52035584
                    Iteration time: 8.27s
                        Total time: 31257.17s
                               ETA: 952920.5s

################################################################################
                    [1m Learning iteration 3176/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.077s, learning 0.221s)
               Value function loss: 0.2770
                    Surrogate loss: -0.0206
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 52.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 52051968
                    Iteration time: 8.30s
                        Total time: 31265.47s
                               ETA: 952863.6s

################################################################################
                    [1m Learning iteration 3177/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.385s, learning 0.163s)
               Value function loss: 0.2566
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3.96
               Mean episode length: 53.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 52068352
                    Iteration time: 8.55s
                        Total time: 31274.02s
                               ETA: 952814.4s

################################################################################
                    [1m Learning iteration 3178/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.317s, learning 0.215s)
               Value function loss: 0.2554
                    Surrogate loss: -0.0209
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 52.46
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 52084736
                    Iteration time: 8.53s
                        Total time: 31282.55s
                               ETA: 952764.7s

################################################################################
                    [1m Learning iteration 3179/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.267s, learning 0.191s)
               Value function loss: 0.2756
                    Surrogate loss: -0.0147
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 51.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 52101120
                    Iteration time: 8.46s
                        Total time: 31291.01s
                               ETA: 952712.7s

################################################################################
                    [1m Learning iteration 3180/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.312s, learning 0.210s)
               Value function loss: 39.3073
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 50.54
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 52117504
                    Iteration time: 8.52s
                        Total time: 31299.53s
                               ETA: 952662.8s

################################################################################
                    [1m Learning iteration 3181/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.267s, learning 0.167s)
               Value function loss: 0.2692
                    Surrogate loss: -0.0279
             Mean action noise std: 0.69
                       Mean reward: 2.92
               Mean episode length: 50.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 52133888
                    Iteration time: 8.43s
                        Total time: 31307.96s
                               ETA: 952610.2s

################################################################################
                    [1m Learning iteration 3182/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.192s, learning 0.171s)
               Value function loss: 16.8414
                    Surrogate loss: 0.0039
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 52.39
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52150272
                    Iteration time: 8.36s
                        Total time: 31316.32s
                               ETA: 952555.4s

################################################################################
                    [1m Learning iteration 3183/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.230s, learning 0.217s)
               Value function loss: 53.8382
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 52.38
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 52166656
                    Iteration time: 8.45s
                        Total time: 31324.77s
                               ETA: 952503.3s

################################################################################
                    [1m Learning iteration 3184/100000 [0m                    

                       Computation: 2045 steps/s (collection: 7.845s, learning 0.163s)
               Value function loss: 0.4334
                    Surrogate loss: -0.0298
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 51.64
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 52183040
                    Iteration time: 8.01s
                        Total time: 31332.78s
                               ETA: 952437.8s

################################################################################
                    [1m Learning iteration 3185/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.055s, learning 0.168s)
               Value function loss: 0.3152
                    Surrogate loss: -0.0162
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 52.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 52199424
                    Iteration time: 8.22s
                        Total time: 31341.00s
                               ETA: 952378.9s

################################################################################
                    [1m Learning iteration 3186/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.232s, learning 0.164s)
               Value function loss: 0.2843
                    Surrogate loss: -0.0214
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 51.79
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 52215808
                    Iteration time: 8.40s
                        Total time: 31349.40s
                               ETA: 952325.3s

################################################################################
                    [1m Learning iteration 3187/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.406s, learning 0.163s)
               Value function loss: 0.2452
                    Surrogate loss: -0.0283
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 50.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52232192
                    Iteration time: 8.57s
                        Total time: 31357.97s
                               ETA: 952276.9s

################################################################################
                    [1m Learning iteration 3188/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.152s, learning 0.277s)
               Value function loss: 0.2374
                    Surrogate loss: -0.0247
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 52248576
                    Iteration time: 8.43s
                        Total time: 31366.40s
                               ETA: 952224.3s

################################################################################
                    [1m Learning iteration 3189/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.226s, learning 0.157s)
               Value function loss: 0.2332
                    Surrogate loss: -0.0261
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 50.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 52264960
                    Iteration time: 8.38s
                        Total time: 31374.78s
                               ETA: 952170.4s

################################################################################
                    [1m Learning iteration 3190/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.151s, learning 0.162s)
               Value function loss: 0.2211
                    Surrogate loss: -0.0232
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.60
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 52281344
                    Iteration time: 8.31s
                        Total time: 31383.09s
                               ETA: 952114.4s

################################################################################
                    [1m Learning iteration 3191/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.277s, learning 0.169s)
               Value function loss: 0.2597
                    Surrogate loss: -0.0217
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 51.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 52297728
                    Iteration time: 8.45s
                        Total time: 31391.54s
                               ETA: 952062.5s

################################################################################
                    [1m Learning iteration 3192/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.040s, learning 0.165s)
               Value function loss: 0.2118
                    Surrogate loss: -0.0260
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 52.72
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 52314112
                    Iteration time: 8.21s
                        Total time: 31399.74s
                               ETA: 952003.2s

################################################################################
                    [1m Learning iteration 3193/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.078s, learning 0.212s)
               Value function loss: 0.1893
                    Surrogate loss: -0.0302
             Mean action noise std: 0.69
                       Mean reward: 3.79
               Mean episode length: 53.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 52330496
                    Iteration time: 8.29s
                        Total time: 31408.03s
                               ETA: 951946.6s

################################################################################
                    [1m Learning iteration 3194/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.237s, learning 0.168s)
               Value function loss: 0.1893
                    Surrogate loss: -0.0214
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 52.59
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 52346880
                    Iteration time: 8.41s
                        Total time: 31416.44s
                               ETA: 951893.5s

################################################################################
                    [1m Learning iteration 3195/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.368s, learning 0.167s)
               Value function loss: 0.2037
                    Surrogate loss: -0.0277
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 53.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52363264
                    Iteration time: 8.53s
                        Total time: 31424.97s
                               ETA: 951844.3s

################################################################################
                    [1m Learning iteration 3196/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.330s, learning 0.213s)
               Value function loss: 39.7498
                    Surrogate loss: 0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 51.66
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 52379648
                    Iteration time: 8.54s
                        Total time: 31433.52s
                               ETA: 951795.5s

################################################################################
                    [1m Learning iteration 3197/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.015s, learning 0.169s)
               Value function loss: 0.2046
                    Surrogate loss: -0.0285
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 50.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 52396032
                    Iteration time: 8.18s
                        Total time: 31441.70s
                               ETA: 951735.7s

################################################################################
                    [1m Learning iteration 3198/100000 [0m                    

                       Computation: 1885 steps/s (collection: 8.523s, learning 0.166s)
               Value function loss: 0.2155
                    Surrogate loss: -0.0256
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 50.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0012
--------------------------------------------------------------------------------
                   Total timesteps: 52412416
                    Iteration time: 8.69s
                        Total time: 31450.39s
                               ETA: 951691.3s

################################################################################
                    [1m Learning iteration 3199/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.145s, learning 0.159s)
               Value function loss: 68.3946
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 51.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52428800
                    Iteration time: 8.30s
                        Total time: 31458.69s
                               ETA: 951635.3s

################################################################################
                    [1m Learning iteration 3200/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.289s, learning 0.165s)
               Value function loss: 0.2440
                    Surrogate loss: -0.0205
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 51.55
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0014
--------------------------------------------------------------------------------
                   Total timesteps: 52445184
                    Iteration time: 8.45s
                        Total time: 31467.15s
                               ETA: 951583.8s

################################################################################
                    [1m Learning iteration 3201/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.427s, learning 0.169s)
               Value function loss: 0.2692
                    Surrogate loss: -0.0210
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 53.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0013
--------------------------------------------------------------------------------
                   Total timesteps: 52461568
                    Iteration time: 8.60s
                        Total time: 31475.74s
                               ETA: 951536.6s

################################################################################
                    [1m Learning iteration 3202/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.144s, learning 0.167s)
               Value function loss: 0.2173
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 6.16
               Mean episode length: 52.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 52477952
                    Iteration time: 8.31s
                        Total time: 31484.05s
                               ETA: 951480.9s

################################################################################
                    [1m Learning iteration 3203/100000 [0m                    

                       Computation: 2021 steps/s (collection: 7.944s, learning 0.159s)
               Value function loss: 0.2213
                    Surrogate loss: -0.0190
             Mean action noise std: 0.69
                       Mean reward: 4.22
               Mean episode length: 53.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0019
--------------------------------------------------------------------------------
                   Total timesteps: 52494336
                    Iteration time: 8.10s
                        Total time: 31492.16s
                               ETA: 951418.9s

################################################################################
                    [1m Learning iteration 3204/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.484s, learning 0.161s)
               Value function loss: 3.8723
                    Surrogate loss: 0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 52510720
                    Iteration time: 8.65s
                        Total time: 31500.80s
                               ETA: 951373.4s

################################################################################
                    [1m Learning iteration 3205/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.392s, learning 0.164s)
               Value function loss: 0.2018
                    Surrogate loss: -0.0290
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 52.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52527104
                    Iteration time: 8.56s
                        Total time: 31509.36s
                               ETA: 951325.1s

################################################################################
                    [1m Learning iteration 3206/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.048s, learning 0.162s)
               Value function loss: 7.2109
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.92
               Mean episode length: 51.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 52543488
                    Iteration time: 8.21s
                        Total time: 31517.57s
                               ETA: 951266.4s

################################################################################
                    [1m Learning iteration 3207/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.273s, learning 0.164s)
               Value function loss: 90.4325
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 2.67
               Mean episode length: 50.69
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52559872
                    Iteration time: 8.44s
                        Total time: 31526.00s
                               ETA: 951214.6s

################################################################################
                    [1m Learning iteration 3208/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.248s, learning 0.163s)
               Value function loss: 18.0853
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 51.53
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0015
--------------------------------------------------------------------------------
                   Total timesteps: 52576256
                    Iteration time: 8.41s
                        Total time: 31534.42s
                               ETA: 951162.1s

################################################################################
                    [1m Learning iteration 3209/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.132s, learning 0.186s)
               Value function loss: 0.2700
                    Surrogate loss: -0.0316
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0016
--------------------------------------------------------------------------------
                   Total timesteps: 52592640
                    Iteration time: 8.32s
                        Total time: 31542.73s
                               ETA: 951106.8s

################################################################################
                    [1m Learning iteration 3210/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.339s, learning 0.169s)
               Value function loss: 0.2295
                    Surrogate loss: -0.0159
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 52609024
                    Iteration time: 8.51s
                        Total time: 31551.24s
                               ETA: 951057.2s

################################################################################
                    [1m Learning iteration 3211/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.352s, learning 0.268s)
               Value function loss: 282.9024
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 51.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 52625408
                    Iteration time: 8.62s
                        Total time: 31559.86s
                               ETA: 951011.0s

################################################################################
                    [1m Learning iteration 3212/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.285s, learning 0.160s)
               Value function loss: 18.2043
                    Surrogate loss: 0.0015
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 52.59
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 52641792
                    Iteration time: 8.45s
                        Total time: 31568.31s
                               ETA: 950959.6s

################################################################################
                    [1m Learning iteration 3213/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.158s, learning 0.166s)
               Value function loss: 0.3192
                    Surrogate loss: -0.0234
             Mean action noise std: 0.69
                       Mean reward: 15.98
               Mean episode length: 51.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 52658176
                    Iteration time: 8.32s
                        Total time: 31576.63s
                               ETA: 950904.6s

################################################################################
                    [1m Learning iteration 3214/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.308s, learning 0.166s)
               Value function loss: 0.2981
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 11.45
               Mean episode length: 52.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 52674560
                    Iteration time: 8.47s
                        Total time: 31585.10s
                               ETA: 950854.1s

################################################################################
                    [1m Learning iteration 3215/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.127s, learning 0.172s)
               Value function loss: 0.2583
                    Surrogate loss: -0.0215
             Mean action noise std: 0.69
                       Mean reward: 3.02
               Mean episode length: 50.27
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 52690944
                    Iteration time: 8.30s
                        Total time: 31593.40s
                               ETA: 950798.4s

################################################################################
                    [1m Learning iteration 3216/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.202s, learning 0.172s)
               Value function loss: 0.2270
                    Surrogate loss: -0.0260
             Mean action noise std: 0.69
                       Mean reward: 3.05
               Mean episode length: 50.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 52707328
                    Iteration time: 8.37s
                        Total time: 31601.78s
                               ETA: 950744.9s

################################################################################
                    [1m Learning iteration 3217/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.170s, learning 0.171s)
               Value function loss: 17.5182
                    Surrogate loss: 0.0071
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 51.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 52723712
                    Iteration time: 8.34s
                        Total time: 31610.12s
                               ETA: 950690.5s

################################################################################
                    [1m Learning iteration 3218/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.224s, learning 0.161s)
               Value function loss: 33.5033
                    Surrogate loss: 0.0005
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 52.78
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 52740096
                    Iteration time: 8.38s
                        Total time: 31618.50s
                               ETA: 950637.5s

################################################################################
                    [1m Learning iteration 3219/100000 [0m                    

                       Computation: 1870 steps/s (collection: 8.591s, learning 0.169s)
               Value function loss: 0.3028
                    Surrogate loss: -0.0166
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 51.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 52756480
                    Iteration time: 8.76s
                        Total time: 31627.26s
                               ETA: 950595.7s

################################################################################
                    [1m Learning iteration 3220/100000 [0m                    

                       Computation: 1220 steps/s (collection: 13.239s, learning 0.180s)
               Value function loss: 0.2439
                    Surrogate loss: -0.0185
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 52.01
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 52772864
                    Iteration time: 13.42s
                        Total time: 31640.68s
                               ETA: 950694.0s

################################################################################
                    [1m Learning iteration 3221/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.093s, learning 0.176s)
               Value function loss: 0.2255
                    Surrogate loss: -0.0219
             Mean action noise std: 0.69
                       Mean reward: 3.06
               Mean episode length: 51.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 52789248
                    Iteration time: 16.27s
                        Total time: 31656.95s
                               ETA: 950877.7s

################################################################################
                    [1m Learning iteration 3222/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.161s, learning 0.164s)
               Value function loss: 54.3059
                    Surrogate loss: 0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 51.75
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 52805632
                    Iteration time: 16.32s
                        Total time: 31673.28s
                               ETA: 951063.1s

################################################################################
                    [1m Learning iteration 3223/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.833s, learning 0.183s)
               Value function loss: 0.2398
                    Surrogate loss: -0.0242
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 52822016
                    Iteration time: 16.02s
                        Total time: 31689.29s
                               ETA: 951239.0s

################################################################################
                    [1m Learning iteration 3224/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.054s, learning 0.177s)
               Value function loss: 0.2595
                    Surrogate loss: -0.0208
             Mean action noise std: 0.69
                       Mean reward: 3.02
               Mean episode length: 50.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 52838400
                    Iteration time: 16.23s
                        Total time: 31705.52s
                               ETA: 951421.3s

################################################################################
                    [1m Learning iteration 3225/100000 [0m                    

                       Computation: 980 steps/s (collection: 16.536s, learning 0.167s)
               Value function loss: 0.2336
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 8.39
               Mean episode length: 52.28
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0028
--------------------------------------------------------------------------------
                   Total timesteps: 52854784
                    Iteration time: 16.70s
                        Total time: 31722.23s
                               ETA: 951617.7s

################################################################################
                    [1m Learning iteration 3226/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.008s, learning 0.174s)
               Value function loss: 0.2373
                    Surrogate loss: -0.0185
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 51.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 52871168
                    Iteration time: 16.18s
                        Total time: 31738.41s
                               ETA: 951798.2s

################################################################################
                    [1m Learning iteration 3227/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.801s, learning 0.164s)
               Value function loss: 63.5047
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 51.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 52887552
                    Iteration time: 15.96s
                        Total time: 31754.37s
                               ETA: 951972.1s

################################################################################
                    [1m Learning iteration 3228/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.111s, learning 0.214s)
               Value function loss: 13.7874
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 52.60
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 52903936
                    Iteration time: 16.33s
                        Total time: 31770.70s
                               ETA: 952156.7s

################################################################################
                    [1m Learning iteration 3229/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.442s, learning 0.168s)
               Value function loss: 0.3298
                    Surrogate loss: -0.0302
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 51.99
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 52920320
                    Iteration time: 16.61s
                        Total time: 31787.31s
                               ETA: 952349.7s

################################################################################
                    [1m Learning iteration 3230/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.191s, learning 0.172s)
               Value function loss: 0.2740
                    Surrogate loss: -0.0156
             Mean action noise std: 0.69
                       Mean reward: 8.28
               Mean episode length: 51.84
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 52936704
                    Iteration time: 16.36s
                        Total time: 31803.67s
                               ETA: 952535.2s

################################################################################
                    [1m Learning iteration 3231/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.036s, learning 0.163s)
               Value function loss: 0.2425
                    Surrogate loss: -0.0181
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 52953088
                    Iteration time: 16.20s
                        Total time: 31819.87s
                               ETA: 952715.7s

################################################################################
                    [1m Learning iteration 3232/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.014s, learning 0.174s)
               Value function loss: 0.2041
                    Surrogate loss: -0.0260
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 51.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 52969472
                    Iteration time: 16.19s
                        Total time: 31836.06s
                               ETA: 952895.7s

################################################################################
                    [1m Learning iteration 3233/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.888s, learning 0.168s)
               Value function loss: 17.7429
                    Surrogate loss: 0.0038
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 52985856
                    Iteration time: 16.06s
                        Total time: 31852.12s
                               ETA: 953071.6s

################################################################################
                    [1m Learning iteration 3234/100000 [0m                    

                       Computation: 981 steps/s (collection: 16.482s, learning 0.217s)
               Value function loss: 4.0778
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 53.80
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 53002240
                    Iteration time: 16.70s
                        Total time: 31868.81s
                               ETA: 953266.7s

################################################################################
                    [1m Learning iteration 3235/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.257s, learning 0.180s)
               Value function loss: 7.1745
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 49.09
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0017
--------------------------------------------------------------------------------
                   Total timesteps: 53018624
                    Iteration time: 16.44s
                        Total time: 31885.25s
                               ETA: 953453.7s

################################################################################
                    [1m Learning iteration 3236/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.716s, learning 0.230s)
               Value function loss: 0.2912
                    Surrogate loss: -0.0277
             Mean action noise std: 0.69
                       Mean reward: 8.23
               Mean episode length: 51.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 53035008
                    Iteration time: 15.95s
                        Total time: 31901.20s
                               ETA: 953626.0s

################################################################################
                    [1m Learning iteration 3237/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.973s, learning 0.184s)
               Value function loss: 53.2478
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 5.80
               Mean episode length: 52.35
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 53051392
                    Iteration time: 16.16s
                        Total time: 31917.35s
                               ETA: 953804.5s

################################################################################
                    [1m Learning iteration 3238/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.196s, learning 0.168s)
               Value function loss: 11.9835
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.82
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0020
--------------------------------------------------------------------------------
                   Total timesteps: 53067776
                    Iteration time: 16.36s
                        Total time: 31933.72s
                               ETA: 953989.0s

################################################################################
                    [1m Learning iteration 3239/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.031s, learning 0.168s)
               Value function loss: 46.7436
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.15
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0018
--------------------------------------------------------------------------------
                   Total timesteps: 53084160
                    Iteration time: 16.20s
                        Total time: 31949.92s
                               ETA: 954168.5s

################################################################################
                    [1m Learning iteration 3240/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.067s, learning 0.180s)
               Value function loss: 0.5671
                    Surrogate loss: -0.0237
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 53100544
                    Iteration time: 16.25s
                        Total time: 31966.16s
                               ETA: 954349.3s

################################################################################
                    [1m Learning iteration 3241/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.738s, learning 0.246s)
               Value function loss: 0.4019
                    Surrogate loss: -0.0220
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 53116928
                    Iteration time: 15.98s
                        Total time: 31982.15s
                               ETA: 954522.1s

################################################################################
                    [1m Learning iteration 3242/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.940s, learning 0.221s)
               Value function loss: 14.9850
                    Surrogate loss: 0.0068
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 52.79
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0021
--------------------------------------------------------------------------------
                   Total timesteps: 53133312
                    Iteration time: 16.16s
                        Total time: 31998.31s
                               ETA: 954700.1s

################################################################################
                    [1m Learning iteration 3243/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.444s, learning 0.175s)
               Value function loss: 62.8132
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 52.69
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0024
--------------------------------------------------------------------------------
                   Total timesteps: 53149696
                    Iteration time: 16.62s
                        Total time: 32014.93s
                               ETA: 954891.6s

################################################################################
                    [1m Learning iteration 3244/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.328s, learning 0.164s)
               Value function loss: 46.0180
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.67
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0022
--------------------------------------------------------------------------------
                   Total timesteps: 53166080
                    Iteration time: 16.49s
                        Total time: 32031.42s
                               ETA: 955079.3s

################################################################################
                    [1m Learning iteration 3245/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.069s, learning 0.171s)
               Value function loss: 17.0525
                    Surrogate loss: 0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 53.10
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 53182464
                    Iteration time: 16.24s
                        Total time: 32047.66s
                               ETA: 955259.2s

################################################################################
                    [1m Learning iteration 3246/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.397s, learning 0.170s)
               Value function loss: 0.7683
                    Surrogate loss: -0.0243
             Mean action noise std: 0.69
                       Mean reward: 15.68
               Mean episode length: 51.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 53198848
                    Iteration time: 16.57s
                        Total time: 32064.23s
                               ETA: 955448.8s

################################################################################
                    [1m Learning iteration 3247/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.377s, learning 0.172s)
               Value function loss: 0.6144
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 53.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 53215232
                    Iteration time: 16.55s
                        Total time: 32080.78s
                               ETA: 955637.8s

################################################################################
                    [1m Learning iteration 3248/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.921s, learning 0.162s)
               Value function loss: 63.0046
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 5.95
               Mean episode length: 52.06
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 53231616
                    Iteration time: 16.08s
                        Total time: 32096.86s
                               ETA: 955812.7s

################################################################################
                    [1m Learning iteration 3249/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.736s, learning 0.214s)
               Value function loss: 0.6073
                    Surrogate loss: -0.0186
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 53.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 53248000
                    Iteration time: 15.95s
                        Total time: 32112.81s
                               ETA: 955983.5s

################################################################################
                    [1m Learning iteration 3250/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.894s, learning 0.171s)
               Value function loss: 17.0240
                    Surrogate loss: 0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 54.28
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 53264384
                    Iteration time: 16.07s
                        Total time: 32128.87s
                               ETA: 956157.7s

################################################################################
                    [1m Learning iteration 3251/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.154s, learning 0.167s)
               Value function loss: 4.4627
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.97
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 53280768
                    Iteration time: 16.32s
                        Total time: 32145.20s
                               ETA: 956339.3s

################################################################################
                    [1m Learning iteration 3252/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.376s, learning 0.217s)
               Value function loss: 22.9006
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 51.16
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 53297152
                    Iteration time: 16.59s
                        Total time: 32161.79s
                               ETA: 956529.0s

################################################################################
                    [1m Learning iteration 3253/100000 [0m                    

                       Computation: 1016 steps/s (collection: 15.950s, learning 0.167s)
               Value function loss: 85.4649
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 53.10
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 53313536
                    Iteration time: 16.12s
                        Total time: 32177.91s
                               ETA: 956704.3s

################################################################################
                    [1m Learning iteration 3254/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.247s, learning 0.164s)
               Value function loss: 1.4334
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.61
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0030
--------------------------------------------------------------------------------
                   Total timesteps: 53329920
                    Iteration time: 16.41s
                        Total time: 32194.32s
                               ETA: 956888.3s

################################################################################
                    [1m Learning iteration 3255/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.013s, learning 0.161s)
               Value function loss: 0.8003
                    Surrogate loss: -0.0134
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 51.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 53346304
                    Iteration time: 16.17s
                        Total time: 32210.49s
                               ETA: 957065.1s

################################################################################
                    [1m Learning iteration 3256/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.181s, learning 0.178s)
               Value function loss: 4.2901
                    Surrogate loss: 0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 50.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 53362688
                    Iteration time: 16.36s
                        Total time: 32226.85s
                               ETA: 957247.3s

################################################################################
                    [1m Learning iteration 3257/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.287s, learning 0.171s)
               Value function loss: 62.9075
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 51.33
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 53379072
                    Iteration time: 16.46s
                        Total time: 32243.31s
                               ETA: 957432.3s

################################################################################
                    [1m Learning iteration 3258/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.228s, learning 0.180s)
               Value function loss: 0.8072
                    Surrogate loss: -0.0225
             Mean action noise std: 0.69
                       Mean reward: 5.61
               Mean episode length: 50.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 53395456
                    Iteration time: 8.41s
                        Total time: 32251.72s
                               ETA: 957378.2s

################################################################################
                    [1m Learning iteration 3259/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.362s, learning 0.171s)
               Value function loss: 62.7912
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 53.56
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 53411840
                    Iteration time: 8.53s
                        Total time: 32260.25s
                               ETA: 957327.8s

################################################################################
                    [1m Learning iteration 3260/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.181s, learning 0.159s)
               Value function loss: 44.8399
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.80
               Mean episode length: 51.65
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 53428224
                    Iteration time: 8.34s
                        Total time: 32268.59s
                               ETA: 957271.8s

################################################################################
                    [1m Learning iteration 3261/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.161s)
               Value function loss: 201.4642
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.15
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 53444608
                    Iteration time: 8.40s
                        Total time: 32276.99s
                               ETA: 957217.6s

################################################################################
                    [1m Learning iteration 3262/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.344s, learning 0.166s)
               Value function loss: 3.2303
                    Surrogate loss: -0.0137
             Mean action noise std: 0.69
                       Mean reward: 16.10
               Mean episode length: 52.74
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 53460992
                    Iteration time: 8.51s
                        Total time: 32285.50s
                               ETA: 957166.7s

################################################################################
                    [1m Learning iteration 3263/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.268s, learning 0.170s)
               Value function loss: 0.9269
                    Surrogate loss: -0.0108
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 53477376
                    Iteration time: 8.44s
                        Total time: 32293.94s
                               ETA: 957113.6s

################################################################################
                    [1m Learning iteration 3264/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.566s, learning 0.165s)
               Value function loss: 29.4950
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.37
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 53493760
                    Iteration time: 8.73s
                        Total time: 32302.67s
                               ETA: 957069.3s

################################################################################
                    [1m Learning iteration 3265/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.988s, learning 0.159s)
               Value function loss: 0.5893
                    Surrogate loss: -0.0236
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 52.18
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 53510144
                    Iteration time: 8.15s
                        Total time: 32310.82s
                               ETA: 957007.7s

################################################################################
                    [1m Learning iteration 3266/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.306s, learning 0.162s)
               Value function loss: 0.3669
                    Surrogate loss: -0.0208
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 53.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 53526528
                    Iteration time: 8.47s
                        Total time: 32319.29s
                               ETA: 956955.6s

################################################################################
                    [1m Learning iteration 3267/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.187s, learning 0.162s)
               Value function loss: 0.3734
                    Surrogate loss: -0.0219
             Mean action noise std: 0.69
                       Mean reward: 8.57
               Mean episode length: 53.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 53542912
                    Iteration time: 8.35s
                        Total time: 32327.64s
                               ETA: 956900.0s

################################################################################
                    [1m Learning iteration 3268/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.165s, learning 0.180s)
               Value function loss: 0.4777
                    Surrogate loss: -0.0206
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 51.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 53559296
                    Iteration time: 8.35s
                        Total time: 32335.98s
                               ETA: 956844.3s

################################################################################
                    [1m Learning iteration 3269/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.103s, learning 0.162s)
               Value function loss: 23.7054
                    Surrogate loss: 0.0053
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 51.10
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0040
--------------------------------------------------------------------------------
                   Total timesteps: 53575680
                    Iteration time: 8.26s
                        Total time: 32344.25s
                               ETA: 956786.3s

################################################################################
                    [1m Learning iteration 3270/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.283s, learning 0.163s)
               Value function loss: 132.4763
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 2.38
               Mean episode length: 49.91
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0037
--------------------------------------------------------------------------------
                   Total timesteps: 53592064
                    Iteration time: 8.45s
                        Total time: 32352.69s
                               ETA: 956733.7s

################################################################################
                    [1m Learning iteration 3271/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.224s, learning 0.191s)
               Value function loss: 206.7206
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 52.22
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 53608448
                    Iteration time: 8.41s
                        Total time: 32361.11s
                               ETA: 956680.1s

################################################################################
                    [1m Learning iteration 3272/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.393s, learning 0.259s)
               Value function loss: 9.8164
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.92
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 53624832
                    Iteration time: 8.65s
                        Total time: 32369.76s
                               ETA: 956633.6s

################################################################################
                    [1m Learning iteration 3273/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.433s, learning 0.164s)
               Value function loss: 9.7249
                    Surrogate loss: -0.0101
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 52.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 53641216
                    Iteration time: 8.60s
                        Total time: 32378.35s
                               ETA: 956585.6s

################################################################################
                    [1m Learning iteration 3274/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.144s, learning 0.210s)
               Value function loss: 264.1032
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 51.58
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 53657600
                    Iteration time: 8.35s
                        Total time: 32386.71s
                               ETA: 956530.3s

################################################################################
                    [1m Learning iteration 3275/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.161s)
               Value function loss: 55.1339
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 5.39
               Mean episode length: 53.18
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 53673984
                    Iteration time: 8.33s
                        Total time: 32395.04s
                               ETA: 956474.4s

################################################################################
                    [1m Learning iteration 3276/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.058s, learning 0.163s)
               Value function loss: 8.4621
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 52.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 53690368
                    Iteration time: 8.22s
                        Total time: 32403.26s
                               ETA: 956415.3s

################################################################################
                    [1m Learning iteration 3277/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.254s, learning 0.161s)
               Value function loss: 0.8950
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 50.44
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 53706752
                    Iteration time: 8.42s
                        Total time: 32411.68s
                               ETA: 956362.0s

################################################################################
                    [1m Learning iteration 3278/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.101s, learning 0.176s)
               Value function loss: 0.4736
                    Surrogate loss: -0.0120
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 52.06
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0060
--------------------------------------------------------------------------------
                   Total timesteps: 53723136
                    Iteration time: 8.28s
                        Total time: 32419.95s
                               ETA: 956304.6s

################################################################################
                    [1m Learning iteration 3279/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.414s, learning 0.161s)
               Value function loss: 17.1936
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 5.85
               Mean episode length: 53.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 53739520
                    Iteration time: 8.57s
                        Total time: 32428.53s
                               ETA: 956256.0s

################################################################################
                    [1m Learning iteration 3280/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.006s, learning 0.161s)
               Value function loss: 216.8311
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 52.38
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 53755904
                    Iteration time: 8.17s
                        Total time: 32436.70s
                               ETA: 956195.4s

################################################################################
                    [1m Learning iteration 3281/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.151s, learning 0.166s)
               Value function loss: 5.7087
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 53.21
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 53772288
                    Iteration time: 8.32s
                        Total time: 32445.01s
                               ETA: 956139.3s

################################################################################
                    [1m Learning iteration 3282/100000 [0m                    

                       Computation: 2052 steps/s (collection: 7.818s, learning 0.165s)
               Value function loss: 17.3930
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 5.10
               Mean episode length: 52.25
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 53788672
                    Iteration time: 7.98s
                        Total time: 32453.00s
                               ETA: 956073.4s

################################################################################
                    [1m Learning iteration 3283/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.235s, learning 0.182s)
               Value function loss: 46.9176
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 52.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 53805056
                    Iteration time: 8.42s
                        Total time: 32461.41s
                               ETA: 956020.2s

################################################################################
                    [1m Learning iteration 3284/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.299s, learning 0.163s)
               Value function loss: 14.7926
                    Surrogate loss: 0.0033
             Mean action noise std: 0.69
                       Mean reward: 15.14
               Mean episode length: 51.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 53821440
                    Iteration time: 8.46s
                        Total time: 32469.87s
                               ETA: 955968.5s

################################################################################
                    [1m Learning iteration 3285/100000 [0m                    

                       Computation: 2013 steps/s (collection: 7.974s, learning 0.164s)
               Value function loss: 10.2309
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 52.44
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 53837824
                    Iteration time: 8.14s
                        Total time: 32478.01s
                               ETA: 955907.2s

################################################################################
                    [1m Learning iteration 3286/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.166s, learning 0.165s)
               Value function loss: 16.3357
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 50.67
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0059
--------------------------------------------------------------------------------
                   Total timesteps: 53854208
                    Iteration time: 8.33s
                        Total time: 32486.34s
                               ETA: 955851.6s

################################################################################
                    [1m Learning iteration 3287/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.214s, learning 0.159s)
               Value function loss: 0.9357
                    Surrogate loss: -0.0202
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 53.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 53870592
                    Iteration time: 8.37s
                        Total time: 32494.72s
                               ETA: 955797.3s

################################################################################
                    [1m Learning iteration 3288/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.425s, learning 0.214s)
               Value function loss: 0.7825
                    Surrogate loss: -0.0120
             Mean action noise std: 0.69
                       Mean reward: 5.49
               Mean episode length: 52.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 53886976
                    Iteration time: 8.64s
                        Total time: 32503.36s
                               ETA: 955750.8s

################################################################################
                    [1m Learning iteration 3289/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.250s, learning 0.167s)
               Value function loss: 203.0747
                    Surrogate loss: 0.0055
             Mean action noise std: 0.69
                       Mean reward: 5.24
               Mean episode length: 52.15
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0053
--------------------------------------------------------------------------------
                   Total timesteps: 53903360
                    Iteration time: 8.42s
                        Total time: 32511.77s
                               ETA: 955697.9s

################################################################################
                    [1m Learning iteration 3290/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.220s, learning 0.165s)
               Value function loss: 0.8276
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 52.03
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 53919744
                    Iteration time: 8.39s
                        Total time: 32520.16s
                               ETA: 955644.0s

################################################################################
                    [1m Learning iteration 3291/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.220s, learning 0.162s)
               Value function loss: 1.0809
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 52.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 53936128
                    Iteration time: 8.38s
                        Total time: 32528.54s
                               ETA: 955590.1s

################################################################################
                    [1m Learning iteration 3292/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.084s, learning 0.162s)
               Value function loss: 0.8294
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 8.01
               Mean episode length: 51.93
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 53952512
                    Iteration time: 8.25s
                        Total time: 32536.79s
                               ETA: 955532.2s

################################################################################
                    [1m Learning iteration 3293/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.341s, learning 0.164s)
               Value function loss: 0.4591
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 3.29
               Mean episode length: 53.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 53968896
                    Iteration time: 8.50s
                        Total time: 32545.29s
                               ETA: 955481.9s

################################################################################
                    [1m Learning iteration 3294/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.158s, learning 0.161s)
               Value function loss: 0.4226
                    Surrogate loss: -0.0174
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 52.75
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 53985280
                    Iteration time: 8.32s
                        Total time: 32553.61s
                               ETA: 955426.2s

################################################################################
                    [1m Learning iteration 3295/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.040s, learning 0.228s)
               Value function loss: 0.3125
                    Surrogate loss: -0.0119
             Mean action noise std: 0.69
                       Mean reward: 2.64
               Mean episode length: 52.03
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 54001664
                    Iteration time: 8.27s
                        Total time: 32561.88s
                               ETA: 955369.0s

################################################################################
                    [1m Learning iteration 3296/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.206s, learning 0.186s)
               Value function loss: 0.3049
                    Surrogate loss: -0.0155
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 52.04
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 54018048
                    Iteration time: 8.39s
                        Total time: 32570.27s
                               ETA: 955315.5s

################################################################################
                    [1m Learning iteration 3297/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.929s, learning 0.166s)
               Value function loss: 275.1570
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 53.00
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 54034432
                    Iteration time: 8.09s
                        Total time: 32578.36s
                               ETA: 955253.3s

################################################################################
                    [1m Learning iteration 3298/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.320s, learning 0.176s)
               Value function loss: 0.4119
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 2.74
               Mean episode length: 52.20
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 54050816
                    Iteration time: 8.50s
                        Total time: 32586.86s
                               ETA: 955203.0s

################################################################################
                    [1m Learning iteration 3299/100000 [0m                    

                       Computation: 1874 steps/s (collection: 8.579s, learning 0.159s)
               Value function loss: 18.1069
                    Surrogate loss: 0.0011
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 52.53
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 54067200
                    Iteration time: 8.74s
                        Total time: 32595.60s
                               ETA: 955159.7s

################################################################################
                    [1m Learning iteration 3300/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.144s, learning 0.162s)
               Value function loss: 169.3762
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 52.87
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0051
--------------------------------------------------------------------------------
                   Total timesteps: 54083584
                    Iteration time: 8.31s
                        Total time: 32603.90s
                               ETA: 955103.8s

################################################################################
                    [1m Learning iteration 3301/100000 [0m                    

                       Computation: 2057 steps/s (collection: 7.797s, learning 0.166s)
               Value function loss: 0.8297
                    Surrogate loss: -0.0161
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 53.21
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54099968
                    Iteration time: 7.96s
                        Total time: 32611.87s
                               ETA: 955037.8s

################################################################################
                    [1m Learning iteration 3302/100000 [0m                    

                       Computation: 2002 steps/s (collection: 8.022s, learning 0.162s)
               Value function loss: 0.5883
                    Surrogate loss: -0.0138
             Mean action noise std: 0.69
                       Mean reward: 2.42
               Mean episode length: 51.87
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 54116352
                    Iteration time: 8.18s
                        Total time: 32620.05s
                               ETA: 954978.4s

################################################################################
                    [1m Learning iteration 3303/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.205s, learning 0.186s)
               Value function loss: 0.4261
                    Surrogate loss: -0.0173
             Mean action noise std: 0.69
                       Mean reward: 8.28
               Mean episode length: 53.57
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 54132736
                    Iteration time: 8.39s
                        Total time: 32628.44s
                               ETA: 954925.1s

################################################################################
                    [1m Learning iteration 3304/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.254s, learning 0.163s)
               Value function loss: 0.5542
                    Surrogate loss: -0.0198
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 52.11
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 54149120
                    Iteration time: 8.42s
                        Total time: 32636.86s
                               ETA: 954872.5s

################################################################################
                    [1m Learning iteration 3305/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.321s, learning 0.165s)
               Value function loss: 131.9076
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 2.69
               Mean episode length: 52.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0050
--------------------------------------------------------------------------------
                   Total timesteps: 54165504
                    Iteration time: 8.49s
                        Total time: 32645.35s
                               ETA: 954822.0s

################################################################################
                    [1m Learning iteration 3306/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.414s, learning 0.212s)
               Value function loss: 0.4290
                    Surrogate loss: -0.0205
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 51.10
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 54181888
                    Iteration time: 8.63s
                        Total time: 32653.97s
                               ETA: 954775.7s

################################################################################
                    [1m Learning iteration 3307/100000 [0m                    

                       Computation: 1871 steps/s (collection: 8.585s, learning 0.170s)
               Value function loss: 0.3926
                    Surrogate loss: -0.0163
             Mean action noise std: 0.69
                       Mean reward: 2.67
               Mean episode length: 53.40
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 54198272
                    Iteration time: 8.76s
                        Total time: 32662.73s
                               ETA: 954733.1s

################################################################################
                    [1m Learning iteration 3308/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.953s, learning 0.161s)
               Value function loss: 0.3465
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 52.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 54214656
                    Iteration time: 8.11s
                        Total time: 32670.84s
                               ETA: 954671.8s

################################################################################
                    [1m Learning iteration 3309/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.349s, learning 0.172s)
               Value function loss: 64.5223
                    Surrogate loss: 0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 51.65
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0042
--------------------------------------------------------------------------------
                   Total timesteps: 54231040
                    Iteration time: 8.52s
                        Total time: 32679.36s
                               ETA: 954622.4s

################################################################################
                    [1m Learning iteration 3310/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.306s, learning 0.157s)
               Value function loss: 0.3172
                    Surrogate loss: -0.0175
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 52.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 54247424
                    Iteration time: 8.46s
                        Total time: 32687.82s
                               ETA: 954571.3s

################################################################################
                    [1m Learning iteration 3311/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.302s, learning 0.170s)
               Value function loss: 0.2813
                    Surrogate loss: -0.0157
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 53.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 54263808
                    Iteration time: 8.47s
                        Total time: 32696.30s
                               ETA: 954520.6s

################################################################################
                    [1m Learning iteration 3312/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.165s)
               Value function loss: 0.2340
                    Surrogate loss: -0.0221
             Mean action noise std: 0.69
                       Mean reward: 8.23
               Mean episode length: 52.35
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 54280192
                    Iteration time: 8.29s
                        Total time: 32704.58s
                               ETA: 954464.5s

################################################################################
                    [1m Learning iteration 3313/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.179s, learning 0.168s)
               Value function loss: 0.2139
                    Surrogate loss: -0.0222
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 54.48
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 54296576
                    Iteration time: 8.35s
                        Total time: 32712.93s
                               ETA: 954410.1s

################################################################################
                    [1m Learning iteration 3314/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.149s, learning 0.263s)
               Value function loss: 0.2457
                    Surrogate loss: -0.0118
             Mean action noise std: 0.69
                       Mean reward: 2.69
               Mean episode length: 51.65
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 54312960
                    Iteration time: 8.41s
                        Total time: 32721.34s
                               ETA: 954357.7s

################################################################################
                    [1m Learning iteration 3315/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.335s, learning 0.217s)
               Value function loss: 0.2873
                    Surrogate loss: -0.0201
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 52.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 54329344
                    Iteration time: 8.55s
                        Total time: 32729.89s
                               ETA: 954309.3s

################################################################################
                    [1m Learning iteration 3316/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.220s, learning 0.165s)
               Value function loss: 0.2110
                    Surrogate loss: -0.0206
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 52.90
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0035
--------------------------------------------------------------------------------
                   Total timesteps: 54345728
                    Iteration time: 8.38s
                        Total time: 32738.28s
                               ETA: 954256.2s

################################################################################
                    [1m Learning iteration 3317/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.949s, learning 0.166s)
               Value function loss: 165.4350
                    Surrogate loss: 0.0052
             Mean action noise std: 0.69
                       Mean reward: 2.75
               Mean episode length: 51.59
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54362112
                    Iteration time: 8.12s
                        Total time: 32746.39s
                               ETA: 954195.2s

################################################################################
                    [1m Learning iteration 3318/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.306s, learning 0.213s)
               Value function loss: 63.1119
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 52.07
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54378496
                    Iteration time: 8.52s
                        Total time: 32754.91s
                               ETA: 954146.0s

################################################################################
                    [1m Learning iteration 3319/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.167s)
               Value function loss: 5.0130
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 52.91
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 54394880
                    Iteration time: 8.20s
                        Total time: 32763.11s
                               ETA: 954087.4s

################################################################################
                    [1m Learning iteration 3320/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.164s)
               Value function loss: 217.4251
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 52.54
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 54411264
                    Iteration time: 8.55s
                        Total time: 32771.66s
                               ETA: 954039.3s

################################################################################
                    [1m Learning iteration 3321/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.176s, learning 0.163s)
               Value function loss: 1.1600
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 52.95
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0032
--------------------------------------------------------------------------------
                   Total timesteps: 54427648
                    Iteration time: 8.34s
                        Total time: 32780.00s
                               ETA: 953984.9s

################################################################################
                    [1m Learning iteration 3322/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.383s, learning 0.163s)
               Value function loss: 120.9499
                    Surrogate loss: 0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 54.21
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 54444032
                    Iteration time: 8.55s
                        Total time: 32788.55s
                               ETA: 953936.6s

################################################################################
                    [1m Learning iteration 3323/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.195s, learning 0.177s)
               Value function loss: 1.1219
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 53.78
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54460416
                    Iteration time: 8.37s
                        Total time: 32796.92s
                               ETA: 953883.3s

################################################################################
                    [1m Learning iteration 3324/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.362s, learning 0.218s)
               Value function loss: 1.0495
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 52.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0044
--------------------------------------------------------------------------------
                   Total timesteps: 54476800
                    Iteration time: 8.58s
                        Total time: 32805.50s
                               ETA: 953836.0s

################################################################################
                    [1m Learning iteration 3325/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.417s, learning 0.172s)
               Value function loss: 17.4841
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 51.36
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 54493184
                    Iteration time: 8.59s
                        Total time: 32814.09s
                               ETA: 953789.0s

################################################################################
                    [1m Learning iteration 3326/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.333s, learning 0.178s)
               Value function loss: 45.4339
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 51.68
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0045
--------------------------------------------------------------------------------
                   Total timesteps: 54509568
                    Iteration time: 8.51s
                        Total time: 32822.60s
                               ETA: 953739.8s

################################################################################
                    [1m Learning iteration 3327/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.262s, learning 0.162s)
               Value function loss: 10.0252
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 53.12
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0041
--------------------------------------------------------------------------------
                   Total timesteps: 54525952
                    Iteration time: 8.42s
                        Total time: 32831.03s
                               ETA: 953688.0s

################################################################################
                    [1m Learning iteration 3328/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.240s, learning 0.162s)
               Value function loss: 0.9223
                    Surrogate loss: -0.0202
             Mean action noise std: 0.69
                       Mean reward: 2.89
               Mean episode length: 52.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0038
--------------------------------------------------------------------------------
                   Total timesteps: 54542336
                    Iteration time: 8.40s
                        Total time: 32839.43s
                               ETA: 953635.7s

################################################################################
                    [1m Learning iteration 3329/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.346s, learning 0.167s)
               Value function loss: 0.7611
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 13.49
               Mean episode length: 53.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0047
--------------------------------------------------------------------------------
                   Total timesteps: 54558720
                    Iteration time: 8.51s
                        Total time: 32847.94s
                               ETA: 953586.6s

################################################################################
                    [1m Learning iteration 3330/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.315s, learning 0.175s)
               Value function loss: 0.5265
                    Surrogate loss: -0.0104
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 51.39
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0046
--------------------------------------------------------------------------------
                   Total timesteps: 54575104
                    Iteration time: 8.49s
                        Total time: 32856.43s
                               ETA: 953536.8s

################################################################################
                    [1m Learning iteration 3331/100000 [0m                    

                       Computation: 2108 steps/s (collection: 7.611s, learning 0.160s)
               Value function loss: 0.3903
                    Surrogate loss: -0.0157
             Mean action noise std: 0.69
                       Mean reward: 2.86
               Mean episode length: 52.57
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0043
--------------------------------------------------------------------------------
                   Total timesteps: 54591488
                    Iteration time: 7.77s
                        Total time: 32864.20s
                               ETA: 953466.3s

################################################################################
                    [1m Learning iteration 3332/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.013s, learning 0.166s)
               Value function loss: 0.3199
                    Surrogate loss: -0.0135
             Mean action noise std: 0.69
                       Mean reward: 3.05
               Mean episode length: 52.87
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0039
--------------------------------------------------------------------------------
                   Total timesteps: 54607872
                    Iteration time: 8.18s
                        Total time: 32872.38s
                               ETA: 953407.6s

################################################################################
                    [1m Learning iteration 3333/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.296s, learning 0.169s)
               Value function loss: 0.3019
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 52.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 54624256
                    Iteration time: 8.46s
                        Total time: 32880.85s
                               ETA: 953357.2s

################################################################################
                    [1m Learning iteration 3334/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.390s, learning 0.172s)
               Value function loss: 0.2540
                    Surrogate loss: -0.0189
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 53.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 54640640
                    Iteration time: 8.56s
                        Total time: 32889.41s
                               ETA: 953309.6s

################################################################################
                    [1m Learning iteration 3335/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.476s, learning 0.167s)
               Value function loss: 9.7830
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 52.23
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 54657024
                    Iteration time: 8.64s
                        Total time: 32898.05s
                               ETA: 953264.5s

################################################################################
                    [1m Learning iteration 3336/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.130s, learning 0.175s)
               Value function loss: 0.2314
                    Surrogate loss: -0.0170
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 52.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54673408
                    Iteration time: 8.30s
                        Total time: 32906.36s
                               ETA: 953209.5s

################################################################################
                    [1m Learning iteration 3337/100000 [0m                    

                       Computation: 1923 steps/s (collection: 8.342s, learning 0.177s)
               Value function loss: 63.9764
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 53.42
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 54689792
                    Iteration time: 8.52s
                        Total time: 32914.88s
                               ETA: 953160.8s

################################################################################
                    [1m Learning iteration 3338/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.026s, learning 0.217s)
               Value function loss: 0.2698
                    Surrogate loss: -0.0224
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 53.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0027
--------------------------------------------------------------------------------
                   Total timesteps: 54706176
                    Iteration time: 8.24s
                        Total time: 32923.12s
                               ETA: 953104.1s

################################################################################
                    [1m Learning iteration 3339/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.121s, learning 0.190s)
               Value function loss: 17.5719
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.81
               Mean episode length: 52.66
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0025
--------------------------------------------------------------------------------
                   Total timesteps: 54722560
                    Iteration time: 8.31s
                        Total time: 32931.43s
                               ETA: 953049.4s

################################################################################
                    [1m Learning iteration 3340/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.122s, learning 0.174s)
               Value function loss: 57.4108
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 53.75
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0023
--------------------------------------------------------------------------------
                   Total timesteps: 54738944
                    Iteration time: 8.30s
                        Total time: 32939.73s
                               ETA: 952994.3s

################################################################################
                    [1m Learning iteration 3341/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.251s, learning 0.163s)
               Value function loss: 106.1619
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 53.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0026
--------------------------------------------------------------------------------
                   Total timesteps: 54755328
                    Iteration time: 8.41s
                        Total time: 32948.14s
                               ETA: 952942.7s

################################################################################
                    [1m Learning iteration 3342/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.229s, learning 0.167s)
               Value function loss: 0.6231
                    Surrogate loss: -0.0163
             Mean action noise std: 0.69
                       Mean reward: 8.30
               Mean episode length: 53.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54771712
                    Iteration time: 8.40s
                        Total time: 32956.54s
                               ETA: 952890.5s

################################################################################
                    [1m Learning iteration 3343/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.197s, learning 0.162s)
               Value function loss: 16.4228
                    Surrogate loss: 0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 53.09
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 54788096
                    Iteration time: 8.36s
                        Total time: 32964.90s
                               ETA: 952837.3s

################################################################################
                    [1m Learning iteration 3344/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.214s, learning 0.165s)
               Value function loss: 0.6742
                    Surrogate loss: -0.0211
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 53.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0036
--------------------------------------------------------------------------------
                   Total timesteps: 54804480
                    Iteration time: 8.38s
                        Total time: 32973.28s
                               ETA: 952784.7s

################################################################################
                    [1m Learning iteration 3345/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.398s, learning 0.211s)
               Value function loss: 0.5088
                    Surrogate loss: -0.0158
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 52.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0033
--------------------------------------------------------------------------------
                   Total timesteps: 54820864
                    Iteration time: 8.61s
                        Total time: 32981.89s
                               ETA: 952738.8s

################################################################################
                    [1m Learning iteration 3346/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.262s, learning 0.168s)
               Value function loss: 90.2771
                    Surrogate loss: 0.0078
             Mean action noise std: 0.69
                       Mean reward: 5.28
               Mean episode length: 51.73
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 54837248
                    Iteration time: 8.43s
                        Total time: 32990.32s
                               ETA: 952687.8s

################################################################################
                    [1m Learning iteration 3347/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.185s, learning 0.162s)
               Value function loss: 4.3155
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 53.59
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 54853632
                    Iteration time: 8.35s
                        Total time: 32998.66s
                               ETA: 952634.3s

################################################################################
                    [1m Learning iteration 3348/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.075s, learning 0.174s)
               Value function loss: 0.7990
                    Surrogate loss: -0.0161
             Mean action noise std: 0.69
                       Mean reward: 3.33
               Mean episode length: 53.12
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54870016
                    Iteration time: 8.25s
                        Total time: 33006.91s
                               ETA: 952578.1s

################################################################################
                    [1m Learning iteration 3349/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.347s, learning 0.177s)
               Value function loss: 124.9130
                    Surrogate loss: 0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 52.69
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54886400
                    Iteration time: 8.52s
                        Total time: 33015.43s
                               ETA: 952529.8s

################################################################################
                    [1m Learning iteration 3350/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.440s, learning 0.165s)
               Value function loss: 97.6170
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 2.95
               Mean episode length: 51.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0034
--------------------------------------------------------------------------------
                   Total timesteps: 54902784
                    Iteration time: 8.60s
                        Total time: 33024.04s
                               ETA: 952483.9s

################################################################################
                    [1m Learning iteration 3351/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.074s, learning 0.176s)
               Value function loss: 130.2806
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.29
               Mean episode length: 53.16
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0031
--------------------------------------------------------------------------------
                   Total timesteps: 54919168
                    Iteration time: 8.25s
                        Total time: 33032.29s
                               ETA: 952427.7s

################################################################################
                    [1m Learning iteration 3352/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.320s, learning 0.168s)
               Value function loss: 59.9048
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 4.25
               Mean episode length: 54.57
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0029
--------------------------------------------------------------------------------
                   Total timesteps: 54935552
                    Iteration time: 8.49s
                        Total time: 33040.78s
                               ETA: 952378.5s

################################################################################
                    [1m Learning iteration 3353/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.170s, learning 0.219s)
               Value function loss: 19.7330
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 55.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0048
--------------------------------------------------------------------------------
                   Total timesteps: 54951936
                    Iteration time: 8.39s
                        Total time: 33049.17s
                               ETA: 952326.4s

################################################################################
                    [1m Learning iteration 3354/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.936s, learning 0.189s)
               Value function loss: 170.0079
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 52.61
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 54968320
                    Iteration time: 8.12s
                        Total time: 33057.29s
                               ETA: 952266.8s

################################################################################
                    [1m Learning iteration 3355/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.299s, learning 0.164s)
               Value function loss: 2.7797
                    Surrogate loss: -0.0164
             Mean action noise std: 0.69
                       Mean reward: 8.64
               Mean episode length: 53.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 54984704
                    Iteration time: 8.46s
                        Total time: 33065.75s
                               ETA: 952216.9s

################################################################################
                    [1m Learning iteration 3356/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.949s, learning 0.162s)
               Value function loss: 30.4271
                    Surrogate loss: 0.0034
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 52.14
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 55001088
                    Iteration time: 8.11s
                        Total time: 33073.87s
                               ETA: 952156.9s

################################################################################
                    [1m Learning iteration 3357/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.418s, learning 0.161s)
               Value function loss: 1.0444
                    Surrogate loss: -0.0242
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 53.63
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 55017472
                    Iteration time: 8.58s
                        Total time: 33082.44s
                               ETA: 952110.4s

################################################################################
                    [1m Learning iteration 3358/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.280s, learning 0.163s)
               Value function loss: 0.9925
                    Surrogate loss: -0.0122
             Mean action noise std: 0.69
                       Mean reward: 9.00
               Mean episode length: 53.87
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 55033856
                    Iteration time: 8.44s
                        Total time: 33090.89s
                               ETA: 952060.0s

################################################################################
                    [1m Learning iteration 3359/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.294s, learning 0.165s)
               Value function loss: 0.5809
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 51.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0063
--------------------------------------------------------------------------------
                   Total timesteps: 55050240
                    Iteration time: 8.46s
                        Total time: 33099.35s
                               ETA: 952010.1s

################################################################################
                    [1m Learning iteration 3360/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.297s, learning 0.164s)
               Value function loss: 130.7821
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 53.11
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 55066624
                    Iteration time: 8.46s
                        Total time: 33107.81s
                               ETA: 951960.3s

################################################################################
                    [1m Learning iteration 3361/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.239s, learning 0.161s)
               Value function loss: 373.0361
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 52.34
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0054
--------------------------------------------------------------------------------
                   Total timesteps: 55083008
                    Iteration time: 8.40s
                        Total time: 33116.21s
                               ETA: 951908.8s

################################################################################
                    [1m Learning iteration 3362/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.348s, learning 0.162s)
               Value function loss: 378.5004
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 53.01
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0049
--------------------------------------------------------------------------------
                   Total timesteps: 55099392
                    Iteration time: 8.51s
                        Total time: 33124.72s
                               ETA: 951860.4s

################################################################################
                    [1m Learning iteration 3363/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.158s, learning 0.162s)
               Value function loss: 5.8210
                    Surrogate loss: -0.0155
             Mean action noise std: 0.69
                       Mean reward: 13.18
               Mean episode length: 52.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 55115776
                    Iteration time: 8.32s
                        Total time: 33133.04s
                               ETA: 951806.6s

################################################################################
                    [1m Learning iteration 3364/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.315s, learning 0.169s)
               Value function loss: 14.1145
                    Surrogate loss: 0.0003
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 53.22
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0055
--------------------------------------------------------------------------------
                   Total timesteps: 55132160
                    Iteration time: 8.48s
                        Total time: 33141.52s
                               ETA: 951757.5s

################################################################################
                    [1m Learning iteration 3365/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.570s, learning 0.167s)
               Value function loss: 2.3849
                    Surrogate loss: -0.0194
             Mean action noise std: 0.69
                       Mean reward: 10.89
               Mean episode length: 52.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0105
--------------------------------------------------------------------------------
                   Total timesteps: 55148544
                    Iteration time: 8.74s
                        Total time: 33150.26s
                               ETA: 951715.8s

################################################################################
                    [1m Learning iteration 3366/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.159s)
               Value function loss: 46.4460
                    Surrogate loss: 0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 53.34
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0097
--------------------------------------------------------------------------------
                   Total timesteps: 55164928
                    Iteration time: 8.49s
                        Total time: 33158.74s
                               ETA: 951666.8s

################################################################################
                    [1m Learning iteration 3367/100000 [0m                    

                       Computation: 2018 steps/s (collection: 7.953s, learning 0.164s)
               Value function loss: 1.5160
                    Surrogate loss: -0.0217
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 52.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 55181312
                    Iteration time: 8.12s
                        Total time: 33166.86s
                               ETA: 951607.3s

################################################################################
                    [1m Learning iteration 3368/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.189s, learning 0.179s)
               Value function loss: 0.9290
                    Surrogate loss: -0.0182
             Mean action noise std: 0.69
                       Mean reward: 6.00
               Mean episode length: 52.41
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 55197696
                    Iteration time: 8.37s
                        Total time: 33175.23s
                               ETA: 951555.0s

################################################################################
                    [1m Learning iteration 3369/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.304s, learning 0.161s)
               Value function loss: 39.5906
                    Surrogate loss: 0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.95
               Mean episode length: 51.95
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 55214080
                    Iteration time: 8.47s
                        Total time: 33183.70s
                               ETA: 951505.5s

################################################################################
                    [1m Learning iteration 3370/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.133s, learning 0.163s)
               Value function loss: 128.1354
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 53.47
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 55230464
                    Iteration time: 8.30s
                        Total time: 33191.99s
                               ETA: 951451.2s

################################################################################
                    [1m Learning iteration 3371/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.114s, learning 0.166s)
               Value function loss: 0.9451
                    Surrogate loss: -0.0166
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 52.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 55246848
                    Iteration time: 8.28s
                        Total time: 33200.27s
                               ETA: 951396.5s

################################################################################
                    [1m Learning iteration 3372/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.114s, learning 0.164s)
               Value function loss: 83.4103
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 52.10
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 55263232
                    Iteration time: 8.28s
                        Total time: 33208.55s
                               ETA: 951341.7s

################################################################################
                    [1m Learning iteration 3373/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.296s, learning 0.164s)
               Value function loss: 0.8801
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 13.65
               Mean episode length: 53.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 55279616
                    Iteration time: 8.46s
                        Total time: 33217.01s
                               ETA: 951292.2s

################################################################################
                    [1m Learning iteration 3374/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.397s, learning 0.163s)
               Value function loss: 63.2310
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.76
               Mean episode length: 53.27
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0072
--------------------------------------------------------------------------------
                   Total timesteps: 55296000
                    Iteration time: 8.56s
                        Total time: 33225.57s
                               ETA: 951245.6s

################################################################################
                    [1m Learning iteration 3375/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.109s, learning 0.171s)
               Value function loss: 56.4441
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 6.03
               Mean episode length: 53.61
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 55312384
                    Iteration time: 8.28s
                        Total time: 33233.85s
                               ETA: 951190.9s

################################################################################
                    [1m Learning iteration 3376/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.302s, learning 0.165s)
               Value function loss: 16.0783
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 51.52
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 55328768
                    Iteration time: 8.47s
                        Total time: 33242.32s
                               ETA: 951141.7s

################################################################################
                    [1m Learning iteration 3377/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.107s, learning 0.163s)
               Value function loss: 51.5873
                    Surrogate loss: 0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 51.75
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 55345152
                    Iteration time: 8.27s
                        Total time: 33250.59s
                               ETA: 951086.8s

################################################################################
                    [1m Learning iteration 3378/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.054s, learning 0.165s)
               Value function loss: 2.4865
                    Surrogate loss: -0.0161
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 52.18
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 55361536
                    Iteration time: 8.22s
                        Total time: 33258.80s
                               ETA: 951030.5s

################################################################################
                    [1m Learning iteration 3379/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.089s, learning 0.175s)
               Value function loss: 104.9166
                    Surrogate loss: 0.0051
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 52.43
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 55377920
                    Iteration time: 8.26s
                        Total time: 33267.07s
                               ETA: 950975.6s

################################################################################
                    [1m Learning iteration 3380/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.299s, learning 0.163s)
               Value function loss: 357.4126
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.78
               Mean episode length: 53.21
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 55394304
                    Iteration time: 8.46s
                        Total time: 33275.53s
                               ETA: 950926.3s

################################################################################
                    [1m Learning iteration 3381/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.304s, learning 0.169s)
               Value function loss: 280.6914
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 52.26
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0065
--------------------------------------------------------------------------------
                   Total timesteps: 55410688
                    Iteration time: 8.47s
                        Total time: 33284.00s
                               ETA: 950877.3s

################################################################################
                    [1m Learning iteration 3382/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.229s, learning 0.165s)
               Value function loss: 40.7308
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 11.31
               Mean episode length: 53.70
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 55427072
                    Iteration time: 8.39s
                        Total time: 33292.40s
                               ETA: 950826.1s

################################################################################
                    [1m Learning iteration 3383/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.276s, learning 0.160s)
               Value function loss: 14.6005
                    Surrogate loss: -0.0094
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 52.83
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0071
--------------------------------------------------------------------------------
                   Total timesteps: 55443456
                    Iteration time: 8.44s
                        Total time: 33300.83s
                               ETA: 950776.2s

################################################################################
                    [1m Learning iteration 3384/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.163s)
               Value function loss: 78.6139
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 54.01
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 55459840
                    Iteration time: 8.24s
                        Total time: 33309.08s
                               ETA: 950720.8s

################################################################################
                    [1m Learning iteration 3385/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.240s, learning 0.218s)
               Value function loss: 3.1690
                    Surrogate loss: -0.0215
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 53.23
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0091
--------------------------------------------------------------------------------
                   Total timesteps: 55476224
                    Iteration time: 8.46s
                        Total time: 33317.54s
                               ETA: 950671.5s

################################################################################
                    [1m Learning iteration 3386/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.209s, learning 0.161s)
               Value function loss: 1.5746
                    Surrogate loss: -0.0101
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 53.30
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 55492608
                    Iteration time: 8.37s
                        Total time: 33325.91s
                               ETA: 950619.8s

################################################################################
                    [1m Learning iteration 3387/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.091s, learning 0.199s)
               Value function loss: 17.0375
                    Surrogate loss: 0.0079
             Mean action noise std: 0.69
                       Mean reward: 3.50
               Mean episode length: 53.58
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 55508992
                    Iteration time: 8.29s
                        Total time: 33334.20s
                               ETA: 950565.7s

################################################################################
                    [1m Learning iteration 3388/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.198s, learning 0.160s)
               Value function loss: 213.9188
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 53.60
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 55525376
                    Iteration time: 8.36s
                        Total time: 33342.55s
                               ETA: 950513.7s

################################################################################
                    [1m Learning iteration 3389/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.117s, learning 0.162s)
               Value function loss: 94.1284
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 53.03
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 55541760
                    Iteration time: 8.28s
                        Total time: 33350.83s
                               ETA: 950459.4s

################################################################################
                    [1m Learning iteration 3390/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.153s, learning 0.158s)
               Value function loss: 0.9060
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 23.38
               Mean episode length: 52.26
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0090
--------------------------------------------------------------------------------
                   Total timesteps: 55558144
                    Iteration time: 8.31s
                        Total time: 33359.14s
                               ETA: 950406.1s

################################################################################
                    [1m Learning iteration 3391/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.442s, learning 0.164s)
               Value function loss: 17.7352
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 50.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 55574528
                    Iteration time: 8.61s
                        Total time: 33367.75s
                               ETA: 950361.1s

################################################################################
                    [1m Learning iteration 3392/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.223s, learning 0.163s)
               Value function loss: 110.4983
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 54.33
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 55590912
                    Iteration time: 8.39s
                        Total time: 33376.14s
                               ETA: 950310.0s

################################################################################
                    [1m Learning iteration 3393/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.196s, learning 0.163s)
               Value function loss: 69.6781
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 3.66
               Mean episode length: 53.89
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 55607296
                    Iteration time: 8.36s
                        Total time: 33384.49s
                               ETA: 950258.1s

################################################################################
                    [1m Learning iteration 3394/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.066s, learning 0.163s)
               Value function loss: 62.2049
                    Surrogate loss: 0.0013
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 52.60
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 55623680
                    Iteration time: 8.23s
                        Total time: 33392.72s
                               ETA: 950202.5s

################################################################################
                    [1m Learning iteration 3395/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.282s, learning 0.163s)
               Value function loss: 25.7883
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 51.79
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 55640064
                    Iteration time: 8.45s
                        Total time: 33401.17s
                               ETA: 950153.1s

################################################################################
                    [1m Learning iteration 3396/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.274s, learning 0.163s)
               Value function loss: 1.6899
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 10.99
               Mean episode length: 53.32
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 55656448
                    Iteration time: 8.44s
                        Total time: 33409.61s
                               ETA: 950103.5s

################################################################################
                    [1m Learning iteration 3397/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.402s, learning 0.176s)
               Value function loss: 53.0650
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 52.77
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 55672832
                    Iteration time: 8.58s
                        Total time: 33418.18s
                               ETA: 950057.9s

################################################################################
                    [1m Learning iteration 3398/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.276s, learning 0.180s)
               Value function loss: 1.1075
                    Surrogate loss: -0.0146
             Mean action noise std: 0.69
                       Mean reward: 4.07
               Mean episode length: 53.66
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0078
--------------------------------------------------------------------------------
                   Total timesteps: 55689216
                    Iteration time: 8.46s
                        Total time: 33426.64s
                               ETA: 950008.9s

################################################################################
                    [1m Learning iteration 3399/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.170s, learning 0.163s)
               Value function loss: 1.2240
                    Surrogate loss: -0.0139
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 53.42
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0076
--------------------------------------------------------------------------------
                   Total timesteps: 55705600
                    Iteration time: 8.33s
                        Total time: 33434.97s
                               ETA: 949956.5s

################################################################################
                    [1m Learning iteration 3400/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.219s, learning 0.164s)
               Value function loss: 15.4995
                    Surrogate loss: 0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 52.03
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0070
--------------------------------------------------------------------------------
                   Total timesteps: 55721984
                    Iteration time: 8.38s
                        Total time: 33443.36s
                               ETA: 949905.4s

################################################################################
                    [1m Learning iteration 3401/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.094s, learning 0.162s)
               Value function loss: 0.7335
                    Surrogate loss: -0.0171
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 53.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 55738368
                    Iteration time: 8.26s
                        Total time: 33451.61s
                               ETA: 949850.8s

################################################################################
                    [1m Learning iteration 3402/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.324s, learning 0.164s)
               Value function loss: 0.6072
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 2.96
               Mean episode length: 51.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0064
--------------------------------------------------------------------------------
                   Total timesteps: 55754752
                    Iteration time: 8.49s
                        Total time: 33460.10s
                               ETA: 949802.8s

################################################################################
                    [1m Learning iteration 3403/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.191s, learning 0.163s)
               Value function loss: 106.0347
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 4.23
               Mean episode length: 55.32
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 55771136
                    Iteration time: 8.35s
                        Total time: 33468.45s
                               ETA: 949751.0s

################################################################################
                    [1m Learning iteration 3404/100000 [0m                    

                       Computation: 2006 steps/s (collection: 8.001s, learning 0.164s)
               Value function loss: 59.9087
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.61
               Mean episode length: 53.24
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 55787520
                    Iteration time: 8.17s
                        Total time: 33476.62s
                               ETA: 949693.9s

################################################################################
                    [1m Learning iteration 3405/100000 [0m                    

                       Computation: 2005 steps/s (collection: 8.008s, learning 0.160s)
               Value function loss: 439.9748
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.33
               Mean episode length: 52.97
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 55803904
                    Iteration time: 8.17s
                        Total time: 33484.79s
                               ETA: 949636.8s

################################################################################
                    [1m Learning iteration 3406/100000 [0m                    

                       Computation: 1067 steps/s (collection: 15.183s, learning 0.165s)
               Value function loss: 229.3897
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 10.92
               Mean episode length: 52.05
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0057
--------------------------------------------------------------------------------
                   Total timesteps: 55820288
                    Iteration time: 15.35s
                        Total time: 33500.14s
                               ETA: 949783.4s

################################################################################
                    [1m Learning iteration 3407/100000 [0m                    

                       Computation: 1023 steps/s (collection: 15.845s, learning 0.166s)
               Value function loss: 152.3943
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 8.54
               Mean episode length: 53.24
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 55836672
                    Iteration time: 16.01s
                        Total time: 33516.15s
                               ETA: 949948.7s

################################################################################
                    [1m Learning iteration 3408/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.371s, learning 0.164s)
               Value function loss: 52.0074
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 54.41
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 55853056
                    Iteration time: 16.53s
                        Total time: 33532.68s
                               ETA: 950128.8s

################################################################################
                    [1m Learning iteration 3409/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.197s, learning 0.169s)
               Value function loss: 222.4412
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 53.00
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 55869440
                    Iteration time: 16.37s
                        Total time: 33549.05s
                               ETA: 950303.9s

################################################################################
                    [1m Learning iteration 3410/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.065s, learning 0.165s)
               Value function loss: 324.2149
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 6.08
               Mean episode length: 53.76
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 55885824
                    Iteration time: 16.23s
                        Total time: 33565.28s
                               ETA: 950475.0s

################################################################################
                    [1m Learning iteration 3411/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.276s, learning 0.168s)
               Value function loss: 37.5980
                    Surrogate loss: -0.0101
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 53.28
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 55902208
                    Iteration time: 16.44s
                        Total time: 33581.72s
                               ETA: 950652.1s

################################################################################
                    [1m Learning iteration 3412/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.115s, learning 0.168s)
               Value function loss: 14.9174
                    Surrogate loss: -0.0137
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 53.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 55918592
                    Iteration time: 16.28s
                        Total time: 33598.01s
                               ETA: 950824.6s

################################################################################
                    [1m Learning iteration 3413/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.357s, learning 0.164s)
               Value function loss: 122.0832
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 19.21
               Mean episode length: 53.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 55934976
                    Iteration time: 16.52s
                        Total time: 33614.53s
                               ETA: 951003.6s

################################################################################
                    [1m Learning iteration 3414/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.782s, learning 0.169s)
               Value function loss: 34.6774
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 52.50
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 55951360
                    Iteration time: 15.95s
                        Total time: 33630.48s
                               ETA: 951166.4s

################################################################################
                    [1m Learning iteration 3415/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.295s, learning 0.176s)
               Value function loss: 256.4938
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 53.05
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 55967744
                    Iteration time: 16.47s
                        Total time: 33646.95s
                               ETA: 951343.8s

################################################################################
                    [1m Learning iteration 3416/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.119s, learning 0.166s)
               Value function loss: 12.5467
                    Surrogate loss: -0.0146
             Mean action noise std: 0.69
                       Mean reward: 8.58
               Mean episode length: 53.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 55984128
                    Iteration time: 16.29s
                        Total time: 33663.23s
                               ETA: 951515.9s

################################################################################
                    [1m Learning iteration 3417/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.070s, learning 0.163s)
               Value function loss: 79.7071
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 6.12
               Mean episode length: 52.59
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 56000512
                    Iteration time: 16.23s
                        Total time: 33679.47s
                               ETA: 951686.4s

################################################################################
                    [1m Learning iteration 3418/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.045s, learning 0.176s)
               Value function loss: 197.1350
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 14.00
               Mean episode length: 54.34
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 56016896
                    Iteration time: 16.22s
                        Total time: 33695.69s
                               ETA: 951856.4s

################################################################################
                    [1m Learning iteration 3419/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.086s, learning 0.162s)
               Value function loss: 49.7306
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 52.19
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 56033280
                    Iteration time: 16.25s
                        Total time: 33711.94s
                               ETA: 952027.1s

################################################################################
                    [1m Learning iteration 3420/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.462s, learning 0.187s)
               Value function loss: 4.0708
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 18.72
               Mean episode length: 53.68
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 56049664
                    Iteration time: 16.65s
                        Total time: 33728.59s
                               ETA: 952209.0s

################################################################################
                    [1m Learning iteration 3421/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.459s, learning 0.210s)
               Value function loss: 62.1831
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.87
               Mean episode length: 54.17
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 56066048
                    Iteration time: 16.67s
                        Total time: 33745.26s
                               ETA: 952391.3s

################################################################################
                    [1m Learning iteration 3422/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.012s, learning 0.162s)
               Value function loss: 641.2356
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 13.98
               Mean episode length: 53.37
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 56082432
                    Iteration time: 16.17s
                        Total time: 33761.43s
                               ETA: 952559.6s

################################################################################
                    [1m Learning iteration 3423/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.481s, learning 0.167s)
               Value function loss: 311.6884
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 54.02
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 56098816
                    Iteration time: 16.65s
                        Total time: 33778.08s
                               ETA: 952741.1s

################################################################################
                    [1m Learning iteration 3424/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.384s, learning 0.164s)
               Value function loss: 276.2806
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 53.07
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 56115200
                    Iteration time: 16.55s
                        Total time: 33794.63s
                               ETA: 952919.6s

################################################################################
                    [1m Learning iteration 3425/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.210s, learning 0.166s)
               Value function loss: 127.3374
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 53.60
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 56131584
                    Iteration time: 16.38s
                        Total time: 33811.00s
                               ETA: 953093.2s

################################################################################
                    [1m Learning iteration 3426/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.316s, learning 0.162s)
               Value function loss: 226.2376
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 19.01
               Mean episode length: 53.75
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 56147968
                    Iteration time: 16.48s
                        Total time: 33827.48s
                               ETA: 953269.6s

################################################################################
                    [1m Learning iteration 3427/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.105s, learning 0.166s)
               Value function loss: 37.2222
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 3.62
               Mean episode length: 53.31
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 56164352
                    Iteration time: 16.27s
                        Total time: 33843.75s
                               ETA: 953440.0s

################################################################################
                    [1m Learning iteration 3428/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.421s, learning 0.171s)
               Value function loss: 7.8653
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 13.55
               Mean episode length: 53.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 56180736
                    Iteration time: 16.59s
                        Total time: 33860.34s
                               ETA: 953619.4s

################################################################################
                    [1m Learning iteration 3429/100000 [0m                    

                       Computation: 989 steps/s (collection: 16.388s, learning 0.164s)
               Value function loss: 18.4634
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 53.50
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 56197120
                    Iteration time: 16.55s
                        Total time: 33876.89s
                               ETA: 953797.6s

################################################################################
                    [1m Learning iteration 3430/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.333s, learning 0.180s)
               Value function loss: 6.5672
                    Surrogate loss: -0.0130
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 54.16
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 56213504
                    Iteration time: 16.51s
                        Total time: 33893.41s
                               ETA: 953974.5s

################################################################################
                    [1m Learning iteration 3431/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.137s, learning 0.185s)
               Value function loss: 131.0073
                    Surrogate loss: 0.0028
             Mean action noise std: 0.69
                       Mean reward: 4.29
               Mean episode length: 55.53
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 56229888
                    Iteration time: 16.32s
                        Total time: 33909.73s
                               ETA: 954145.9s

################################################################################
                    [1m Learning iteration 3432/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.179s, learning 0.167s)
               Value function loss: 17.3660
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.77
               Mean episode length: 54.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 56246272
                    Iteration time: 16.35s
                        Total time: 33926.08s
                               ETA: 954317.9s

################################################################################
                    [1m Learning iteration 3433/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.021s, learning 0.206s)
               Value function loss: 263.1352
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.66
               Mean episode length: 54.12
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 56262656
                    Iteration time: 16.23s
                        Total time: 33942.30s
                               ETA: 954486.4s

################################################################################
                    [1m Learning iteration 3434/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.291s, learning 0.167s)
               Value function loss: 780.1197
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 6.47
               Mean episode length: 55.10
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 56279040
                    Iteration time: 16.46s
                        Total time: 33958.76s
                               ETA: 954661.3s

################################################################################
                    [1m Learning iteration 3435/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.213s, learning 0.182s)
               Value function loss: 246.6129
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 13.72
               Mean episode length: 53.51
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 56295424
                    Iteration time: 16.40s
                        Total time: 33975.16s
                               ETA: 954834.4s

################################################################################
                    [1m Learning iteration 3436/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.001s, learning 0.174s)
               Value function loss: 305.6753
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 18.55
               Mean episode length: 52.89
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 56311808
                    Iteration time: 16.17s
                        Total time: 33991.33s
                               ETA: 955001.1s

################################################################################
                    [1m Learning iteration 3437/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.248s, learning 0.166s)
               Value function loss: 250.8014
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 11.03
               Mean episode length: 53.02
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0192
--------------------------------------------------------------------------------
                   Total timesteps: 56328192
                    Iteration time: 16.41s
                        Total time: 34007.74s
                               ETA: 955174.5s

################################################################################
                    [1m Learning iteration 3438/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.461s, learning 0.213s)
               Value function loss: 553.1459
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.51
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 56344576
                    Iteration time: 16.67s
                        Total time: 34024.42s
                               ETA: 955355.0s

################################################################################
                    [1m Learning iteration 3439/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.173s, learning 0.161s)
               Value function loss: 487.7692
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 15.98
               Mean episode length: 53.23
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0218
--------------------------------------------------------------------------------
                   Total timesteps: 56360960
                    Iteration time: 16.33s
                        Total time: 34040.75s
                               ETA: 955525.9s

################################################################################
                    [1m Learning iteration 3440/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.985s, learning 0.176s)
               Value function loss: 204.9192
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 53.72
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0215
--------------------------------------------------------------------------------
                   Total timesteps: 56377344
                    Iteration time: 16.16s
                        Total time: 34056.91s
                               ETA: 955691.8s

################################################################################
                    [1m Learning iteration 3441/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.122s, learning 0.205s)
               Value function loss: 321.6432
                    Surrogate loss: -0.0005
             Mean action noise std: 0.69
                       Mean reward: 14.31
               Mean episode length: 54.44
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0241
--------------------------------------------------------------------------------
                   Total timesteps: 56393728
                    Iteration time: 16.33s
                        Total time: 34073.24s
                               ETA: 955862.3s

################################################################################
                    [1m Learning iteration 3442/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.014s, learning 0.218s)
               Value function loss: 108.0116
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 53.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 56410112
                    Iteration time: 16.23s
                        Total time: 34089.47s
                               ETA: 956030.0s

################################################################################
                    [1m Learning iteration 3443/100000 [0m                    

                       Computation: 1124 steps/s (collection: 14.394s, learning 0.172s)
               Value function loss: 240.8917
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 53.01
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 56426496
                    Iteration time: 14.57s
                        Total time: 34104.04s
                               ETA: 956150.9s

################################################################################
                    [1m Learning iteration 3444/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.392s, learning 0.168s)
               Value function loss: 129.1125
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 8.58
               Mean episode length: 52.32
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 56442880
                    Iteration time: 8.56s
                        Total time: 34112.60s
                               ETA: 956103.4s

################################################################################
                    [1m Learning iteration 3445/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.320s, learning 0.170s)
               Value function loss: 118.0608
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 52.51
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 56459264
                    Iteration time: 8.49s
                        Total time: 34121.09s
                               ETA: 956053.9s

################################################################################
                    [1m Learning iteration 3446/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.521s, learning 0.178s)
               Value function loss: 106.9199
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.26
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 56475648
                    Iteration time: 8.70s
                        Total time: 34129.79s
                               ETA: 956010.3s

################################################################################
                    [1m Learning iteration 3447/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.217s, learning 0.164s)
               Value function loss: 33.2959
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 52.11
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0242
--------------------------------------------------------------------------------
                   Total timesteps: 56492032
                    Iteration time: 8.38s
                        Total time: 34138.17s
                               ETA: 955957.8s

################################################################################
                    [1m Learning iteration 3448/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.370s, learning 0.177s)
               Value function loss: 7.9370
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 53.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0223
--------------------------------------------------------------------------------
                   Total timesteps: 56508416
                    Iteration time: 8.55s
                        Total time: 34146.71s
                               ETA: 955910.0s

################################################################################
                    [1m Learning iteration 3449/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.461s, learning 0.167s)
               Value function loss: 217.7977
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 54.47
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 56524800
                    Iteration time: 8.63s
                        Total time: 34155.34s
                               ETA: 955864.5s

################################################################################
                    [1m Learning iteration 3450/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.294s, learning 0.207s)
               Value function loss: 3.0588
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 53.75
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0198
--------------------------------------------------------------------------------
                   Total timesteps: 56541184
                    Iteration time: 8.50s
                        Total time: 34163.84s
                               ETA: 955815.5s

################################################################################
                    [1m Learning iteration 3451/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.415s, learning 0.213s)
               Value function loss: 2.2792
                    Surrogate loss: -0.0104
             Mean action noise std: 0.69
                       Mean reward: 3.80
               Mean episode length: 54.51
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 56557568
                    Iteration time: 8.63s
                        Total time: 34172.47s
                               ETA: 955770.0s

################################################################################
                    [1m Learning iteration 3452/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.143s, learning 0.161s)
               Value function loss: 126.3157
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 53.84
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 56573952
                    Iteration time: 8.30s
                        Total time: 34180.78s
                               ETA: 955715.5s

################################################################################
                    [1m Learning iteration 3453/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.475s, learning 0.163s)
               Value function loss: 3.8867
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 3.79
               Mean episode length: 53.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 56590336
                    Iteration time: 8.64s
                        Total time: 34189.41s
                               ETA: 955670.3s

################################################################################
                    [1m Learning iteration 3454/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.104s, learning 0.161s)
               Value function loss: 2.7391
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 53.58
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 56606720
                    Iteration time: 8.27s
                        Total time: 34197.68s
                               ETA: 955614.8s

################################################################################
                    [1m Learning iteration 3455/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.393s, learning 0.167s)
               Value function loss: 2.3562
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 11.40
               Mean episode length: 54.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 56623104
                    Iteration time: 8.56s
                        Total time: 34206.24s
                               ETA: 955567.5s

################################################################################
                    [1m Learning iteration 3456/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.274s, learning 0.164s)
               Value function loss: 19.9157
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 52.66
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 56639488
                    Iteration time: 8.44s
                        Total time: 34214.68s
                               ETA: 955516.8s

################################################################################
                    [1m Learning iteration 3457/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.395s, learning 0.184s)
               Value function loss: 106.2597
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 53.12
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 56655872
                    Iteration time: 8.58s
                        Total time: 34223.26s
                               ETA: 955470.1s

################################################################################
                    [1m Learning iteration 3458/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.308s, learning 0.164s)
               Value function loss: 36.1088
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0124
--------------------------------------------------------------------------------
                   Total timesteps: 56672256
                    Iteration time: 8.47s
                        Total time: 34231.73s
                               ETA: 955420.5s

################################################################################
                    [1m Learning iteration 3459/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.228s, learning 0.166s)
               Value function loss: 74.7204
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.91
               Mean episode length: 52.95
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 56688640
                    Iteration time: 8.39s
                        Total time: 34240.12s
                               ETA: 955368.7s

################################################################################
                    [1m Learning iteration 3460/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.245s, learning 0.161s)
               Value function loss: 34.6557
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 16.46
               Mean episode length: 53.87
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 56705024
                    Iteration time: 8.41s
                        Total time: 34248.53s
                               ETA: 955317.2s

################################################################################
                    [1m Learning iteration 3461/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.155s, learning 0.160s)
               Value function loss: 285.0655
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 3.82
               Mean episode length: 54.55
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 56721408
                    Iteration time: 8.31s
                        Total time: 34256.84s
                               ETA: 955263.2s

################################################################################
                    [1m Learning iteration 3462/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.183s, learning 0.164s)
               Value function loss: 218.9660
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 6.78
               Mean episode length: 55.42
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 56737792
                    Iteration time: 8.35s
                        Total time: 34265.19s
                               ETA: 955210.2s

################################################################################
                    [1m Learning iteration 3463/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.327s, learning 0.163s)
               Value function loss: 229.6894
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 52.14
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 56754176
                    Iteration time: 8.49s
                        Total time: 34273.68s
                               ETA: 955161.1s

################################################################################
                    [1m Learning iteration 3464/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.560s, learning 0.160s)
               Value function loss: 166.8655
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 23.40
               Mean episode length: 52.29
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 56770560
                    Iteration time: 8.72s
                        Total time: 34282.40s
                               ETA: 955118.5s

################################################################################
                    [1m Learning iteration 3465/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.314s, learning 0.166s)
               Value function loss: 243.7797
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 53.27
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 56786944
                    Iteration time: 8.48s
                        Total time: 34290.88s
                               ETA: 955069.2s

################################################################################
                    [1m Learning iteration 3466/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.384s, learning 0.165s)
               Value function loss: 96.0280
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3.65
               Mean episode length: 53.04
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 56803328
                    Iteration time: 8.55s
                        Total time: 34299.43s
                               ETA: 955021.9s

################################################################################
                    [1m Learning iteration 3467/100000 [0m                    

                       Computation: 1896 steps/s (collection: 8.479s, learning 0.160s)
               Value function loss: 305.8576
                    Surrogate loss: 0.0043
             Mean action noise std: 0.69
                       Mean reward: 9.39
               Mean episode length: 55.79
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 56819712
                    Iteration time: 8.64s
                        Total time: 34308.07s
                               ETA: 954977.1s

################################################################################
                    [1m Learning iteration 3468/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.281s, learning 0.161s)
               Value function loss: 377.0413
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 18.78
               Mean episode length: 53.38
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 56836096
                    Iteration time: 8.44s
                        Total time: 34316.51s
                               ETA: 954926.8s

################################################################################
                    [1m Learning iteration 3469/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.063s, learning 0.158s)
               Value function loss: 427.4381
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3.74
               Mean episode length: 54.16
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 56852480
                    Iteration time: 8.22s
                        Total time: 34324.73s
                               ETA: 954870.4s

################################################################################
                    [1m Learning iteration 3470/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.392s, learning 0.162s)
               Value function loss: 47.2494
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 53.03
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 56868864
                    Iteration time: 8.55s
                        Total time: 34333.28s
                               ETA: 954823.3s

################################################################################
                    [1m Learning iteration 3471/100000 [0m                    

                       Computation: 2031 steps/s (collection: 7.901s, learning 0.165s)
               Value function loss: 15.0460
                    Surrogate loss: -0.0159
             Mean action noise std: 0.69
                       Mean reward: 28.71
               Mean episode length: 53.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0188
--------------------------------------------------------------------------------
                   Total timesteps: 56885248
                    Iteration time: 8.07s
                        Total time: 34341.35s
                               ETA: 954762.7s

################################################################################
                    [1m Learning iteration 3472/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.166s)
               Value function loss: 6.0565
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 8.70
               Mean episode length: 53.79
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 56901632
                    Iteration time: 8.43s
                        Total time: 34349.77s
                               ETA: 954712.1s

################################################################################
                    [1m Learning iteration 3473/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.171s, learning 0.162s)
               Value function loss: 182.5089
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 4.00
               Mean episode length: 54.79
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 56918016
                    Iteration time: 8.33s
                        Total time: 34358.11s
                               ETA: 954658.9s

################################################################################
                    [1m Learning iteration 3474/100000 [0m                    

                       Computation: 1985 steps/s (collection: 8.087s, learning 0.166s)
               Value function loss: 140.3328
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 53.26
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 56934400
                    Iteration time: 8.25s
                        Total time: 34366.36s
                               ETA: 954603.5s

################################################################################
                    [1m Learning iteration 3475/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.263s, learning 0.164s)
               Value function loss: 49.4705
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 3.57
               Mean episode length: 53.54
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 56950784
                    Iteration time: 8.43s
                        Total time: 34374.79s
                               ETA: 954553.0s

################################################################################
                    [1m Learning iteration 3476/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.277s, learning 0.165s)
               Value function loss: 362.2969
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.95
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 56967168
                    Iteration time: 8.44s
                        Total time: 34383.23s
                               ETA: 954502.9s

################################################################################
                    [1m Learning iteration 3477/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.382s, learning 0.176s)
               Value function loss: 630.8436
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 14.07
               Mean episode length: 54.11
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 56983552
                    Iteration time: 8.56s
                        Total time: 34391.79s
                               ETA: 954456.1s

################################################################################
                    [1m Learning iteration 3478/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.462s, learning 0.165s)
               Value function loss: 14.6277
                    Surrogate loss: -0.0144
             Mean action noise std: 0.69
                       Mean reward: 16.14
               Mean episode length: 53.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 56999936
                    Iteration time: 8.63s
                        Total time: 34400.41s
                               ETA: 954411.2s

################################################################################
                    [1m Learning iteration 3479/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.265s, learning 0.163s)
               Value function loss: 44.8985
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 54.18
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 57016320
                    Iteration time: 8.43s
                        Total time: 34408.84s
                               ETA: 954360.8s

################################################################################
                    [1m Learning iteration 3480/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.077s, learning 0.214s)
               Value function loss: 135.6995
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 38.78
               Mean episode length: 53.86
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 57032704
                    Iteration time: 8.29s
                        Total time: 34417.13s
                               ETA: 954306.7s

################################################################################
                    [1m Learning iteration 3481/100000 [0m                    

                       Computation: 1882 steps/s (collection: 8.527s, learning 0.177s)
               Value function loss: 247.7850
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.75
               Mean episode length: 54.01
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 57049088
                    Iteration time: 8.70s
                        Total time: 34425.84s
                               ETA: 954264.0s

################################################################################
                    [1m Learning iteration 3482/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.227s, learning 0.161s)
               Value function loss: 25.4189
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 8.77
               Mean episode length: 53.25
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 57065472
                    Iteration time: 8.39s
                        Total time: 34434.22s
                               ETA: 954212.6s

################################################################################
                    [1m Learning iteration 3483/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.471s, learning 0.158s)
               Value function loss: 82.8315
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 26.44
               Mean episode length: 55.16
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 57081856
                    Iteration time: 8.63s
                        Total time: 34442.85s
                               ETA: 954167.9s

################################################################################
                    [1m Learning iteration 3484/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.469s, learning 0.164s)
               Value function loss: 148.7596
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 53.59
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 57098240
                    Iteration time: 8.63s
                        Total time: 34451.49s
                               ETA: 954123.3s

################################################################################
                    [1m Learning iteration 3485/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.032s, learning 0.165s)
               Value function loss: 403.9798
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 8.87
               Mean episode length: 53.47
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 57114624
                    Iteration time: 8.20s
                        Total time: 34459.68s
                               ETA: 954066.6s

################################################################################
                    [1m Learning iteration 3486/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.258s, learning 0.175s)
               Value function loss: 255.9599
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 8.21
               Mean episode length: 53.54
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 57131008
                    Iteration time: 8.43s
                        Total time: 34468.11s
                               ETA: 954016.5s

################################################################################
                    [1m Learning iteration 3487/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.324s, learning 0.161s)
               Value function loss: 413.4118
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 6.20
               Mean episode length: 53.67
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 57147392
                    Iteration time: 8.48s
                        Total time: 34476.60s
                               ETA: 953967.9s

################################################################################
                    [1m Learning iteration 3488/100000 [0m                    

                       Computation: 1975 steps/s (collection: 8.127s, learning 0.165s)
               Value function loss: 628.4423
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 11.14
               Mean episode length: 53.86
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 57163776
                    Iteration time: 8.29s
                        Total time: 34484.89s
                               ETA: 953914.0s

################################################################################
                    [1m Learning iteration 3489/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.218s, learning 0.168s)
               Value function loss: 616.7424
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 24.04
               Mean episode length: 54.93
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 57180160
                    Iteration time: 8.39s
                        Total time: 34493.28s
                               ETA: 953862.7s

################################################################################
                    [1m Learning iteration 3490/100000 [0m                    

                       Computation: 1901 steps/s (collection: 8.407s, learning 0.208s)
               Value function loss: 306.8163
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 10.87
               Mean episode length: 52.39
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0200
--------------------------------------------------------------------------------
                   Total timesteps: 57196544
                    Iteration time: 8.61s
                        Total time: 34501.89s
                               ETA: 953817.7s

################################################################################
                    [1m Learning iteration 3491/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.319s, learning 0.162s)
               Value function loss: 355.6272
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 53.19
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 57212928
                    Iteration time: 8.48s
                        Total time: 34510.37s
                               ETA: 953769.1s

################################################################################
                    [1m Learning iteration 3492/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.522s, learning 0.208s)
               Value function loss: 401.6239
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 19.42
               Mean episode length: 54.88
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 57229312
                    Iteration time: 8.73s
                        Total time: 34519.10s
                               ETA: 953727.3s

################################################################################
                    [1m Learning iteration 3493/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.331s, learning 0.161s)
               Value function loss: 550.3120
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 4.17
               Mean episode length: 55.48
                  Mean reward/step: 0.33
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 57245696
                    Iteration time: 8.49s
                        Total time: 34527.60s
                               ETA: 953679.1s

################################################################################
                    [1m Learning iteration 3494/100000 [0m                    

                       Computation: 1895 steps/s (collection: 8.474s, learning 0.168s)
               Value function loss: 763.9745
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 23.66
               Mean episode length: 52.78
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0269
--------------------------------------------------------------------------------
                   Total timesteps: 57262080
                    Iteration time: 8.64s
                        Total time: 34536.24s
                               ETA: 953635.0s

################################################################################
                    [1m Learning iteration 3495/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.067s, learning 0.188s)
               Value function loss: 286.0710
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 19.29
               Mean episode length: 55.54
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 57278464
                    Iteration time: 8.26s
                        Total time: 34544.49s
                               ETA: 953580.2s

################################################################################
                    [1m Learning iteration 3496/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.181s, learning 0.165s)
               Value function loss: 189.8887
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 28.90
               Mean episode length: 53.63
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0302
--------------------------------------------------------------------------------
                   Total timesteps: 57294848
                    Iteration time: 8.35s
                        Total time: 34552.84s
                               ETA: 953527.9s

################################################################################
                    [1m Learning iteration 3497/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.166s)
               Value function loss: 144.4068
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 53.15
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 57311232
                    Iteration time: 8.36s
                        Total time: 34561.20s
                               ETA: 953476.1s

################################################################################
                    [1m Learning iteration 3498/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.519s, learning 0.164s)
               Value function loss: 662.1703
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 4.09
               Mean episode length: 54.32
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0292
--------------------------------------------------------------------------------
                   Total timesteps: 57327616
                    Iteration time: 8.68s
                        Total time: 34569.88s
                               ETA: 953433.2s

################################################################################
                    [1m Learning iteration 3499/100000 [0m                    

                       Computation: 1909 steps/s (collection: 8.421s, learning 0.161s)
               Value function loss: 312.8931
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 53.35
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0278
--------------------------------------------------------------------------------
                   Total timesteps: 57344000
                    Iteration time: 8.58s
                        Total time: 34578.46s
                               ETA: 953387.6s

################################################################################
                    [1m Learning iteration 3500/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.483s, learning 0.171s)
               Value function loss: 597.4073
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 3.93
               Mean episode length: 53.29
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0279
--------------------------------------------------------------------------------
                   Total timesteps: 57360384
                    Iteration time: 8.65s
                        Total time: 34587.12s
                               ETA: 953343.9s

################################################################################
                    [1m Learning iteration 3501/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.170s, learning 0.214s)
               Value function loss: 458.1759
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 8.56
               Mean episode length: 53.59
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 57376768
                    Iteration time: 8.38s
                        Total time: 34595.50s
                               ETA: 953292.8s

################################################################################
                    [1m Learning iteration 3502/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.315s, learning 0.163s)
               Value function loss: 471.0025
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 7.46
               Mean episode length: 56.25
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 57393152
                    Iteration time: 8.48s
                        Total time: 34603.98s
                               ETA: 953244.3s

################################################################################
                    [1m Learning iteration 3503/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.324s, learning 0.242s)
               Value function loss: 335.8341
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 4.24
               Mean episode length: 54.37
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 57409536
                    Iteration time: 8.57s
                        Total time: 34612.55s
                               ETA: 953198.3s

################################################################################
                    [1m Learning iteration 3504/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.299s, learning 0.171s)
               Value function loss: 451.5303
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 34.44
               Mean episode length: 55.88
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0317
--------------------------------------------------------------------------------
                   Total timesteps: 57425920
                    Iteration time: 8.47s
                        Total time: 34621.02s
                               ETA: 953149.6s

################################################################################
                    [1m Learning iteration 3505/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.274s, learning 0.159s)
               Value function loss: 266.4356
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 26.20
               Mean episode length: 54.01
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 57442304
                    Iteration time: 8.43s
                        Total time: 34629.45s
                               ETA: 953100.0s

################################################################################
                    [1m Learning iteration 3506/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.202s, learning 0.165s)
               Value function loss: 172.4673
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 53.43
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0309
--------------------------------------------------------------------------------
                   Total timesteps: 57458688
                    Iteration time: 8.37s
                        Total time: 34637.81s
                               ETA: 953048.6s

################################################################################
                    [1m Learning iteration 3507/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.403s, learning 0.163s)
               Value function loss: 26.1262
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 26.74
               Mean episode length: 54.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 57475072
                    Iteration time: 8.57s
                        Total time: 34646.38s
                               ETA: 953002.6s

################################################################################
                    [1m Learning iteration 3508/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.239s, learning 0.162s)
               Value function loss: 65.3305
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 4.31
               Mean episode length: 54.80
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 57491456
                    Iteration time: 8.40s
                        Total time: 34654.78s
                               ETA: 952952.2s

################################################################################
                    [1m Learning iteration 3509/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.252s, learning 0.165s)
               Value function loss: 11.3719
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 4.09
               Mean episode length: 54.83
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0293
--------------------------------------------------------------------------------
                   Total timesteps: 57507840
                    Iteration time: 8.42s
                        Total time: 34663.20s
                               ETA: 952902.2s

################################################################################
                    [1m Learning iteration 3510/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.241s, learning 0.159s)
               Value function loss: 5.4756
                    Surrogate loss: -0.0138
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 53.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0271
--------------------------------------------------------------------------------
                   Total timesteps: 57524224
                    Iteration time: 8.40s
                        Total time: 34671.60s
                               ETA: 952851.8s

################################################################################
                    [1m Learning iteration 3511/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.245s, learning 0.165s)
               Value function loss: 679.0470
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 53.42
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 57540608
                    Iteration time: 8.41s
                        Total time: 34680.01s
                               ETA: 952801.6s

################################################################################
                    [1m Learning iteration 3512/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.117s, learning 0.160s)
               Value function loss: 241.7530
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 52.78
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 57556992
                    Iteration time: 8.28s
                        Total time: 34688.29s
                               ETA: 952747.9s

################################################################################
                    [1m Learning iteration 3513/100000 [0m                    

                       Computation: 1898 steps/s (collection: 8.465s, learning 0.165s)
               Value function loss: 420.1539
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 21.09
               Mean episode length: 53.55
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0245
--------------------------------------------------------------------------------
                   Total timesteps: 57573376
                    Iteration time: 8.63s
                        Total time: 34696.92s
                               ETA: 952703.8s

################################################################################
                    [1m Learning iteration 3514/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.298s, learning 0.208s)
               Value function loss: 314.5697
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 14.18
               Mean episode length: 54.28
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 57589760
                    Iteration time: 8.51s
                        Total time: 34705.42s
                               ETA: 952656.4s

################################################################################
                    [1m Learning iteration 3515/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.199s, learning 0.210s)
               Value function loss: 624.0249
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 16.38
               Mean episode length: 54.27
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0231
--------------------------------------------------------------------------------
                   Total timesteps: 57606144
                    Iteration time: 8.41s
                        Total time: 34713.83s
                               ETA: 952606.4s

################################################################################
                    [1m Learning iteration 3516/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.323s, learning 0.166s)
               Value function loss: 594.3894
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 16.30
               Mean episode length: 53.40
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 57622528
                    Iteration time: 8.49s
                        Total time: 34722.32s
                               ETA: 952558.5s

################################################################################
                    [1m Learning iteration 3517/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.259s, learning 0.211s)
               Value function loss: 314.7392
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 52.87
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 57638912
                    Iteration time: 8.47s
                        Total time: 34730.79s
                               ETA: 952510.2s

################################################################################
                    [1m Learning iteration 3518/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.098s, learning 0.160s)
               Value function loss: 122.4433
                    Surrogate loss: -0.0079
             Mean action noise std: 0.69
                       Mean reward: 36.53
               Mean episode length: 54.32
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0294
--------------------------------------------------------------------------------
                   Total timesteps: 57655296
                    Iteration time: 8.26s
                        Total time: 34739.05s
                               ETA: 952456.0s

################################################################################
                    [1m Learning iteration 3519/100000 [0m                    

                       Computation: 1990 steps/s (collection: 8.070s, learning 0.161s)
               Value function loss: 75.4864
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 53.00
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 57671680
                    Iteration time: 8.23s
                        Total time: 34747.28s
                               ETA: 952401.2s

################################################################################
                    [1m Learning iteration 3520/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.193s, learning 0.161s)
               Value function loss: 396.4597
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 53.37
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0278
--------------------------------------------------------------------------------
                   Total timesteps: 57688064
                    Iteration time: 8.35s
                        Total time: 34755.63s
                               ETA: 952349.7s

################################################################################
                    [1m Learning iteration 3521/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.161s)
               Value function loss: 724.6121
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 4.28
               Mean episode length: 54.33
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 57704448
                    Iteration time: 8.24s
                        Total time: 34763.88s
                               ETA: 952295.3s

################################################################################
                    [1m Learning iteration 3522/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.096s, learning 0.275s)
               Value function loss: 670.1990
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 26.38
               Mean episode length: 53.29
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0234
Mean episode consecutive_successes: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 57720832
                    Iteration time: 8.37s
                        Total time: 34772.25s
                               ETA: 952244.4s

################################################################################
                    [1m Learning iteration 3523/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.162s, learning 0.162s)
               Value function loss: 509.0996
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 52.34
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0234
Mean episode consecutive_successes: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 57737216
                    Iteration time: 8.32s
                        Total time: 34780.57s
                               ETA: 952192.2s

################################################################################
                    [1m Learning iteration 3524/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.359s, learning 0.211s)
               Value function loss: 429.0058
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 36.37
               Mean episode length: 53.68
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0300
--------------------------------------------------------------------------------
                   Total timesteps: 57753600
                    Iteration time: 8.57s
                        Total time: 34789.14s
                               ETA: 952146.7s

################################################################################
                    [1m Learning iteration 3525/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.208s, learning 0.161s)
               Value function loss: 466.9073
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 18.87
               Mean episode length: 54.15
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0322
--------------------------------------------------------------------------------
                   Total timesteps: 57769984
                    Iteration time: 8.37s
                        Total time: 34797.51s
                               ETA: 952095.8s

################################################################################
                    [1m Learning iteration 3526/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.326s, learning 0.265s)
               Value function loss: 450.3866
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 8.65
               Mean episode length: 52.59
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0315
--------------------------------------------------------------------------------
                   Total timesteps: 57786368
                    Iteration time: 8.59s
                        Total time: 34806.10s
                               ETA: 952051.0s

################################################################################
                    [1m Learning iteration 3527/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.398s, learning 0.212s)
               Value function loss: 489.1772
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 52.15
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0301
--------------------------------------------------------------------------------
                   Total timesteps: 57802752
                    Iteration time: 8.61s
                        Total time: 34814.71s
                               ETA: 952006.7s

################################################################################
                    [1m Learning iteration 3528/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.214s, learning 0.166s)
               Value function loss: 43.5805
                    Surrogate loss: -0.0136
             Mean action noise std: 0.69
                       Mean reward: 6.37
               Mean episode length: 53.77
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0314
--------------------------------------------------------------------------------
                   Total timesteps: 57819136
                    Iteration time: 8.38s
                        Total time: 34823.09s
                               ETA: 951956.1s

################################################################################
                    [1m Learning iteration 3529/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.213s, learning 0.211s)
               Value function loss: 248.9696
                    Surrogate loss: 0.0032
             Mean action noise std: 0.69
                       Mean reward: 19.10
               Mean episode length: 54.97
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0308
--------------------------------------------------------------------------------
                   Total timesteps: 57835520
                    Iteration time: 8.42s
                        Total time: 34831.51s
                               ETA: 951906.8s

################################################################################
                    [1m Learning iteration 3530/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.365s, learning 0.247s)
               Value function loss: 80.7886
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 4.13
               Mean episode length: 54.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 57851904
                    Iteration time: 8.61s
                        Total time: 34840.13s
                               ETA: 951862.6s

################################################################################
                    [1m Learning iteration 3531/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.107s, learning 0.163s)
               Value function loss: 69.0916
                    Surrogate loss: 0.0005
             Mean action noise std: 0.69
                       Mean reward: 13.69
               Mean episode length: 53.77
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 57868288
                    Iteration time: 8.27s
                        Total time: 34848.40s
                               ETA: 951809.2s

################################################################################
                    [1m Learning iteration 3532/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.031s, learning 0.166s)
               Value function loss: 117.8299
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 53.33
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 57884672
                    Iteration time: 8.20s
                        Total time: 34856.59s
                               ETA: 951753.7s

################################################################################
                    [1m Learning iteration 3533/100000 [0m                    

                       Computation: 1932 steps/s (collection: 8.312s, learning 0.166s)
               Value function loss: 639.4895
                    Surrogate loss: -0.0009
             Mean action noise std: 0.69
                       Mean reward: 16.15
               Mean episode length: 53.34
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0281
--------------------------------------------------------------------------------
                   Total timesteps: 57901056
                    Iteration time: 8.48s
                        Total time: 34865.07s
                               ETA: 951705.9s

################################################################################
                    [1m Learning iteration 3534/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.123s, learning 0.161s)
               Value function loss: 491.1126
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 6.56
               Mean episode length: 54.63
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0262
--------------------------------------------------------------------------------
                   Total timesteps: 57917440
                    Iteration time: 8.28s
                        Total time: 34873.35s
                               ETA: 951652.9s

################################################################################
                    [1m Learning iteration 3535/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.246s, learning 0.160s)
               Value function loss: 744.2539
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 16.49
               Mean episode length: 55.46
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0267
--------------------------------------------------------------------------------
                   Total timesteps: 57933824
                    Iteration time: 8.41s
                        Total time: 34881.76s
                               ETA: 951603.3s

################################################################################
                    [1m Learning iteration 3536/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.169s, learning 0.160s)
               Value function loss: 436.2852
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 52.46
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 57950208
                    Iteration time: 8.33s
                        Total time: 34890.09s
                               ETA: 951551.5s

################################################################################
                    [1m Learning iteration 3537/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.049s, learning 0.160s)
               Value function loss: 317.0473
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 53.14
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 57966592
                    Iteration time: 8.21s
                        Total time: 34898.30s
                               ETA: 951496.5s

################################################################################
                    [1m Learning iteration 3538/100000 [0m                    

                       Computation: 1860 steps/s (collection: 8.640s, learning 0.169s)
               Value function loss: 711.9249
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 3.92
               Mean episode length: 53.99
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 57982976
                    Iteration time: 8.81s
                        Total time: 34907.11s
                               ETA: 951457.9s

################################################################################
                    [1m Learning iteration 3539/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.146s, learning 0.214s)
               Value function loss: 388.3887
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 53.13
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0321
--------------------------------------------------------------------------------
                   Total timesteps: 57999360
                    Iteration time: 8.36s
                        Total time: 34915.47s
                               ETA: 951407.0s

################################################################################
                    [1m Learning iteration 3540/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.192s, learning 0.166s)
               Value function loss: 368.5220
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 16.15
               Mean episode length: 53.20
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0319
--------------------------------------------------------------------------------
                   Total timesteps: 58015744
                    Iteration time: 8.36s
                        Total time: 34923.83s
                               ETA: 951356.2s

################################################################################
                    [1m Learning iteration 3541/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.303s, learning 0.210s)
               Value function loss: 134.6606
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 20.64
               Mean episode length: 52.67
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 58032128
                    Iteration time: 8.51s
                        Total time: 34932.34s
                               ETA: 951309.6s

################################################################################
                    [1m Learning iteration 3542/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.963s, learning 0.169s)
               Value function loss: 23.0799
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 23.52
               Mean episode length: 53.08
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0333
--------------------------------------------------------------------------------
                   Total timesteps: 58048512
                    Iteration time: 8.13s
                        Total time: 34940.47s
                               ETA: 951252.6s

################################################################################
                    [1m Learning iteration 3543/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.224s, learning 0.164s)
               Value function loss: 558.5706
                    Surrogate loss: 0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 53.20
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0329
--------------------------------------------------------------------------------
                   Total timesteps: 58064896
                    Iteration time: 8.39s
                        Total time: 34948.86s
                               ETA: 951202.6s

################################################################################
                    [1m Learning iteration 3544/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.150s, learning 0.162s)
               Value function loss: 98.1218
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 4.49
               Mean episode length: 54.85
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0303
--------------------------------------------------------------------------------
                   Total timesteps: 58081280
                    Iteration time: 8.31s
                        Total time: 34957.17s
                               ETA: 951150.6s

################################################################################
                    [1m Learning iteration 3545/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.220s, learning 0.218s)
               Value function loss: 262.5275
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 3.98
               Mean episode length: 53.88
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 58097664
                    Iteration time: 8.44s
                        Total time: 34965.61s
                               ETA: 951102.0s

################################################################################
                    [1m Learning iteration 3546/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.383s, learning 0.162s)
               Value function loss: 175.0918
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 49.65
               Mean episode length: 55.03
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 58114048
                    Iteration time: 8.55s
                        Total time: 34974.15s
                               ETA: 951056.4s

################################################################################
                    [1m Learning iteration 3547/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.501s, learning 0.168s)
               Value function loss: 196.8441
                    Surrogate loss: 0.0036
             Mean action noise std: 0.69
                       Mean reward: 3.78
               Mean episode length: 53.22
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0286
--------------------------------------------------------------------------------
                   Total timesteps: 58130432
                    Iteration time: 8.67s
                        Total time: 34982.82s
                               ETA: 951014.1s

################################################################################
                    [1m Learning iteration 3548/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.164s)
               Value function loss: 184.0824
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 13.53
               Mean episode length: 53.75
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0289
--------------------------------------------------------------------------------
                   Total timesteps: 58146816
                    Iteration time: 8.48s
                        Total time: 34991.30s
                               ETA: 950966.6s

################################################################################
                    [1m Learning iteration 3549/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.224s, learning 0.167s)
               Value function loss: 910.1997
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.03
                  Mean reward/step: 0.37
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0267
--------------------------------------------------------------------------------
                   Total timesteps: 58163200
                    Iteration time: 8.39s
                        Total time: 34999.69s
                               ETA: 950916.9s

################################################################################
                    [1m Learning iteration 3550/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.179s, learning 0.163s)
               Value function loss: 18.3628
                    Surrogate loss: -0.0140
             Mean action noise std: 0.69
                       Mean reward: 26.58
               Mean episode length: 54.04
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0279
--------------------------------------------------------------------------------
                   Total timesteps: 58179584
                    Iteration time: 8.34s
                        Total time: 35008.03s
                               ETA: 950865.8s

################################################################################
                    [1m Learning iteration 3551/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.229s, learning 0.165s)
               Value function loss: 19.0502
                    Surrogate loss: 0.0320
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 53.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 58195968
                    Iteration time: 8.39s
                        Total time: 35016.42s
                               ETA: 950816.2s

################################################################################
                    [1m Learning iteration 3552/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.279s, learning 0.163s)
               Value function loss: 193.6324
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 23.69
               Mean episode length: 53.39
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 58212352
                    Iteration time: 8.44s
                        Total time: 35024.87s
                               ETA: 950767.9s

################################################################################
                    [1m Learning iteration 3553/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.269s, learning 0.174s)
               Value function loss: 250.7526
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 53.34
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 58228736
                    Iteration time: 8.44s
                        Total time: 35033.31s
                               ETA: 950719.6s

################################################################################
                    [1m Learning iteration 3554/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.301s, learning 0.164s)
               Value function loss: 24.8367
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 53.30
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 58245120
                    Iteration time: 8.46s
                        Total time: 35041.77s
                               ETA: 950672.0s

################################################################################
                    [1m Learning iteration 3555/100000 [0m                    

                       Computation: 1938 steps/s (collection: 8.278s, learning 0.174s)
               Value function loss: 413.4382
                    Surrogate loss: 0.0024
             Mean action noise std: 0.69
                       Mean reward: 18.76
               Mean episode length: 54.22
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 58261504
                    Iteration time: 8.45s
                        Total time: 35050.22s
                               ETA: 950624.0s

################################################################################
                    [1m Learning iteration 3556/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.176s, learning 0.194s)
               Value function loss: 259.6595
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 15.85
               Mean episode length: 52.54
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 58277888
                    Iteration time: 8.37s
                        Total time: 35058.60s
                               ETA: 950573.8s

################################################################################
                    [1m Learning iteration 3557/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.338s, learning 0.159s)
               Value function loss: 104.2976
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.59
               Mean episode length: 53.31
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 58294272
                    Iteration time: 8.50s
                        Total time: 35067.09s
                               ETA: 950527.2s

################################################################################
                    [1m Learning iteration 3558/100000 [0m                    

                       Computation: 1854 steps/s (collection: 8.668s, learning 0.165s)
               Value function loss: 127.9423
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 53.88
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0236
--------------------------------------------------------------------------------
                   Total timesteps: 58310656
                    Iteration time: 8.83s
                        Total time: 35075.93s
                               ETA: 950489.6s

################################################################################
                    [1m Learning iteration 3559/100000 [0m                    

                       Computation: 2019 steps/s (collection: 7.938s, learning 0.174s)
               Value function loss: 516.5595
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 8.10
               Mean episode length: 53.29
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 58327040
                    Iteration time: 8.11s
                        Total time: 35084.04s
                               ETA: 950432.5s

################################################################################
                    [1m Learning iteration 3560/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.315s, learning 0.160s)
               Value function loss: 224.4532
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 53.34
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0229
--------------------------------------------------------------------------------
                   Total timesteps: 58343424
                    Iteration time: 8.47s
                        Total time: 35092.51s
                               ETA: 950385.3s

################################################################################
                    [1m Learning iteration 3561/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.196s, learning 0.215s)
               Value function loss: 403.1269
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 8.86
               Mean episode length: 54.44
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 58359808
                    Iteration time: 8.41s
                        Total time: 35100.92s
                               ETA: 950336.3s

################################################################################
                    [1m Learning iteration 3562/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.279s, learning 0.217s)
               Value function loss: 301.7020
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 8.72
               Mean episode length: 53.98
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0213
--------------------------------------------------------------------------------
                   Total timesteps: 58376192
                    Iteration time: 8.50s
                        Total time: 35109.42s
                               ETA: 950289.7s

################################################################################
                    [1m Learning iteration 3563/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.167s)
               Value function loss: 505.5703
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 21.10
               Mean episode length: 54.17
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0221
--------------------------------------------------------------------------------
                   Total timesteps: 58392576
                    Iteration time: 8.29s
                        Total time: 35117.71s
                               ETA: 950237.5s

################################################################################
                    [1m Learning iteration 3564/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.214s, learning 0.213s)
               Value function loss: 253.2320
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 13.64
               Mean episode length: 53.50
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0249
--------------------------------------------------------------------------------
                   Total timesteps: 58408960
                    Iteration time: 8.43s
                        Total time: 35126.14s
                               ETA: 950189.1s

################################################################################
                    [1m Learning iteration 3565/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.355s, learning 0.212s)
               Value function loss: 639.7027
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 52.72
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 58425344
                    Iteration time: 8.57s
                        Total time: 35134.71s
                               ETA: 950144.5s

################################################################################
                    [1m Learning iteration 3566/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.272s, learning 0.162s)
               Value function loss: 235.9612
                    Surrogate loss: -0.0060
             Mean action noise std: 0.69
                       Mean reward: 15.66
               Mean episode length: 53.54
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0254
--------------------------------------------------------------------------------
                   Total timesteps: 58441728
                    Iteration time: 8.43s
                        Total time: 35143.14s
                               ETA: 950096.3s

################################################################################
                    [1m Learning iteration 3567/100000 [0m                    

                       Computation: 2069 steps/s (collection: 7.756s, learning 0.161s)
               Value function loss: 357.5773
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 41.28
               Mean episode length: 54.30
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0299
--------------------------------------------------------------------------------
                   Total timesteps: 58458112
                    Iteration time: 7.92s
                        Total time: 35151.06s
                               ETA: 950034.2s

################################################################################
                    [1m Learning iteration 3568/100000 [0m                    

                       Computation: 2001 steps/s (collection: 8.018s, learning 0.169s)
               Value function loss: 227.7657
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 54.21
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0275
--------------------------------------------------------------------------------
                   Total timesteps: 58474496
                    Iteration time: 8.19s
                        Total time: 35159.24s
                               ETA: 949979.4s

################################################################################
                    [1m Learning iteration 3569/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.296s, learning 0.160s)
               Value function loss: 102.8991
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 53.07
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0257
--------------------------------------------------------------------------------
                   Total timesteps: 58490880
                    Iteration time: 8.46s
                        Total time: 35167.70s
                               ETA: 949931.8s

################################################################################
                    [1m Learning iteration 3570/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.015s, learning 0.164s)
               Value function loss: 490.2316
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 52.87
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0267
--------------------------------------------------------------------------------
                   Total timesteps: 58507264
                    Iteration time: 8.18s
                        Total time: 35175.88s
                               ETA: 949876.8s

################################################################################
                    [1m Learning iteration 3571/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.095s, learning 0.165s)
               Value function loss: 301.7029
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 54.84
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0284
--------------------------------------------------------------------------------
                   Total timesteps: 58523648
                    Iteration time: 8.26s
                        Total time: 35184.14s
                               ETA: 949824.0s

################################################################################
                    [1m Learning iteration 3572/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.570s, learning 0.162s)
               Value function loss: 246.1109
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 5.61
               Mean episode length: 52.65
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 58540032
                    Iteration time: 8.73s
                        Total time: 35192.87s
                               ETA: 949784.0s

################################################################################
                    [1m Learning iteration 3573/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.398s, learning 0.180s)
               Value function loss: 309.8289
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 11.11
               Mean episode length: 53.86
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0300
--------------------------------------------------------------------------------
                   Total timesteps: 58556416
                    Iteration time: 8.58s
                        Total time: 35201.45s
                               ETA: 949739.8s

################################################################################
                    [1m Learning iteration 3574/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.143s, learning 0.162s)
               Value function loss: 381.5798
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 26.52
               Mean episode length: 54.19
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0301
--------------------------------------------------------------------------------
                   Total timesteps: 58572800
                    Iteration time: 8.30s
                        Total time: 35209.75s
                               ETA: 949688.3s

################################################################################
                    [1m Learning iteration 3575/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.303s, learning 0.165s)
               Value function loss: 338.6128
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 16.77
               Mean episode length: 54.99
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0297
--------------------------------------------------------------------------------
                   Total timesteps: 58589184
                    Iteration time: 8.47s
                        Total time: 35218.22s
                               ETA: 949641.3s

################################################################################
                    [1m Learning iteration 3576/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.918s, learning 0.208s)
               Value function loss: 1571.4011
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 52.21
                  Mean reward/step: 0.68
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 58605568
                    Iteration time: 8.13s
                        Total time: 35226.35s
                               ETA: 949585.0s

################################################################################
                    [1m Learning iteration 3577/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.130s, learning 0.254s)
               Value function loss: 619.2776
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 23.78
               Mean episode length: 53.80
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0285
--------------------------------------------------------------------------------
                   Total timesteps: 58621952
                    Iteration time: 8.38s
                        Total time: 35234.73s
                               ETA: 949535.7s

################################################################################
                    [1m Learning iteration 3578/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.244s, learning 0.161s)
               Value function loss: 666.5100
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 26.69
               Mean episode length: 54.39
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0298
Mean episode consecutive_successes: 0.0296
--------------------------------------------------------------------------------
                   Total timesteps: 58638336
                    Iteration time: 8.40s
                        Total time: 35243.14s
                               ETA: 949487.0s

################################################################################
                    [1m Learning iteration 3579/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.162s, learning 0.209s)
               Value function loss: 307.4473
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 26.10
               Mean episode length: 52.80
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0370
--------------------------------------------------------------------------------
                   Total timesteps: 58654720
                    Iteration time: 8.37s
                        Total time: 35251.51s
                               ETA: 949437.4s

################################################################################
                    [1m Learning iteration 3580/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.213s, learning 0.206s)
               Value function loss: 47.5436
                    Surrogate loss: -0.0115
             Mean action noise std: 0.69
                       Mean reward: 4.24
               Mean episode length: 55.38
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0356
--------------------------------------------------------------------------------
                   Total timesteps: 58671104
                    Iteration time: 8.42s
                        Total time: 35259.93s
                               ETA: 949389.0s

################################################################################
                    [1m Learning iteration 3581/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.137s, learning 0.171s)
               Value function loss: 514.2081
                    Surrogate loss: -0.0011
             Mean action noise std: 0.69
                       Mean reward: 13.67
               Mean episode length: 53.69
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 58687488
                    Iteration time: 8.31s
                        Total time: 35268.24s
                               ETA: 949337.8s

################################################################################
                    [1m Learning iteration 3582/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.453s, learning 0.211s)
               Value function loss: 561.5383
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 3.65
               Mean episode length: 53.59
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 58703872
                    Iteration time: 8.66s
                        Total time: 35276.90s
                               ETA: 949296.2s

################################################################################
                    [1m Learning iteration 3583/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.215s, learning 0.174s)
               Value function loss: 124.7206
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 53.86
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 58720256
                    Iteration time: 8.39s
                        Total time: 35285.29s
                               ETA: 949247.1s

################################################################################
                    [1m Learning iteration 3584/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.340s, learning 0.170s)
               Value function loss: 66.9404
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 13.85
               Mean episode length: 54.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0330
--------------------------------------------------------------------------------
                   Total timesteps: 58736640
                    Iteration time: 8.51s
                        Total time: 35293.80s
                               ETA: 949201.4s

################################################################################
                    [1m Learning iteration 3585/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.319s, learning 0.266s)
               Value function loss: 496.4202
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 15.57
               Mean episode length: 52.81
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0323
--------------------------------------------------------------------------------
                   Total timesteps: 58753024
                    Iteration time: 8.59s
                        Total time: 35302.38s
                               ETA: 949157.7s

################################################################################
                    [1m Learning iteration 3586/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.252s, learning 0.161s)
               Value function loss: 186.0721
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 51.83
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0298
--------------------------------------------------------------------------------
                   Total timesteps: 58769408
                    Iteration time: 8.41s
                        Total time: 35310.80s
                               ETA: 949109.3s

################################################################################
                    [1m Learning iteration 3587/100000 [0m                    

                       Computation: 2030 steps/s (collection: 7.909s, learning 0.161s)
               Value function loss: 44.4586
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 54.20
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 58785792
                    Iteration time: 8.07s
                        Total time: 35318.87s
                               ETA: 949051.8s

################################################################################
                    [1m Learning iteration 3588/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.114s, learning 0.160s)
               Value function loss: 68.2625
                    Surrogate loss: -0.0082
             Mean action noise std: 0.69
                       Mean reward: 26.65
               Mean episode length: 55.13
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0298
--------------------------------------------------------------------------------
                   Total timesteps: 58802176
                    Iteration time: 8.27s
                        Total time: 35327.14s
                               ETA: 948999.8s

################################################################################
                    [1m Learning iteration 3589/100000 [0m                    

                       Computation: 2015 steps/s (collection: 7.967s, learning 0.162s)
               Value function loss: 142.5584
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.92
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0286
--------------------------------------------------------------------------------
                   Total timesteps: 58818560
                    Iteration time: 8.13s
                        Total time: 35335.27s
                               ETA: 948944.0s

################################################################################
                    [1m Learning iteration 3590/100000 [0m                    

                       Computation: 1993 steps/s (collection: 8.042s, learning 0.178s)
               Value function loss: 390.7102
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 5.91
               Mean episode length: 54.06
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 58834944
                    Iteration time: 8.22s
                        Total time: 35343.49s
                               ETA: 948890.5s

################################################################################
                    [1m Learning iteration 3591/100000 [0m                    

                       Computation: 2005 steps/s (collection: 7.999s, learning 0.172s)
               Value function loss: 91.1151
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 3.64
               Mean episode length: 53.60
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0252
--------------------------------------------------------------------------------
                   Total timesteps: 58851328
                    Iteration time: 8.17s
                        Total time: 35351.66s
                               ETA: 948835.8s

################################################################################
                    [1m Learning iteration 3592/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.101s, learning 0.158s)
               Value function loss: 235.3076
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 13.65
               Mean episode length: 54.70
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 58867712
                    Iteration time: 8.26s
                        Total time: 35359.92s
                               ETA: 948783.5s

################################################################################
                    [1m Learning iteration 3593/100000 [0m                    

                       Computation: 2026 steps/s (collection: 7.918s, learning 0.165s)
               Value function loss: 269.8488
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 41.59
               Mean episode length: 53.84
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 58884096
                    Iteration time: 8.08s
                        Total time: 35368.00s
                               ETA: 948726.5s

################################################################################
                    [1m Learning iteration 3594/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.378s, learning 0.161s)
               Value function loss: 245.4689
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 4.07
               Mean episode length: 54.39
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0242
--------------------------------------------------------------------------------
                   Total timesteps: 58900480
                    Iteration time: 8.54s
                        Total time: 35376.54s
                               ETA: 948681.8s

################################################################################
                    [1m Learning iteration 3595/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.344s, learning 0.160s)
               Value function loss: 859.8669
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 14.50
               Mean episode length: 54.80
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 58916864
                    Iteration time: 8.50s
                        Total time: 35385.05s
                               ETA: 948636.1s

################################################################################
                    [1m Learning iteration 3596/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.154s, learning 0.175s)
               Value function loss: 597.3603
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 54.50
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 58933248
                    Iteration time: 8.33s
                        Total time: 35393.38s
                               ETA: 948585.8s

################################################################################
                    [1m Learning iteration 3597/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.270s, learning 0.179s)
               Value function loss: 612.6349
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 52.88
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0249
Mean episode consecutive_successes: 0.0239
--------------------------------------------------------------------------------
                   Total timesteps: 58949632
                    Iteration time: 8.45s
                        Total time: 35401.83s
                               ETA: 948538.7s

################################################################################
                    [1m Learning iteration 3598/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.287s, learning 0.171s)
               Value function loss: 392.0556
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 43.92
               Mean episode length: 53.98
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0286
--------------------------------------------------------------------------------
                   Total timesteps: 58966016
                    Iteration time: 8.46s
                        Total time: 35410.28s
                               ETA: 948491.9s

################################################################################
                    [1m Learning iteration 3599/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.056s, learning 0.210s)
               Value function loss: 892.1756
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 34.28
               Mean episode length: 53.50
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0309
--------------------------------------------------------------------------------
                   Total timesteps: 58982400
                    Iteration time: 8.27s
                        Total time: 35418.55s
                               ETA: 948439.9s

################################################################################
                    [1m Learning iteration 3600/100000 [0m                    

                       Computation: 2010 steps/s (collection: 7.980s, learning 0.168s)
               Value function loss: 358.1098
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 11.46
               Mean episode length: 54.71
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0304
--------------------------------------------------------------------------------
                   Total timesteps: 58998784
                    Iteration time: 8.15s
                        Total time: 35426.70s
                               ETA: 948384.8s

################################################################################
                    [1m Learning iteration 3601/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.195s, learning 0.169s)
               Value function loss: 788.5088
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 46.47
               Mean episode length: 53.92
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0220
Mean episode consecutive_successes: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 59015168
                    Iteration time: 8.36s
                        Total time: 35435.06s
                               ETA: 948335.5s

################################################################################
                    [1m Learning iteration 3602/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.234s, learning 0.162s)
               Value function loss: 284.7562
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 16.02
               Mean episode length: 53.39
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0340
--------------------------------------------------------------------------------
                   Total timesteps: 59031552
                    Iteration time: 8.40s
                        Total time: 35443.46s
                               ETA: 948287.1s

################################################################################
                    [1m Learning iteration 3603/100000 [0m                    

                       Computation: 2034 steps/s (collection: 7.893s, learning 0.159s)
               Value function loss: 341.1741
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 16.02
               Mean episode length: 53.59
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0327
--------------------------------------------------------------------------------
                   Total timesteps: 59047936
                    Iteration time: 8.05s
                        Total time: 35451.51s
                               ETA: 948229.5s

################################################################################
                    [1m Learning iteration 3604/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.963s, learning 0.159s)
               Value function loss: 349.8045
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 31.31
               Mean episode length: 53.58
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0349
--------------------------------------------------------------------------------
                   Total timesteps: 59064320
                    Iteration time: 8.12s
                        Total time: 35459.63s
                               ETA: 948173.8s

################################################################################
                    [1m Learning iteration 3605/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.189s, learning 0.210s)
               Value function loss: 72.7812
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 19.70
               Mean episode length: 55.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0347
--------------------------------------------------------------------------------
                   Total timesteps: 59080704
                    Iteration time: 8.40s
                        Total time: 35468.03s
                               ETA: 948125.6s

################################################################################
                    [1m Learning iteration 3606/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.148s, learning 0.211s)
               Value function loss: 256.1938
                    Surrogate loss: 0.0022
             Mean action noise std: 0.69
                       Mean reward: 23.11
               Mean episode length: 51.18
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 59097088
                    Iteration time: 8.36s
                        Total time: 35476.39s
                               ETA: 948076.3s

################################################################################
                    [1m Learning iteration 3607/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.379s, learning 0.258s)
               Value function loss: 121.1283
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 4.12
               Mean episode length: 53.87
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0338
--------------------------------------------------------------------------------
                   Total timesteps: 59113472
                    Iteration time: 8.64s
                        Total time: 35485.03s
                               ETA: 948034.4s

################################################################################
                    [1m Learning iteration 3608/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.236s, learning 0.226s)
               Value function loss: 597.5351
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 4.04
               Mean episode length: 53.52
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0328
--------------------------------------------------------------------------------
                   Total timesteps: 59129856
                    Iteration time: 8.46s
                        Total time: 35493.49s
                               ETA: 947987.9s

################################################################################
                    [1m Learning iteration 3609/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.410s, learning 0.202s)
               Value function loss: 173.1645
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 11.00
               Mean episode length: 53.52
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 59146240
                    Iteration time: 8.61s
                        Total time: 35502.10s
                               ETA: 947945.4s

################################################################################
                    [1m Learning iteration 3610/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.462s, learning 0.166s)
               Value function loss: 907.3002
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 21.74
               Mean episode length: 54.39
                  Mean reward/step: 0.40
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 59162624
                    Iteration time: 16.63s
                        Total time: 35518.73s
                               ETA: 948117.0s

################################################################################
                    [1m Learning iteration 3611/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.372s, learning 0.172s)
               Value function loss: 313.1504
                    Surrogate loss: -0.0083
             Mean action noise std: 0.69
                       Mean reward: 23.31
               Mean episode length: 51.58
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0315
--------------------------------------------------------------------------------
                   Total timesteps: 59179008
                    Iteration time: 16.54s
                        Total time: 35535.27s
                               ETA: 948286.1s

################################################################################
                    [1m Learning iteration 3612/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.074s, learning 0.172s)
               Value function loss: 49.9542
                    Surrogate loss: -0.0099
             Mean action noise std: 0.69
                       Mean reward: 41.73
               Mean episode length: 53.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0344
--------------------------------------------------------------------------------
                   Total timesteps: 59195392
                    Iteration time: 16.25s
                        Total time: 35551.52s
                               ETA: 948447.2s

################################################################################
                    [1m Learning iteration 3613/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.129s, learning 0.171s)
               Value function loss: 343.2580
                    Surrogate loss: 0.0038
             Mean action noise std: 0.69
                       Mean reward: 14.41
               Mean episode length: 54.87
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 59211776
                    Iteration time: 16.30s
                        Total time: 35567.82s
                               ETA: 948609.7s

################################################################################
                    [1m Learning iteration 3614/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.095s, learning 0.169s)
               Value function loss: 22.0718
                    Surrogate loss: -0.0137
             Mean action noise std: 0.69
                       Mean reward: 4.11
               Mean episode length: 53.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 59228160
                    Iteration time: 16.26s
                        Total time: 35584.08s
                               ETA: 948771.1s

################################################################################
                    [1m Learning iteration 3615/100000 [0m                    

                       Computation: 1003 steps/s (collection: 16.112s, learning 0.211s)
               Value function loss: 109.6498
                    Surrogate loss: 0.0028
             Mean action noise std: 0.69
                       Mean reward: 9.01
               Mean episode length: 55.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0332
--------------------------------------------------------------------------------
                   Total timesteps: 59244544
                    Iteration time: 16.32s
                        Total time: 35600.41s
                               ETA: 948933.9s

################################################################################
                    [1m Learning iteration 3616/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.369s, learning 0.170s)
               Value function loss: 233.4260
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 53.26
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0307
--------------------------------------------------------------------------------
                   Total timesteps: 59260928
                    Iteration time: 16.54s
                        Total time: 35616.95s
                               ETA: 949102.5s

################################################################################
                    [1m Learning iteration 3617/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.866s, learning 0.201s)
               Value function loss: 652.3200
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 52.58
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0283
--------------------------------------------------------------------------------
                   Total timesteps: 59277312
                    Iteration time: 16.07s
                        Total time: 35633.01s
                               ETA: 949258.3s

################################################################################
                    [1m Learning iteration 3618/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.172s, learning 0.168s)
               Value function loss: 272.6097
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 26.46
               Mean episode length: 53.46
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0171
Mean episode consecutive_successes: 0.0291
--------------------------------------------------------------------------------
                   Total timesteps: 59293696
                    Iteration time: 16.34s
                        Total time: 35649.35s
                               ETA: 949421.4s

################################################################################
                    [1m Learning iteration 3619/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.015s, learning 0.165s)
               Value function loss: 359.1617
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 4.09
               Mean episode length: 54.40
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0280
--------------------------------------------------------------------------------
                   Total timesteps: 59310080
                    Iteration time: 16.18s
                        Total time: 35665.53s
                               ETA: 949580.0s

################################################################################
                    [1m Learning iteration 3620/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.009s, learning 0.212s)
               Value function loss: 267.6986
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 29.83
               Mean episode length: 54.55
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0186
Mean episode consecutive_successes: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 59326464
                    Iteration time: 16.22s
                        Total time: 35681.75s
                               ETA: 949739.7s

################################################################################
                    [1m Learning iteration 3621/100000 [0m                    

                       Computation: 1032 steps/s (collection: 15.707s, learning 0.164s)
               Value function loss: 794.2766
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 12.44
               Mean episode length: 55.30
                  Mean reward/step: 0.44
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 59342848
                    Iteration time: 15.87s
                        Total time: 35697.63s
                               ETA: 949890.0s

################################################################################
                    [1m Learning iteration 3622/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.177s, learning 0.170s)
               Value function loss: 301.0071
                    Surrogate loss: -0.0087
             Mean action noise std: 0.69
                       Mean reward: 9.27
               Mean episode length: 55.22
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0322
--------------------------------------------------------------------------------
                   Total timesteps: 59359232
                    Iteration time: 16.35s
                        Total time: 35713.97s
                               ETA: 950052.8s

################################################################################
                    [1m Learning iteration 3623/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.329s, learning 0.164s)
               Value function loss: 55.5474
                    Surrogate loss: -0.0123
             Mean action noise std: 0.69
                       Mean reward: 41.78
               Mean episode length: 54.43
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0336
--------------------------------------------------------------------------------
                   Total timesteps: 59375616
                    Iteration time: 16.49s
                        Total time: 35730.47s
                               ETA: 950219.4s

################################################################################
                    [1m Learning iteration 3624/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.177s, learning 0.166s)
               Value function loss: 179.4243
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 21.72
               Mean episode length: 54.42
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0361
--------------------------------------------------------------------------------
                   Total timesteps: 59392000
                    Iteration time: 16.34s
                        Total time: 35746.81s
                               ETA: 950381.9s

################################################################################
                    [1m Learning iteration 3625/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.912s, learning 0.171s)
               Value function loss: 1012.5060
                    Surrogate loss: 0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 53.99
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0341
--------------------------------------------------------------------------------
                   Total timesteps: 59408384
                    Iteration time: 16.08s
                        Total time: 35762.89s
                               ETA: 950537.4s

################################################################################
                    [1m Learning iteration 3626/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.416s, learning 0.265s)
               Value function loss: 181.3910
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 52.21
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0326
--------------------------------------------------------------------------------
                   Total timesteps: 59424768
                    Iteration time: 16.68s
                        Total time: 35779.57s
                               ETA: 950708.7s

################################################################################
                    [1m Learning iteration 3627/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.098s, learning 0.168s)
               Value function loss: 645.3456
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 26.81
               Mean episode length: 53.90
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0344
--------------------------------------------------------------------------------
                   Total timesteps: 59441152
                    Iteration time: 16.27s
                        Total time: 35795.84s
                               ETA: 950868.9s

################################################################################
                    [1m Learning iteration 3628/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.141s, learning 0.161s)
               Value function loss: 924.3846
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 52.96
                  Mean reward/step: 0.50
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0278
Mean episode consecutive_successes: 0.0330
--------------------------------------------------------------------------------
                   Total timesteps: 59457536
                    Iteration time: 16.30s
                        Total time: 35812.14s
                               ETA: 951029.9s

################################################################################
                    [1m Learning iteration 3629/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.417s, learning 0.165s)
               Value function loss: 509.8408
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 19.25
               Mean episode length: 54.29
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0322
Mean episode consecutive_successes: 0.0343
--------------------------------------------------------------------------------
                   Total timesteps: 59473920
                    Iteration time: 16.58s
                        Total time: 35828.72s
                               ETA: 951198.3s

################################################################################
                    [1m Learning iteration 3630/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.086s, learning 0.165s)
               Value function loss: 428.9736
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 21.30
               Mean episode length: 53.62
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0269
Mean episode consecutive_successes: 0.0362
--------------------------------------------------------------------------------
                   Total timesteps: 59490304
                    Iteration time: 16.25s
                        Total time: 35844.97s
                               ETA: 951357.8s

################################################################################
                    [1m Learning iteration 3631/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.288s, learning 0.172s)
               Value function loss: 148.7663
                    Surrogate loss: -0.0060
             Mean action noise std: 0.69
                       Mean reward: 54.29
               Mean episode length: 53.70
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0417
--------------------------------------------------------------------------------
                   Total timesteps: 59506688
                    Iteration time: 16.46s
                        Total time: 35861.43s
                               ETA: 951522.7s

################################################################################
                    [1m Learning iteration 3632/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.283s, learning 0.193s)
               Value function loss: 470.7329
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 15.80
               Mean episode length: 52.53
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0416
--------------------------------------------------------------------------------
                   Total timesteps: 59523072
                    Iteration time: 16.48s
                        Total time: 35877.91s
                               ETA: 951688.0s

################################################################################
                    [1m Learning iteration 3633/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.855s, learning 0.223s)
               Value function loss: 357.2334
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 51.01
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0415
--------------------------------------------------------------------------------
                   Total timesteps: 59539456
                    Iteration time: 16.08s
                        Total time: 35893.99s
                               ETA: 951842.6s

################################################################################
                    [1m Learning iteration 3634/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.200s, learning 0.175s)
               Value function loss: 476.1840
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 3.90
               Mean episode length: 53.39
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0405
--------------------------------------------------------------------------------
                   Total timesteps: 59555840
                    Iteration time: 16.37s
                        Total time: 35910.36s
                               ETA: 952005.0s

################################################################################
                    [1m Learning iteration 3635/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.867s, learning 0.261s)
               Value function loss: 583.8795
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 53.57
                  Mean reward/step: 0.36
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0181
Mean episode consecutive_successes: 0.0381
--------------------------------------------------------------------------------
                   Total timesteps: 59572224
                    Iteration time: 16.13s
                        Total time: 35926.49s
                               ETA: 952160.7s

################################################################################
                    [1m Learning iteration 3636/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.291s, learning 0.165s)
               Value function loss: 264.3863
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 5.95
               Mean episode length: 51.93
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0372
--------------------------------------------------------------------------------
                   Total timesteps: 59588608
                    Iteration time: 16.46s
                        Total time: 35942.95s
                               ETA: 952325.0s

################################################################################
                    [1m Learning iteration 3637/100000 [0m                    

                       Computation: 1041 steps/s (collection: 15.559s, learning 0.178s)
               Value function loss: 156.5636
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 52.09
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0354
--------------------------------------------------------------------------------
                   Total timesteps: 59604992
                    Iteration time: 15.74s
                        Total time: 35958.68s
                               ETA: 952470.2s

################################################################################
                    [1m Learning iteration 3638/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.181s, learning 0.182s)
               Value function loss: 355.4073
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 26.88
               Mean episode length: 54.07
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0400
--------------------------------------------------------------------------------
                   Total timesteps: 59621376
                    Iteration time: 16.36s
                        Total time: 35975.05s
                               ETA: 952631.9s

################################################################################
                    [1m Learning iteration 3639/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.921s, learning 0.161s)
               Value function loss: 278.2329
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 54.19
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0378
--------------------------------------------------------------------------------
                   Total timesteps: 59637760
                    Iteration time: 16.08s
                        Total time: 35991.13s
                               ETA: 952786.1s

################################################################################
                    [1m Learning iteration 3640/100000 [0m                    

                       Computation: 1017 steps/s (collection: 15.902s, learning 0.205s)
               Value function loss: 77.3217
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 19.04
               Mean episode length: 52.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0364
--------------------------------------------------------------------------------
                   Total timesteps: 59654144
                    Iteration time: 16.11s
                        Total time: 36007.24s
                               ETA: 952940.8s

################################################################################
                    [1m Learning iteration 3641/100000 [0m                    

                       Computation: 992 steps/s (collection: 16.330s, learning 0.179s)
               Value function loss: 473.6295
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 14.01
               Mean episode length: 53.83
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0369
--------------------------------------------------------------------------------
                   Total timesteps: 59670528
                    Iteration time: 16.51s
                        Total time: 36023.75s
                               ETA: 953106.0s

################################################################################
                    [1m Learning iteration 3642/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.197s, learning 0.168s)
               Value function loss: 374.8420
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 8.63
               Mean episode length: 53.05
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 59686912
                    Iteration time: 16.37s
                        Total time: 36040.11s
                               ETA: 953267.4s

################################################################################
                    [1m Learning iteration 3643/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.281s, learning 0.169s)
               Value function loss: 1248.8658
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 19.07
               Mean episode length: 53.43
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0205
Mean episode consecutive_successes: 0.0336
--------------------------------------------------------------------------------
                   Total timesteps: 59703296
                    Iteration time: 16.45s
                        Total time: 36056.56s
                               ETA: 953430.8s

################################################################################
                    [1m Learning iteration 3644/100000 [0m                    

                       Computation: 1014 steps/s (collection: 15.918s, learning 0.234s)
               Value function loss: 510.8555
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.90
               Mean episode length: 54.01
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0244
Mean episode consecutive_successes: 0.0310
--------------------------------------------------------------------------------
                   Total timesteps: 59719680
                    Iteration time: 16.15s
                        Total time: 36072.71s
                               ETA: 953586.4s

################################################################################
                    [1m Learning iteration 3645/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.133s, learning 0.171s)
               Value function loss: 279.4195
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 21.03
               Mean episode length: 52.07
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0357
--------------------------------------------------------------------------------
                   Total timesteps: 59736064
                    Iteration time: 16.30s
                        Total time: 36089.02s
                               ETA: 953745.8s

################################################################################
                    [1m Learning iteration 3646/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.310s, learning 0.172s)
               Value function loss: 618.1227
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.63
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0360
--------------------------------------------------------------------------------
                   Total timesteps: 59752448
                    Iteration time: 16.48s
                        Total time: 36105.50s
                               ETA: 953909.9s

################################################################################
                    [1m Learning iteration 3647/100000 [0m                    

                       Computation: 1214 steps/s (collection: 13.332s, learning 0.159s)
               Value function loss: 177.6737
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 19.48
               Mean episode length: 54.58
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0402
--------------------------------------------------------------------------------
                   Total timesteps: 59768832
                    Iteration time: 13.49s
                        Total time: 36118.99s
                               ETA: 953994.8s

################################################################################
                    [1m Learning iteration 3648/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.381s, learning 0.165s)
               Value function loss: 277.6102
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 21.29
               Mean episode length: 53.27
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0389
--------------------------------------------------------------------------------
                   Total timesteps: 59785216
                    Iteration time: 8.55s
                        Total time: 36127.54s
                               ETA: 953949.1s

################################################################################
                    [1m Learning iteration 3649/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.238s, learning 0.163s)
               Value function loss: 522.8071
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 6.70
               Mean episode length: 54.18
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0376
--------------------------------------------------------------------------------
                   Total timesteps: 59801600
                    Iteration time: 8.40s
                        Total time: 36135.94s
                               ETA: 953899.7s

################################################################################
                    [1m Learning iteration 3650/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.246s, learning 0.159s)
               Value function loss: 385.9669
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 3.71
               Mean episode length: 53.24
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0355
--------------------------------------------------------------------------------
                   Total timesteps: 59817984
                    Iteration time: 8.40s
                        Total time: 36144.34s
                               ETA: 953850.3s

################################################################################
                    [1m Learning iteration 3651/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.310s, learning 0.165s)
               Value function loss: 98.6707
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 3.58
               Mean episode length: 52.90
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0380
--------------------------------------------------------------------------------
                   Total timesteps: 59834368
                    Iteration time: 8.48s
                        Total time: 36152.82s
                               ETA: 953802.8s

################################################################################
                    [1m Learning iteration 3652/100000 [0m                    

                       Computation: 1889 steps/s (collection: 8.507s, learning 0.163s)
               Value function loss: 190.4383
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 3.97
               Mean episode length: 54.57
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0356
--------------------------------------------------------------------------------
                   Total timesteps: 59850752
                    Iteration time: 8.67s
                        Total time: 36161.49s
                               ETA: 953760.5s

################################################################################
                    [1m Learning iteration 3653/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.544s, learning 0.163s)
               Value function loss: 138.9906
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.89
               Mean episode length: 53.56
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0354
--------------------------------------------------------------------------------
                   Total timesteps: 59867136
                    Iteration time: 8.71s
                        Total time: 36170.19s
                               ETA: 953719.1s

################################################################################
                    [1m Learning iteration 3654/100000 [0m                    

                       Computation: 1922 steps/s (collection: 8.360s, learning 0.163s)
               Value function loss: 482.3724
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 18.99
               Mean episode length: 53.78
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0345
--------------------------------------------------------------------------------
                   Total timesteps: 59883520
                    Iteration time: 8.52s
                        Total time: 36178.72s
                               ETA: 953673.0s

################################################################################
                    [1m Learning iteration 3655/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.297s, learning 0.162s)
               Value function loss: 170.0108
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 3.92
               Mean episode length: 53.81
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0318
--------------------------------------------------------------------------------
                   Total timesteps: 59899904
                    Iteration time: 8.46s
                        Total time: 36187.18s
                               ETA: 953625.1s

################################################################################
                    [1m Learning iteration 3656/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.209s, learning 0.171s)
               Value function loss: 596.6470
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 12.28
               Mean episode length: 54.90
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 59916288
                    Iteration time: 8.38s
                        Total time: 36195.56s
                               ETA: 953575.2s

################################################################################
                    [1m Learning iteration 3657/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.298s, learning 0.163s)
               Value function loss: 1091.7623
                    Surrogate loss: 0.0000
             Mean action noise std: 0.69
                       Mean reward: 3.95
               Mean episode length: 54.55
                  Mean reward/step: 0.41
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0215
Mean episode consecutive_successes: 0.0309
--------------------------------------------------------------------------------
                   Total timesteps: 59932672
                    Iteration time: 8.46s
                        Total time: 36204.02s
                               ETA: 953527.5s

################################################################################
                    [1m Learning iteration 3658/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.257s, learning 0.175s)
               Value function loss: 160.9015
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 4.22
               Mean episode length: 54.51
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0210
Mean episode consecutive_successes: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 59949056
                    Iteration time: 8.43s
                        Total time: 36212.45s
                               ETA: 953479.0s

################################################################################
                    [1m Learning iteration 3659/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.404s, learning 0.256s)
               Value function loss: 480.7795
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 4.00
               Mean episode length: 54.09
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0190
Mean episode consecutive_successes: 0.0316
--------------------------------------------------------------------------------
                   Total timesteps: 59965440
                    Iteration time: 8.66s
                        Total time: 36221.11s
                               ETA: 953436.6s

################################################################################
                    [1m Learning iteration 3660/100000 [0m                    

                       Computation: 1924 steps/s (collection: 8.353s, learning 0.161s)
               Value function loss: 482.7412
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.66
               Mean episode length: 53.90
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0346
--------------------------------------------------------------------------------
                   Total timesteps: 59981824
                    Iteration time: 8.51s
                        Total time: 36229.62s
                               ETA: 953390.3s

################################################################################
                    [1m Learning iteration 3661/100000 [0m                    

                       Computation: 1893 steps/s (collection: 8.441s, learning 0.212s)
               Value function loss: 213.6943
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 18.49
               Mean episode length: 53.31
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0336
--------------------------------------------------------------------------------
                   Total timesteps: 59998208
                    Iteration time: 8.65s
                        Total time: 36238.27s
                               ETA: 953347.7s

################################################################################
                    [1m Learning iteration 3662/100000 [0m                    

                       Computation: 1949 steps/s (collection: 8.244s, learning 0.160s)
               Value function loss: 432.6762
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.33
               Mean episode length: 53.51
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0332
--------------------------------------------------------------------------------
                   Total timesteps: 60014592
                    Iteration time: 8.40s
                        Total time: 36246.68s
                               ETA: 953298.5s

################################################################################
                    [1m Learning iteration 3663/100000 [0m                    

                       Computation: 1925 steps/s (collection: 8.346s, learning 0.164s)
               Value function loss: 394.2424
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 13.35
               Mean episode length: 51.69
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0344
--------------------------------------------------------------------------------
                   Total timesteps: 60030976
                    Iteration time: 8.51s
                        Total time: 36255.19s
                               ETA: 953252.2s

################################################################################
                    [1m Learning iteration 3664/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.172s, learning 0.216s)
               Value function loss: 41.9717
                    Surrogate loss: -0.0146
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 52.19
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0334
--------------------------------------------------------------------------------
                   Total timesteps: 60047360
                    Iteration time: 8.39s
                        Total time: 36263.58s
                               ETA: 953202.7s

################################################################################
                    [1m Learning iteration 3665/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.425s, learning 0.173s)
               Value function loss: 60.2179
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 6.00
               Mean episode length: 53.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0342
--------------------------------------------------------------------------------
                   Total timesteps: 60063744
                    Iteration time: 8.60s
                        Total time: 36272.18s
                               ETA: 953158.8s

################################################################################
                    [1m Learning iteration 3666/100000 [0m                    

                       Computation: 2028 steps/s (collection: 7.911s, learning 0.166s)
               Value function loss: 430.0315
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 3.41
               Mean episode length: 52.90
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0335
--------------------------------------------------------------------------------
                   Total timesteps: 60080128
                    Iteration time: 8.08s
                        Total time: 36280.25s
                               ETA: 953101.1s

################################################################################
                    [1m Learning iteration 3667/100000 [0m                    

                       Computation: 1950 steps/s (collection: 8.240s, learning 0.161s)
               Value function loss: 25.3968
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 6.54
               Mean episode length: 54.45
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 60096512
                    Iteration time: 8.40s
                        Total time: 36288.65s
                               ETA: 953052.0s

################################################################################
                    [1m Learning iteration 3668/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.336s, learning 0.160s)
               Value function loss: 14.9222
                    Surrogate loss: -0.0116
             Mean action noise std: 0.69
                       Mean reward: 18.24
               Mean episode length: 52.40
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0306
--------------------------------------------------------------------------------
                   Total timesteps: 60112896
                    Iteration time: 8.50s
                        Total time: 36297.15s
                               ETA: 953005.4s

################################################################################
                    [1m Learning iteration 3669/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.090s, learning 0.217s)
               Value function loss: 24.8151
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 53.22
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0290
--------------------------------------------------------------------------------
                   Total timesteps: 60129280
                    Iteration time: 8.31s
                        Total time: 36305.46s
                               ETA: 952953.9s

################################################################################
                    [1m Learning iteration 3670/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.193s, learning 0.159s)
               Value function loss: 10.3356
                    Surrogate loss: -0.0087
             Mean action noise std: 0.69
                       Mean reward: 4.27
               Mean episode length: 54.74
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 60145664
                    Iteration time: 8.35s
                        Total time: 36313.81s
                               ETA: 952903.6s

################################################################################
                    [1m Learning iteration 3671/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.243s, learning 0.165s)
               Value function loss: 71.6639
                    Surrogate loss: -0.0017
             Mean action noise std: 0.69
                       Mean reward: 9.17
               Mean episode length: 54.07
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 60162048
                    Iteration time: 8.41s
                        Total time: 36322.22s
                               ETA: 952854.8s

################################################################################
                    [1m Learning iteration 3672/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.395s, learning 0.170s)
               Value function loss: 541.5529
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 8.59
               Mean episode length: 53.96
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0239
--------------------------------------------------------------------------------
                   Total timesteps: 60178432
                    Iteration time: 8.57s
                        Total time: 36330.78s
                               ETA: 952810.1s

################################################################################
                    [1m Learning iteration 3673/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.206s, learning 0.169s)
               Value function loss: 356.2066
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 13.51
               Mean episode length: 52.68
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0229
--------------------------------------------------------------------------------
                   Total timesteps: 60194816
                    Iteration time: 8.38s
                        Total time: 36339.16s
                               ETA: 952760.4s

################################################################################
                    [1m Learning iteration 3674/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.403s, learning 0.164s)
               Value function loss: 138.6163
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 3.99
               Mean episode length: 54.49
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 60211200
                    Iteration time: 8.57s
                        Total time: 36347.72s
                               ETA: 952715.9s

################################################################################
                    [1m Learning iteration 3675/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.277s, learning 0.161s)
               Value function loss: 578.4781
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 9.05
               Mean episode length: 53.94
                  Mean reward/step: 0.30
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0156
Mean episode consecutive_successes: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 60227584
                    Iteration time: 8.44s
                        Total time: 36356.16s
                               ETA: 952667.9s

################################################################################
                    [1m Learning iteration 3676/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.342s, learning 0.162s)
               Value function loss: 423.8726
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 8.70
               Mean episode length: 52.55
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0242
--------------------------------------------------------------------------------
                   Total timesteps: 60243968
                    Iteration time: 8.50s
                        Total time: 36364.67s
                               ETA: 952621.7s

################################################################################
                    [1m Learning iteration 3677/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.038s, learning 0.215s)
               Value function loss: 513.0958
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 16.12
               Mean episode length: 52.77
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0248
--------------------------------------------------------------------------------
                   Total timesteps: 60260352
                    Iteration time: 8.25s
                        Total time: 36372.92s
                               ETA: 952569.0s

################################################################################
                    [1m Learning iteration 3678/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.446s, learning 0.253s)
               Value function loss: 112.7329
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 3.32
               Mean episode length: 53.20
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0264
--------------------------------------------------------------------------------
                   Total timesteps: 60276736
                    Iteration time: 8.70s
                        Total time: 36381.62s
                               ETA: 952527.9s

################################################################################
                    [1m Learning iteration 3679/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.162s)
               Value function loss: 186.3002
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 3.74
               Mean episode length: 52.67
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0251
--------------------------------------------------------------------------------
                   Total timesteps: 60293120
                    Iteration time: 8.37s
                        Total time: 36389.99s
                               ETA: 952478.2s

################################################################################
                    [1m Learning iteration 3680/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.229s, learning 0.210s)
               Value function loss: 589.3012
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 52.86
                  Mean reward/step: 0.32
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0263
--------------------------------------------------------------------------------
                   Total timesteps: 60309504
                    Iteration time: 8.44s
                        Total time: 36398.43s
                               ETA: 952430.4s

################################################################################
                    [1m Learning iteration 3681/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.369s, learning 0.160s)
               Value function loss: 224.2468
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 13.45
               Mean episode length: 53.21
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0259
--------------------------------------------------------------------------------
                   Total timesteps: 60325888
                    Iteration time: 8.53s
                        Total time: 36406.95s
                               ETA: 952385.0s

################################################################################
                    [1m Learning iteration 3682/100000 [0m                    

                       Computation: 1899 steps/s (collection: 8.355s, learning 0.269s)
               Value function loss: 762.1323
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 44.41
               Mean episode length: 53.38
                  Mean reward/step: 0.30
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0278
--------------------------------------------------------------------------------
                   Total timesteps: 60342272
                    Iteration time: 8.62s
                        Total time: 36415.58s
                               ETA: 952342.0s

################################################################################
                    [1m Learning iteration 3683/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.523s, learning 0.212s)
               Value function loss: 142.9738
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 18.91
               Mean episode length: 54.47
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 60358656
                    Iteration time: 8.73s
                        Total time: 36424.31s
                               ETA: 952302.0s

################################################################################
                    [1m Learning iteration 3684/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.129s, learning 0.162s)
               Value function loss: 584.9174
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 6.73
               Mean episode length: 54.34
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0277
--------------------------------------------------------------------------------
                   Total timesteps: 60375040
                    Iteration time: 8.29s
                        Total time: 36432.60s
                               ETA: 952250.4s

################################################################################
                    [1m Learning iteration 3685/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.294s, learning 0.162s)
               Value function loss: 252.4579
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 21.02
               Mean episode length: 52.66
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0287
--------------------------------------------------------------------------------
                   Total timesteps: 60391424
                    Iteration time: 8.46s
                        Total time: 36441.06s
                               ETA: 952203.1s

################################################################################
                    [1m Learning iteration 3686/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.259s, learning 0.171s)
               Value function loss: 526.5979
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 52.67
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.19
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0273
--------------------------------------------------------------------------------
                   Total timesteps: 60407808
                    Iteration time: 8.43s
                        Total time: 36449.49s
                               ETA: 952155.2s

################################################################################
                    [1m Learning iteration 3687/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.271s, learning 0.172s)
               Value function loss: 321.3698
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 16.79
               Mean episode length: 54.12
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0282
--------------------------------------------------------------------------------
                   Total timesteps: 60424192
                    Iteration time: 8.44s
                        Total time: 36457.93s
                               ETA: 952107.6s

################################################################################
                    [1m Learning iteration 3688/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.049s, learning 0.161s)
               Value function loss: 81.0419
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 53.24
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0271
--------------------------------------------------------------------------------
                   Total timesteps: 60440576
                    Iteration time: 8.21s
                        Total time: 36466.14s
                               ETA: 952054.0s

################################################################################
                    [1m Learning iteration 3689/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.328s, learning 0.174s)
               Value function loss: 328.2211
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 5.89
               Mean episode length: 52.08
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0287
--------------------------------------------------------------------------------
                   Total timesteps: 60456960
                    Iteration time: 8.50s
                        Total time: 36474.65s
                               ETA: 952008.0s

################################################################################
                    [1m Learning iteration 3690/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.284s, learning 0.163s)
               Value function loss: 404.7171
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 3.65
               Mean episode length: 53.32
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 60473344
                    Iteration time: 8.45s
                        Total time: 36483.09s
                               ETA: 951960.6s

################################################################################
                    [1m Learning iteration 3691/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.125s, learning 0.164s)
               Value function loss: 401.5493
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 52.59
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 60489728
                    Iteration time: 8.29s
                        Total time: 36491.38s
                               ETA: 951909.1s

################################################################################
                    [1m Learning iteration 3692/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.161s)
               Value function loss: 77.3491
                    Surrogate loss: -0.0114
             Mean action noise std: 0.69
                       Mean reward: 30.96
               Mean episode length: 52.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0268
--------------------------------------------------------------------------------
                   Total timesteps: 60506112
                    Iteration time: 8.37s
                        Total time: 36499.75s
                               ETA: 951859.7s

################################################################################
                    [1m Learning iteration 3693/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.125s, learning 0.196s)
               Value function loss: 58.8591
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3.94
               Mean episode length: 52.07
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0253
--------------------------------------------------------------------------------
                   Total timesteps: 60522496
                    Iteration time: 8.32s
                        Total time: 36508.07s
                               ETA: 951809.1s

################################################################################
                    [1m Learning iteration 3694/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.075s, learning 0.161s)
               Value function loss: 536.6148
                    Surrogate loss: 0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 52.38
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0234
--------------------------------------------------------------------------------
                   Total timesteps: 60538880
                    Iteration time: 8.24s
                        Total time: 36516.31s
                               ETA: 951756.3s

################################################################################
                    [1m Learning iteration 3695/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.111s, learning 0.161s)
               Value function loss: 92.0152
                    Surrogate loss: -0.0105
             Mean action noise std: 0.69
                       Mean reward: 3.97
               Mean episode length: 52.77
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0235
--------------------------------------------------------------------------------
                   Total timesteps: 60555264
                    Iteration time: 8.27s
                        Total time: 36524.58s
                               ETA: 951704.5s

################################################################################
                    [1m Learning iteration 3696/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.345s, learning 0.161s)
               Value function loss: 29.8305
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 13.83
               Mean episode length: 53.52
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 60571648
                    Iteration time: 8.51s
                        Total time: 36533.09s
                               ETA: 951658.7s

################################################################################
                    [1m Learning iteration 3697/100000 [0m                    

                       Computation: 1876 steps/s (collection: 8.574s, learning 0.159s)
               Value function loss: 70.5635
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 24.12
               Mean episode length: 53.69
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0244
--------------------------------------------------------------------------------
                   Total timesteps: 60588032
                    Iteration time: 8.73s
                        Total time: 36541.82s
                               ETA: 951618.9s

################################################################################
                    [1m Learning iteration 3698/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.233s, learning 0.161s)
               Value function loss: 464.6606
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.93
               Mean episode length: 53.41
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 60604416
                    Iteration time: 8.39s
                        Total time: 36550.21s
                               ETA: 951570.3s

################################################################################
                    [1m Learning iteration 3699/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.413s, learning 0.162s)
               Value function loss: 232.1509
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 4.02
               Mean episode length: 53.43
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 60620800
                    Iteration time: 8.58s
                        Total time: 36558.79s
                               ETA: 951526.5s

################################################################################
                    [1m Learning iteration 3700/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.190s, learning 0.162s)
               Value function loss: 360.8305
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 21.08
               Mean episode length: 52.43
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 60637184
                    Iteration time: 8.35s
                        Total time: 36567.14s
                               ETA: 951476.8s

################################################################################
                    [1m Learning iteration 3701/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.191s, learning 0.170s)
               Value function loss: 84.7068
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 11.32
               Mean episode length: 52.35
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0219
--------------------------------------------------------------------------------
                   Total timesteps: 60653568
                    Iteration time: 8.36s
                        Total time: 36575.50s
                               ETA: 951427.4s

################################################################################
                    [1m Learning iteration 3702/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.385s, learning 0.159s)
               Value function loss: 26.5061
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 11.13
               Mean episode length: 52.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 60669952
                    Iteration time: 8.54s
                        Total time: 36584.05s
                               ETA: 951382.8s

################################################################################
                    [1m Learning iteration 3703/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.539s, learning 0.161s)
               Value function loss: 23.6296
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 52.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 60686336
                    Iteration time: 8.70s
                        Total time: 36592.75s
                               ETA: 951342.2s

################################################################################
                    [1m Learning iteration 3704/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.367s, learning 0.171s)
               Value function loss: 310.5904
                    Surrogate loss: -0.0001
             Mean action noise std: 0.69
                       Mean reward: 8.41
               Mean episode length: 52.84
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0202
--------------------------------------------------------------------------------
                   Total timesteps: 60702720
                    Iteration time: 8.54s
                        Total time: 36601.28s
                               ETA: 951297.5s

################################################################################
                    [1m Learning iteration 3705/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.160s)
               Value function loss: 235.9440
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3.52
               Mean episode length: 52.51
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0186
--------------------------------------------------------------------------------
                   Total timesteps: 60719104
                    Iteration time: 8.62s
                        Total time: 36609.90s
                               ETA: 951254.9s

################################################################################
                    [1m Learning iteration 3706/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.402s, learning 0.160s)
               Value function loss: 127.3672
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 51.96
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 60735488
                    Iteration time: 8.56s
                        Total time: 36618.47s
                               ETA: 951210.8s

################################################################################
                    [1m Learning iteration 3707/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.435s, learning 0.161s)
               Value function loss: 202.5716
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 8.90
               Mean episode length: 53.81
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0182
--------------------------------------------------------------------------------
                   Total timesteps: 60751872
                    Iteration time: 8.60s
                        Total time: 36627.06s
                               ETA: 951167.7s

################################################################################
                    [1m Learning iteration 3708/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.537s, learning 0.163s)
               Value function loss: 475.8222
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 15.96
               Mean episode length: 52.48
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 60768256
                    Iteration time: 8.70s
                        Total time: 36635.76s
                               ETA: 951127.2s

################################################################################
                    [1m Learning iteration 3709/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.363s, learning 0.175s)
               Value function loss: 168.6532
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 53.81
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 60784640
                    Iteration time: 8.54s
                        Total time: 36644.30s
                               ETA: 951082.6s

################################################################################
                    [1m Learning iteration 3710/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.123s, learning 0.167s)
               Value function loss: 185.3938
                    Surrogate loss: -0.0047
             Mean action noise std: 0.69
                       Mean reward: 16.10
               Mean episode length: 52.57
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0190
--------------------------------------------------------------------------------
                   Total timesteps: 60801024
                    Iteration time: 8.29s
                        Total time: 36652.59s
                               ETA: 951031.5s

################################################################################
                    [1m Learning iteration 3711/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.882s, learning 0.159s)
               Value function loss: 248.0632
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 52.24
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 60817408
                    Iteration time: 8.04s
                        Total time: 36660.63s
                               ETA: 950974.0s

################################################################################
                    [1m Learning iteration 3712/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.151s, learning 0.159s)
               Value function loss: 33.3727
                    Surrogate loss: -0.0087
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 51.58
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 60833792
                    Iteration time: 8.31s
                        Total time: 36668.94s
                               ETA: 950923.5s

################################################################################
                    [1m Learning iteration 3713/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.254s, learning 0.171s)
               Value function loss: 24.8678
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 3.50
               Mean episode length: 51.81
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 60850176
                    Iteration time: 8.42s
                        Total time: 36677.37s
                               ETA: 950876.0s

################################################################################
                    [1m Learning iteration 3714/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.152s, learning 0.180s)
               Value function loss: 191.6426
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 53.11
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 60866560
                    Iteration time: 8.33s
                        Total time: 36685.70s
                               ETA: 950826.2s

################################################################################
                    [1m Learning iteration 3715/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.174s, learning 0.220s)
               Value function loss: 20.8420
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3.95
               Mean episode length: 52.68
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 60882944
                    Iteration time: 8.39s
                        Total time: 36694.09s
                               ETA: 950777.9s

################################################################################
                    [1m Learning iteration 3716/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.415s, learning 0.195s)
               Value function loss: 132.8456
                    Surrogate loss: 0.0042
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 52.53
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 60899328
                    Iteration time: 8.61s
                        Total time: 36702.70s
                               ETA: 950735.3s

################################################################################
                    [1m Learning iteration 3717/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.492s, learning 0.168s)
               Value function loss: 288.1539
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 13.54
               Mean episode length: 52.38
                  Mean reward/step: 0.21
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 60915712
                    Iteration time: 8.66s
                        Total time: 36711.36s
                               ETA: 950694.0s

################################################################################
                    [1m Learning iteration 3718/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.382s, learning 0.166s)
               Value function loss: 304.2135
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 52.51
                  Mean reward/step: 0.22
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 60932096
                    Iteration time: 8.55s
                        Total time: 36719.91s
                               ETA: 950649.8s

################################################################################
                    [1m Learning iteration 3719/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.373s, learning 0.165s)
               Value function loss: 274.7536
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 3.60
               Mean episode length: 52.57
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 60948480
                    Iteration time: 8.54s
                        Total time: 36728.45s
                               ETA: 950605.3s

################################################################################
                    [1m Learning iteration 3720/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.091s, learning 0.166s)
               Value function loss: 41.9238
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 3.68
               Mean episode length: 52.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0162
--------------------------------------------------------------------------------
                   Total timesteps: 60964864
                    Iteration time: 8.26s
                        Total time: 36736.71s
                               ETA: 950553.6s

################################################################################
                    [1m Learning iteration 3721/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.306s, learning 0.167s)
               Value function loss: 20.3377
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 52.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0176
--------------------------------------------------------------------------------
                   Total timesteps: 60981248
                    Iteration time: 8.47s
                        Total time: 36745.18s
                               ETA: 950507.6s

################################################################################
                    [1m Learning iteration 3722/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.382s, learning 0.171s)
               Value function loss: 314.3390
                    Surrogate loss: 0.0071
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 51.51
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 60997632
                    Iteration time: 8.55s
                        Total time: 36753.73s
                               ETA: 950463.6s

################################################################################
                    [1m Learning iteration 3723/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.391s, learning 0.166s)
               Value function loss: 505.5079
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 3.66
               Mean episode length: 53.18
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 61014016
                    Iteration time: 8.56s
                        Total time: 36762.29s
                               ETA: 950419.7s

################################################################################
                    [1m Learning iteration 3724/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.423s, learning 0.170s)
               Value function loss: 823.6316
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 51.37
                  Mean reward/step: 0.42
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0225
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 61030400
                    Iteration time: 8.59s
                        Total time: 36770.88s
                               ETA: 950376.8s

################################################################################
                    [1m Learning iteration 3725/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.329s, learning 0.177s)
               Value function loss: 336.8909
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 21.25
               Mean episode length: 53.01
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0200
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 61046784
                    Iteration time: 8.51s
                        Total time: 36779.39s
                               ETA: 950331.6s

################################################################################
                    [1m Learning iteration 3726/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.132s, learning 0.171s)
               Value function loss: 116.3233
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 22.13
               Mean episode length: 54.34
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0184
--------------------------------------------------------------------------------
                   Total timesteps: 61063168
                    Iteration time: 8.30s
                        Total time: 36787.69s
                               ETA: 950281.3s

################################################################################
                    [1m Learning iteration 3727/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.315s, learning 0.183s)
               Value function loss: 387.8289
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 54.07
               Mean episode length: 53.01
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0230
--------------------------------------------------------------------------------
                   Total timesteps: 61079552
                    Iteration time: 8.50s
                        Total time: 36796.19s
                               ETA: 950235.9s

################################################################################
                    [1m Learning iteration 3728/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.324s, learning 0.167s)
               Value function loss: 380.9833
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 11.13
               Mean episode length: 53.07
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0243
--------------------------------------------------------------------------------
                   Total timesteps: 61095936
                    Iteration time: 8.49s
                        Total time: 36804.68s
                               ETA: 950190.5s

################################################################################
                    [1m Learning iteration 3729/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.082s, learning 0.161s)
               Value function loss: 349.6457
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 15.73
               Mean episode length: 51.75
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 61112320
                    Iteration time: 8.24s
                        Total time: 36812.92s
                               ETA: 950138.6s

################################################################################
                    [1m Learning iteration 3730/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.360s, learning 0.170s)
               Value function loss: 198.5845
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 3.36
               Mean episode length: 52.45
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0232
--------------------------------------------------------------------------------
                   Total timesteps: 61128704
                    Iteration time: 8.53s
                        Total time: 36821.45s
                               ETA: 950094.1s

################################################################################
                    [1m Learning iteration 3731/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.403s, learning 0.160s)
               Value function loss: 170.3151
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 16.20
               Mean episode length: 51.54
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0240
--------------------------------------------------------------------------------
                   Total timesteps: 61145088
                    Iteration time: 8.56s
                        Total time: 36830.02s
                               ETA: 950050.6s

################################################################################
                    [1m Learning iteration 3732/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.203s, learning 0.174s)
               Value function loss: 242.6652
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 3.70
               Mean episode length: 52.51
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0233
--------------------------------------------------------------------------------
                   Total timesteps: 61161472
                    Iteration time: 8.38s
                        Total time: 36838.39s
                               ETA: 950002.2s

################################################################################
                    [1m Learning iteration 3733/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.052s, learning 0.172s)
               Value function loss: 91.9819
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 11.53
               Mean episode length: 52.85
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 61177856
                    Iteration time: 8.22s
                        Total time: 36846.62s
                               ETA: 949950.0s

################################################################################
                    [1m Learning iteration 3734/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.337s, learning 0.166s)
               Value function loss: 841.3334
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.40
               Mean episode length: 52.83
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0206
--------------------------------------------------------------------------------
                   Total timesteps: 61194240
                    Iteration time: 8.50s
                        Total time: 36855.12s
                               ETA: 949904.9s

################################################################################
                    [1m Learning iteration 3735/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.377s, learning 0.172s)
               Value function loss: 360.7917
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 16.10
               Mean episode length: 52.50
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0204
--------------------------------------------------------------------------------
                   Total timesteps: 61210624
                    Iteration time: 8.55s
                        Total time: 36863.67s
                               ETA: 949861.1s

################################################################################
                    [1m Learning iteration 3736/100000 [0m                    

                       Computation: 1878 steps/s (collection: 8.521s, learning 0.201s)
               Value function loss: 431.9415
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 31.60
               Mean episode length: 52.87
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0217
--------------------------------------------------------------------------------
                   Total timesteps: 61227008
                    Iteration time: 8.72s
                        Total time: 36872.39s
                               ETA: 949821.7s

################################################################################
                    [1m Learning iteration 3737/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.356s, learning 0.207s)
               Value function loss: 57.5672
                    Surrogate loss: -0.0121
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 51.76
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 61243392
                    Iteration time: 8.56s
                        Total time: 36880.95s
                               ETA: 949778.3s

################################################################################
                    [1m Learning iteration 3738/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.453s, learning 0.184s)
               Value function loss: 282.7327
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 16.10
               Mean episode length: 51.78
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0224
--------------------------------------------------------------------------------
                   Total timesteps: 61259776
                    Iteration time: 8.64s
                        Total time: 36889.59s
                               ETA: 949736.8s

################################################################################
                    [1m Learning iteration 3739/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.242s, learning 0.166s)
               Value function loss: 265.7078
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 51.00
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0225
--------------------------------------------------------------------------------
                   Total timesteps: 61276160
                    Iteration time: 8.41s
                        Total time: 36898.00s
                               ETA: 949689.4s

################################################################################
                    [1m Learning iteration 3740/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.253s, learning 0.157s)
               Value function loss: 187.9927
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 50.90
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0208
--------------------------------------------------------------------------------
                   Total timesteps: 61292544
                    Iteration time: 8.41s
                        Total time: 36906.41s
                               ETA: 949642.1s

################################################################################
                    [1m Learning iteration 3741/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.121s, learning 0.168s)
               Value function loss: 158.7979
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 23.41
               Mean episode length: 51.21
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0228
--------------------------------------------------------------------------------
                   Total timesteps: 61308928
                    Iteration time: 8.29s
                        Total time: 36914.70s
                               ETA: 949591.6s

################################################################################
                    [1m Learning iteration 3742/100000 [0m                    

                       Computation: 1917 steps/s (collection: 8.382s, learning 0.162s)
               Value function loss: 164.0897
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 5.95
               Mean episode length: 52.08
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0214
--------------------------------------------------------------------------------
                   Total timesteps: 61325312
                    Iteration time: 8.54s
                        Total time: 36923.24s
                               ETA: 949547.8s

################################################################################
                    [1m Learning iteration 3743/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.052s, learning 0.172s)
               Value function loss: 347.0496
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 23.42
               Mean episode length: 52.18
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0220
--------------------------------------------------------------------------------
                   Total timesteps: 61341696
                    Iteration time: 8.22s
                        Total time: 36931.47s
                               ETA: 949495.7s

################################################################################
                    [1m Learning iteration 3744/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.197s, learning 0.160s)
               Value function loss: 34.6074
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 3.67
               Mean episode length: 52.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0203
--------------------------------------------------------------------------------
                   Total timesteps: 61358080
                    Iteration time: 8.36s
                        Total time: 36939.82s
                               ETA: 949447.1s

################################################################################
                    [1m Learning iteration 3745/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.397s, learning 0.173s)
               Value function loss: 178.1657
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 11.10
               Mean episode length: 52.22
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0205
--------------------------------------------------------------------------------
                   Total timesteps: 61374464
                    Iteration time: 8.57s
                        Total time: 36948.39s
                               ETA: 949404.0s

################################################################################
                    [1m Learning iteration 3746/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.145s, learning 0.170s)
               Value function loss: 75.2946
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 3.83
               Mean episode length: 53.56
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0201
--------------------------------------------------------------------------------
                   Total timesteps: 61390848
                    Iteration time: 8.32s
                        Total time: 36956.71s
                               ETA: 949354.4s

################################################################################
                    [1m Learning iteration 3747/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.444s, learning 0.165s)
               Value function loss: 32.3353
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 50.99
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 61407232
                    Iteration time: 8.61s
                        Total time: 36965.32s
                               ETA: 949312.4s

################################################################################
                    [1m Learning iteration 3748/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.537s, learning 0.161s)
               Value function loss: 139.8634
                    Surrogate loss: -0.0008
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 50.77
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0187
--------------------------------------------------------------------------------
                   Total timesteps: 61423616
                    Iteration time: 8.70s
                        Total time: 36974.02s
                               ETA: 949272.6s

################################################################################
                    [1m Learning iteration 3749/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.408s, learning 0.160s)
               Value function loss: 266.2785
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 8.72
               Mean episode length: 52.60
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0181
--------------------------------------------------------------------------------
                   Total timesteps: 61440000
                    Iteration time: 8.57s
                        Total time: 36982.58s
                               ETA: 949229.5s

################################################################################
                    [1m Learning iteration 3750/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.207s, learning 0.165s)
               Value function loss: 36.7424
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 3.36
               Mean episode length: 51.78
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 61456384
                    Iteration time: 8.37s
                        Total time: 36990.96s
                               ETA: 949181.4s

################################################################################
                    [1m Learning iteration 3751/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.261s, learning 0.172s)
               Value function loss: 13.6930
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 51.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0180
--------------------------------------------------------------------------------
                   Total timesteps: 61472768
                    Iteration time: 8.43s
                        Total time: 36999.39s
                               ETA: 949134.9s

################################################################################
                    [1m Learning iteration 3752/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.082s, learning 0.175s)
               Value function loss: 19.1581
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 52.62
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 61489152
                    Iteration time: 8.26s
                        Total time: 37007.65s
                               ETA: 949083.9s

################################################################################
                    [1m Learning iteration 3753/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.272s, learning 0.176s)
               Value function loss: 70.9976
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 52.12
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 61505536
                    Iteration time: 8.45s
                        Total time: 37016.09s
                               ETA: 949037.8s

################################################################################
                    [1m Learning iteration 3754/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.299s, learning 0.162s)
               Value function loss: 16.4913
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 3.53
               Mean episode length: 52.81
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 61521920
                    Iteration time: 8.46s
                        Total time: 37024.55s
                               ETA: 948992.1s

################################################################################
                    [1m Learning iteration 3755/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.519s, learning 0.164s)
               Value function loss: 3.7259
                    Surrogate loss: 0.0169
             Mean action noise std: 0.69
                       Mean reward: 11.16
               Mean episode length: 52.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 61538304
                    Iteration time: 8.68s
                        Total time: 37033.24s
                               ETA: 948952.1s

################################################################################
                    [1m Learning iteration 3756/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.255s, learning 0.161s)
               Value function loss: 3.3509
                    Surrogate loss: 0.0091
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 51.14
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 61554688
                    Iteration time: 8.42s
                        Total time: 37041.65s
                               ETA: 948905.2s

################################################################################
                    [1m Learning iteration 3757/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.993s, learning 0.170s)
               Value function loss: 2.5404
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.73
               Mean episode length: 53.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 61571072
                    Iteration time: 8.16s
                        Total time: 37049.82s
                               ETA: 948851.9s

################################################################################
                    [1m Learning iteration 3758/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.411s, learning 0.211s)
               Value function loss: 2.4581
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 3.51
               Mean episode length: 51.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 61587456
                    Iteration time: 8.62s
                        Total time: 37058.44s
                               ETA: 948810.4s

################################################################################
                    [1m Learning iteration 3759/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.351s, learning 0.215s)
               Value function loss: 4.7356
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 2.73
               Mean episode length: 49.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 61603840
                    Iteration time: 8.57s
                        Total time: 37067.00s
                               ETA: 948767.4s

################################################################################
                    [1m Learning iteration 3760/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.105s, learning 0.216s)
               Value function loss: 2.4248
                    Surrogate loss: -0.0117
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 51.02
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 61620224
                    Iteration time: 8.32s
                        Total time: 37075.32s
                               ETA: 948718.2s

################################################################################
                    [1m Learning iteration 3761/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.390s, learning 0.204s)
               Value function loss: 20.3268
                    Surrogate loss: -0.0002
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 51.94
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 61636608
                    Iteration time: 8.59s
                        Total time: 37083.92s
                               ETA: 948676.0s

################################################################################
                    [1m Learning iteration 3762/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.326s, learning 0.159s)
               Value function loss: 5.7251
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 51.05
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 61652992
                    Iteration time: 8.48s
                        Total time: 37092.40s
                               ETA: 948631.1s

################################################################################
                    [1m Learning iteration 3763/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.289s, learning 0.161s)
               Value function loss: 63.0940
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 51.49
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 61669376
                    Iteration time: 8.45s
                        Total time: 37100.85s
                               ETA: 948585.2s

################################################################################
                    [1m Learning iteration 3764/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.240s, learning 0.176s)
               Value function loss: 441.2714
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 51.68
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 61685760
                    Iteration time: 8.42s
                        Total time: 37109.27s
                               ETA: 948538.5s

################################################################################
                    [1m Learning iteration 3765/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.093s, learning 0.172s)
               Value function loss: 357.5880
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 50.50
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 61702144
                    Iteration time: 8.26s
                        Total time: 37117.53s
                               ETA: 948488.0s

################################################################################
                    [1m Learning iteration 3766/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.188s, learning 0.171s)
               Value function loss: 261.7934
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 51.22
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0075
--------------------------------------------------------------------------------
                   Total timesteps: 61718528
                    Iteration time: 8.36s
                        Total time: 37125.89s
                               ETA: 948439.9s

################################################################################
                    [1m Learning iteration 3767/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.306s, learning 0.162s)
               Value function loss: 244.2110
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 49.84
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 61734912
                    Iteration time: 8.47s
                        Total time: 37134.36s
                               ETA: 948394.6s

################################################################################
                    [1m Learning iteration 3768/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.211s, learning 0.162s)
               Value function loss: 32.9184
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.85
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 61751296
                    Iteration time: 8.37s
                        Total time: 37142.73s
                               ETA: 948346.9s

################################################################################
                    [1m Learning iteration 3769/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.304s, learning 0.169s)
               Value function loss: 292.4996
                    Surrogate loss: -0.0004
             Mean action noise std: 0.69
                       Mean reward: 16.08
               Mean episode length: 51.44
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 61767680
                    Iteration time: 8.47s
                        Total time: 37151.21s
                               ETA: 948301.8s

################################################################################
                    [1m Learning iteration 3770/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.421s, learning 0.172s)
               Value function loss: 56.0383
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 50.57
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 61784064
                    Iteration time: 8.59s
                        Total time: 37159.80s
                               ETA: 948259.7s

################################################################################
                    [1m Learning iteration 3771/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.134s, learning 0.166s)
               Value function loss: 69.1683
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 51.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 61800448
                    Iteration time: 8.30s
                        Total time: 37168.10s
                               ETA: 948210.2s

################################################################################
                    [1m Learning iteration 3772/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.212s, learning 0.173s)
               Value function loss: 64.3145
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 3.50
               Mean episode length: 51.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 61816832
                    Iteration time: 8.38s
                        Total time: 37176.48s
                               ETA: 948162.9s

################################################################################
                    [1m Learning iteration 3773/100000 [0m                    

                       Computation: 1959 steps/s (collection: 8.190s, learning 0.169s)
               Value function loss: 69.9103
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.24
               Mean episode length: 50.71
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 61833216
                    Iteration time: 8.36s
                        Total time: 37184.84s
                               ETA: 948114.9s

################################################################################
                    [1m Learning iteration 3774/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.402s, learning 0.175s)
               Value function loss: 408.7526
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.33
               Mean episode length: 50.87
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 61849600
                    Iteration time: 8.58s
                        Total time: 37193.42s
                               ETA: 948072.6s

################################################################################
                    [1m Learning iteration 3775/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.098s, learning 0.162s)
               Value function loss: 142.9072
                    Surrogate loss: -0.0050
             Mean action noise std: 0.69
                       Mean reward: 6.23
               Mean episode length: 52.21
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 61865984
                    Iteration time: 8.26s
                        Total time: 37201.68s
                               ETA: 948022.1s

################################################################################
                    [1m Learning iteration 3776/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.259s, learning 0.162s)
               Value function loss: 71.6160
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 50.82
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 61882368
                    Iteration time: 8.42s
                        Total time: 37210.10s
                               ETA: 947975.8s

################################################################################
                    [1m Learning iteration 3777/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.171s, learning 0.162s)
               Value function loss: 154.0778
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 49.44
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 61898752
                    Iteration time: 8.33s
                        Total time: 37218.43s
                               ETA: 947927.3s

################################################################################
                    [1m Learning iteration 3778/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.011s, learning 0.210s)
               Value function loss: 8.7252
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 50.91
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 61915136
                    Iteration time: 8.22s
                        Total time: 37226.66s
                               ETA: 947876.0s

################################################################################
                    [1m Learning iteration 3779/100000 [0m                    

                       Computation: 1912 steps/s (collection: 8.355s, learning 0.210s)
               Value function loss: 13.5651
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.97
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 61931520
                    Iteration time: 8.57s
                        Total time: 37235.22s
                               ETA: 947833.4s

################################################################################
                    [1m Learning iteration 3780/100000 [0m                    

                       Computation: 1863 steps/s (collection: 8.575s, learning 0.220s)
               Value function loss: 6.7829
                    Surrogate loss: -0.0081
             Mean action noise std: 0.69
                       Mean reward: 3.29
               Mean episode length: 51.52
                  Mean reward/step: 0.07
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 61947904
                    Iteration time: 8.79s
                        Total time: 37244.01s
                               ETA: 947796.6s

################################################################################
                    [1m Learning iteration 3781/100000 [0m                    

                       Computation: 1903 steps/s (collection: 8.395s, learning 0.211s)
               Value function loss: 2.0395
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 51.35
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0110
--------------------------------------------------------------------------------
                   Total timesteps: 61964288
                    Iteration time: 8.61s
                        Total time: 37252.62s
                               ETA: 947755.1s

################################################################################
                    [1m Learning iteration 3782/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.333s, learning 0.209s)
               Value function loss: 3.4274
                    Surrogate loss: -0.0111
             Mean action noise std: 0.69
                       Mean reward: 3.03
               Mean episode length: 49.61
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 61980672
                    Iteration time: 8.54s
                        Total time: 37261.16s
                               ETA: 947712.0s

################################################################################
                    [1m Learning iteration 3783/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.428s, learning 0.160s)
               Value function loss: 4.4695
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 3.57
               Mean episode length: 50.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0094
--------------------------------------------------------------------------------
                   Total timesteps: 61997056
                    Iteration time: 8.59s
                        Total time: 37269.75s
                               ETA: 947670.1s

################################################################################
                    [1m Learning iteration 3784/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.207s, learning 0.208s)
               Value function loss: 159.8524
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 50.42
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 62013440
                    Iteration time: 8.41s
                        Total time: 37278.17s
                               ETA: 947623.8s

################################################################################
                    [1m Learning iteration 3785/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.149s, learning 0.161s)
               Value function loss: 118.3423
                    Surrogate loss: -0.0020
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 50.83
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 62029824
                    Iteration time: 8.31s
                        Total time: 37286.48s
                               ETA: 947574.8s

################################################################################
                    [1m Learning iteration 3786/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.495s, learning 0.163s)
               Value function loss: 295.9406
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 13.68
               Mean episode length: 51.65
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 62046208
                    Iteration time: 8.66s
                        Total time: 37295.13s
                               ETA: 947534.7s

################################################################################
                    [1m Learning iteration 3787/100000 [0m                    

                       Computation: 1890 steps/s (collection: 8.446s, learning 0.219s)
               Value function loss: 38.7089
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 50.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0081
--------------------------------------------------------------------------------
                   Total timesteps: 62062592
                    Iteration time: 8.66s
                        Total time: 37303.80s
                               ETA: 947494.8s

################################################################################
                    [1m Learning iteration 3788/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.145s, learning 0.162s)
               Value function loss: 6.8515
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 50.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 62078976
                    Iteration time: 8.31s
                        Total time: 37312.11s
                               ETA: 947445.8s

################################################################################
                    [1m Learning iteration 3789/100000 [0m                    

                       Computation: 1928 steps/s (collection: 8.332s, learning 0.162s)
               Value function loss: 92.8263
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 8.87
               Mean episode length: 52.26
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 62095360
                    Iteration time: 8.49s
                        Total time: 37320.60s
                               ETA: 947401.6s

################################################################################
                    [1m Learning iteration 3790/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.375s, learning 0.178s)
               Value function loss: 202.2855
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.22
               Mean episode length: 50.60
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 62111744
                    Iteration time: 8.55s
                        Total time: 37329.15s
                               ETA: 947359.0s

################################################################################
                    [1m Learning iteration 3791/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.287s, learning 0.162s)
               Value function loss: 82.7302
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 11.32
               Mean episode length: 51.68
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 62128128
                    Iteration time: 8.45s
                        Total time: 37337.60s
                               ETA: 947313.7s

################################################################################
                    [1m Learning iteration 3792/100000 [0m                    

                       Computation: 2000 steps/s (collection: 8.026s, learning 0.164s)
               Value function loss: 211.9851
                    Surrogate loss: -0.0023
             Mean action noise std: 0.69
                       Mean reward: 2.81
               Mean episode length: 49.86
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0098
--------------------------------------------------------------------------------
                   Total timesteps: 62144512
                    Iteration time: 8.19s
                        Total time: 37345.79s
                               ETA: 947261.8s

################################################################################
                    [1m Learning iteration 3793/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.293s, learning 0.162s)
               Value function loss: 13.1168
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.54
               Mean episode length: 51.42
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 62160896
                    Iteration time: 8.45s
                        Total time: 37354.25s
                               ETA: 947216.7s

################################################################################
                    [1m Learning iteration 3794/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.320s, learning 0.162s)
               Value function loss: 3.4604
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 23.48
               Mean episode length: 50.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 62177280
                    Iteration time: 8.48s
                        Total time: 37362.73s
                               ETA: 947172.2s

################################################################################
                    [1m Learning iteration 3795/100000 [0m                    

                       Computation: 1980 steps/s (collection: 8.078s, learning 0.193s)
               Value function loss: 3.6408
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 51.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 62193664
                    Iteration time: 8.27s
                        Total time: 37371.00s
                               ETA: 947122.5s

################################################################################
                    [1m Learning iteration 3796/100000 [0m                    

                       Computation: 1886 steps/s (collection: 8.479s, learning 0.207s)
               Value function loss: 9.2047
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 50.32
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 62210048
                    Iteration time: 8.69s
                        Total time: 37379.69s
                               ETA: 947083.3s

################################################################################
                    [1m Learning iteration 3797/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.093s, learning 0.163s)
               Value function loss: 13.3346
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.60
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0092
--------------------------------------------------------------------------------
                   Total timesteps: 62226432
                    Iteration time: 8.26s
                        Total time: 37387.94s
                               ETA: 947033.2s

################################################################################
                    [1m Learning iteration 3798/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.161s, learning 0.163s)
               Value function loss: 3.8919
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.53
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 62242816
                    Iteration time: 8.32s
                        Total time: 37396.27s
                               ETA: 946984.9s

################################################################################
                    [1m Learning iteration 3799/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.370s, learning 0.184s)
               Value function loss: 63.5251
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.98
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 62259200
                    Iteration time: 8.55s
                        Total time: 37404.82s
                               ETA: 946942.4s

################################################################################
                    [1m Learning iteration 3800/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.141s, learning 0.167s)
               Value function loss: 330.2738
                    Surrogate loss: -0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 51.18
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 62275584
                    Iteration time: 8.31s
                        Total time: 37413.13s
                               ETA: 946893.7s

################################################################################
                    [1m Learning iteration 3801/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.215s, learning 0.163s)
               Value function loss: 75.8018
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 50.83
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 62291968
                    Iteration time: 8.38s
                        Total time: 37421.51s
                               ETA: 946846.8s

################################################################################
                    [1m Learning iteration 3802/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.140s, learning 0.169s)
               Value function loss: 13.6646
                    Surrogate loss: -0.0087
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 49.89
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0067
--------------------------------------------------------------------------------
                   Total timesteps: 62308352
                    Iteration time: 8.31s
                        Total time: 37429.81s
                               ETA: 946798.1s

################################################################################
                    [1m Learning iteration 3803/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.271s, learning 0.161s)
               Value function loss: 10.1608
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 51.45
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0087
--------------------------------------------------------------------------------
                   Total timesteps: 62324736
                    Iteration time: 8.43s
                        Total time: 37438.25s
                               ETA: 946752.6s

################################################################################
                    [1m Learning iteration 3804/100000 [0m                    

                       Computation: 1725 steps/s (collection: 9.319s, learning 0.174s)
               Value function loss: 17.8473
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 50.90
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0080
--------------------------------------------------------------------------------
                   Total timesteps: 62341120
                    Iteration time: 9.49s
                        Total time: 37447.74s
                               ETA: 946734.0s

################################################################################
                    [1m Learning iteration 3805/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.533s, learning 0.190s)
               Value function loss: 1.1939
                    Surrogate loss: -0.0142
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.05
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 62357504
                    Iteration time: 16.72s
                        Total time: 37464.46s
                               ETA: 946898.0s

################################################################################
                    [1m Learning iteration 3806/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.271s, learning 0.166s)
               Value function loss: 1.5653
                    Surrogate loss: -0.0108
             Mean action noise std: 0.69
                       Mean reward: 3.56
               Mean episode length: 51.86
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0069
--------------------------------------------------------------------------------
                   Total timesteps: 62373888
                    Iteration time: 16.44s
                        Total time: 37480.90s
                               ETA: 947054.8s

################################################################################
                    [1m Learning iteration 3807/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.085s, learning 0.216s)
               Value function loss: 1.3744
                    Surrogate loss: -0.0135
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 51.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0066
--------------------------------------------------------------------------------
                   Total timesteps: 62390272
                    Iteration time: 16.30s
                        Total time: 37497.20s
                               ETA: 947208.1s

################################################################################
                    [1m Learning iteration 3808/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.131s, learning 0.165s)
               Value function loss: 1.8315
                    Surrogate loss: -0.0079
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 51.26
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 62406656
                    Iteration time: 16.30s
                        Total time: 37513.50s
                               ETA: 947361.1s

################################################################################
                    [1m Learning iteration 3809/100000 [0m                    

                       Computation: 1022 steps/s (collection: 15.849s, learning 0.174s)
               Value function loss: 284.1399
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 50.17
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0056
--------------------------------------------------------------------------------
                   Total timesteps: 62423040
                    Iteration time: 16.02s
                        Total time: 37529.52s
                               ETA: 947507.1s

################################################################################
                    [1m Learning iteration 3810/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.179s, learning 0.166s)
               Value function loss: 83.6969
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 51.21
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0052
--------------------------------------------------------------------------------
                   Total timesteps: 62439424
                    Iteration time: 16.35s
                        Total time: 37545.86s
                               ETA: 947661.2s

################################################################################
                    [1m Learning iteration 3811/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.052s, learning 0.172s)
               Value function loss: 490.6956
                    Surrogate loss: -0.0029
             Mean action noise std: 0.69
                       Mean reward: 16.01
               Mean episode length: 50.91
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0061
--------------------------------------------------------------------------------
                   Total timesteps: 62455808
                    Iteration time: 16.22s
                        Total time: 37562.09s
                               ETA: 947812.1s

################################################################################
                    [1m Learning iteration 3812/100000 [0m                    

                       Computation: 1007 steps/s (collection: 16.044s, learning 0.215s)
               Value function loss: 428.1341
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 16.09
               Mean episode length: 52.63
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 62472192
                    Iteration time: 16.26s
                        Total time: 37578.35s
                               ETA: 947963.8s

################################################################################
                    [1m Learning iteration 3813/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.511s, learning 0.168s)
               Value function loss: 156.1845
                    Surrogate loss: -0.0054
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.43
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0077
--------------------------------------------------------------------------------
                   Total timesteps: 62488576
                    Iteration time: 16.68s
                        Total time: 37595.03s
                               ETA: 948126.1s

################################################################################
                    [1m Learning iteration 3814/100000 [0m                    

                       Computation: 1013 steps/s (collection: 15.977s, learning 0.189s)
               Value function loss: 252.5824
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.76
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0099
--------------------------------------------------------------------------------
                   Total timesteps: 62504960
                    Iteration time: 16.17s
                        Total time: 37611.19s
                               ETA: 948275.2s

################################################################################
                    [1m Learning iteration 3815/100000 [0m                    

                       Computation: 1002 steps/s (collection: 16.178s, learning 0.163s)
               Value function loss: 196.8067
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 18.13
               Mean episode length: 51.21
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 62521344
                    Iteration time: 16.34s
                        Total time: 37627.53s
                               ETA: 948428.8s

################################################################################
                    [1m Learning iteration 3816/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.284s, learning 0.160s)
               Value function loss: 119.9645
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3.13
               Mean episode length: 51.27
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 62537728
                    Iteration time: 16.44s
                        Total time: 37643.98s
                               ETA: 948584.8s

################################################################################
                    [1m Learning iteration 3817/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.084s, learning 0.157s)
               Value function loss: 122.1451
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 2.72
               Mean episode length: 50.30
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 62554112
                    Iteration time: 16.24s
                        Total time: 37660.22s
                               ETA: 948735.7s

################################################################################
                    [1m Learning iteration 3818/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.062s, learning 0.163s)
               Value function loss: 233.0764
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 51.48
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 62570496
                    Iteration time: 16.23s
                        Total time: 37676.44s
                               ETA: 948886.0s

################################################################################
                    [1m Learning iteration 3819/100000 [0m                    

                       Computation: 1018 steps/s (collection: 15.917s, learning 0.168s)
               Value function loss: 153.2162
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 52.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 62586880
                    Iteration time: 16.08s
                        Total time: 37692.53s
                               ETA: 949032.7s

################################################################################
                    [1m Learning iteration 3820/100000 [0m                    

                       Computation: 1005 steps/s (collection: 16.124s, learning 0.162s)
               Value function loss: 294.4906
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 18.42
               Mean episode length: 51.16
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 62603264
                    Iteration time: 16.29s
                        Total time: 37708.81s
                               ETA: 949184.4s

################################################################################
                    [1m Learning iteration 3821/100000 [0m                    

                       Computation: 1015 steps/s (collection: 15.970s, learning 0.171s)
               Value function loss: 57.0522
                    Surrogate loss: -0.0085
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 50.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 62619648
                    Iteration time: 16.14s
                        Total time: 37724.96s
                               ETA: 949332.4s

################################################################################
                    [1m Learning iteration 3822/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.021s, learning 0.174s)
               Value function loss: 22.9457
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 51.13
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 62636032
                    Iteration time: 16.19s
                        Total time: 37741.15s
                               ETA: 949481.6s

################################################################################
                    [1m Learning iteration 3823/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.313s, learning 0.166s)
               Value function loss: 11.0436
                    Surrogate loss: 0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.62
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 62652416
                    Iteration time: 16.48s
                        Total time: 37757.63s
                               ETA: 949637.9s

################################################################################
                    [1m Learning iteration 3824/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.049s, learning 0.163s)
               Value function loss: 3.6088
                    Surrogate loss: -0.0107
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 51.64
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 62668800
                    Iteration time: 16.21s
                        Total time: 37773.84s
                               ETA: 949787.4s

################################################################################
                    [1m Learning iteration 3825/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.240s, learning 0.211s)
               Value function loss: 1.8801
                    Surrogate loss: -0.0184
             Mean action noise std: 0.69
                       Mean reward: 3.45
               Mean episode length: 52.15
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 62685184
                    Iteration time: 16.45s
                        Total time: 37790.29s
                               ETA: 949942.9s

################################################################################
                    [1m Learning iteration 3826/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.234s, learning 0.177s)
               Value function loss: 0.8535
                    Surrogate loss: -0.0123
             Mean action noise std: 0.69
                       Mean reward: 2.74
               Mean episode length: 49.90
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 62701568
                    Iteration time: 16.41s
                        Total time: 37806.70s
                               ETA: 950097.2s

################################################################################
                    [1m Learning iteration 3827/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.223s, learning 0.182s)
               Value function loss: 1.1933
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 2.91
               Mean episode length: 50.40
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 62717952
                    Iteration time: 16.40s
                        Total time: 37823.11s
                               ETA: 950251.3s

################################################################################
                    [1m Learning iteration 3828/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.434s, learning 0.167s)
               Value function loss: 289.4747
                    Surrogate loss: 0.0000
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 51.92
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 62734336
                    Iteration time: 16.60s
                        Total time: 37839.71s
                               ETA: 950410.2s

################################################################################
                    [1m Learning iteration 3829/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.014s, learning 0.171s)
               Value function loss: 128.5481
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.42
               Mean episode length: 52.25
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 62750720
                    Iteration time: 16.18s
                        Total time: 37855.90s
                               ETA: 950558.6s

################################################################################
                    [1m Learning iteration 3830/100000 [0m                    

                       Computation: 979 steps/s (collection: 16.544s, learning 0.179s)
               Value function loss: 50.9806
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 20.86
               Mean episode length: 51.64
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0089
--------------------------------------------------------------------------------
                   Total timesteps: 62767104
                    Iteration time: 16.72s
                        Total time: 37872.62s
                               ETA: 950720.4s

################################################################################
                    [1m Learning iteration 3831/100000 [0m                    

                       Computation: 1011 steps/s (collection: 16.023s, learning 0.168s)
               Value function loss: 529.4149
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 3.06
               Mean episode length: 51.91
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0082
--------------------------------------------------------------------------------
                   Total timesteps: 62783488
                    Iteration time: 16.19s
                        Total time: 37888.81s
                               ETA: 950868.7s

################################################################################
                    [1m Learning iteration 3832/100000 [0m                    

                       Computation: 1012 steps/s (collection: 16.022s, learning 0.160s)
               Value function loss: 153.2604
                    Surrogate loss: -0.0063
             Mean action noise std: 0.69
                       Mean reward: 3.44
               Mean episode length: 50.88
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 62799872
                    Iteration time: 16.18s
                        Total time: 37904.99s
                               ETA: 951016.8s

################################################################################
                    [1m Learning iteration 3833/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.238s, learning 0.171s)
               Value function loss: 245.1337
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 23.47
               Mean episode length: 51.51
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 62816256
                    Iteration time: 16.41s
                        Total time: 37921.40s
                               ETA: 951170.4s

################################################################################
                    [1m Learning iteration 3834/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.207s, learning 0.177s)
               Value function loss: 182.8631
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 11.23
               Mean episode length: 52.37
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 62832640
                    Iteration time: 16.38s
                        Total time: 37937.79s
                               ETA: 951323.4s

################################################################################
                    [1m Learning iteration 3835/100000 [0m                    

                       Computation: 984 steps/s (collection: 16.469s, learning 0.177s)
               Value function loss: 493.4694
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 51.77
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 62849024
                    Iteration time: 16.65s
                        Total time: 37954.43s
                               ETA: 951482.8s

################################################################################
                    [1m Learning iteration 3836/100000 [0m                    

                       Computation: 1027 steps/s (collection: 15.776s, learning 0.170s)
               Value function loss: 188.9547
                    Surrogate loss: -0.0084
             Mean action noise std: 0.69
                       Mean reward: 21.13
               Mean episode length: 51.94
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 62865408
                    Iteration time: 15.95s
                        Total time: 37970.38s
                               ETA: 951624.5s

################################################################################
                    [1m Learning iteration 3837/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.301s, learning 0.168s)
               Value function loss: 277.9857
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 51.89
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0113
--------------------------------------------------------------------------------
                   Total timesteps: 62881792
                    Iteration time: 16.47s
                        Total time: 37986.85s
                               ETA: 951779.4s

################################################################################
                    [1m Learning iteration 3838/100000 [0m                    

                       Computation: 991 steps/s (collection: 16.345s, learning 0.173s)
               Value function loss: 487.2754
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 52.23
                  Mean reward/step: 0.28
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0104
--------------------------------------------------------------------------------
                   Total timesteps: 62898176
                    Iteration time: 16.52s
                        Total time: 38003.37s
                               ETA: 951935.3s

################################################################################
                    [1m Learning iteration 3839/100000 [0m                    

                       Computation: 1024 steps/s (collection: 15.831s, learning 0.169s)
               Value function loss: 748.2236
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 23.59
               Mean episode length: 52.84
                  Mean reward/step: 0.35
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 62914560
                    Iteration time: 16.00s
                        Total time: 38019.37s
                               ETA: 952078.2s

################################################################################
                    [1m Learning iteration 3840/100000 [0m                    

                       Computation: 995 steps/s (collection: 16.170s, learning 0.283s)
               Value function loss: 141.0028
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 13.89
               Mean episode length: 52.70
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 62930944
                    Iteration time: 16.45s
                        Total time: 38035.82s
                               ETA: 952232.3s

################################################################################
                    [1m Learning iteration 3841/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.266s, learning 0.178s)
               Value function loss: 63.8851
                    Surrogate loss: -0.0067
             Mean action noise std: 0.69
                       Mean reward: 18.27
               Mean episode length: 51.77
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 62947328
                    Iteration time: 16.44s
                        Total time: 38052.26s
                               ETA: 952386.1s

################################################################################
                    [1m Learning iteration 3842/100000 [0m                    

                       Computation: 1273 steps/s (collection: 12.694s, learning 0.168s)
               Value function loss: 17.6276
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 3.76
               Mean episode length: 52.27
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 62963712
                    Iteration time: 12.86s
                        Total time: 38065.13s
                               ETA: 952450.2s

################################################################################
                    [1m Learning iteration 3843/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.110s, learning 0.168s)
               Value function loss: 19.5524
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 50.48
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 62980096
                    Iteration time: 8.28s
                        Total time: 38073.40s
                               ETA: 952399.6s

################################################################################
                    [1m Learning iteration 3844/100000 [0m                    

                       Computation: 1964 steps/s (collection: 8.179s, learning 0.160s)
               Value function loss: 20.5979
                    Surrogate loss: -0.0000
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 51.31
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 62996480
                    Iteration time: 8.34s
                        Total time: 38081.74s
                               ETA: 952350.6s

################################################################################
                    [1m Learning iteration 3845/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.143s, learning 0.160s)
               Value function loss: 7.2316
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 6.20
               Mean episode length: 52.73
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 63012864
                    Iteration time: 8.30s
                        Total time: 38090.05s
                               ETA: 952300.6s

################################################################################
                    [1m Learning iteration 3846/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.472s, learning 0.161s)
               Value function loss: 239.8924
                    Surrogate loss: 0.0012
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 50.71
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 63029248
                    Iteration time: 8.63s
                        Total time: 38098.68s
                               ETA: 952259.0s

################################################################################
                    [1m Learning iteration 3847/100000 [0m                    

                       Computation: 1998 steps/s (collection: 8.036s, learning 0.163s)
               Value function loss: 830.2029
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 50.78
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 63045632
                    Iteration time: 8.20s
                        Total time: 38106.88s
                               ETA: 952206.5s

################################################################################
                    [1m Learning iteration 3848/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.063s, learning 0.162s)
               Value function loss: 211.6123
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 51.85
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 63062016
                    Iteration time: 8.23s
                        Total time: 38115.10s
                               ETA: 952154.7s

################################################################################
                    [1m Learning iteration 3849/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.104s, learning 0.221s)
               Value function loss: 509.6932
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 38.57
               Mean episode length: 51.43
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0161
Mean episode consecutive_successes: 0.0157
--------------------------------------------------------------------------------
                   Total timesteps: 63078400
                    Iteration time: 8.32s
                        Total time: 38123.43s
                               ETA: 952105.4s

################################################################################
                    [1m Learning iteration 3850/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.373s, learning 0.162s)
               Value function loss: 224.1987
                    Surrogate loss: -0.0064
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.24
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 63094784
                    Iteration time: 8.53s
                        Total time: 38131.96s
                               ETA: 952061.3s

################################################################################
                    [1m Learning iteration 3851/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.164s, learning 0.170s)
               Value function loss: 609.6683
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 50.14
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 63111168
                    Iteration time: 8.33s
                        Total time: 38140.30s
                               ETA: 952012.3s

################################################################################
                    [1m Learning iteration 3852/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.118s, learning 0.160s)
               Value function loss: 277.3286
                    Surrogate loss: -0.0070
             Mean action noise std: 0.69
                       Mean reward: 13.08
               Mean episode length: 49.95
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 63127552
                    Iteration time: 8.28s
                        Total time: 38148.57s
                               ETA: 951961.9s

################################################################################
                    [1m Learning iteration 3853/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.045s, learning 0.160s)
               Value function loss: 78.9653
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 49.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 63143936
                    Iteration time: 8.20s
                        Total time: 38156.78s
                               ETA: 951909.6s

################################################################################
                    [1m Learning iteration 3854/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.588s, learning 0.162s)
               Value function loss: 20.4204
                    Surrogate loss: -0.0106
             Mean action noise std: 0.69
                       Mean reward: 20.73
               Mean episode length: 50.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0183
--------------------------------------------------------------------------------
                   Total timesteps: 63160320
                    Iteration time: 8.75s
                        Total time: 38165.53s
                               ETA: 951871.0s

################################################################################
                    [1m Learning iteration 3855/100000 [0m                    

                       Computation: 1965 steps/s (collection: 8.157s, learning 0.178s)
               Value function loss: 19.8019
                    Surrogate loss: -0.0108
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 51.23
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0169
--------------------------------------------------------------------------------
                   Total timesteps: 63176704
                    Iteration time: 8.33s
                        Total time: 38173.86s
                               ETA: 951822.1s

################################################################################
                    [1m Learning iteration 3856/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.370s, learning 0.161s)
               Value function loss: 14.7442
                    Surrogate loss: -0.0088
             Mean action noise std: 0.69
                       Mean reward: 3.37
               Mean episode length: 51.20
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 63193088
                    Iteration time: 8.53s
                        Total time: 38182.39s
                               ETA: 951778.1s

################################################################################
                    [1m Learning iteration 3857/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.212s, learning 0.213s)
               Value function loss: 551.1714
                    Surrogate loss: 0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 50.68
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 63209472
                    Iteration time: 8.43s
                        Total time: 38190.82s
                               ETA: 951731.4s

################################################################################
                    [1m Learning iteration 3858/100000 [0m                    

                       Computation: 1881 steps/s (collection: 8.496s, learning 0.211s)
               Value function loss: 50.4783
                    Surrogate loss: -0.0078
             Mean action noise std: 0.69
                       Mean reward: 3.49
               Mean episode length: 51.30
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 63225856
                    Iteration time: 8.71s
                        Total time: 38199.53s
                               ETA: 951691.8s

################################################################################
                    [1m Learning iteration 3859/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.288s, learning 0.169s)
               Value function loss: 4.4635
                    Surrogate loss: -0.0159
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.75
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 63242240
                    Iteration time: 8.46s
                        Total time: 38207.98s
                               ETA: 951646.0s

################################################################################
                    [1m Learning iteration 3860/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.278s, learning 0.162s)
               Value function loss: 476.4532
                    Surrogate loss: 0.0009
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 50.13
                  Mean reward/step: 0.19
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 63258624
                    Iteration time: 8.44s
                        Total time: 38216.42s
                               ETA: 951599.8s

################################################################################
                    [1m Learning iteration 3861/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.339s, learning 0.160s)
               Value function loss: 109.7379
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.23
               Mean episode length: 51.61
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 63275008
                    Iteration time: 8.50s
                        Total time: 38224.92s
                               ETA: 951555.1s

################################################################################
                    [1m Learning iteration 3862/100000 [0m                    

                       Computation: 2017 steps/s (collection: 7.939s, learning 0.183s)
               Value function loss: 376.1368
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 23.54
               Mean episode length: 52.41
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 63291392
                    Iteration time: 8.12s
                        Total time: 38233.04s
                               ETA: 951501.0s

################################################################################
                    [1m Learning iteration 3863/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.433s, learning 0.161s)
               Value function loss: 308.5990
                    Surrogate loss: -0.0066
             Mean action noise std: 0.69
                       Mean reward: 21.09
               Mean episode length: 51.47
                  Mean reward/step: 0.22
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 63307776
                    Iteration time: 8.59s
                        Total time: 38241.64s
                               ETA: 951458.7s

################################################################################
                    [1m Learning iteration 3864/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.433s, learning 0.171s)
               Value function loss: 46.4366
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 30.61
               Mean episode length: 50.17
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 63324160
                    Iteration time: 8.60s
                        Total time: 38250.24s
                               ETA: 951416.6s

################################################################################
                    [1m Learning iteration 3865/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.338s, learning 0.223s)
               Value function loss: 11.3582
                    Surrogate loss: -0.0127
             Mean action noise std: 0.69
                       Mean reward: 18.67
               Mean episode length: 51.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 63340544
                    Iteration time: 8.56s
                        Total time: 38258.80s
                               ETA: 951373.5s

################################################################################
                    [1m Learning iteration 3866/100000 [0m                    

                       Computation: 1989 steps/s (collection: 8.071s, learning 0.164s)
               Value function loss: 7.4312
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 48.96
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 63356928
                    Iteration time: 8.23s
                        Total time: 38267.04s
                               ETA: 951322.3s

################################################################################
                    [1m Learning iteration 3867/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.298s, learning 0.162s)
               Value function loss: 4.7714
                    Surrogate loss: -0.0139
             Mean action noise std: 0.69
                       Mean reward: 3.18
               Mean episode length: 50.33
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 63373312
                    Iteration time: 8.46s
                        Total time: 38275.50s
                               ETA: 951276.7s

################################################################################
                    [1m Learning iteration 3868/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.358s, learning 0.168s)
               Value function loss: 6.0914
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 3.36
               Mean episode length: 49.97
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0128
--------------------------------------------------------------------------------
                   Total timesteps: 63389696
                    Iteration time: 8.53s
                        Total time: 38284.02s
                               ETA: 951232.8s

################################################################################
                    [1m Learning iteration 3869/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.242s, learning 0.167s)
               Value function loss: 1089.1712
                    Surrogate loss: 0.0046
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 50.23
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0118
--------------------------------------------------------------------------------
                   Total timesteps: 63406080
                    Iteration time: 8.41s
                        Total time: 38292.43s
                               ETA: 951186.0s

################################################################################
                    [1m Learning iteration 3870/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.090s, learning 0.167s)
               Value function loss: 456.2101
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.99
               Mean episode length: 49.51
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 63422464
                    Iteration time: 8.26s
                        Total time: 38300.69s
                               ETA: 951135.5s

################################################################################
                    [1m Learning iteration 3871/100000 [0m                    

                       Computation: 1867 steps/s (collection: 8.612s, learning 0.159s)
               Value function loss: 96.5180
                    Surrogate loss: -0.0050
             Mean action noise std: 0.69
                       Mean reward: 23.72
               Mean episode length: 51.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 63438848
                    Iteration time: 8.77s
                        Total time: 38309.46s
                               ETA: 951097.7s

################################################################################
                    [1m Learning iteration 3872/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.459s, learning 0.163s)
               Value function loss: 8.8150
                    Surrogate loss: -0.0136
             Mean action noise std: 0.69
                       Mean reward: 28.27
               Mean episode length: 51.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 63455232
                    Iteration time: 8.62s
                        Total time: 38318.09s
                               ETA: 951056.3s

################################################################################
                    [1m Learning iteration 3873/100000 [0m                    

                       Computation: 2014 steps/s (collection: 7.967s, learning 0.164s)
               Value function loss: 5.4123
                    Surrogate loss: -0.0137
             Mean action noise std: 0.69
                       Mean reward: 23.39
               Mean episode length: 50.80
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 63471616
                    Iteration time: 8.13s
                        Total time: 38326.22s
                               ETA: 951002.6s

################################################################################
                    [1m Learning iteration 3874/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.144s, learning 0.188s)
               Value function loss: 4.5731
                    Surrogate loss: -0.0129
             Mean action noise std: 0.69
                       Mean reward: 3.85
               Mean episode length: 52.82
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 63488000
                    Iteration time: 8.33s
                        Total time: 38334.55s
                               ETA: 950954.0s

################################################################################
                    [1m Learning iteration 3875/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.126s, learning 0.260s)
               Value function loss: 5.1897
                    Surrogate loss: -0.0092
             Mean action noise std: 0.69
                       Mean reward: 3.69
               Mean episode length: 52.16
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 63504384
                    Iteration time: 8.39s
                        Total time: 38342.93s
                               ETA: 950906.8s

################################################################################
                    [1m Learning iteration 3876/100000 [0m                    

                       Computation: 1872 steps/s (collection: 8.576s, learning 0.175s)
               Value function loss: 3.3438
                    Surrogate loss: -0.0125
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 50.99
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 63520768
                    Iteration time: 8.75s
                        Total time: 38351.69s
                               ETA: 950868.6s

################################################################################
                    [1m Learning iteration 3877/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.264s, learning 0.163s)
               Value function loss: 2.6068
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 3.05
               Mean episode length: 50.17
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 63537152
                    Iteration time: 8.43s
                        Total time: 38360.11s
                               ETA: 950822.4s

################################################################################
                    [1m Learning iteration 3878/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.455s, learning 0.167s)
               Value function loss: 1.1115
                    Surrogate loss: -0.0159
             Mean action noise std: 0.69
                       Mean reward: 3.28
               Mean episode length: 50.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 63553536
                    Iteration time: 8.62s
                        Total time: 38368.73s
                               ETA: 950781.0s

################################################################################
                    [1m Learning iteration 3879/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.390s, learning 0.165s)
               Value function loss: 475.7358
                    Surrogate loss: 0.0027
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 49.35
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 63569920
                    Iteration time: 8.56s
                        Total time: 38377.29s
                               ETA: 950738.0s

################################################################################
                    [1m Learning iteration 3880/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.251s, learning 0.159s)
               Value function loss: 471.7761
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 50.42
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0088
--------------------------------------------------------------------------------
                   Total timesteps: 63586304
                    Iteration time: 8.41s
                        Total time: 38385.70s
                               ETA: 950691.4s

################################################################################
                    [1m Learning iteration 3881/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.234s, learning 0.163s)
               Value function loss: 236.6602
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 12.92
               Mean episode length: 49.53
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0137
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 63602688
                    Iteration time: 8.40s
                        Total time: 38394.10s
                               ETA: 950644.5s

################################################################################
                    [1m Learning iteration 3882/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.207s, learning 0.171s)
               Value function loss: 4.5961
                    Surrogate loss: -0.0063
             Mean action noise std: 0.69
                       Mean reward: 13.60
               Mean episode length: 50.56
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0114
--------------------------------------------------------------------------------
                   Total timesteps: 63619072
                    Iteration time: 8.38s
                        Total time: 38402.47s
                               ETA: 950597.2s

################################################################################
                    [1m Learning iteration 3883/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.138s, learning 0.173s)
               Value function loss: 3.1358
                    Surrogate loss: -0.0089
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 49.25
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 63635456
                    Iteration time: 8.31s
                        Total time: 38410.79s
                               ETA: 950548.3s

################################################################################
                    [1m Learning iteration 3884/100000 [0m                    

                       Computation: 1955 steps/s (collection: 8.215s, learning 0.163s)
               Value function loss: 3.2093
                    Surrogate loss: -0.0139
             Mean action noise std: 0.69
                       Mean reward: 3.43
               Mean episode length: 51.68
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 63651840
                    Iteration time: 8.38s
                        Total time: 38419.16s
                               ETA: 950501.0s

################################################################################
                    [1m Learning iteration 3885/100000 [0m                    

                       Computation: 1915 steps/s (collection: 8.393s, learning 0.160s)
               Value function loss: 212.2552
                    Surrogate loss: 0.0023
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 50.48
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 63668224
                    Iteration time: 8.55s
                        Total time: 38427.72s
                               ETA: 950458.1s

################################################################################
                    [1m Learning iteration 3886/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.497s, learning 0.160s)
               Value function loss: 343.8485
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.50
               Mean episode length: 50.62
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 63684608
                    Iteration time: 8.66s
                        Total time: 38436.37s
                               ETA: 950417.7s

################################################################################
                    [1m Learning iteration 3887/100000 [0m                    

                       Computation: 1988 steps/s (collection: 8.070s, learning 0.168s)
               Value function loss: 434.3838
                    Surrogate loss: -0.0040
             Mean action noise std: 0.69
                       Mean reward: 3.39
               Mean episode length: 50.97
                  Mean reward/step: 0.25
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 63700992
                    Iteration time: 8.24s
                        Total time: 38444.61s
                               ETA: 950367.0s

################################################################################
                    [1m Learning iteration 3888/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.460s, learning 0.161s)
               Value function loss: 46.2974
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 3.01
               Mean episode length: 49.61
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0095
--------------------------------------------------------------------------------
                   Total timesteps: 63717376
                    Iteration time: 8.62s
                        Total time: 38453.23s
                               ETA: 950325.8s

################################################################################
                    [1m Learning iteration 3889/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.280s, learning 0.188s)
               Value function loss: 108.5307
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 13.66
               Mean episode length: 51.20
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 63733760
                    Iteration time: 8.47s
                        Total time: 38461.70s
                               ETA: 950280.9s

################################################################################
                    [1m Learning iteration 3890/100000 [0m                    

                       Computation: 1900 steps/s (collection: 8.458s, learning 0.162s)
               Value function loss: 255.4280
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 3.02
               Mean episode length: 49.70
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 63750144
                    Iteration time: 8.62s
                        Total time: 38470.32s
                               ETA: 950239.7s

################################################################################
                    [1m Learning iteration 3891/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.279s, learning 0.169s)
               Value function loss: 242.3824
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 8.56
               Mean episode length: 50.45
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 63766528
                    Iteration time: 8.45s
                        Total time: 38478.77s
                               ETA: 950194.3s

################################################################################
                    [1m Learning iteration 3892/100000 [0m                    

                       Computation: 1974 steps/s (collection: 8.131s, learning 0.168s)
               Value function loss: 20.0564
                    Surrogate loss: -0.0109
             Mean action noise std: 0.69
                       Mean reward: 18.59
               Mean episode length: 50.34
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 63782912
                    Iteration time: 8.30s
                        Total time: 38487.07s
                               ETA: 950145.2s

################################################################################
                    [1m Learning iteration 3893/100000 [0m                    

                       Computation: 2016 steps/s (collection: 7.956s, learning 0.167s)
               Value function loss: 173.7333
                    Surrogate loss: -0.0022
             Mean action noise std: 0.69
                       Mean reward: 3.72
               Mean episode length: 50.71
                  Mean reward/step: 0.14
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 63799296
                    Iteration time: 8.12s
                        Total time: 38495.19s
                               ETA: 950091.8s

################################################################################
                    [1m Learning iteration 3894/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.367s, learning 0.163s)
               Value function loss: 224.9673
                    Surrogate loss: -0.0039
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 48.76
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 63815680
                    Iteration time: 8.53s
                        Total time: 38503.72s
                               ETA: 950048.5s

################################################################################
                    [1m Learning iteration 3895/100000 [0m                    

                       Computation: 2001 steps/s (collection: 7.988s, learning 0.198s)
               Value function loss: 48.8870
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 49.49
                  Mean reward/step: 0.11
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 63832064
                    Iteration time: 8.19s
                        Total time: 38511.91s
                               ETA: 949996.7s

################################################################################
                    [1m Learning iteration 3896/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.345s, learning 0.210s)
               Value function loss: 399.0932
                    Surrogate loss: 0.0031
             Mean action noise std: 0.69
                       Mean reward: 3.35
               Mean episode length: 50.82
                  Mean reward/step: 0.15
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 63848448
                    Iteration time: 8.56s
                        Total time: 38520.46s
                               ETA: 949954.0s

################################################################################
                    [1m Learning iteration 3897/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.259s, learning 0.165s)
               Value function loss: 45.4657
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 50.60
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 63864832
                    Iteration time: 8.42s
                        Total time: 38528.89s
                               ETA: 949908.1s

################################################################################
                    [1m Learning iteration 3898/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.205s, learning 0.161s)
               Value function loss: 369.1745
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 26.20
               Mean episode length: 51.74
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 63881216
                    Iteration time: 8.37s
                        Total time: 38537.26s
                               ETA: 949860.8s

################################################################################
                    [1m Learning iteration 3899/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.130s, learning 0.219s)
               Value function loss: 561.4045
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 50.34
                  Mean reward/step: 0.26
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 63897600
                    Iteration time: 8.35s
                        Total time: 38545.60s
                               ETA: 949813.1s

################################################################################
                    [1m Learning iteration 3900/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.186s, learning 0.168s)
               Value function loss: 163.0637
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 30.86
               Mean episode length: 49.47
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 63913984
                    Iteration time: 8.35s
                        Total time: 38553.96s
                               ETA: 949765.5s

################################################################################
                    [1m Learning iteration 3901/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.272s, learning 0.163s)
               Value function loss: 495.9961
                    Surrogate loss: -0.0028
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 49.43
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 63930368
                    Iteration time: 8.43s
                        Total time: 38562.39s
                               ETA: 949720.0s

################################################################################
                    [1m Learning iteration 3902/100000 [0m                    

                       Computation: 1952 steps/s (collection: 8.227s, learning 0.162s)
               Value function loss: 82.0451
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 50.36
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 63946752
                    Iteration time: 8.39s
                        Total time: 38570.78s
                               ETA: 949673.3s

################################################################################
                    [1m Learning iteration 3903/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.988s, learning 0.179s)
               Value function loss: 27.4290
                    Surrogate loss: -0.0103
             Mean action noise std: 0.69
                       Mean reward: 23.24
               Mean episode length: 50.13
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 63963136
                    Iteration time: 8.17s
                        Total time: 38578.95s
                               ETA: 949621.2s

################################################################################
                    [1m Learning iteration 3904/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.315s, learning 0.166s)
               Value function loss: 441.3535
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 3.47
               Mean episode length: 50.91
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0153
--------------------------------------------------------------------------------
                   Total timesteps: 63979520
                    Iteration time: 8.48s
                        Total time: 38587.43s
                               ETA: 949576.9s

################################################################################
                    [1m Learning iteration 3905/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.154s, learning 0.175s)
               Value function loss: 77.0379
                    Surrogate loss: -0.0055
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 49.35
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0141
--------------------------------------------------------------------------------
                   Total timesteps: 63995904
                    Iteration time: 8.33s
                        Total time: 38595.76s
                               ETA: 949528.8s

################################################################################
                    [1m Learning iteration 3906/100000 [0m                    

                       Computation: 2024 steps/s (collection: 7.917s, learning 0.178s)
               Value function loss: 48.4915
                    Surrogate loss: -0.0060
             Mean action noise std: 0.69
                       Mean reward: 3.20
               Mean episode length: 50.06
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 64012288
                    Iteration time: 8.09s
                        Total time: 38603.85s
                               ETA: 949475.0s

################################################################################
                    [1m Learning iteration 3907/100000 [0m                    

                       Computation: 1939 steps/s (collection: 8.280s, learning 0.168s)
               Value function loss: 190.1758
                    Surrogate loss: -0.0026
             Mean action noise std: 0.69
                       Mean reward: 8.55
               Mean episode length: 51.14
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 64028672
                    Iteration time: 8.45s
                        Total time: 38612.30s
                               ETA: 949429.8s

################################################################################
                    [1m Learning iteration 3908/100000 [0m                    

                       Computation: 1971 steps/s (collection: 8.142s, learning 0.167s)
               Value function loss: 323.4110
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.10
               Mean episode length: 49.22
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 64045056
                    Iteration time: 8.31s
                        Total time: 38620.61s
                               ETA: 949381.4s

################################################################################
                    [1m Learning iteration 3909/100000 [0m                    

                       Computation: 1999 steps/s (collection: 8.017s, learning 0.179s)
               Value function loss: 18.4804
                    Surrogate loss: -0.0106
             Mean action noise std: 0.69
                       Mean reward: 3.63
               Mean episode length: 50.57
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 64061440
                    Iteration time: 8.20s
                        Total time: 38628.81s
                               ETA: 949330.1s

################################################################################
                    [1m Learning iteration 3910/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.342s, learning 0.192s)
               Value function loss: 3.7354
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.05
               Mean episode length: 49.98
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0150
--------------------------------------------------------------------------------
                   Total timesteps: 64077824
                    Iteration time: 8.53s
                        Total time: 38637.34s
                               ETA: 949287.1s

################################################################################
                    [1m Learning iteration 3911/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.113s, learning 0.179s)
               Value function loss: 3.7083
                    Surrogate loss: -0.0142
             Mean action noise std: 0.69
                       Mean reward: 3.25
               Mean episode length: 51.02
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 64094208
                    Iteration time: 8.29s
                        Total time: 38645.63s
                               ETA: 949238.3s

################################################################################
                    [1m Learning iteration 3912/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.223s, learning 0.165s)
               Value function loss: 329.9130
                    Surrogate loss: 0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 50.85
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 64110592
                    Iteration time: 8.39s
                        Total time: 38654.02s
                               ETA: 949191.8s

################################################################################
                    [1m Learning iteration 3913/100000 [0m                    

                       Computation: 2007 steps/s (collection: 7.989s, learning 0.172s)
               Value function loss: 888.1512
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 50.98
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 64126976
                    Iteration time: 8.16s
                        Total time: 38662.18s
                               ETA: 949139.7s

################################################################################
                    [1m Learning iteration 3914/100000 [0m                    

                       Computation: 1995 steps/s (collection: 8.047s, learning 0.163s)
               Value function loss: 673.1674
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 50.89
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 64143360
                    Iteration time: 8.21s
                        Total time: 38670.39s
                               ETA: 949088.9s

################################################################################
                    [1m Learning iteration 3915/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.540s, learning 0.160s)
               Value function loss: 898.5056
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.07
               Mean episode length: 50.48
                  Mean reward/step: 0.34
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 64159744
                    Iteration time: 8.70s
                        Total time: 38679.09s
                               ETA: 949050.1s

################################################################################
                    [1m Learning iteration 3916/100000 [0m                    

                       Computation: 1879 steps/s (collection: 8.543s, learning 0.173s)
               Value function loss: 418.3980
                    Surrogate loss: -0.0074
             Mean action noise std: 0.69
                       Mean reward: 3.93
               Mean episode length: 52.68
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 64176128
                    Iteration time: 8.72s
                        Total time: 38687.81s
                               ETA: 949011.8s

################################################################################
                    [1m Learning iteration 3917/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.261s, learning 0.173s)
               Value function loss: 319.9043
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 25.89
               Mean episode length: 51.77
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0210
--------------------------------------------------------------------------------
                   Total timesteps: 64192512
                    Iteration time: 8.43s
                        Total time: 38696.24s
                               ETA: 948966.5s

################################################################################
                    [1m Learning iteration 3918/100000 [0m                    

                       Computation: 1977 steps/s (collection: 8.111s, learning 0.175s)
               Value function loss: 59.4453
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.95
               Mean episode length: 51.08
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0212
--------------------------------------------------------------------------------
                   Total timesteps: 64208896
                    Iteration time: 8.29s
                        Total time: 38704.53s
                               ETA: 948917.6s

################################################################################
                    [1m Learning iteration 3919/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.358s, learning 0.173s)
               Value function loss: 316.3122
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.27
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0196
--------------------------------------------------------------------------------
                   Total timesteps: 64225280
                    Iteration time: 8.53s
                        Total time: 38713.06s
                               ETA: 948874.8s

################################################################################
                    [1m Learning iteration 3920/100000 [0m                    

                       Computation: 1982 steps/s (collection: 8.101s, learning 0.162s)
               Value function loss: 53.8153
                    Surrogate loss: -0.0088
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 51.04
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0191
--------------------------------------------------------------------------------
                   Total timesteps: 64241664
                    Iteration time: 8.26s
                        Total time: 38721.32s
                               ETA: 948825.4s

################################################################################
                    [1m Learning iteration 3921/100000 [0m                    

                       Computation: 1954 steps/s (collection: 8.218s, learning 0.166s)
               Value function loss: 32.8481
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 51.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0185
--------------------------------------------------------------------------------
                   Total timesteps: 64258048
                    Iteration time: 8.38s
                        Total time: 38729.70s
                               ETA: 948779.0s

################################################################################
                    [1m Learning iteration 3922/100000 [0m                    

                       Computation: 1908 steps/s (collection: 8.375s, learning 0.208s)
               Value function loss: 85.8457
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 50.45
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0171
--------------------------------------------------------------------------------
                   Total timesteps: 64274432
                    Iteration time: 8.58s
                        Total time: 38738.29s
                               ETA: 948737.5s

################################################################################
                    [1m Learning iteration 3923/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.303s, learning 0.160s)
               Value function loss: 10.3870
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 51.54
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 64290816
                    Iteration time: 8.46s
                        Total time: 38746.75s
                               ETA: 948693.0s

################################################################################
                    [1m Learning iteration 3924/100000 [0m                    

                       Computation: 2020 steps/s (collection: 7.943s, learning 0.166s)
               Value function loss: 2.8330
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 5.88
               Mean episode length: 51.52
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 64307200
                    Iteration time: 8.11s
                        Total time: 38754.86s
                               ETA: 948640.0s

################################################################################
                    [1m Learning iteration 3925/100000 [0m                    

                       Computation: 1883 steps/s (collection: 8.492s, learning 0.205s)
               Value function loss: 3.0388
                    Surrogate loss: -0.0077
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 50.49
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 64323584
                    Iteration time: 8.70s
                        Total time: 38763.56s
                               ETA: 948601.3s

################################################################################
                    [1m Learning iteration 3926/100000 [0m                    

                       Computation: 1905 steps/s (collection: 8.423s, learning 0.174s)
               Value function loss: 133.8204
                    Surrogate loss: 0.0006
             Mean action noise std: 0.69
                       Mean reward: 3.15
               Mean episode length: 50.62
                  Mean reward/step: 0.11
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 64339968
                    Iteration time: 8.60s
                        Total time: 38772.15s
                               ETA: 948560.2s

################################################################################
                    [1m Learning iteration 3927/100000 [0m                    

                       Computation: 1969 steps/s (collection: 8.102s, learning 0.216s)
               Value function loss: 6.5906
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 51.30
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 64356352
                    Iteration time: 8.32s
                        Total time: 38780.47s
                               ETA: 948512.3s

################################################################################
                    [1m Learning iteration 3928/100000 [0m                    

                       Computation: 1989 steps/s (collection: 7.959s, learning 0.277s)
               Value function loss: 109.6843
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 50.45
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 64372736
                    Iteration time: 8.24s
                        Total time: 38788.71s
                               ETA: 948462.4s

################################################################################
                    [1m Learning iteration 3929/100000 [0m                    

                       Computation: 1897 steps/s (collection: 8.407s, learning 0.225s)
               Value function loss: 477.4721
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 50.27
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 64389120
                    Iteration time: 8.63s
                        Total time: 38797.34s
                               ETA: 948422.2s

################################################################################
                    [1m Learning iteration 3930/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.297s, learning 0.172s)
               Value function loss: 246.7755
                    Surrogate loss: -0.0024
             Mean action noise std: 0.69
                       Mean reward: 2.91
               Mean episode length: 50.80
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0111
--------------------------------------------------------------------------------
                   Total timesteps: 64405504
                    Iteration time: 8.47s
                        Total time: 38805.81s
                               ETA: 948378.0s

################################################################################
                    [1m Learning iteration 3931/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.297s, learning 0.169s)
               Value function loss: 10.0634
                    Surrogate loss: -0.0045
             Mean action noise std: 0.69
                       Mean reward: 3.14
               Mean episode length: 50.87
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 64421888
                    Iteration time: 8.47s
                        Total time: 38814.27s
                               ETA: 948333.8s

################################################################################
                    [1m Learning iteration 3932/100000 [0m                    

                       Computation: 1904 steps/s (collection: 8.320s, learning 0.282s)
               Value function loss: 259.3527
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.43
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 64438272
                    Iteration time: 8.60s
                        Total time: 38822.88s
                               ETA: 948292.9s

################################################################################
                    [1m Learning iteration 3933/100000 [0m                    

                       Computation: 1921 steps/s (collection: 8.336s, learning 0.189s)
               Value function loss: 215.9314
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 3.34
               Mean episode length: 51.39
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 64454656
                    Iteration time: 8.52s
                        Total time: 38831.40s
                               ETA: 948250.2s

################################################################################
                    [1m Learning iteration 3934/100000 [0m                    

                       Computation: 1864 steps/s (collection: 8.572s, learning 0.217s)
               Value function loss: 244.5999
                    Surrogate loss: -0.0019
             Mean action noise std: 0.69
                       Mean reward: 15.83
               Mean episode length: 50.20
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 64471040
                    Iteration time: 8.79s
                        Total time: 38840.19s
                               ETA: 948213.9s

################################################################################
                    [1m Learning iteration 3935/100000 [0m                    

                       Computation: 1888 steps/s (collection: 8.471s, learning 0.204s)
               Value function loss: 8.0507
                    Surrogate loss: -0.0080
             Mean action noise std: 0.69
                       Mean reward: 27.94
               Mean episode length: 50.66
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 64487424
                    Iteration time: 8.68s
                        Total time: 38848.86s
                               ETA: 948174.8s

################################################################################
                    [1m Learning iteration 3936/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.307s, learning 0.265s)
               Value function loss: 140.7648
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 51.27
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 64503808
                    Iteration time: 8.57s
                        Total time: 38857.44s
                               ETA: 948133.3s

################################################################################
                    [1m Learning iteration 3937/100000 [0m                    

                       Computation: 1961 steps/s (collection: 8.165s, learning 0.189s)
               Value function loss: 21.8146
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.02
               Mean episode length: 50.80
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 64520192
                    Iteration time: 8.35s
                        Total time: 38865.79s
                               ETA: 948086.4s

################################################################################
                    [1m Learning iteration 3938/100000 [0m                    

                       Computation: 2006 steps/s (collection: 7.999s, learning 0.167s)
               Value function loss: 291.3112
                    Surrogate loss: -0.0003
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 49.56
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0034
Mean episode consecutive_successes: 0.0120
--------------------------------------------------------------------------------
                   Total timesteps: 64536576
                    Iteration time: 8.17s
                        Total time: 38873.96s
                               ETA: 948035.0s

################################################################################
                    [1m Learning iteration 3939/100000 [0m                    

                       Computation: 1970 steps/s (collection: 8.148s, learning 0.168s)
               Value function loss: 2.9796
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 50.19
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 64552960
                    Iteration time: 8.32s
                        Total time: 38882.27s
                               ETA: 947987.3s

################################################################################
                    [1m Learning iteration 3940/100000 [0m                    

                       Computation: 1942 steps/s (collection: 8.264s, learning 0.171s)
               Value function loss: 180.2413
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 15.73
               Mean episode length: 50.88
                  Mean reward/step: 0.14
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 64569344
                    Iteration time: 8.44s
                        Total time: 38890.71s
                               ETA: 947942.5s

################################################################################
                    [1m Learning iteration 3941/100000 [0m                    

                       Computation: 1957 steps/s (collection: 8.203s, learning 0.169s)
               Value function loss: 244.0687
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 50.98
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 64585728
                    Iteration time: 8.37s
                        Total time: 38899.08s
                               ETA: 947896.1s

################################################################################
                    [1m Learning iteration 3942/100000 [0m                    

                       Computation: 2003 steps/s (collection: 8.008s, learning 0.171s)
               Value function loss: 5.3965
                    Surrogate loss: -0.0059
             Mean action noise std: 0.69
                       Mean reward: 2.92
               Mean episode length: 49.94
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 64602112
                    Iteration time: 8.18s
                        Total time: 38907.26s
                               ETA: 947845.1s

################################################################################
                    [1m Learning iteration 3943/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.058s, learning 0.157s)
               Value function loss: 128.9087
                    Surrogate loss: -0.0007
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 49.64
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 64618496
                    Iteration time: 8.21s
                        Total time: 38915.47s
                               ETA: 947795.0s

################################################################################
                    [1m Learning iteration 3944/100000 [0m                    

                       Computation: 1887 steps/s (collection: 8.508s, learning 0.173s)
               Value function loss: 178.1902
                    Surrogate loss: -0.0031
             Mean action noise std: 0.69
                       Mean reward: 2.79
               Mean episode length: 50.26
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 64634880
                    Iteration time: 8.68s
                        Total time: 38924.15s
                               ETA: 947756.3s

################################################################################
                    [1m Learning iteration 3945/100000 [0m                    

                       Computation: 1910 steps/s (collection: 8.416s, learning 0.161s)
               Value function loss: 3.7912
                    Surrogate loss: -0.0184
             Mean action noise std: 0.69
                       Mean reward: 3.19
               Mean episode length: 50.81
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 64651264
                    Iteration time: 8.58s
                        Total time: 38932.73s
                               ETA: 947715.0s

################################################################################
                    [1m Learning iteration 3946/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.374s, learning 0.160s)
               Value function loss: 129.4522
                    Surrogate loss: 0.0016
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 50.00
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 64667648
                    Iteration time: 8.53s
                        Total time: 38941.27s
                               ETA: 947672.7s

################################################################################
                    [1m Learning iteration 3947/100000 [0m                    

                       Computation: 1951 steps/s (collection: 8.227s, learning 0.169s)
               Value function loss: 456.2692
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.07
                  Mean reward/step: 0.20
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0115
--------------------------------------------------------------------------------
                   Total timesteps: 64684032
                    Iteration time: 8.40s
                        Total time: 38949.66s
                               ETA: 947627.1s

################################################################################
                    [1m Learning iteration 3948/100000 [0m                    

                       Computation: 1920 steps/s (collection: 8.359s, learning 0.171s)
               Value function loss: 10.2897
                    Surrogate loss: -0.0150
             Mean action noise std: 0.69
                       Mean reward: 2.57
               Mean episode length: 49.49
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0106
--------------------------------------------------------------------------------
                   Total timesteps: 64700416
                    Iteration time: 8.53s
                        Total time: 38958.19s
                               ETA: 947584.8s

################################################################################
                    [1m Learning iteration 3949/100000 [0m                    

                       Computation: 1914 steps/s (collection: 8.399s, learning 0.161s)
               Value function loss: 4.4062
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 3.21
               Mean episode length: 50.50
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 64716800
                    Iteration time: 8.56s
                        Total time: 38966.75s
                               ETA: 947543.2s

################################################################################
                    [1m Learning iteration 3950/100000 [0m                    

                       Computation: 1979 steps/s (collection: 8.108s, learning 0.170s)
               Value function loss: 19.1788
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 50.38
                  Mean reward/step: 0.08
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0005
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 64733184
                    Iteration time: 8.28s
                        Total time: 38975.03s
                               ETA: 947494.7s

################################################################################
                    [1m Learning iteration 3951/100000 [0m                    

                       Computation: 1891 steps/s (collection: 8.442s, learning 0.221s)
               Value function loss: 620.0012
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 49.70
                  Mean reward/step: 0.24
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 64749568
                    Iteration time: 8.66s
                        Total time: 38983.69s
                               ETA: 947455.6s

################################################################################
                    [1m Learning iteration 3952/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.271s, learning 0.169s)
               Value function loss: 5.5079
                    Surrogate loss: -0.0149
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 50.43
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0103
--------------------------------------------------------------------------------
                   Total timesteps: 64765952
                    Iteration time: 8.44s
                        Total time: 38992.13s
                               ETA: 947411.2s

################################################################################
                    [1m Learning iteration 3953/100000 [0m                    

                       Computation: 1935 steps/s (collection: 8.303s, learning 0.164s)
               Value function loss: 162.5555
                    Surrogate loss: -0.0010
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 49.48
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0126
--------------------------------------------------------------------------------
                   Total timesteps: 64782336
                    Iteration time: 8.47s
                        Total time: 39000.60s
                               ETA: 947367.4s

################################################################################
                    [1m Learning iteration 3954/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.296s, learning 0.187s)
               Value function loss: 572.9099
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 3.38
               Mean episode length: 50.81
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0116
--------------------------------------------------------------------------------
                   Total timesteps: 64798720
                    Iteration time: 8.48s
                        Total time: 39009.08s
                               ETA: 947324.0s

################################################################################
                    [1m Learning iteration 3955/100000 [0m                    

                       Computation: 1902 steps/s (collection: 8.448s, learning 0.165s)
               Value function loss: 21.7651
                    Surrogate loss: -0.0096
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 49.99
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 64815104
                    Iteration time: 8.61s
                        Total time: 39017.69s
                               ETA: 947283.7s

################################################################################
                    [1m Learning iteration 3956/100000 [0m                    

                       Computation: 1992 steps/s (collection: 8.047s, learning 0.174s)
               Value function loss: 7.5006
                    Surrogate loss: -0.0050
             Mean action noise std: 0.69
                       Mean reward: 3.17
               Mean episode length: 51.00
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0107
--------------------------------------------------------------------------------
                   Total timesteps: 64831488
                    Iteration time: 8.22s
                        Total time: 39025.92s
                               ETA: 947234.0s

################################################################################
                    [1m Learning iteration 3957/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.205s, learning 0.162s)
               Value function loss: 156.3054
                    Surrogate loss: 0.0007
             Mean action noise std: 0.69
                       Mean reward: 10.50
               Mean episode length: 50.54
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.85
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 64847872
                    Iteration time: 8.37s
                        Total time: 39034.28s
                               ETA: 947187.9s

################################################################################
                    [1m Learning iteration 3958/100000 [0m                    

                       Computation: 1962 steps/s (collection: 8.187s, learning 0.163s)
               Value function loss: 6.5271
                    Surrogate loss: -0.0160
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 51.20
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0122
--------------------------------------------------------------------------------
                   Total timesteps: 64864256
                    Iteration time: 8.35s
                        Total time: 39042.63s
                               ETA: 947141.3s

################################################################################
                    [1m Learning iteration 3959/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.165s)
               Value function loss: 3.8370
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 2.56
               Mean episode length: 49.37
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0112
--------------------------------------------------------------------------------
                   Total timesteps: 64880640
                    Iteration time: 8.42s
                        Total time: 39051.06s
                               ETA: 947096.6s

################################################################################
                    [1m Learning iteration 3960/100000 [0m                    

                       Computation: 1991 steps/s (collection: 8.056s, learning 0.170s)
               Value function loss: 4.1335
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 50.71
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0119
--------------------------------------------------------------------------------
                   Total timesteps: 64897024
                    Iteration time: 8.23s
                        Total time: 39059.28s
                               ETA: 947047.1s

################################################################################
                    [1m Learning iteration 3961/100000 [0m                    

                       Computation: 1944 steps/s (collection: 8.261s, learning 0.164s)
               Value function loss: 2.7048
                    Surrogate loss: -0.0130
             Mean action noise std: 0.69
                       Mean reward: 3.31
               Mean episode length: 50.70
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0109
--------------------------------------------------------------------------------
                   Total timesteps: 64913408
                    Iteration time: 8.42s
                        Total time: 39067.71s
                               ETA: 947002.4s

################################################################################
                    [1m Learning iteration 3962/100000 [0m                    

                       Computation: 2037 steps/s (collection: 7.874s, learning 0.165s)
               Value function loss: 1.4810
                    Surrogate loss: -0.0192
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 50.29
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0101
--------------------------------------------------------------------------------
                   Total timesteps: 64929792
                    Iteration time: 8.04s
                        Total time: 39075.75s
                               ETA: 946948.4s

################################################################################
                    [1m Learning iteration 3963/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.281s, learning 0.163s)
               Value function loss: 0.9181
                    Surrogate loss: -0.0235
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 51.34
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0093
--------------------------------------------------------------------------------
                   Total timesteps: 64946176
                    Iteration time: 8.44s
                        Total time: 39084.19s
                               ETA: 946904.2s

################################################################################
                    [1m Learning iteration 3964/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.292s, learning 0.164s)
               Value function loss: 1.0725
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 3.55
               Mean episode length: 52.96
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0086
--------------------------------------------------------------------------------
                   Total timesteps: 64962560
                    Iteration time: 8.46s
                        Total time: 39092.65s
                               ETA: 946860.4s

################################################################################
                    [1m Learning iteration 3965/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.159s, learning 0.171s)
               Value function loss: 1.4380
                    Surrogate loss: -0.0187
             Mean action noise std: 0.69
                       Mean reward: 2.58
               Mean episode length: 50.27
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0079
--------------------------------------------------------------------------------
                   Total timesteps: 64978944
                    Iteration time: 8.33s
                        Total time: 39100.98s
                               ETA: 946813.5s

################################################################################
                    [1m Learning iteration 3966/100000 [0m                    

                       Computation: 1968 steps/s (collection: 8.161s, learning 0.162s)
               Value function loss: 109.0213
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 49.59
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.08
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0073
--------------------------------------------------------------------------------
                   Total timesteps: 64995328
                    Iteration time: 8.32s
                        Total time: 39109.30s
                               ETA: 946766.5s

################################################################################
                    [1m Learning iteration 3967/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.243s, learning 0.165s)
               Value function loss: 239.9310
                    Surrogate loss: -0.0018
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 49.69
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 65011712
                    Iteration time: 8.41s
                        Total time: 39117.71s
                               ETA: 946721.5s

################################################################################
                    [1m Learning iteration 3968/100000 [0m                    

                       Computation: 1973 steps/s (collection: 8.135s, learning 0.168s)
               Value function loss: 205.6997
                    Surrogate loss: -0.0027
             Mean action noise std: 0.69
                       Mean reward: 2.61
               Mean episode length: 49.93
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0062
--------------------------------------------------------------------------------
                   Total timesteps: 65028096
                    Iteration time: 8.30s
                        Total time: 39126.01s
                               ETA: 946674.0s

################################################################################
                    [1m Learning iteration 3969/100000 [0m                    

                       Computation: 1984 steps/s (collection: 8.085s, learning 0.173s)
               Value function loss: 71.8702
                    Surrogate loss: -0.0043
             Mean action noise std: 0.69
                       Mean reward: 2.81
               Mean episode length: 50.91
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0058
--------------------------------------------------------------------------------
                   Total timesteps: 65044480
                    Iteration time: 8.26s
                        Total time: 39134.27s
                               ETA: 946625.4s

################################################################################
                    [1m Learning iteration 3970/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.245s, learning 0.166s)
               Value function loss: 128.2965
                    Surrogate loss: -0.0037
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 51.40
                  Mean reward/step: 0.10
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0074
--------------------------------------------------------------------------------
                   Total timesteps: 65060864
                    Iteration time: 8.41s
                        Total time: 39142.68s
                               ETA: 946580.6s

################################################################################
                    [1m Learning iteration 3971/100000 [0m                    

                       Computation: 1966 steps/s (collection: 8.172s, learning 0.160s)
               Value function loss: 840.5099
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 50.81
                  Mean reward/step: 0.36
       Mean episode length/episode: 6.93
            Mean episode successes: 0.0151
Mean episode consecutive_successes: 0.0068
--------------------------------------------------------------------------------
                   Total timesteps: 65077248
                    Iteration time: 8.33s
                        Total time: 39151.01s
                               ETA: 946533.8s

################################################################################
                    [1m Learning iteration 3972/100000 [0m                    

                       Computation: 1934 steps/s (collection: 8.305s, learning 0.164s)
               Value function loss: 909.7350
                    Surrogate loss: -0.0035
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 50.41
                  Mean reward/step: 0.31
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0085
--------------------------------------------------------------------------------
                   Total timesteps: 65093632
                    Iteration time: 8.47s
                        Total time: 39159.48s
                               ETA: 946490.4s

################################################################################
                    [1m Learning iteration 3973/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.183s, learning 0.174s)
               Value function loss: 264.5593
                    Surrogate loss: -0.0073
             Mean action noise std: 0.69
                       Mean reward: 12.96
               Mean episode length: 50.32
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0176
Mean episode consecutive_successes: 0.0102
--------------------------------------------------------------------------------
                   Total timesteps: 65110016
                    Iteration time: 8.36s
                        Total time: 39167.84s
                               ETA: 946444.3s

################################################################################
                    [1m Learning iteration 3974/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.161s, learning 0.167s)
               Value function loss: 21.9490
                    Surrogate loss: -0.0100
             Mean action noise std: 0.69
                       Mean reward: 2.79
               Mean episode length: 49.85
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0122
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 65126400
                    Iteration time: 8.33s
                        Total time: 39176.16s
                               ETA: 946397.6s

################################################################################
                    [1m Learning iteration 3975/100000 [0m                    

                       Computation: 1948 steps/s (collection: 8.249s, learning 0.161s)
               Value function loss: 164.1312
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 30.35
               Mean episode length: 51.14
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 65142784
                    Iteration time: 8.41s
                        Total time: 39184.57s
                               ETA: 946352.8s

################################################################################
                    [1m Learning iteration 3976/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.321s, learning 0.179s)
               Value function loss: 396.8602
                    Surrogate loss: -0.0021
             Mean action noise std: 0.69
                       Mean reward: 3.26
               Mean episode length: 51.15
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 65159168
                    Iteration time: 8.50s
                        Total time: 39193.07s
                               ETA: 946310.2s

################################################################################
                    [1m Learning iteration 3977/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.125s, learning 0.163s)
               Value function loss: 132.2407
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 2.93
               Mean episode length: 51.44
                  Mean reward/step: 0.09
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 65175552
                    Iteration time: 8.29s
                        Total time: 39201.36s
                               ETA: 946262.6s

################################################################################
                    [1m Learning iteration 3978/100000 [0m                    

                       Computation: 1947 steps/s (collection: 8.247s, learning 0.164s)
               Value function loss: 693.6306
                    Surrogate loss: -0.0025
             Mean action noise std: 0.69
                       Mean reward: 33.12
               Mean episode length: 50.84
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 65191936
                    Iteration time: 8.41s
                        Total time: 39209.77s
                               ETA: 946217.9s

################################################################################
                    [1m Learning iteration 3979/100000 [0m                    

                       Computation: 1956 steps/s (collection: 8.204s, learning 0.172s)
               Value function loss: 181.5487
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 3.08
               Mean episode length: 50.40
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0098
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 65208320
                    Iteration time: 8.38s
                        Total time: 39218.15s
                               ETA: 946172.4s

################################################################################
                    [1m Learning iteration 3980/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.325s, learning 0.174s)
               Value function loss: 98.2886
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 2.71
               Mean episode length: 49.70
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 65224704
                    Iteration time: 8.50s
                        Total time: 39226.65s
                               ETA: 946129.9s

################################################################################
                    [1m Learning iteration 3981/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.311s, learning 0.163s)
               Value function loss: 572.5511
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.45
               Mean episode length: 49.61
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 65241088
                    Iteration time: 8.47s
                        Total time: 39235.12s
                               ETA: 946086.7s

################################################################################
                    [1m Learning iteration 3982/100000 [0m                    

                       Computation: 1958 steps/s (collection: 8.198s, learning 0.166s)
               Value function loss: 229.4539
                    Surrogate loss: -0.0091
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 49.64
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0160
--------------------------------------------------------------------------------
                   Total timesteps: 65257472
                    Iteration time: 8.36s
                        Total time: 39243.49s
                               ETA: 946041.0s

################################################################################
                    [1m Learning iteration 3983/100000 [0m                    

                       Computation: 1945 steps/s (collection: 8.258s, learning 0.161s)
               Value function loss: 77.1977
                    Surrogate loss: -0.0094
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 51.16
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 65273856
                    Iteration time: 8.42s
                        Total time: 39251.91s
                               ETA: 945996.6s

################################################################################
                    [1m Learning iteration 3984/100000 [0m                    

                       Computation: 1978 steps/s (collection: 8.121s, learning 0.162s)
               Value function loss: 263.2096
                    Surrogate loss: -0.0013
             Mean action noise std: 0.69
                       Mean reward: 3.12
               Mean episode length: 51.76
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0166
--------------------------------------------------------------------------------
                   Total timesteps: 65290240
                    Iteration time: 8.28s
                        Total time: 39260.19s
                               ETA: 945948.9s

################################################################################
                    [1m Learning iteration 3985/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.278s, learning 0.184s)
               Value function loss: 197.9494
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 3.48
               Mean episode length: 51.25
                  Mean reward/step: 0.17
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0174
--------------------------------------------------------------------------------
                   Total timesteps: 65306624
                    Iteration time: 8.46s
                        Total time: 39268.65s
                               ETA: 945905.6s

################################################################################
                    [1m Learning iteration 3986/100000 [0m                    

                       Computation: 1981 steps/s (collection: 8.105s, learning 0.165s)
               Value function loss: 80.9665
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 2.38
               Mean episode length: 48.88
                  Mean reward/step: 0.06
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 65323008
                    Iteration time: 8.27s
                        Total time: 39276.92s
                               ETA: 945857.6s

################################################################################
                    [1m Learning iteration 3987/100000 [0m                    

                       Computation: 1994 steps/s (collection: 8.049s, learning 0.168s)
               Value function loss: 567.6506
                    Surrogate loss: 0.0025
             Mean action noise std: 0.69
                       Mean reward: 12.91
               Mean episode length: 50.89
                  Mean reward/step: 0.21
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0179
--------------------------------------------------------------------------------
                   Total timesteps: 65339392
                    Iteration time: 8.22s
                        Total time: 39285.14s
                               ETA: 945808.4s

################################################################################
                    [1m Learning iteration 3988/100000 [0m                    

                       Computation: 1933 steps/s (collection: 8.306s, learning 0.167s)
               Value function loss: 87.3100
                    Surrogate loss: -0.0091
             Mean action noise std: 0.69
                       Mean reward: 2.78
               Mean episode length: 49.91
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0165
--------------------------------------------------------------------------------
                   Total timesteps: 65355776
                    Iteration time: 8.47s
                        Total time: 39293.61s
                               ETA: 945765.4s

################################################################################
                    [1m Learning iteration 3989/100000 [0m                    

                       Computation: 1911 steps/s (collection: 8.404s, learning 0.166s)
               Value function loss: 353.9727
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 25.73
               Mean episode length: 50.57
                  Mean reward/step: 0.16
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 65372160
                    Iteration time: 8.57s
                        Total time: 39302.18s
                               ETA: 945724.7s

################################################################################
                    [1m Learning iteration 3990/100000 [0m                    

                       Computation: 2032 steps/s (collection: 7.893s, learning 0.167s)
               Value function loss: 23.9839
                    Surrogate loss: -0.0139
             Mean action noise std: 0.69
                       Mean reward: 2.77
               Mean episode length: 50.51
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0173
--------------------------------------------------------------------------------
                   Total timesteps: 65388544
                    Iteration time: 8.06s
                        Total time: 39310.24s
                               ETA: 945671.8s

################################################################################
                    [1m Learning iteration 3991/100000 [0m                    

                       Computation: 1931 steps/s (collection: 8.266s, learning 0.216s)
               Value function loss: 536.4682
                    Surrogate loss: 0.0004
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 50.02
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0177
--------------------------------------------------------------------------------
                   Total timesteps: 65404928
                    Iteration time: 8.48s
                        Total time: 39318.72s
                               ETA: 945629.1s

################################################################################
                    [1m Learning iteration 3992/100000 [0m                    

                       Computation: 1916 steps/s (collection: 8.340s, learning 0.208s)
               Value function loss: 17.8568
                    Surrogate loss: -0.0154
             Mean action noise std: 0.69
                       Mean reward: 20.39
               Mean episode length: 52.03
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 65421312
                    Iteration time: 8.55s
                        Total time: 39327.27s
                               ETA: 945588.0s

################################################################################
                    [1m Learning iteration 3993/100000 [0m                    

                       Computation: 1976 steps/s (collection: 8.124s, learning 0.166s)
               Value function loss: 318.4410
                    Surrogate loss: 0.0001
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 49.78
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 65437696
                    Iteration time: 8.29s
                        Total time: 39335.56s
                               ETA: 945540.6s

################################################################################
                    [1m Learning iteration 3994/100000 [0m                    

                       Computation: 1930 steps/s (collection: 8.319s, learning 0.168s)
               Value function loss: 67.9084
                    Surrogate loss: -0.0068
             Mean action noise std: 0.69
                       Mean reward: 2.91
               Mean episode length: 50.42
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 65454080
                    Iteration time: 8.49s
                        Total time: 39344.05s
                               ETA: 945498.1s

################################################################################
                    [1m Learning iteration 3995/100000 [0m                    

                       Computation: 1936 steps/s (collection: 8.246s, learning 0.214s)
               Value function loss: 599.6736
                    Surrogate loss: -0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 51.51
                  Mean reward/step: 0.27
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0168
--------------------------------------------------------------------------------
                   Total timesteps: 65470464
                    Iteration time: 8.46s
                        Total time: 39352.51s
                               ETA: 945454.9s

################################################################################
                    [1m Learning iteration 3996/100000 [0m                    

                       Computation: 1918 steps/s (collection: 8.376s, learning 0.165s)
               Value function loss: 106.9987
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 2.88
               Mean episode length: 50.68
                  Mean reward/step: 0.13
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 65486848
                    Iteration time: 8.54s
                        Total time: 39361.05s
                               ETA: 945413.6s

################################################################################
                    [1m Learning iteration 3997/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.216s, learning 0.203s)
               Value function loss: 15.7012
                    Surrogate loss: -0.0112
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 50.55
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0158
--------------------------------------------------------------------------------
                   Total timesteps: 65503232
                    Iteration time: 8.42s
                        Total time: 39369.47s
                               ETA: 945369.5s

################################################################################
                    [1m Learning iteration 3998/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.258s, learning 0.172s)
               Value function loss: 8.2809
                    Surrogate loss: -0.0058
             Mean action noise std: 0.69
                       Mean reward: 3.00
               Mean episode length: 50.38
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0175
--------------------------------------------------------------------------------
                   Total timesteps: 65519616
                    Iteration time: 8.43s
                        Total time: 39377.90s
                               ETA: 945325.6s

################################################################################
                    [1m Learning iteration 3999/100000 [0m                    

                       Computation: 1946 steps/s (collection: 8.205s, learning 0.213s)
               Value function loss: 7.4766
                    Surrogate loss: -0.0057
             Mean action noise std: 0.69
                       Mean reward: 2.72
               Mean episode length: 51.54
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0161
--------------------------------------------------------------------------------
                   Total timesteps: 65536000
                    Iteration time: 8.42s
                        Total time: 39386.32s
                               ETA: 945281.5s

################################################################################
                    [1m Learning iteration 4000/100000 [0m                    

                       Computation: 1983 steps/s (collection: 8.094s, learning 0.168s)
               Value function loss: 5.2472
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 2.82
               Mean episode length: 50.85
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 65552384
                    Iteration time: 8.26s
                        Total time: 39394.58s
                               ETA: 945233.6s

################################################################################
                    [1m Learning iteration 4001/100000 [0m                    

                       Computation: 1972 steps/s (collection: 8.135s, learning 0.172s)
               Value function loss: 5.0480
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 51.43
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0137
--------------------------------------------------------------------------------
                   Total timesteps: 65568768
                    Iteration time: 8.31s
                        Total time: 39402.89s
                               ETA: 945186.8s

################################################################################
                    [1m Learning iteration 4002/100000 [0m                    

                       Computation: 1217 steps/s (collection: 13.292s, learning 0.170s)
               Value function loss: 4.9593
                    Surrogate loss: -0.0095
             Mean action noise std: 0.69
                       Mean reward: 3.16
               Mean episode length: 51.71
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 65585152
                    Iteration time: 13.46s
                        Total time: 39416.35s
                               ETA: 945263.7s

################################################################################
                    [1m Learning iteration 4003/100000 [0m                    

                       Computation: 1033 steps/s (collection: 15.681s, learning 0.167s)
               Value function loss: 300.3181
                    Surrogate loss: 0.0014
             Mean action noise std: 0.69
                       Mean reward: 2.85
               Mean episode length: 50.87
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0117
--------------------------------------------------------------------------------
                   Total timesteps: 65601536
                    Iteration time: 15.85s
                        Total time: 39432.20s
                               ETA: 945397.7s

################################################################################
                    [1m Learning iteration 4004/100000 [0m                    

                       Computation: 1009 steps/s (collection: 16.009s, learning 0.214s)
               Value function loss: 303.4020
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 2.86
               Mean episode length: 51.92
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0088
Mean episode consecutive_successes: 0.0108
--------------------------------------------------------------------------------
                   Total timesteps: 65617920
                    Iteration time: 16.22s
                        Total time: 39448.42s
                               ETA: 945540.7s

################################################################################
                    [1m Learning iteration 4005/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.180s, learning 0.172s)
               Value function loss: 403.6147
                    Surrogate loss: -0.0036
             Mean action noise std: 0.69
                       Mean reward: 2.41
               Mean episode length: 50.17
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0100
--------------------------------------------------------------------------------
                   Total timesteps: 65634304
                    Iteration time: 16.35s
                        Total time: 39464.77s
                               ETA: 945686.6s

################################################################################
                    [1m Learning iteration 4006/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.264s, learning 0.170s)
               Value function loss: 73.0626
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 28.11
               Mean episode length: 51.06
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0155
--------------------------------------------------------------------------------
                   Total timesteps: 65650688
                    Iteration time: 16.43s
                        Total time: 39481.20s
                               ETA: 945834.5s

################################################################################
                    [1m Learning iteration 4007/100000 [0m                    

                       Computation: 994 steps/s (collection: 16.306s, learning 0.163s)
               Value function loss: 285.7094
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.91
               Mean episode length: 51.22
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 65667072
                    Iteration time: 16.47s
                        Total time: 39497.67s
                               ETA: 945983.1s

################################################################################
                    [1m Learning iteration 4008/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.169s, learning 0.202s)
               Value function loss: 133.9175
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 3.27
               Mean episode length: 52.09
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0134
--------------------------------------------------------------------------------
                   Total timesteps: 65683456
                    Iteration time: 16.37s
                        Total time: 39514.04s
                               ETA: 946129.3s

################################################################################
                    [1m Learning iteration 4009/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.388s, learning 0.160s)
               Value function loss: 134.7866
                    Surrogate loss: -0.0067
             Mean action noise std: 0.69
                       Mean reward: 3.09
               Mean episode length: 53.46
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0129
--------------------------------------------------------------------------------
                   Total timesteps: 65699840
                    Iteration time: 16.55s
                        Total time: 39530.59s
                               ETA: 946279.6s

################################################################################
                    [1m Learning iteration 4010/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.911s, learning 0.164s)
               Value function loss: 96.3881
                    Surrogate loss: -0.0015
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 50.72
                  Mean reward/step: 0.08
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0010
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 65716224
                    Iteration time: 16.08s
                        Total time: 39546.67s
                               ETA: 946418.5s

################################################################################
                    [1m Learning iteration 4011/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.241s, learning 0.170s)
               Value function loss: 406.2207
                    Surrogate loss: -0.0030
             Mean action noise std: 0.69
                       Mean reward: 2.88
               Mean episode length: 50.51
                  Mean reward/step: 0.19
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 65732608
                    Iteration time: 16.41s
                        Total time: 39563.08s
                               ETA: 946565.4s

################################################################################
                    [1m Learning iteration 4012/100000 [0m                    

                       Computation: 999 steps/s (collection: 16.231s, learning 0.159s)
               Value function loss: 448.8489
                    Surrogate loss: -0.0053
             Mean action noise std: 0.69
                       Mean reward: 2.92
               Mean episode length: 52.03
                  Mean reward/step: 0.25
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0117
Mean episode consecutive_successes: 0.0123
--------------------------------------------------------------------------------
                   Total timesteps: 65748992
                    Iteration time: 16.39s
                        Total time: 39579.47s
                               ETA: 946711.7s

################################################################################
                    [1m Learning iteration 4013/100000 [0m                    

                       Computation: 1020 steps/s (collection: 15.840s, learning 0.222s)
               Value function loss: 298.4431
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 40.60
               Mean episode length: 52.82
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 65765376
                    Iteration time: 16.06s
                        Total time: 39595.53s
                               ETA: 946850.1s

################################################################################
                    [1m Learning iteration 4014/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.257s, learning 0.182s)
               Value function loss: 514.2293
                    Surrogate loss: -0.0060
             Mean action noise std: 0.69
                       Mean reward: 2.88
               Mean episode length: 53.01
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0145
--------------------------------------------------------------------------------
                   Total timesteps: 65781760
                    Iteration time: 16.44s
                        Total time: 39611.97s
                               ETA: 946997.4s

################################################################################
                    [1m Learning iteration 4015/100000 [0m                    

                       Computation: 1008 steps/s (collection: 16.076s, learning 0.165s)
               Value function loss: 319.8058
                    Surrogate loss: -0.0061
             Mean action noise std: 0.69
                       Mean reward: 2.31
               Mean episode length: 52.10
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0149
--------------------------------------------------------------------------------
                   Total timesteps: 65798144
                    Iteration time: 16.24s
                        Total time: 39628.21s
                               ETA: 947139.9s

################################################################################
                    [1m Learning iteration 4016/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.310s, learning 0.178s)
               Value function loss: 262.0785
                    Surrogate loss: -0.0102
             Mean action noise std: 0.69
                       Mean reward: 30.20
               Mean episode length: 51.08
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0163
--------------------------------------------------------------------------------
                   Total timesteps: 65814528
                    Iteration time: 16.49s
                        Total time: 39644.70s
                               ETA: 947288.2s

################################################################################
                    [1m Learning iteration 4017/100000 [0m                    

                       Computation: 1006 steps/s (collection: 16.117s, learning 0.167s)
               Value function loss: 327.2330
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 20.07
               Mean episode length: 52.11
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0172
--------------------------------------------------------------------------------
                   Total timesteps: 65830912
                    Iteration time: 16.28s
                        Total time: 39660.98s
                               ETA: 947431.6s

################################################################################
                    [1m Learning iteration 4018/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.239s, learning 0.163s)
               Value function loss: 427.1552
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 22.83
               Mean episode length: 50.47
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0178
--------------------------------------------------------------------------------
                   Total timesteps: 65847296
                    Iteration time: 16.40s
                        Total time: 39677.39s
                               ETA: 947577.7s

################################################################################
                    [1m Learning iteration 4019/100000 [0m                    

                       Computation: 1025 steps/s (collection: 15.803s, learning 0.177s)
               Value function loss: 58.4234
                    Surrogate loss: -0.0093
             Mean action noise std: 0.69
                       Mean reward: 2.55
               Mean episode length: 50.41
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0164
--------------------------------------------------------------------------------
                   Total timesteps: 65863680
                    Iteration time: 15.98s
                        Total time: 39693.37s
                               ETA: 947713.7s

################################################################################
                    [1m Learning iteration 4020/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.286s, learning 0.297s)
               Value function loss: 26.2427
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 3.04
               Mean episode length: 53.25
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0049
Mean episode consecutive_successes: 0.0152
--------------------------------------------------------------------------------
                   Total timesteps: 65880064
                    Iteration time: 16.58s
                        Total time: 39709.95s
                               ETA: 947863.9s

################################################################################
                    [1m Learning iteration 4021/100000 [0m                    

                       Computation: 1000 steps/s (collection: 16.209s, learning 0.166s)
               Value function loss: 11.7330
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 2.44
               Mean episode length: 49.95
                  Mean reward/step: 0.05
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0167
--------------------------------------------------------------------------------
                   Total timesteps: 65896448
                    Iteration time: 16.38s
                        Total time: 39726.32s
                               ETA: 948009.2s

################################################################################
                    [1m Learning iteration 4022/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.439s, learning 0.168s)
               Value function loss: 13.6786
                    Surrogate loss: -0.0067
             Mean action noise std: 0.69
                       Mean reward: 2.66
               Mean episode length: 51.00
                  Mean reward/step: 0.05
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 65912832
                    Iteration time: 16.61s
                        Total time: 39742.93s
                               ETA: 948159.8s

################################################################################
                    [1m Learning iteration 4023/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.277s, learning 0.168s)
               Value function loss: 104.9550
                    Surrogate loss: 0.0002
             Mean action noise std: 0.69
                       Mean reward: 2.65
               Mean episode length: 52.54
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0015
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 65929216
                    Iteration time: 16.45s
                        Total time: 39759.38s
                               ETA: 948306.6s

################################################################################
                    [1m Learning iteration 4024/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.201s, learning 0.164s)
               Value function loss: 144.0300
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 2.72
               Mean episode length: 51.64
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0131
--------------------------------------------------------------------------------
                   Total timesteps: 65945600
                    Iteration time: 16.37s
                        Total time: 39775.74s
                               ETA: 948451.3s

################################################################################
                    [1m Learning iteration 4025/100000 [0m                    

                       Computation: 985 steps/s (collection: 16.410s, learning 0.213s)
               Value function loss: 99.1668
                    Surrogate loss: -0.0046
             Mean action noise std: 0.69
                       Mean reward: 2.47
               Mean episode length: 50.51
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0121
--------------------------------------------------------------------------------
                   Total timesteps: 65961984
                    Iteration time: 16.62s
                        Total time: 39792.37s
                               ETA: 948602.2s

################################################################################
                    [1m Learning iteration 4026/100000 [0m                    

                       Computation: 993 steps/s (collection: 16.319s, learning 0.169s)
               Value function loss: 47.2544
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 20.71
               Mean episode length: 51.52
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0143
--------------------------------------------------------------------------------
                   Total timesteps: 65978368
                    Iteration time: 16.49s
                        Total time: 39808.85s
                               ETA: 948749.7s

################################################################################
                    [1m Learning iteration 4027/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.419s, learning 0.187s)
               Value function loss: 34.0495
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 2.68
               Mean episode length: 49.92
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0024
Mean episode consecutive_successes: 0.0132
--------------------------------------------------------------------------------
                   Total timesteps: 65994752
                    Iteration time: 16.61s
                        Total time: 39825.46s
                               ETA: 948899.9s

################################################################################
                    [1m Learning iteration 4028/100000 [0m                    

                       Computation: 973 steps/s (collection: 16.622s, learning 0.200s)
               Value function loss: 289.4531
                    Surrogate loss: -0.0012
             Mean action noise std: 0.69
                       Mean reward: 2.70
               Mean episode length: 50.83
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0136
--------------------------------------------------------------------------------
                   Total timesteps: 66011136
                    Iteration time: 16.82s
                        Total time: 39842.28s
                               ETA: 949055.2s

################################################################################
                    [1m Learning iteration 4029/100000 [0m                    

                       Computation: 1004 steps/s (collection: 16.090s, learning 0.219s)
               Value function loss: 191.0616
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 3.11
               Mean episode length: 51.67
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0068
Mean episode consecutive_successes: 0.0125
--------------------------------------------------------------------------------
                   Total timesteps: 66027520
                    Iteration time: 16.31s
                        Total time: 39858.59s
                               ETA: 949198.2s

################################################################################
                    [1m Learning iteration 4030/100000 [0m                    

                       Computation: 998 steps/s (collection: 16.234s, learning 0.168s)
               Value function loss: 557.0643
                    Surrogate loss: -0.0038
             Mean action noise std: 0.69
                       Mean reward: 30.39
               Mean episode length: 50.13
                  Mean reward/step: 0.24
       Mean episode length/episode: 7.06
            Mean episode successes: 0.0127
Mean episode consecutive_successes: 0.0146
--------------------------------------------------------------------------------
                   Total timesteps: 66043904
                    Iteration time: 16.40s
                        Total time: 39874.99s
                               ETA: 949343.3s

################################################################################
                    [1m Learning iteration 4031/100000 [0m                    

                       Computation: 1026 steps/s (collection: 15.797s, learning 0.162s)
               Value function loss: 197.6885
                    Surrogate loss: -0.0097
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 51.86
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0135
--------------------------------------------------------------------------------
                   Total timesteps: 66060288
                    Iteration time: 15.96s
                        Total time: 39890.95s
                               ETA: 949477.9s

################################################################################
                    [1m Learning iteration 4032/100000 [0m                    

                       Computation: 990 steps/s (collection: 16.370s, learning 0.179s)
               Value function loss: 265.7803
                    Surrogate loss: -0.0076
             Mean action noise std: 0.69
                       Mean reward: 2.83
               Mean episode length: 51.24
                  Mean reward/step: 0.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0020
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 66076672
                    Iteration time: 16.55s
                        Total time: 39907.50s
                               ETA: 949626.3s

################################################################################
                    [1m Learning iteration 4033/100000 [0m                    

                       Computation: 988 steps/s (collection: 16.380s, learning 0.197s)
               Value function loss: 294.9713
                    Surrogate loss: 0.0021
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 50.98
                  Mean reward/step: 0.18
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0139
--------------------------------------------------------------------------------
                   Total timesteps: 66093056
                    Iteration time: 16.58s
                        Total time: 39924.08s
                               ETA: 949775.4s

################################################################################
                    [1m Learning iteration 4034/100000 [0m                    

                       Computation: 982 steps/s (collection: 16.500s, learning 0.169s)
               Value function loss: 273.4485
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 3.95
               Mean episode length: 52.83
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0073
Mean episode consecutive_successes: 0.0138
--------------------------------------------------------------------------------
                   Total timesteps: 66109440
                    Iteration time: 16.67s
                        Total time: 39940.75s
                               ETA: 949926.5s

################################################################################
                    [1m Learning iteration 4035/100000 [0m                    

                       Computation: 996 steps/s (collection: 16.264s, learning 0.173s)
               Value function loss: 82.6025
                    Surrogate loss: -0.0048
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 51.29
                  Mean reward/step: 0.09
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0127
--------------------------------------------------------------------------------
                   Total timesteps: 66125824
                    Iteration time: 16.44s
                        Total time: 39957.18s
                               ETA: 950072.1s

################################################################################
                    [1m Learning iteration 4036/100000 [0m                    

                       Computation: 1010 steps/s (collection: 16.049s, learning 0.161s)
               Value function loss: 142.9459
                    Surrogate loss: -0.0042
             Mean action noise std: 0.69
                       Mean reward: 33.45
               Mean episode length: 52.28
                  Mean reward/step: 0.10
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0063
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 66142208
                    Iteration time: 16.21s
                        Total time: 39973.39s
                               ETA: 950212.2s

################################################################################
                    [1m Learning iteration 4037/100000 [0m                    

                       Computation: 986 steps/s (collection: 16.441s, learning 0.171s)
               Value function loss: 498.3353
                    Surrogate loss: -0.0049
             Mean action noise std: 0.69
                       Mean reward: 2.90
               Mean episode length: 51.68
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0144
--------------------------------------------------------------------------------
                   Total timesteps: 66158592
                    Iteration time: 16.61s
                        Total time: 39990.00s
                               ETA: 950361.8s

################################################################################
                    [1m Learning iteration 4038/100000 [0m                    

                       Computation: 1001 steps/s (collection: 16.180s, learning 0.177s)
               Value function loss: 89.3913
                    Surrogate loss: 0.0059
             Mean action noise std: 0.69
                       Mean reward: 2.97
               Mean episode length: 50.46
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0133
--------------------------------------------------------------------------------
                   Total timesteps: 66174976
                    Iteration time: 16.36s
                        Total time: 40006.36s
                               ETA: 950505.2s

################################################################################
                    [1m Learning iteration 4039/100000 [0m                    

                       Computation: 987 steps/s (collection: 16.429s, learning 0.167s)
               Value function loss: 136.2221
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 2.53
               Mean episode length: 50.16
                  Mean reward/step: 0.12
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0029
Mean episode consecutive_successes: 0.0156
--------------------------------------------------------------------------------
                   Total timesteps: 66191360
                    Iteration time: 16.60s
                        Total time: 40022.96s
                               ETA: 950654.2s

################################################################################
                    [1m Learning iteration 4040/100000 [0m                    

                       Computation: 1919 steps/s (collection: 8.365s, learning 0.169s)
               Value function loss: 142.4445
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 2.81
               Mean episode length: 51.01
                  Mean reward/step: 0.16
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0054
Mean episode consecutive_successes: 0.0148
--------------------------------------------------------------------------------
                   Total timesteps: 66207744
                    Iteration time: 8.53s
                        Total time: 40031.49s
                               ETA: 950611.7s

################################################################################
                    [1m Learning iteration 4041/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.076s, learning 0.169s)
               Value function loss: 155.3488
                    Surrogate loss: -0.0063
             Mean action noise std: 0.69
                       Mean reward: 2.87
               Mean episode length: 52.69
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0039
Mean episode consecutive_successes: 0.0154
--------------------------------------------------------------------------------
                   Total timesteps: 66224128
                    Iteration time: 8.24s
                        Total time: 40039.74s
                               ETA: 950562.3s

################################################################################
                    [1m Learning iteration 4042/100000 [0m                    

                       Computation: 1953 steps/s (collection: 8.214s, learning 0.171s)
               Value function loss: 284.6812
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 2.84
               Mean episode length: 51.74
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0142
--------------------------------------------------------------------------------
                   Total timesteps: 66240512
                    Iteration time: 8.39s
                        Total time: 40048.12s
                               ETA: 950516.3s

################################################################################
                    [1m Learning iteration 4043/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.487s, learning 0.169s)
               Value function loss: 817.7177
                    Surrogate loss: -0.0033
             Mean action noise std: 0.69
                       Mean reward: 3.30
               Mean episode length: 52.35
                  Mean reward/step: 0.33
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0107
Mean episode consecutive_successes: 0.0151
--------------------------------------------------------------------------------
                   Total timesteps: 66256896
                    Iteration time: 8.66s
                        Total time: 40056.78s
                               ETA: 950476.8s

################################################################################
                    [1m Learning iteration 4044/100000 [0m                    

                       Computation: 1906 steps/s (collection: 8.427s, learning 0.168s)
               Value function loss: 1028.8433
                    Surrogate loss: -0.0067
             Mean action noise std: 0.69
                       Mean reward: 2.76
               Mean episode length: 50.79
                  Mean reward/step: 0.54
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0264
Mean episode consecutive_successes: 0.0140
--------------------------------------------------------------------------------
                   Total timesteps: 66273280
                    Iteration time: 8.60s
                        Total time: 40065.37s
                               ETA: 950435.8s

################################################################################
                    [1m Learning iteration 4045/100000 [0m                    

                       Computation: 1937 steps/s (collection: 8.293s, learning 0.162s)
               Value function loss: 404.3560
                    Surrogate loss: -0.0098
             Mean action noise std: 0.69
                       Mean reward: 2.92
               Mean episode length: 51.35
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0269
Mean episode consecutive_successes: 0.0147
--------------------------------------------------------------------------------
                   Total timesteps: 66289664
                    Iteration time: 8.45s
                        Total time: 40073.83s
                               ETA: 950391.5s

################################################################################
                    [1m Learning iteration 4046/100000 [0m                    

                       Computation: 1940 steps/s (collection: 8.274s, learning 0.170s)
               Value function loss: 462.9032
                    Surrogate loss: -0.0006
             Mean action noise std: 0.69
                       Mean reward: 88.38
               Mean episode length: 52.49
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0238
--------------------------------------------------------------------------------
                   Total timesteps: 66306048
                    Iteration time: 8.44s
                        Total time: 40082.27s
                               ETA: 950347.0s

################################################################################
                    [1m Learning iteration 4047/100000 [0m                    

                       Computation: 1967 steps/s (collection: 8.116s, learning 0.209s)
               Value function loss: 1099.5285
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 2.67
               Mean episode length: 51.26
                  Mean reward/step: 0.45
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0241
--------------------------------------------------------------------------------
                   Total timesteps: 66322432
                    Iteration time: 8.33s
                        Total time: 40090.60s
                               ETA: 950299.6s

################################################################################
                    [1m Learning iteration 4048/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.290s, learning 0.212s)
               Value function loss: 1033.6415
                    Surrogate loss: -0.0069
             Mean action noise std: 0.69
                       Mean reward: 2.98
               Mean episode length: 52.39
                  Mean reward/step: 0.48
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0283
Mean episode consecutive_successes: 0.0222
--------------------------------------------------------------------------------
                   Total timesteps: 66338816
                    Iteration time: 8.50s
                        Total time: 40099.10s
                               ETA: 950256.5s

################################################################################
                    [1m Learning iteration 4049/100000 [0m                    

                       Computation: 1960 steps/s (collection: 8.148s, learning 0.208s)
               Value function loss: 254.0796
                    Surrogate loss: -0.0072
             Mean action noise std: 0.69
                       Mean reward: 2.63
               Mean episode length: 51.54
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0146
Mean episode consecutive_successes: 0.0281
--------------------------------------------------------------------------------
                   Total timesteps: 66355200
                    Iteration time: 8.36s
                        Total time: 40107.45s
                               ETA: 950209.9s

################################################################################
                    [1m Learning iteration 4050/100000 [0m                    

                       Computation: 1943 steps/s (collection: 8.262s, learning 0.167s)
               Value function loss: 91.7731
                    Surrogate loss: -0.0044
             Mean action noise std: 0.69
                       Mean reward: 42.93
               Mean episode length: 52.19
                  Mean reward/step: 0.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0083
Mean episode consecutive_successes: 0.0334
--------------------------------------------------------------------------------
                   Total timesteps: 66371584
                    Iteration time: 8.43s
                        Total time: 40115.88s
                               ETA: 950165.1s

################################################################################
                    [1m Learning iteration 4051/100000 [0m                    

                       Computation: 1907 steps/s (collection: 8.367s, learning 0.221s)
               Value function loss: 538.9065
                    Surrogate loss: -0.0041
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 52.43
                  Mean reward/step: 0.18
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0044
Mean episode consecutive_successes: 0.0308
--------------------------------------------------------------------------------
                   Total timesteps: 66387968
                    Iteration time: 8.59s
                        Total time: 40124.47s
                               ETA: 950124.1s

################################################################################
                    [1m Learning iteration 4052/100000 [0m                    

                       Computation: 1929 steps/s (collection: 8.317s, learning 0.176s)
               Value function loss: 1125.1244
                    Surrogate loss: -0.0032
             Mean action noise std: 0.69
                       Mean reward: 2.59
               Mean episode length: 51.54
                  Mean reward/step: 0.34
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0093
Mean episode consecutive_successes: 0.0311
--------------------------------------------------------------------------------
                   Total timesteps: 66404352
                    Iteration time: 8.49s
                        Total time: 40132.96s
                               ETA: 950080.8s

################################################################################
                    [1m Learning iteration 4053/100000 [0m                    

                       Computation: 1986 steps/s (collection: 8.085s, learning 0.162s)
               Value function loss: 690.6792
                    Surrogate loss: -0.0065
             Mean action noise std: 0.69
                       Mean reward: 2.37
               Mean episode length: 51.72
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0166
Mean episode consecutive_successes: 0.0287
--------------------------------------------------------------------------------
                   Total timesteps: 66420736
                    Iteration time: 8.25s
                        Total time: 40141.21s
                               ETA: 950031.7s

################################################################################
                    [1m Learning iteration 4054/100000 [0m                    

                       Computation: 1913 steps/s (collection: 8.390s, learning 0.171s)
               Value function loss: 547.2345
                    Surrogate loss: -0.0052
             Mean action noise std: 0.69
                       Mean reward: 2.62
               Mean episode length: 51.53
                  Mean reward/step: 0.15
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0195
Mean episode consecutive_successes: 0.0265
--------------------------------------------------------------------------------
                   Total timesteps: 66437120
                    Iteration time: 8.56s
                        Total time: 40149.77s
                               ETA: 949990.1s

################################################################################
                    [1m Learning iteration 4055/100000 [0m                    

                       Computation: 1927 steps/s (collection: 8.322s, learning 0.176s)
               Value function loss: 230.9734
                    Surrogate loss: -0.0086
             Mean action noise std: 0.69
                       Mean reward: 28.29
               Mean episode length: 52.48
                  Mean reward/step: 0.12
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0339
--------------------------------------------------------------------------------
                   Total timesteps: 66453504
                    Iteration time: 8.50s
                        Total time: 40158.27s
                               ETA: 949947.0s

################################################################################
                    [1m Learning iteration 4056/100000 [0m                    

                       Computation: 1875 steps/s (collection: 8.460s, learning 0.274s)
               Value function loss: 535.5102
                    Surrogate loss: -0.0062
             Mean action noise std: 0.69
                       Mean reward: 2.60
               Mean episode length: 51.10
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0112
Mean episode consecutive_successes: 0.0313
--------------------------------------------------------------------------------
                   Total timesteps: 66469888
                    Iteration time: 8.73s
                        Total time: 40167.00s
                               ETA: 949909.5s

################################################################################
                    [1m Learning iteration 4057/100000 [0m                    

                       Computation: 1941 steps/s (collection: 8.229s, learning 0.209s)
               Value function loss: 406.6004
                    Surrogate loss: -0.0075
             Mean action noise std: 0.69
                       Mean reward: 27.88
               Mean episode length: 51.46
                  Mean reward/step: 0.20
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0078
Mean episode consecutive_successes: 0.0324
--------------------------------------------------------------------------------
                   Total timesteps: 66486272
                    Iteration time: 8.44s
                        Total time: 40175.44s
                               ETA: 949865.0s

################################################################################
                    [1m Learning iteration 4058/100000 [0m                    

                       Computation: 1926 steps/s (collection: 8.328s, learning 0.174s)
               Value function loss: 375.4658
                    Surrogate loss: -0.0071
             Mean action noise std: 0.69
                       Mean reward: 8.39
               Mean episode length: 51.86
                  Mean reward/step: 0.13
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0326
--------------------------------------------------------------------------------
                   Total timesteps: 66502656
                    Iteration time: 8.50s
                        Total time: 40183.94s
                               ETA: 949822.1s

################################################################################
                    [1m Learning iteration 4059/100000 [0m                    

                       Computation: 1987 steps/s (collection: 8.072s, learning 0.170s)
               Value function loss: 460.6362
                    Surrogate loss: -0.0051
             Mean action noise std: 0.69
                       Mean reward: 2.74
               Mean episode length: 53.14
                  Mean reward/step: 0.29
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0103
Mean episode consecutive_successes: 0.0312
--------------------------------------------------------------------------------
                   Total timesteps: 66519040
                    Iteration time: 8.24s
                        Total time: 40192.18s
                               ETA: 949773.0s

################################################################################
                    [1m Learning iteration 4060/100000 [0m                    

                       Computation: 1892 steps/s (collection: 8.492s, learning 0.165s)
               Value function loss: 384.8188
                    Surrogate loss: -0.0090
             Mean action noise std: 0.69
                       Mean reward: 2.94
               Mean episode length: 51.36
                  Mean reward/step: 0.17
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0142
Mean episode consecutive_successes: 0.0288
--------------------------------------------------------------------------------
                   Total timesteps: 66535424
                    Iteration time: 8.66s
                        Total time: 40200.84s
                               ETA: 949733.7s

################################################################################
                    [1m Learning iteration 4061/100000 [0m                    

                       Computation: 1963 steps/s (collection: 8.177s, learning 0.167s)
               Value function loss: 688.9364
                    Surrogate loss: -0.0034
             Mean action noise std: 0.69
                       Mean reward: 39.96
               Mean episode length: 51.36
                  Mean reward/step: 0.23
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0059
Mean episode consecutive_successes: 0.0331
--------------------------------------------------------------------------------
                   Total timesteps: 66551808
                    Iteration time: 8.34s
                        Total time: 40209.18s
                               ETA: 949687.1s

################################################################################
                    [1m Learning iteration 4062/100000 [0m                    

                       Computation: 1996 steps/s (collection: 8.036s, learning 0.172s)
               Value function loss: 761.9405
                    Surrogate loss: -0.0056
             Mean action noise std: 0.69
                       Mean reward: 2.34
               Mean episode length: 51.44
                  Mean reward/step: 0.28
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0132
Mean episode consecutive_successes: 0.0305
--------------------------------------------------------------------------------
                   Total timesteps: 66568192
                    Iteration time: 8.21s
                        Total time: 40217.39s
                               ETA: 949637.2s

Importing module 'gym_37' (/data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/gym_37.so)
Setting GYM_USD_PLUG_INFO_PATH to /data/zihan/software/isaacgym/python/isaacgym/_bindings/linux-x86_64/usd/plugInfo.json
PyTorch version 1.11.0.dev20211118+cu113
Device count 8
/data/zihan/software/isaacgym/python/isaacgym/_bindings/src/gymtorch
Using /data/zihan/.cache/torch_extensions/py37_cu113 as PyTorch extensions root...
Emitting ninja build file /data/zihan/.cache/torch_extensions/py37_cu113/gymtorch/build.ninja...
Building extension module gymtorch...
Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)
ninja: no work to do.
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:6: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.
  if not hasattr(tensorboard, '__version__') or LooseVersion(tensorboard.__version__) < LooseVersion('1.15'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:568: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.object, string),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:569: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  (np.bool, bool),
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:100: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object: SlowAppendObjectArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorboard/util/tensor_util.py:101: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool: SlowAppendBoolArrayToTensorProto,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py:15: DeprecationWarning: the imp module is deprecated in favour of importlib; see the module's documentation for alternative uses
  import imp
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/util/nest.py:1286: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  _pywrap_tensorflow.RegisterType("Mapping", _collections.Mapping)
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:593: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:601: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:106: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. 
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.object:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/tensor_util.py:108: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
  np.bool:
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/object_identity.py:61: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class ObjectIdentityDictionary(collections.MutableMapping):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/training/tracking/data_structures.py:374: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated, and in 3.8 it will stop working
  class _ListWrapper(List, collections.MutableSequence,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:23: DeprecationWarning: NEAREST is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.NEAREST or Dither.NONE instead.
  'nearest': pil_image.NEAREST,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:24: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.
  'bilinear': pil_image.BILINEAR,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:25: DeprecationWarning: BICUBIC is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BICUBIC instead.
  'bicubic': pil_image.BICUBIC,
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:28: DeprecationWarning: HAMMING is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.HAMMING instead.
  if hasattr(pil_image, 'HAMMING'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:30: DeprecationWarning: BOX is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BOX instead.
  if hasattr(pil_image, 'BOX'):
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/keras_preprocessing/image/utils.py:33: DeprecationWarning: LANCZOS is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.LANCZOS instead.
  if hasattr(pil_image, 'LANCZOS'):
wandb: Currently logged in as: quantumiracle. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041915-3j61i0sc
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandBottleCap_ppo_20221020041913
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/3j61i0sc
wandb: wandb version 0.13.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.13.0
wandb: Run data is saved locally in /data/zihan/research/DexterousHands/bi-dexhands/wandb/run-20221020_041914-14olyl33
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run ShadowHandBottleCap_ppo_20221020041912
wandb: â­ï¸ View project at https://wandb.ai/quantumiracle/bi-dexhands
wandb: ðŸš€ View run at https://wandb.ai/quantumiracle/bi-dexhands/runs/14olyl33
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:7
GPU Pipeline: enabled
Not connected to PVD
+++ Using GPU PhysX
Physics Engine: PhysX
Physics Device: cuda:4
GPU Pipeline: enabled
JointSpec type free not yet supported!
JointSpec type free not yet supported!
JointSpec type free not yet supported!
JointSpec type free not yet supported!
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint8 = np.dtype([("qint8", np.int8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint8 = np.dtype([("quint8", np.uint8, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint16 = np.dtype([("qint16", np.int16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_quint16 = np.dtype([("quint16", np.uint16, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  _np_qint32 = np.dtype([("qint32", np.int32, 1)])
/data/zihan/anaconda3/envs/x/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.
  np_resource = np.dtype([("resource", np.ubyte, 1)])
Loading extension module gymtorch...
raw:  Namespace(algo='ppo', cfg_env='Base', cfg_train='Base', checkpoint='Base', compute_device_id=4, datatype='random', episode_length=0, experiment='Base', flex=False, graphics_device_id=4, headless=False, horovod=False, logdir='logs/', max_iterations=0, metadata=False, minibatch_size=-1, model_dir='', num_envs=0, num_threads=0, physics_engine=SimType.SIM_PHYSX, physx=False, pipeline='gpu', play=False, randomize=False, record_video=True, record_video_interval=30, resume=0, rl_device='cuda:4', seed=None, sim_device='cuda:4', sim_device_type='cuda', slices=0, steps_num=-1, subscenes=0, task='ShadowHandBottleCap', task_type='Python', test=False, torch_deterministic=False, use_gpu=True, use_gpu_pipeline=True, wandb_activate=True, wandb_entity='quantumiracle', wandb_group='', wandb_project='bi-dexhands')
{'env': {'env_name': 'shadow_hand_bottle_cap', 'numEnvs': 2048, 'envSpacing': 1.5, 'episodeLength': 125, 'enableDebugVis': False, 'aggregateMode': 1, 'stiffnessScale': 1.0, 'forceLimitScale': 1.0, 'useRelativeControl': False, 'dofSpeedScale': 20.0, 'actionsMovingAverage': 1.0, 'controlFrequencyInv': 1, 'startPositionNoise': 0.0, 'startRotationNoise': 0.0, 'resetPositionNoise': 0.0, 'resetRotationNoise': 0.0, 'resetDofPosRandomInterval': 0.0, 'resetDofVelRandomInterval': 0.0, 'distRewardScale': 20, 'transition_scale': 0.1, 'orientation_scale': 0.1, 'rotRewardScale': 1.0, 'rotEps': 0.1, 'actionPenaltyScale': -0.0002, 'reachGoalBonus': 250, 'fallDistance': 0.4, 'fallPenalty': 0.0, 'objectType': 'pot', 'observationType': 'full_state', 'handAgentIndex': '[[0, 1, 2, 3, 4, 5]]', 'asymmetric_observations': False, 'successTolerance': 0.1, 'printNumSuccesses': False, 'maxConsecutiveSuccesses': 0, 'asset': {'assetRoot': '../assets', 'assetFileName': 'mjcf/open_ai_assets/hand/shadow_hand.xml', 'assetFileNameBlock': 'urdf/objects/cube_multicolor.urdf', 'assetFileNameEgg': 'mjcf/open_ai_assets/hand/egg.xml', 'assetFileNamePen': 'mjcf/open_ai_assets/hand/pen.xml'}}, 'task': {'randomize': False, 'randomization_params': {'frequency': 600, 'observations': {'range': [0, 0.002], 'range_correlated': [0, 0.001], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'actions': {'range': [0.0, 0.05], 'range_correlated': [0, 0.015], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}, 'sim_params': {'gravity': {'range': [0, 0.4], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 40000}}, 'actor_params': {'hand': {'color': True, 'tendon_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'dof_properties': {'damping': {'range': [0.3, 3.0], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'stiffness': {'range': [0.75, 1.5], 'operation': 'scaling', 'distribution': 'loguniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'lower': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}, 'upper': {'range': [0, 0.01], 'operation': 'additive', 'distribution': 'gaussian', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}, 'object': {'scale': {'range': [0.95, 1.05], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}, 'rigid_body_properties': {'mass': {'range': [0.5, 1.5], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}, 'rigid_shape_properties': {'friction': {'num_buckets': 250, 'range': [0.7, 1.3], 'operation': 'scaling', 'distribution': 'uniform', 'schedule': 'linear', 'schedule_steps': 30000}}}}}}, 'sim': {'substeps': 2, 'physx': {'num_threads': 4, 'solver_type': 1, 'num_position_iterations': 4, 'num_velocity_iterations': 0, 'contact_offset': 0.002, 'rest_offset': 0.0, 'bounce_threshold_velocity': 0.2, 'max_depenetration_velocity': 1000.0, 'default_buffer_size_multiplier': 5.0}, 'flex': {'num_outer_iterations': 5, 'num_inner_iterations': 20, 'warm_start': 0.8, 'relaxation': 0.75}}, 'name': 'ShadowHandBottleCap', 'headless': False, 'wandb_activate': True, 'wandb_project': 'bi-dexhands', 'wandb_name': 'ShadowHandBottleCap_ppo_20221020041912', 'algo': 'ppo', 'seed': -1, 'clip_observations': 5.0, 'clip_actions': 1.0, 'policy': {'pi_hid_sizes': [1024, 1024, 512], 'vf_hid_sizes': [1024, 1024, 512], 'activation': 'elu'}, 'learn': {'agent_name': 'shadow_hand', 'test': False, 'resume': 0, 'save_interval': 1000, 'print_log': True, 'max_iterations': 100000, 'cliprange': 0.2, 'ent_coef': 0, 'nsteps': 8, 'noptepochs': 5, 'nminibatches': 4, 'max_grad_norm': 1, 'optim_stepsize': 0.0003, 'schedule': 'adaptive', 'desired_kl': 0.016, 'gamma': 0.96, 'lam': 0.95, 'init_noise_std': 0.8, 'log_interval': 1, 'asymmetric': False}}
Setting seed: 2574
Algorithm:  ppo
Python
Averaging factor:  0.01
Obs type: full_state
self.num_shadow_hand_bodies:  26
self.num_shadow_hand_shapes:  22
self.num_shadow_hand_dofs:  24
self.num_shadow_hand_actuators:  20
self.num_shadow_hand_tendons:  4
RL device:  cuda:4
Sequential(
  (0): Linear(in_features=420, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=52, bias=True)
)
Sequential(
  (0): Linear(in_features=420, out_features=1024, bias=True)
  (1): ELU(alpha=1.0)
  (2): Linear(in_features=1024, out_features=1024, bias=True)
  (3): ELU(alpha=1.0)
  (4): Linear(in_features=1024, out_features=512, bias=True)
  (5): ELU(alpha=1.0)
  (6): Linear(in_features=512, out_features=1, bias=True)
)
################################################################################
                     [1m Learning iteration 0/100000 [0m                      

                       Computation: 728 steps/s (collection: 20.992s, learning 1.507s)
               Value function loss: 8.7326
                    Surrogate loss: 0.0618
             Mean action noise std: 0.80
                  Mean reward/step: 1.45
       Mean episode length/episode: 8.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384
                    Iteration time: 22.50s
                        Total time: 22.50s
                               ETA: 2249895.2s

################################################################################
                     [1m Learning iteration 1/100000 [0m                      

                       Computation: 715 steps/s (collection: 22.349s, learning 0.556s)
               Value function loss: 11.1501
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: 18.05
               Mean episode length: 14.83
                  Mean reward/step: 1.26
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768
                    Iteration time: 22.91s
                        Total time: 45.40s
                               ETA: 2270194.0s

################################################################################
                     [1m Learning iteration 2/100000 [0m                      

                       Computation: 706 steps/s (collection: 23.025s, learning 0.167s)
               Value function loss: 11.9557
                    Surrogate loss: -0.0165
             Mean action noise std: 0.80
                       Mean reward: 29.61
               Mean episode length: 22.86
                  Mean reward/step: 1.17
       Mean episode length/episode: 5.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152
                    Iteration time: 23.19s
                        Total time: 68.60s
                               ETA: 2286478.6s

################################################################################
                     [1m Learning iteration 3/100000 [0m                      

                       Computation: 678 steps/s (collection: 23.974s, learning 0.174s)
               Value function loss: 13.6377
                    Surrogate loss: -0.0052
             Mean action noise std: 0.80
                       Mean reward: 35.12
               Mean episode length: 29.13
                  Mean reward/step: 1.28
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 65536
                    Iteration time: 24.15s
                        Total time: 92.74s
                               ETA: 2318513.4s

################################################################################
                     [1m Learning iteration 4/100000 [0m                      

                       Computation: 672 steps/s (collection: 24.203s, learning 0.161s)
               Value function loss: 8.8963
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 43.06
               Mean episode length: 35.83
                  Mean reward/step: 1.21
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 81920
                    Iteration time: 24.36s
                        Total time: 117.11s
                               ETA: 2342065.9s

################################################################################
                     [1m Learning iteration 5/100000 [0m                      

                       Computation: 690 steps/s (collection: 23.528s, learning 0.192s)
               Value function loss: 8.0169
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 51.12
               Mean episode length: 43.01
                  Mean reward/step: 1.19
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 98304
                    Iteration time: 23.72s
                        Total time: 140.83s
                               ETA: 2347013.8s

################################################################################
                     [1m Learning iteration 6/100000 [0m                      

                       Computation: 683 steps/s (collection: 23.785s, learning 0.170s)
               Value function loss: 8.1330
                    Surrogate loss: -0.0078
             Mean action noise std: 0.80
                       Mean reward: 59.86
               Mean episode length: 50.66
                  Mean reward/step: 1.20
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 114688
                    Iteration time: 23.96s
                        Total time: 164.78s
                               ETA: 2353902.1s

################################################################################
                     [1m Learning iteration 7/100000 [0m                      

                       Computation: 689 steps/s (collection: 23.585s, learning 0.182s)
               Value function loss: 6.6351
                    Surrogate loss: -0.0056
             Mean action noise std: 0.80
                       Mean reward: 64.85
               Mean episode length: 54.59
                  Mean reward/step: 1.22
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 131072
                    Iteration time: 23.77s
                        Total time: 188.55s
                               ETA: 2356709.9s

################################################################################
                     [1m Learning iteration 8/100000 [0m                      

                       Computation: 682 steps/s (collection: 23.842s, learning 0.175s)
               Value function loss: 6.8236
                    Surrogate loss: -0.0102
             Mean action noise std: 0.80
                       Mean reward: 72.51
               Mean episode length: 61.05
                  Mean reward/step: 1.23
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 147456
                    Iteration time: 24.02s
                        Total time: 212.57s
                               ETA: 2361668.4s

################################################################################
                     [1m Learning iteration 9/100000 [0m                      

                       Computation: 666 steps/s (collection: 24.273s, learning 0.314s)
               Value function loss: 4.9776
                    Surrogate loss: -0.0034
             Mean action noise std: 0.80
                       Mean reward: 79.10
               Mean episode length: 65.44
                  Mean reward/step: 1.25
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 163840
                    Iteration time: 24.59s
                        Total time: 237.15s
                               ETA: 2371328.6s

################################################################################
                     [1m Learning iteration 10/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.888s, learning 0.215s)
               Value function loss: 6.8427
                    Surrogate loss: 0.0065
             Mean action noise std: 0.80
                       Mean reward: 88.94
               Mean episode length: 73.49
                  Mean reward/step: 1.27
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 180224
                    Iteration time: 24.10s
                        Total time: 261.26s
                               ETA: 2374826.6s

################################################################################
                     [1m Learning iteration 11/100000 [0m                     

                       Computation: 665 steps/s (collection: 24.384s, learning 0.224s)
               Value function loss: 4.0985
                    Surrogate loss: -0.0047
             Mean action noise std: 0.80
                       Mean reward: 92.42
               Mean episode length: 76.92
                  Mean reward/step: 1.29
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 196608
                    Iteration time: 24.61s
                        Total time: 285.86s
                               ETA: 2381944.5s

################################################################################
                     [1m Learning iteration 12/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.797s, learning 0.170s)
               Value function loss: 2.8753
                    Surrogate loss: -0.0021
             Mean action noise std: 0.80
                       Mean reward: 96.72
               Mean episode length: 80.20
                  Mean reward/step: 1.33
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 212992
                    Iteration time: 23.97s
                        Total time: 309.83s
                               ETA: 2383034.0s

################################################################################
                     [1m Learning iteration 13/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.549s, learning 0.170s)
               Value function loss: 2.6304
                    Surrogate loss: 0.0009
             Mean action noise std: 0.80
                       Mean reward: 102.06
               Mean episode length: 85.61
                  Mean reward/step: 1.36
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 229376
                    Iteration time: 23.72s
                        Total time: 333.55s
                               ETA: 2382198.2s

################################################################################
                     [1m Learning iteration 14/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.837s, learning 0.197s)
               Value function loss: 2.5150
                    Surrogate loss: 0.0021
             Mean action noise std: 0.80
                       Mean reward: 104.67
               Mean episode length: 87.30
                  Mean reward/step: 1.38
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 245760
                    Iteration time: 24.03s
                        Total time: 357.59s
                               ETA: 2383569.4s

################################################################################
                     [1m Learning iteration 15/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.726s, learning 0.242s)
               Value function loss: 46.2429
                    Surrogate loss: -0.0091
             Mean action noise std: 0.80
                       Mean reward: 152.78
               Mean episode length: 122.50
                  Mean reward/step: 1.39
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 262144
                    Iteration time: 23.97s
                        Total time: 381.55s
                               ETA: 2384351.9s

################################################################################
                     [1m Learning iteration 16/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.894s, learning 0.172s)
               Value function loss: 2.9954
                    Surrogate loss: 0.0306
             Mean action noise std: 0.80
                       Mean reward: 150.42
               Mean episode length: 120.53
                  Mean reward/step: 1.42
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 278528
                    Iteration time: 24.07s
                        Total time: 405.62s
                               ETA: 2385615.5s

################################################################################
                     [1m Learning iteration 17/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.321s, learning 0.188s)
               Value function loss: 98.5655
                    Surrogate loss: 0.0328
             Mean action noise std: 0.80
                       Mean reward: 161.85
               Mean episode length: 125.00
                  Mean reward/step: 1.42
       Mean episode length/episode: 6.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 294912
                    Iteration time: 24.51s
                        Total time: 430.13s
                               ETA: 2389199.2s

################################################################################
                     [1m Learning iteration 18/100000 [0m                     

                       Computation: 678 steps/s (collection: 23.957s, learning 0.206s)
               Value function loss: 32.6032
                    Surrogate loss: -0.0058
             Mean action noise std: 0.80
                       Mean reward: 164.26
               Mean episode length: 123.99
                  Mean reward/step: 1.45
       Mean episode length/episode: 6.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 311296
                    Iteration time: 24.16s
                        Total time: 454.29s
                               ETA: 2390580.4s

################################################################################
                     [1m Learning iteration 19/100000 [0m                     

                       Computation: 677 steps/s (collection: 24.031s, learning 0.163s)
               Value function loss: 13.5778
                    Surrogate loss: -0.0104
             Mean action noise std: 0.80
                       Mean reward: 164.39
               Mean episode length: 121.81
                  Mean reward/step: 1.47
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 327680
                    Iteration time: 24.19s
                        Total time: 478.49s
                               ETA: 2391976.2s

################################################################################
                     [1m Learning iteration 20/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.877s, learning 0.159s)
               Value function loss: 13.5644
                    Surrogate loss: -0.0008
             Mean action noise std: 0.80
                       Mean reward: 151.93
               Mean episode length: 110.72
                  Mean reward/step: 1.49
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 344064
                    Iteration time: 24.04s
                        Total time: 502.52s
                               ETA: 2392484.4s

################################################################################
                     [1m Learning iteration 21/100000 [0m                     

                       Computation: 677 steps/s (collection: 23.993s, learning 0.177s)
               Value function loss: 14.2172
                    Surrogate loss: 0.0022
             Mean action noise std: 0.80
                       Mean reward: 153.84
               Mean episode length: 108.71
                  Mean reward/step: 1.52
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 360448
                    Iteration time: 24.17s
                        Total time: 526.69s
                               ETA: 2393550.2s

################################################################################
                     [1m Learning iteration 22/100000 [0m                     

                       Computation: 689 steps/s (collection: 23.570s, learning 0.179s)
               Value function loss: 13.6645
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 168.41
               Mean episode length: 117.86
                  Mean reward/step: 1.54
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 376832
                    Iteration time: 23.75s
                        Total time: 550.44s
                               ETA: 2392693.1s

################################################################################
                     [1m Learning iteration 23/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.564s, learning 0.172s)
               Value function loss: 10.7051
                    Surrogate loss: 0.0032
             Mean action noise std: 0.80
                       Mean reward: 164.94
               Mean episode length: 113.87
                  Mean reward/step: 1.56
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 393216
                    Iteration time: 23.74s
                        Total time: 574.18s
                               ETA: 2391850.9s

################################################################################
                     [1m Learning iteration 24/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.809s, learning 0.178s)
               Value function loss: 9.9135
                    Surrogate loss: -0.0050
             Mean action noise std: 0.80
                       Mean reward: 166.19
               Mean episode length: 113.10
                  Mean reward/step: 1.58
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 409600
                    Iteration time: 23.99s
                        Total time: 598.16s
                               ETA: 2392079.8s

################################################################################
                     [1m Learning iteration 25/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.874s, learning 0.198s)
               Value function loss: 8.5420
                    Surrogate loss: 0.0009
             Mean action noise std: 0.80
                       Mean reward: 174.04
               Mean episode length: 117.78
                  Mean reward/step: 1.59
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 425984
                    Iteration time: 24.07s
                        Total time: 622.24s
                               ETA: 2392615.6s

################################################################################
                     [1m Learning iteration 26/100000 [0m                     

                       Computation: 678 steps/s (collection: 23.948s, learning 0.195s)
               Value function loss: 9.1752
                    Surrogate loss: -0.0010
             Mean action noise std: 0.80
                       Mean reward: 170.08
               Mean episode length: 114.27
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 442368
                    Iteration time: 24.14s
                        Total time: 646.38s
                               ETA: 2393374.5s

################################################################################
                     [1m Learning iteration 27/100000 [0m                     

                       Computation: 693 steps/s (collection: 23.477s, learning 0.159s)
               Value function loss: 5.2288
                    Surrogate loss: 0.0131
             Mean action noise std: 0.80
                       Mean reward: 168.71
               Mean episode length: 112.23
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 458752
                    Iteration time: 23.64s
                        Total time: 670.02s
                               ETA: 2392267.8s

################################################################################
                     [1m Learning iteration 28/100000 [0m                     

                       Computation: 682 steps/s (collection: 23.826s, learning 0.176s)
               Value function loss: 6.4614
                    Surrogate loss: 0.0015
             Mean action noise std: 0.80
                       Mean reward: 170.17
               Mean episode length: 112.34
                  Mean reward/step: 1.64
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 475136
                    Iteration time: 24.00s
                        Total time: 694.02s
                               ETA: 2392495.3s

################################################################################
                     [1m Learning iteration 29/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.504s, learning 0.200s)
               Value function loss: 5.2831
                    Surrogate loss: 0.0037
             Mean action noise std: 0.80
                       Mean reward: 169.26
               Mean episode length: 111.29
                  Mean reward/step: 1.68
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 491520
                    Iteration time: 23.70s
                        Total time: 717.72s
                               ETA: 2391714.1s

################################################################################
                     [1m Learning iteration 30/100000 [0m                     

                       Computation: 696 steps/s (collection: 23.317s, learning 0.203s)
               Value function loss: 4.3398
                    Surrogate loss: 0.0339
             Mean action noise std: 0.80
                       Mean reward: 174.20
               Mean episode length: 113.25
                  Mean reward/step: 1.67
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 507904
                    Iteration time: 23.52s
                        Total time: 741.24s
                               ETA: 2390386.3s

################################################################################
                     [1m Learning iteration 31/100000 [0m                     

                       Computation: 699 steps/s (collection: 23.140s, learning 0.283s)
               Value function loss: 24.1543
                    Surrogate loss: 0.0089
             Mean action noise std: 0.80
                       Mean reward: 191.08
               Mean episode length: 122.09
                  Mean reward/step: 1.62
       Mean episode length/episode: 6.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 524288
                    Iteration time: 23.42s
                        Total time: 764.66s
                               ETA: 2388837.2s

################################################################################
                     [1m Learning iteration 32/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.717s, learning 0.194s)
               Value function loss: 7.5146
                    Surrogate loss: 0.0008
             Mean action noise std: 0.80
                       Mean reward: 189.00
               Mean episode length: 121.27
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 540672
                    Iteration time: 23.91s
                        Total time: 788.58s
                               ETA: 2388860.3s

################################################################################
                     [1m Learning iteration 33/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.555s, learning 0.162s)
               Value function loss: 178.9035
                    Surrogate loss: 0.0312
             Mean action noise std: 0.80
                       Mean reward: 199.91
               Mean episode length: 125.00
                  Mean reward/step: 1.59
       Mean episode length/episode: 5.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 557056
                    Iteration time: 23.72s
                        Total time: 812.29s
                               ETA: 2388307.4s

################################################################################
                     [1m Learning iteration 34/100000 [0m                     

                       Computation: 698 steps/s (collection: 23.243s, learning 0.212s)
               Value function loss: 19.7440
                    Surrogate loss: -0.0152
             Mean action noise std: 0.80
                       Mean reward: 195.11
               Mean episode length: 123.02
                  Mean reward/step: 1.60
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 573440
                    Iteration time: 23.46s
                        Total time: 835.75s
                               ETA: 2387039.9s

################################################################################
                     [1m Learning iteration 35/100000 [0m                     

                       Computation: 679 steps/s (collection: 23.874s, learning 0.226s)
               Value function loss: 16.4027
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                       Mean reward: 186.54
               Mean episode length: 116.47
                  Mean reward/step: 1.62
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 589824
                    Iteration time: 24.10s
                        Total time: 859.85s
                               ETA: 2387631.6s

################################################################################
                     [1m Learning iteration 36/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.502s, learning 0.186s)
               Value function loss: 18.1649
                    Surrogate loss: -0.0007
             Mean action noise std: 0.80
                       Mean reward: 186.41
               Mean episode length: 115.29
                  Mean reward/step: 1.66
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 606208
                    Iteration time: 23.69s
                        Total time: 883.54s
                               ETA: 2387075.3s

################################################################################
                     [1m Learning iteration 37/100000 [0m                     

                       Computation: 879 steps/s (collection: 18.420s, learning 0.216s)
               Value function loss: 19.8941
                    Surrogate loss: -0.0021
             Mean action noise std: 0.80
                       Mean reward: 190.32
               Mean episode length: 116.54
                  Mean reward/step: 1.70
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 622592
                    Iteration time: 18.64s
                        Total time: 902.17s
                               ETA: 2373259.3s

################################################################################
                     [1m Learning iteration 38/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.920s, learning 0.199s)
               Value function loss: 13.0254
                    Surrogate loss: 0.0105
             Mean action noise std: 0.80
                       Mean reward: 194.74
               Mean episode length: 117.71
                  Mean reward/step: 1.74
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 638976
                    Iteration time: 12.12s
                        Total time: 914.29s
                               ETA: 2343446.3s

################################################################################
                     [1m Learning iteration 39/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.997s, learning 0.190s)
               Value function loss: 15.2423
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 200.33
               Mean episode length: 118.60
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 655360
                    Iteration time: 12.19s
                        Total time: 926.48s
                               ETA: 2315294.8s

################################################################################
                     [1m Learning iteration 40/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.164s, learning 0.227s)
               Value function loss: 12.0682
                    Surrogate loss: 0.0068
             Mean action noise std: 0.80
                       Mean reward: 194.35
               Mean episode length: 115.82
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 671744
                    Iteration time: 12.39s
                        Total time: 938.87s
                               ETA: 2289011.4s

################################################################################
                     [1m Learning iteration 41/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.622s, learning 0.168s)
               Value function loss: 13.8526
                    Surrogate loss: 0.0048
             Mean action noise std: 0.80
                       Mean reward: 194.54
               Mean episode length: 115.69
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 688128
                    Iteration time: 11.79s
                        Total time: 950.66s
                               ETA: 2262550.6s

################################################################################
                     [1m Learning iteration 42/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.943s, learning 0.169s)
               Value function loss: 10.7448
                    Surrogate loss: -0.0054
             Mean action noise std: 0.80
                       Mean reward: 196.51
               Mean episode length: 116.08
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 704512
                    Iteration time: 12.11s
                        Total time: 962.77s
                               ETA: 2238066.4s

################################################################################
                     [1m Learning iteration 43/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.681s, learning 0.167s)
               Value function loss: 8.9456
                    Surrogate loss: 0.0016
             Mean action noise std: 0.80
                       Mean reward: 197.70
               Mean episode length: 115.91
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 720896
                    Iteration time: 11.85s
                        Total time: 974.62s
                               ETA: 2214094.7s

################################################################################
                     [1m Learning iteration 44/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.737s, learning 0.167s)
               Value function loss: 7.8516
                    Surrogate loss: 0.0068
             Mean action noise std: 0.80
                       Mean reward: 197.09
               Mean episode length: 116.05
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 737280
                    Iteration time: 11.90s
                        Total time: 986.52s
                               ETA: 2191312.4s

################################################################################
                     [1m Learning iteration 45/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.705s, learning 0.168s)
               Value function loss: 7.8762
                    Surrogate loss: 0.0025
             Mean action noise std: 0.80
                       Mean reward: 198.70
               Mean episode length: 116.05
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 753664
                    Iteration time: 11.87s
                        Total time: 998.40s
                               ETA: 2169452.6s

################################################################################
                     [1m Learning iteration 46/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.922s, learning 0.176s)
               Value function loss: 122.1354
                    Surrogate loss: 0.0078
             Mean action noise std: 0.80
                       Mean reward: 224.57
               Mean episode length: 125.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 770048
                    Iteration time: 12.10s
                        Total time: 1010.50s
                               ETA: 2149001.3s

################################################################################
                     [1m Learning iteration 47/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.851s, learning 0.168s)
               Value function loss: 17.4151
                    Surrogate loss: 0.0023
             Mean action noise std: 0.80
                       Mean reward: 222.13
               Mean episode length: 123.54
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 786432
                    Iteration time: 12.02s
                        Total time: 1022.51s
                               ETA: 2129236.3s

################################################################################
                     [1m Learning iteration 48/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.776s, learning 0.174s)
               Value function loss: 61.5143
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 225.22
               Mean episode length: 123.93
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 802816
                    Iteration time: 11.95s
                        Total time: 1034.46s
                               ETA: 2110137.8s

################################################################################
                     [1m Learning iteration 49/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.396s, learning 0.155s)
               Value function loss: 121.7823
                    Surrogate loss: -0.0026
             Mean action noise std: 0.80
                       Mean reward: 228.10
               Mean episode length: 124.67
                  Mean reward/step: 1.75
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 819200
                    Iteration time: 12.55s
                        Total time: 1047.01s
                               ETA: 2093003.7s

################################################################################
                     [1m Learning iteration 50/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.609s, learning 0.195s)
               Value function loss: 23.8042
                    Surrogate loss: -0.0157
             Mean action noise std: 0.80
                       Mean reward: 220.56
               Mean episode length: 121.10
                  Mean reward/step: 1.76
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 835584
                    Iteration time: 11.80s
                        Total time: 1058.82s
                               ETA: 2075077.4s

################################################################################
                     [1m Learning iteration 51/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.957s, learning 0.163s)
               Value function loss: 23.9554
                    Surrogate loss: -0.0176
             Mean action noise std: 0.80
                       Mean reward: 210.09
               Mean episode length: 113.55
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 851968
                    Iteration time: 12.12s
                        Total time: 1070.94s
                               ETA: 2058448.3s

################################################################################
                     [1m Learning iteration 52/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.338s, learning 0.219s)
               Value function loss: 23.3640
                    Surrogate loss: -0.0136
             Mean action noise std: 0.80
                       Mean reward: 215.34
               Mean episode length: 115.28
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 868352
                    Iteration time: 11.56s
                        Total time: 1082.50s
                               ETA: 2041384.0s

################################################################################
                     [1m Learning iteration 53/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.737s, learning 0.167s)
               Value function loss: 20.6373
                    Surrogate loss: -0.0162
             Mean action noise std: 0.80
                       Mean reward: 224.25
               Mean episode length: 121.13
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 884736
                    Iteration time: 11.90s
                        Total time: 1094.40s
                               ETA: 2025592.5s

################################################################################
                     [1m Learning iteration 54/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.797s, learning 0.173s)
               Value function loss: 15.2794
                    Surrogate loss: -0.0113
             Mean action noise std: 0.80
                       Mean reward: 228.59
               Mean episode length: 123.01
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 901120
                    Iteration time: 11.97s
                        Total time: 1106.37s
                               ETA: 2010496.5s

################################################################################
                     [1m Learning iteration 55/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.764s, learning 0.166s)
               Value function loss: 14.2769
                    Surrogate loss: -0.0098
             Mean action noise std: 0.80
                       Mean reward: 224.66
               Mean episode length: 119.75
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 917504
                    Iteration time: 11.93s
                        Total time: 1118.30s
                               ETA: 1995866.9s

################################################################################
                     [1m Learning iteration 56/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.637s, learning 0.166s)
               Value function loss: 10.7545
                    Surrogate loss: 0.0060
             Mean action noise std: 0.80
                       Mean reward: 223.94
               Mean episode length: 119.17
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 933888
                    Iteration time: 11.80s
                        Total time: 1130.10s
                               ETA: 1981527.4s

################################################################################
                     [1m Learning iteration 57/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.224s, learning 0.180s)
               Value function loss: 13.0120
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 222.76
               Mean episode length: 117.99
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 950272
                    Iteration time: 12.40s
                        Total time: 1142.51s
                               ETA: 1968717.3s

################################################################################
                     [1m Learning iteration 58/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.677s, learning 0.173s)
               Value function loss: 10.4854
                    Surrogate loss: -0.0016
             Mean action noise std: 0.80
                       Mean reward: 223.16
               Mean episode length: 117.37
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 966656
                    Iteration time: 11.85s
                        Total time: 1154.36s
                               ETA: 1955404.0s

################################################################################
                     [1m Learning iteration 59/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.795s, learning 0.205s)
               Value function loss: 10.8539
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 218.15
               Mean episode length: 114.87
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 983040
                    Iteration time: 12.00s
                        Total time: 1166.36s
                               ETA: 1942781.5s

################################################################################
                     [1m Learning iteration 60/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.123s, learning 0.173s)
               Value function loss: 7.8900
                    Surrogate loss: -0.0057
             Mean action noise std: 0.80
                       Mean reward: 216.67
               Mean episode length: 114.68
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 999424
                    Iteration time: 12.30s
                        Total time: 1178.65s
                               ETA: 1931060.0s

################################################################################
                     [1m Learning iteration 61/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.326s, learning 0.166s)
               Value function loss: 7.1405
                    Surrogate loss: -0.0106
             Mean action noise std: 0.80
                       Mean reward: 220.92
               Mean episode length: 116.36
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1015808
                    Iteration time: 11.49s
                        Total time: 1190.15s
                               ETA: 1918419.0s

################################################################################
                     [1m Learning iteration 62/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.560s, learning 0.171s)
               Value function loss: 57.9732
                    Surrogate loss: 0.0015
             Mean action noise std: 0.80
                       Mean reward: 236.65
               Mean episode length: 124.50
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1032192
                    Iteration time: 11.73s
                        Total time: 1201.88s
                               ETA: 1906557.7s

################################################################################
                     [1m Learning iteration 63/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.839s, learning 0.212s)
               Value function loss: 9.5644
                    Surrogate loss: -0.0100
             Mean action noise std: 0.80
                       Mean reward: 229.18
               Mean episode length: 121.78
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1048576
                    Iteration time: 12.05s
                        Total time: 1213.93s
                               ETA: 1895567.7s

################################################################################
                     [1m Learning iteration 64/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.742s, learning 0.163s)
               Value function loss: 154.9450
                    Surrogate loss: -0.0035
             Mean action noise std: 0.80
                       Mean reward: 242.67
               Mean episode length: 125.00
                  Mean reward/step: 1.88
       Mean episode length/episode: 6.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1064960
                    Iteration time: 11.90s
                        Total time: 1225.83s
                               ETA: 1884690.0s

################################################################################
                     [1m Learning iteration 65/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.610s, learning 0.222s)
               Value function loss: 47.0635
                    Surrogate loss: -0.0150
             Mean action noise std: 0.80
                       Mean reward: 238.68
               Mean episode length: 124.02
                  Mean reward/step: 1.77
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1081344
                    Iteration time: 11.83s
                        Total time: 1237.67s
                               ETA: 1874032.2s

################################################################################
                     [1m Learning iteration 66/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.029s, learning 0.253s)
               Value function loss: 21.6454
                    Surrogate loss: -0.0197
             Mean action noise std: 0.80
                       Mean reward: 235.87
               Mean episode length: 121.71
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1097728
                    Iteration time: 12.28s
                        Total time: 1249.95s
                               ETA: 1864361.3s

################################################################################
                     [1m Learning iteration 67/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.906s, learning 0.179s)
               Value function loss: 19.8947
                    Surrogate loss: -0.0200
             Mean action noise std: 0.80
                       Mean reward: 236.56
               Mean episode length: 121.82
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1114112
                    Iteration time: 12.09s
                        Total time: 1262.03s
                               ETA: 1854686.4s

################################################################################
                     [1m Learning iteration 68/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.959s, learning 0.159s)
               Value function loss: 19.2146
                    Surrogate loss: -0.0037
             Mean action noise std: 0.80
                       Mean reward: 243.75
               Mean episode length: 124.85
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1130496
                    Iteration time: 12.12s
                        Total time: 1274.15s
                               ETA: 1845338.8s

################################################################################
                     [1m Learning iteration 69/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.476s, learning 0.181s)
               Value function loss: 15.7173
                    Surrogate loss: -0.0148
             Mean action noise std: 0.80
                       Mean reward: 234.41
               Mean episode length: 121.18
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1146880
                    Iteration time: 11.66s
                        Total time: 1285.81s
                               ETA: 1835600.1s

################################################################################
                     [1m Learning iteration 70/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.330s, learning 0.162s)
               Value function loss: 11.7246
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                       Mean reward: 242.70
               Mean episode length: 124.18
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1163264
                    Iteration time: 11.49s
                        Total time: 1297.30s
                               ETA: 1825903.0s

################################################################################
                     [1m Learning iteration 71/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.592s, learning 0.181s)
               Value function loss: 11.6563
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 244.07
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1179648
                    Iteration time: 11.77s
                        Total time: 1309.07s
                               ETA: 1816864.5s

################################################################################
                     [1m Learning iteration 72/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.570s, learning 0.179s)
               Value function loss: 9.5746
                    Surrogate loss: -0.0109
             Mean action noise std: 0.80
                       Mean reward: 242.03
               Mean episode length: 124.35
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1196032
                    Iteration time: 11.75s
                        Total time: 1320.82s
                               ETA: 1808040.5s

################################################################################
                     [1m Learning iteration 73/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.815s, learning 0.174s)
               Value function loss: 9.5279
                    Surrogate loss: -0.0136
             Mean action noise std: 0.80
                       Mean reward: 239.29
               Mean episode length: 123.07
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1212416
                    Iteration time: 11.99s
                        Total time: 1332.81s
                               ETA: 1799780.0s

################################################################################
                     [1m Learning iteration 74/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.870s, learning 0.282s)
               Value function loss: 8.1577
                    Surrogate loss: -0.0150
             Mean action noise std: 0.80
                       Mean reward: 235.79
               Mean episode length: 122.38
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1228800
                    Iteration time: 12.15s
                        Total time: 1344.96s
                               ETA: 1791956.2s

################################################################################
                     [1m Learning iteration 75/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.037s, learning 0.175s)
               Value function loss: 8.8463
                    Surrogate loss: -0.0146
             Mean action noise std: 0.80
                       Mean reward: 235.85
               Mean episode length: 122.25
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1245184
                    Iteration time: 12.21s
                        Total time: 1357.17s
                               ETA: 1784416.6s

################################################################################
                     [1m Learning iteration 76/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.495s, learning 0.178s)
               Value function loss: 12.4725
                    Surrogate loss: -0.0123
             Mean action noise std: 0.80
                       Mean reward: 232.68
               Mean episode length: 121.25
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1261568
                    Iteration time: 11.67s
                        Total time: 1368.85s
                               ETA: 1776373.4s

################################################################################
                     [1m Learning iteration 77/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.787s, learning 0.176s)
               Value function loss: 46.0643
                    Surrogate loss: -0.0017
             Mean action noise std: 0.80
                       Mean reward: 243.20
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1277952
                    Iteration time: 11.96s
                        Total time: 1380.81s
                               ETA: 1768907.1s

################################################################################
                     [1m Learning iteration 78/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.640s, learning 0.160s)
               Value function loss: 21.1143
                    Surrogate loss: -0.0055
             Mean action noise std: 0.80
                       Mean reward: 242.65
               Mean episode length: 124.68
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1294336
                    Iteration time: 11.80s
                        Total time: 1392.61s
                               ETA: 1761423.4s

################################################################################
                     [1m Learning iteration 79/100000 [0m                     

                       Computation: 1346 steps/s (collection: 12.002s, learning 0.169s)
               Value function loss: 13.0289
                    Surrogate loss: -0.0172
             Mean action noise std: 0.80
                       Mean reward: 238.22
               Mean episode length: 123.49
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1310720
                    Iteration time: 12.17s
                        Total time: 1404.78s
                               ETA: 1754589.6s

################################################################################
                     [1m Learning iteration 80/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.479s, learning 0.166s)
               Value function loss: 146.1086
                    Surrogate loss: -0.0063
             Mean action noise std: 0.80
                       Mean reward: 244.38
               Mean episode length: 125.00
                  Mean reward/step: 1.83
       Mean episode length/episode: 5.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1327104
                    Iteration time: 11.64s
                        Total time: 1416.43s
                               ETA: 1747275.5s

################################################################################
                     [1m Learning iteration 81/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.371s, learning 0.156s)
               Value function loss: 31.7601
                    Surrogate loss: -0.0209
             Mean action noise std: 0.80
                       Mean reward: 241.97
               Mean episode length: 123.92
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1343488
                    Iteration time: 11.53s
                        Total time: 1427.95s
                               ETA: 1739995.9s

################################################################################
                     [1m Learning iteration 82/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.813s, learning 0.201s)
               Value function loss: 24.4614
                    Surrogate loss: -0.0190
             Mean action noise std: 0.80
                       Mean reward: 236.67
               Mean episode length: 121.70
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1359872
                    Iteration time: 12.01s
                        Total time: 1439.97s
                               ETA: 1733477.6s

################################################################################
                     [1m Learning iteration 83/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.406s, learning 0.177s)
               Value function loss: 21.8863
                    Surrogate loss: -0.0189
             Mean action noise std: 0.80
                       Mean reward: 236.35
               Mean episode length: 120.92
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1376256
                    Iteration time: 12.58s
                        Total time: 1452.55s
                               ETA: 1727791.3s

################################################################################
                     [1m Learning iteration 84/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.066s, learning 0.208s)
               Value function loss: 18.8177
                    Surrogate loss: -0.0187
             Mean action noise std: 0.80
                       Mean reward: 241.59
               Mean episode length: 124.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1392640
                    Iteration time: 12.27s
                        Total time: 1464.82s
                               ETA: 1721875.3s

################################################################################
                     [1m Learning iteration 85/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.989s, learning 0.176s)
               Value function loss: 13.7906
                    Surrogate loss: -0.0169
             Mean action noise std: 0.80
                       Mean reward: 236.14
               Mean episode length: 121.09
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1409024
                    Iteration time: 12.16s
                        Total time: 1476.99s
                               ETA: 1715968.7s

################################################################################
                     [1m Learning iteration 86/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.759s, learning 0.215s)
               Value function loss: 13.4268
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: 236.61
               Mean episode length: 122.32
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1425408
                    Iteration time: 11.97s
                        Total time: 1488.96s
                               ETA: 1709978.7s

################################################################################
                     [1m Learning iteration 87/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.790s, learning 0.170s)
               Value function loss: 10.7156
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: 236.11
               Mean episode length: 122.22
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1441792
                    Iteration time: 11.96s
                        Total time: 1500.92s
                               ETA: 1704109.7s

################################################################################
                     [1m Learning iteration 88/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.696s, learning 0.228s)
               Value function loss: 12.6392
                    Surrogate loss: -0.0137
             Mean action noise std: 0.80
                       Mean reward: 239.95
               Mean episode length: 122.57
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1458176
                    Iteration time: 11.92s
                        Total time: 1512.85s
                               ETA: 1698330.5s

################################################################################
                     [1m Learning iteration 89/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.995s, learning 0.177s)
               Value function loss: 8.6087
                    Surrogate loss: -0.0098
             Mean action noise std: 0.80
                       Mean reward: 242.44
               Mean episode length: 123.24
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1474560
                    Iteration time: 12.17s
                        Total time: 1525.02s
                               ETA: 1692955.8s

################################################################################
                     [1m Learning iteration 90/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.079s, learning 0.170s)
               Value function loss: 7.9418
                    Surrogate loss: -0.0132
             Mean action noise std: 0.80
                       Mean reward: 245.14
               Mean episode length: 124.29
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1490944
                    Iteration time: 12.25s
                        Total time: 1537.27s
                               ETA: 1687783.0s

################################################################################
                     [1m Learning iteration 91/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.217s, learning 0.196s)
               Value function loss: 6.9984
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 240.54
               Mean episode length: 122.78
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1507328
                    Iteration time: 12.41s
                        Total time: 1549.68s
                               ETA: 1682900.8s

################################################################################
                     [1m Learning iteration 92/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.740s, learning 0.206s)
               Value function loss: 6.1525
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: 240.16
               Mean episode length: 122.78
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1523712
                    Iteration time: 11.95s
                        Total time: 1561.63s
                               ETA: 1677622.0s

################################################################################
                     [1m Learning iteration 93/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.570s, learning 0.176s)
               Value function loss: 77.9289
                    Surrogate loss: 0.0076
             Mean action noise std: 0.80
                       Mean reward: 246.66
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1540096
                    Iteration time: 11.75s
                        Total time: 1573.37s
                               ETA: 1672242.0s

################################################################################
                     [1m Learning iteration 94/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.021s, learning 0.211s)
               Value function loss: 7.7769
                    Surrogate loss: -0.0136
             Mean action noise std: 0.80
                       Mean reward: 246.37
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1556480
                    Iteration time: 12.23s
                        Total time: 1585.60s
                               ETA: 1667486.6s

################################################################################
                     [1m Learning iteration 95/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.021s, learning 0.163s)
               Value function loss: 98.1921
                    Surrogate loss: -0.0077
             Mean action noise std: 0.80
                       Mean reward: 249.03
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1572864
                    Iteration time: 12.18s
                        Total time: 1597.79s
                               ETA: 1662780.5s

################################################################################
                     [1m Learning iteration 96/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.750s, learning 0.174s)
               Value function loss: 94.7092
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 249.51
               Mean episode length: 125.00
                  Mean reward/step: 1.84
       Mean episode length/episode: 6.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1589248
                    Iteration time: 11.92s
                        Total time: 1609.71s
                               ETA: 1657903.3s

################################################################################
                     [1m Learning iteration 97/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.521s, learning 0.164s)
               Value function loss: 30.6801
                    Surrogate loss: -0.0183
             Mean action noise std: 0.80
                       Mean reward: 246.38
               Mean episode length: 123.93
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1605632
                    Iteration time: 11.68s
                        Total time: 1621.40s
                               ETA: 1652881.0s

################################################################################
                     [1m Learning iteration 98/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.766s, learning 0.199s)
               Value function loss: 34.0259
                    Surrogate loss: -0.0148
             Mean action noise std: 0.80
                       Mean reward: 248.54
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1622016
                    Iteration time: 11.97s
                        Total time: 1633.36s
                               ETA: 1648243.6s

################################################################################
                     [1m Learning iteration 99/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.917s, learning 0.162s)
               Value function loss: 30.6077
                    Surrogate loss: -0.0094
             Mean action noise std: 0.80
                       Mean reward: 241.29
               Mean episode length: 121.67
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1638400
                    Iteration time: 12.08s
                        Total time: 1645.44s
                               ETA: 1643812.6s

################################################################################
                    [1m Learning iteration 100/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.545s, learning 0.166s)
               Value function loss: 26.0465
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                       Mean reward: 243.23
               Mean episode length: 122.13
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1654784
                    Iteration time: 11.71s
                        Total time: 1657.15s
                               ETA: 1639105.2s

################################################################################
                    [1m Learning iteration 101/100000 [0m                     

                       Computation: 1313 steps/s (collection: 12.264s, learning 0.205s)
               Value function loss: 17.6880
                    Surrogate loss: -0.0179
             Mean action noise std: 0.80
                       Mean reward: 245.77
               Mean episode length: 123.53
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1671168
                    Iteration time: 12.47s
                        Total time: 1669.62s
                               ETA: 1635231.6s

################################################################################
                    [1m Learning iteration 102/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.527s, learning 0.161s)
               Value function loss: 17.9916
                    Surrogate loss: -0.0156
             Mean action noise std: 0.80
                       Mean reward: 240.67
               Mean episode length: 120.53
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1687552
                    Iteration time: 11.69s
                        Total time: 1681.31s
                               ETA: 1630675.3s

################################################################################
                    [1m Learning iteration 103/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.825s, learning 0.192s)
               Value function loss: 12.1098
                    Surrogate loss: -0.0122
             Mean action noise std: 0.80
                       Mean reward: 241.56
               Mean episode length: 121.30
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1703936
                    Iteration time: 12.02s
                        Total time: 1693.33s
                               ETA: 1626522.1s

################################################################################
                    [1m Learning iteration 104/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.321s, learning 0.274s)
               Value function loss: 16.5696
                    Surrogate loss: -0.0126
             Mean action noise std: 0.80
                       Mean reward: 247.73
               Mean episode length: 123.62
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1720320
                    Iteration time: 11.60s
                        Total time: 1704.92s
                               ETA: 1622046.9s

################################################################################
                    [1m Learning iteration 105/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.834s, learning 0.174s)
               Value function loss: 7.4560
                    Surrogate loss: -0.0104
             Mean action noise std: 0.80
                       Mean reward: 248.52
               Mean episode length: 124.04
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1736704
                    Iteration time: 12.01s
                        Total time: 1716.93s
                               ETA: 1618044.7s

################################################################################
                    [1m Learning iteration 106/100000 [0m                     

                       Computation: 1296 steps/s (collection: 12.378s, learning 0.262s)
               Value function loss: 11.7687
                    Surrogate loss: -0.0167
             Mean action noise std: 0.80
                       Mean reward: 245.59
               Mean episode length: 122.56
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1753088
                    Iteration time: 12.64s
                        Total time: 1729.57s
                               ETA: 1614706.9s

################################################################################
                    [1m Learning iteration 107/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.132s, learning 0.161s)
               Value function loss: 7.9625
                    Surrogate loss: -0.0130
             Mean action noise std: 0.80
                       Mean reward: 243.88
               Mean episode length: 121.93
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1769472
                    Iteration time: 12.29s
                        Total time: 1741.86s
                               ETA: 1611110.5s

################################################################################
                    [1m Learning iteration 108/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.938s, learning 0.170s)
               Value function loss: 6.6714
                    Surrogate loss: -0.0119
             Mean action noise std: 0.80
                       Mean reward: 240.69
               Mean episode length: 120.88
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1785856
                    Iteration time: 12.11s
                        Total time: 1753.97s
                               ETA: 1607409.7s

################################################################################
                    [1m Learning iteration 109/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.819s, learning 0.165s)
               Value function loss: 64.5844
                    Surrogate loss: -0.0050
             Mean action noise std: 0.80
                       Mean reward: 247.94
               Mean episode length: 124.60
                  Mean reward/step: 2.00
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1802240
                    Iteration time: 11.98s
                        Total time: 1765.95s
                               ETA: 1603663.6s

################################################################################
                    [1m Learning iteration 110/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.998s, learning 0.172s)
               Value function loss: 9.6143
                    Surrogate loss: -0.0057
             Mean action noise std: 0.80
                       Mean reward: 243.50
               Mean episode length: 123.18
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1818624
                    Iteration time: 12.17s
                        Total time: 1778.12s
                               ETA: 1600151.9s

################################################################################
                    [1m Learning iteration 111/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.588s, learning 0.187s)
               Value function loss: 188.5604
                    Surrogate loss: 0.0180
             Mean action noise std: 0.80
                       Mean reward: 251.98
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.06
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1835008
                    Iteration time: 11.78s
                        Total time: 1789.90s
                               ETA: 1596350.7s

################################################################################
                    [1m Learning iteration 112/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.304s, learning 0.161s)
               Value function loss: 26.0837
                    Surrogate loss: -0.0161
             Mean action noise std: 0.80
                       Mean reward: 253.14
               Mean episode length: 125.00
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1851392
                    Iteration time: 12.47s
                        Total time: 1802.36s
                               ETA: 1593226.5s

################################################################################
                    [1m Learning iteration 113/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.024s, learning 0.161s)
               Value function loss: 20.9540
                    Surrogate loss: -0.0231
             Mean action noise std: 0.80
                       Mean reward: 249.82
               Mean episode length: 123.25
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1867776
                    Iteration time: 12.18s
                        Total time: 1814.55s
                               ETA: 1589911.3s

################################################################################
                    [1m Learning iteration 114/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.947s, learning 0.255s)
               Value function loss: 22.8102
                    Surrogate loss: -0.0083
             Mean action noise std: 0.80
                       Mean reward: 244.37
               Mean episode length: 121.91
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1884160
                    Iteration time: 12.20s
                        Total time: 1826.75s
                               ETA: 1586667.9s

################################################################################
                    [1m Learning iteration 115/100000 [0m                     

                       Computation: 1303 steps/s (collection: 12.414s, learning 0.160s)
               Value function loss: 24.7612
                    Surrogate loss: -0.0144
             Mean action noise std: 0.80
                       Mean reward: 252.71
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1900544
                    Iteration time: 12.57s
                        Total time: 1839.32s
                               ETA: 1583801.1s

################################################################################
                    [1m Learning iteration 116/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.541s, learning 0.205s)
               Value function loss: 18.1553
                    Surrogate loss: -0.0189
             Mean action noise std: 0.80
                       Mean reward: 251.27
               Mean episode length: 123.52
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1916928
                    Iteration time: 11.75s
                        Total time: 1851.07s
                               ETA: 1580275.8s

################################################################################
                    [1m Learning iteration 117/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.835s, learning 0.196s)
               Value function loss: 16.5746
                    Surrogate loss: -0.0176
             Mean action noise std: 0.80
                       Mean reward: 251.20
               Mean episode length: 123.48
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1933312
                    Iteration time: 12.03s
                        Total time: 1863.10s
                               ETA: 1577052.2s

################################################################################
                    [1m Learning iteration 118/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.683s, learning 0.168s)
               Value function loss: 14.1194
                    Surrogate loss: -0.0156
             Mean action noise std: 0.80
                       Mean reward: 255.30
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1949696
                    Iteration time: 11.85s
                        Total time: 1874.95s
                               ETA: 1573731.3s

################################################################################
                    [1m Learning iteration 119/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.545s, learning 0.181s)
               Value function loss: 13.5296
                    Surrogate loss: -0.0128
             Mean action noise std: 0.80
                       Mean reward: 254.01
               Mean episode length: 124.58
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1966080
                    Iteration time: 11.73s
                        Total time: 1886.68s
                               ETA: 1570361.0s

################################################################################
                    [1m Learning iteration 120/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.905s, learning 0.279s)
               Value function loss: 11.1730
                    Surrogate loss: -0.0131
             Mean action noise std: 0.80
                       Mean reward: 253.52
               Mean episode length: 124.58
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1982464
                    Iteration time: 12.18s
                        Total time: 1898.86s
                               ETA: 1567424.9s

################################################################################
                    [1m Learning iteration 121/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.784s, learning 0.298s)
               Value function loss: 9.9662
                    Surrogate loss: -0.0086
             Mean action noise std: 0.80
                       Mean reward: 251.08
               Mean episode length: 123.67
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 1998848
                    Iteration time: 12.08s
                        Total time: 1910.94s
                               ETA: 1564452.8s

################################################################################
                    [1m Learning iteration 122/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.953s, learning 0.199s)
               Value function loss: 7.9976
                    Surrogate loss: -0.0045
             Mean action noise std: 0.80
                       Mean reward: 252.12
               Mean episode length: 123.67
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2015232
                    Iteration time: 12.15s
                        Total time: 1923.10s
                               ETA: 1561585.7s

################################################################################
                    [1m Learning iteration 123/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.640s, learning 0.163s)
               Value function loss: 9.9567
                    Surrogate loss: -0.0102
             Mean action noise std: 0.80
                       Mean reward: 251.05
               Mean episode length: 123.37
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2031616
                    Iteration time: 11.80s
                        Total time: 1934.90s
                               ETA: 1558483.9s

################################################################################
                    [1m Learning iteration 124/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.654s, learning 0.161s)
               Value function loss: 110.6413
                    Surrogate loss: 0.0252
             Mean action noise std: 0.80
                       Mean reward: 254.43
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2048000
                    Iteration time: 11.81s
                        Total time: 1946.71s
                               ETA: 1555440.7s

################################################################################
                    [1m Learning iteration 125/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.574s, learning 0.310s)
               Value function loss: 10.2833
                    Surrogate loss: -0.0107
             Mean action noise std: 0.80
                       Mean reward: 253.15
               Mean episode length: 123.95
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2064384
                    Iteration time: 11.88s
                        Total time: 1958.60s
                               ETA: 1552500.9s

################################################################################
                    [1m Learning iteration 126/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.099s, learning 0.193s)
               Value function loss: 23.7088
                    Surrogate loss: -0.0125
             Mean action noise std: 0.80
                       Mean reward: 252.06
               Mean episode length: 123.95
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2080768
                    Iteration time: 12.29s
                        Total time: 1970.89s
                               ETA: 1549927.5s

################################################################################
                    [1m Learning iteration 127/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.432s, learning 0.164s)
               Value function loss: 150.7797
                    Surrogate loss: 0.0116
             Mean action noise std: 0.80
                       Mean reward: 254.90
               Mean episode length: 125.00
                  Mean reward/step: 1.92
       Mean episode length/episode: 5.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2097152
                    Iteration time: 11.60s
                        Total time: 1982.49s
                               ETA: 1546851.1s

################################################################################
                    [1m Learning iteration 128/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.450s, learning 0.183s)
               Value function loss: 24.0813
                    Surrogate loss: -0.0189
             Mean action noise std: 0.80
                       Mean reward: 255.57
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2113536
                    Iteration time: 11.63s
                        Total time: 1994.12s
                               ETA: 1543851.0s

################################################################################
                    [1m Learning iteration 129/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.691s, learning 0.189s)
               Value function loss: 25.9785
                    Surrogate loss: -0.0152
             Mean action noise std: 0.80
                       Mean reward: 251.16
               Mean episode length: 122.86
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2129920
                    Iteration time: 11.88s
                        Total time: 2006.00s
                               ETA: 1541086.1s

################################################################################
                    [1m Learning iteration 130/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.984s, learning 0.159s)
               Value function loss: 27.4214
                    Surrogate loss: -0.0115
             Mean action noise std: 0.80
                       Mean reward: 255.46
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2146304
                    Iteration time: 12.14s
                        Total time: 2018.14s
                               ETA: 1538564.0s

################################################################################
                    [1m Learning iteration 131/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.659s, learning 0.222s)
               Value function loss: 24.6490
                    Surrogate loss: -0.0119
             Mean action noise std: 0.80
                       Mean reward: 256.57
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2162688
                    Iteration time: 11.88s
                        Total time: 2030.02s
                               ETA: 1535882.1s

################################################################################
                    [1m Learning iteration 132/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.625s, learning 0.165s)
               Value function loss: 17.8898
                    Surrogate loss: -0.0157
             Mean action noise std: 0.80
                       Mean reward: 254.55
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2179072
                    Iteration time: 11.79s
                        Total time: 2041.81s
                               ETA: 1533171.2s

################################################################################
                    [1m Learning iteration 133/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.748s, learning 0.173s)
               Value function loss: 17.9968
                    Surrogate loss: -0.0172
             Mean action noise std: 0.80
                       Mean reward: 256.26
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2195456
                    Iteration time: 11.92s
                        Total time: 2053.73s
                               ETA: 1530598.7s

################################################################################
                    [1m Learning iteration 134/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.969s, learning 0.197s)
               Value function loss: 12.4094
                    Surrogate loss: -0.0097
             Mean action noise std: 0.80
                       Mean reward: 249.63
               Mean episode length: 122.27
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2211840
                    Iteration time: 12.17s
                        Total time: 2065.90s
                               ETA: 1528245.8s

################################################################################
                    [1m Learning iteration 135/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.122s, learning 0.160s)
               Value function loss: 16.8486
                    Surrogate loss: -0.0087
             Mean action noise std: 0.80
                       Mean reward: 252.24
               Mean episode length: 123.24
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2228224
                    Iteration time: 12.28s
                        Total time: 2078.18s
                               ETA: 1526011.7s

################################################################################
                    [1m Learning iteration 136/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.955s, learning 0.174s)
               Value function loss: 9.8376
                    Surrogate loss: -0.0066
             Mean action noise std: 0.80
                       Mean reward: 253.74
               Mean episode length: 124.95
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2244608
                    Iteration time: 12.13s
                        Total time: 2090.31s
                               ETA: 1523699.3s

################################################################################
                    [1m Learning iteration 137/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.757s, learning 0.236s)
               Value function loss: 9.3263
                    Surrogate loss: -0.0114
             Mean action noise std: 0.80
                       Mean reward: 253.48
               Mean episode length: 124.95
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2260992
                    Iteration time: 11.99s
                        Total time: 2102.30s
                               ETA: 1521321.6s

################################################################################
                    [1m Learning iteration 138/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.877s, learning 0.163s)
               Value function loss: 11.0371
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 250.77
               Mean episode length: 124.45
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2277376
                    Iteration time: 12.04s
                        Total time: 2114.34s
                               ETA: 1519011.4s

################################################################################
                    [1m Learning iteration 139/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.807s, learning 0.164s)
               Value function loss: 6.4949
                    Surrogate loss: -0.0023
             Mean action noise std: 0.80
                       Mean reward: 251.77
               Mean episode length: 124.45
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2293760
                    Iteration time: 11.97s
                        Total time: 2126.31s
                               ETA: 1516684.7s

################################################################################
                    [1m Learning iteration 140/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.349s, learning 0.229s)
               Value function loss: 83.2046
                    Surrogate loss: 0.0139
             Mean action noise std: 0.80
                       Mean reward: 253.76
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2310144
                    Iteration time: 11.58s
                        Total time: 2137.89s
                               ETA: 1514112.4s

################################################################################
                    [1m Learning iteration 141/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.047s, learning 0.235s)
               Value function loss: 10.1763
                    Surrogate loss: -0.0124
             Mean action noise std: 0.80
                       Mean reward: 252.59
               Mean episode length: 123.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2326528
                    Iteration time: 12.28s
                        Total time: 2150.17s
                               ETA: 1512072.0s

################################################################################
                    [1m Learning iteration 142/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.735s, learning 0.176s)
               Value function loss: 155.7956
                    Surrogate loss: 0.0120
             Mean action noise std: 0.80
                       Mean reward: 254.56
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 6.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2342912
                    Iteration time: 11.91s
                        Total time: 2162.09s
                               ETA: 1509800.9s

################################################################################
                    [1m Learning iteration 143/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.624s, learning 0.164s)
               Value function loss: 59.2612
                    Surrogate loss: -0.0136
             Mean action noise std: 0.80
                       Mean reward: 256.98
               Mean episode length: 125.00
                  Mean reward/step: 1.90
       Mean episode length/episode: 6.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2359296
                    Iteration time: 11.79s
                        Total time: 2173.87s
                               ETA: 1507475.1s

################################################################################
                    [1m Learning iteration 144/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.679s, learning 0.199s)
               Value function loss: 25.1026
                    Surrogate loss: -0.0209
             Mean action noise std: 0.80
                       Mean reward: 254.82
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2375680
                    Iteration time: 11.88s
                        Total time: 2185.75s
                               ETA: 1505243.0s

################################################################################
                    [1m Learning iteration 145/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.745s, learning 0.245s)
               Value function loss: 26.4178
                    Surrogate loss: -0.0174
             Mean action noise std: 0.80
                       Mean reward: 254.75
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2392064
                    Iteration time: 11.99s
                        Total time: 2197.74s
                               ETA: 1503118.6s

################################################################################
                    [1m Learning iteration 146/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.806s, learning 0.164s)
               Value function loss: 25.1238
                    Surrogate loss: -0.0186
             Mean action noise std: 0.80
                       Mean reward: 254.66
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2408448
                    Iteration time: 11.97s
                        Total time: 2209.71s
                               ETA: 1501009.6s

################################################################################
                    [1m Learning iteration 147/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.882s, learning 0.179s)
               Value function loss: 21.2685
                    Surrogate loss: -0.0153
             Mean action noise std: 0.80
                       Mean reward: 256.34
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2424832
                    Iteration time: 12.06s
                        Total time: 2221.77s
                               ETA: 1498989.9s

################################################################################
                    [1m Learning iteration 148/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.453s, learning 0.241s)
               Value function loss: 16.0682
                    Surrogate loss: -0.0160
             Mean action noise std: 0.80
                       Mean reward: 255.53
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2441216
                    Iteration time: 11.69s
                        Total time: 2233.47s
                               ETA: 1496751.7s

################################################################################
                    [1m Learning iteration 149/100000 [0m                     

                       Computation: 1435 steps/s (collection: 11.240s, learning 0.172s)
               Value function loss: 14.8299
                    Surrogate loss: -0.0120
             Mean action noise std: 0.80
                       Mean reward: 255.06
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2457600
                    Iteration time: 11.41s
                        Total time: 2244.88s
                               ETA: 1494355.2s

################################################################################
                    [1m Learning iteration 150/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.945s, learning 0.210s)
               Value function loss: 12.3036
                    Surrogate loss: -0.0071
             Mean action noise std: 0.80
                       Mean reward: 251.15
               Mean episode length: 123.73
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2473984
                    Iteration time: 12.15s
                        Total time: 2257.03s
                               ETA: 1492481.4s

################################################################################
                    [1m Learning iteration 151/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.714s, learning 0.157s)
               Value function loss: 14.4892
                    Surrogate loss: -0.0031
             Mean action noise std: 0.80
                       Mean reward: 251.54
               Mean episode length: 123.73
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2490368
                    Iteration time: 11.87s
                        Total time: 2268.90s
                               ETA: 1490445.6s

################################################################################
                    [1m Learning iteration 152/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.788s, learning 0.160s)
               Value function loss: 8.6449
                    Surrogate loss: -0.0021
             Mean action noise std: 0.80
                       Mean reward: 252.41
               Mean episode length: 123.85
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2506752
                    Iteration time: 11.95s
                        Total time: 2280.85s
                               ETA: 1488487.0s

################################################################################
                    [1m Learning iteration 153/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.800s, learning 0.193s)
               Value function loss: 9.7430
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 254.74
               Mean episode length: 124.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2523136
                    Iteration time: 11.99s
                        Total time: 2292.85s
                               ETA: 1486582.6s

################################################################################
                    [1m Learning iteration 154/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.689s, learning 0.274s)
               Value function loss: 8.9802
                    Surrogate loss: -0.0122
             Mean action noise std: 0.80
                       Mean reward: 252.77
               Mean episode length: 123.41
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2539520
                    Iteration time: 11.96s
                        Total time: 2304.81s
                               ETA: 1484683.1s

################################################################################
                    [1m Learning iteration 155/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.834s, learning 0.227s)
               Value function loss: 6.0863
                    Surrogate loss: -0.0120
             Mean action noise std: 0.80
                       Mean reward: 254.53
               Mean episode length: 123.95
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2555904
                    Iteration time: 12.06s
                        Total time: 2316.87s
                               ETA: 1482870.8s

################################################################################
                    [1m Learning iteration 156/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.721s, learning 0.164s)
               Value function loss: 33.4656
                    Surrogate loss: -0.0133
             Mean action noise std: 0.80
                       Mean reward: 256.11
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2572288
                    Iteration time: 11.88s
                        Total time: 2328.75s
                               ETA: 1480969.0s

################################################################################
                    [1m Learning iteration 157/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.874s, learning 0.207s)
               Value function loss: 11.8779
                    Surrogate loss: -0.0076
             Mean action noise std: 0.80
                       Mean reward: 254.54
               Mean episode length: 124.17
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2588672
                    Iteration time: 12.08s
                        Total time: 2340.84s
                               ETA: 1479215.2s

################################################################################
                    [1m Learning iteration 158/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.634s, learning 0.164s)
               Value function loss: 191.8676
                    Surrogate loss: 0.0272
             Mean action noise std: 0.80
                       Mean reward: 256.91
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 5.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2605056
                    Iteration time: 11.80s
                        Total time: 2352.63s
                               ETA: 1477305.8s

################################################################################
                    [1m Learning iteration 159/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.013s, learning 0.164s)
               Value function loss: 21.9298
                    Surrogate loss: -0.0152
             Mean action noise std: 0.80
                       Mean reward: 258.74
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2621440
                    Iteration time: 12.18s
                        Total time: 2364.81s
                               ETA: 1475656.5s

################################################################################
                    [1m Learning iteration 160/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.977s, learning 0.166s)
               Value function loss: 18.6949
                    Surrogate loss: -0.0210
             Mean action noise std: 0.80
                       Mean reward: 258.72
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2637824
                    Iteration time: 12.14s
                        Total time: 2376.95s
                               ETA: 1474006.7s

################################################################################
                    [1m Learning iteration 161/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.938s, learning 0.222s)
               Value function loss: 23.1762
                    Surrogate loss: -0.0196
             Mean action noise std: 0.80
                       Mean reward: 258.91
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2654208
                    Iteration time: 12.16s
                        Total time: 2389.11s
                               ETA: 1472386.9s

################################################################################
                    [1m Learning iteration 162/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.035s, learning 0.189s)
               Value function loss: 24.6700
                    Surrogate loss: -0.0185
             Mean action noise std: 0.80
                       Mean reward: 257.05
               Mean episode length: 124.14
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2670592
                    Iteration time: 12.22s
                        Total time: 2401.34s
                               ETA: 1470826.3s

################################################################################
                    [1m Learning iteration 163/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.137s, learning 0.163s)
               Value function loss: 15.1858
                    Surrogate loss: -0.0123
             Mean action noise std: 0.80
                       Mean reward: 256.88
               Mean episode length: 124.14
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2686976
                    Iteration time: 12.30s
                        Total time: 2413.64s
                               ETA: 1469330.8s

################################################################################
                    [1m Learning iteration 164/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.002s, learning 0.172s)
               Value function loss: 16.9337
                    Surrogate loss: -0.0108
             Mean action noise std: 0.80
                       Mean reward: 257.83
               Mean episode length: 124.20
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2703360
                    Iteration time: 12.17s
                        Total time: 2425.81s
                               ETA: 1467777.5s

################################################################################
                    [1m Learning iteration 165/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.189s, learning 0.202s)
               Value function loss: 13.5516
                    Surrogate loss: -0.0100
             Mean action noise std: 0.80
                       Mean reward: 257.99
               Mean episode length: 124.31
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2719744
                    Iteration time: 12.39s
                        Total time: 2438.20s
                               ETA: 1466373.1s

################################################################################
                    [1m Learning iteration 166/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.794s, learning 0.200s)
               Value function loss: 15.3802
                    Surrogate loss: -0.0099
             Mean action noise std: 0.80
                       Mean reward: 256.66
               Mean episode length: 123.72
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2736128
                    Iteration time: 11.99s
                        Total time: 2450.20s
                               ETA: 1464748.0s

################################################################################
                    [1m Learning iteration 167/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.065s, learning 0.318s)
               Value function loss: 11.6429
                    Surrogate loss: 0.0139
             Mean action noise std: 0.80
                       Mean reward: 258.12
               Mean episode length: 123.86
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2752512
                    Iteration time: 12.38s
                        Total time: 2462.58s
                               ETA: 1463373.5s

################################################################################
                    [1m Learning iteration 168/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.895s, learning 0.179s)
               Value function loss: 11.7948
                    Surrogate loss: -0.0044
             Mean action noise std: 0.80
                       Mean reward: 251.01
               Mean episode length: 121.84
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2768896
                    Iteration time: 12.07s
                        Total time: 2474.65s
                               ETA: 1461832.5s

################################################################################
                    [1m Learning iteration 169/100000 [0m                     

                       Computation: 1295 steps/s (collection: 12.455s, learning 0.191s)
               Value function loss: 8.1443
                    Surrogate loss: -0.0111
             Mean action noise std: 0.80
                       Mean reward: 250.74
               Mean episode length: 122.05
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2785280
                    Iteration time: 12.65s
                        Total time: 2487.30s
                               ETA: 1460645.6s

################################################################################
                    [1m Learning iteration 170/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.047s, learning 0.166s)
               Value function loss: 8.2299
                    Surrogate loss: -0.0087
             Mean action noise std: 0.80
                       Mean reward: 251.77
               Mean episode length: 122.60
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2801664
                    Iteration time: 12.21s
                        Total time: 2499.51s
                               ETA: 1459219.4s

################################################################################
                    [1m Learning iteration 171/100000 [0m                     

                       Computation: 1302 steps/s (collection: 12.378s, learning 0.204s)
               Value function loss: 92.3476
                    Surrogate loss: -0.0012
             Mean action noise std: 0.80
                       Mean reward: 259.99
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2818048
                    Iteration time: 12.58s
                        Total time: 2512.10s
                               ETA: 1458023.6s

################################################################################
                    [1m Learning iteration 172/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.579s, learning 0.159s)
               Value function loss: 10.2300
                    Surrogate loss: -0.0049
             Mean action noise std: 0.80
                       Mean reward: 256.41
               Mean episode length: 123.98
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2834432
                    Iteration time: 11.74s
                        Total time: 2523.83s
                               ETA: 1456354.8s

################################################################################
                    [1m Learning iteration 173/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.656s, learning 0.170s)
               Value function loss: 59.6146
                    Surrogate loss: 0.0058
             Mean action noise std: 0.80
                       Mean reward: 263.44
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2850816
                    Iteration time: 11.83s
                        Total time: 2535.66s
                               ETA: 1454755.4s

################################################################################
                    [1m Learning iteration 174/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.933s, learning 0.188s)
               Value function loss: 129.3709
                    Surrogate loss: 0.0097
             Mean action noise std: 0.80
                       Mean reward: 260.91
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2867200
                    Iteration time: 12.12s
                        Total time: 2547.78s
                               ETA: 1453342.7s

################################################################################
                    [1m Learning iteration 175/100000 [0m                     

                       Computation: 1339 steps/s (collection: 11.969s, learning 0.260s)
               Value function loss: 24.7608
                    Surrogate loss: -0.0144
             Mean action noise std: 0.80
                       Mean reward: 255.29
               Mean episode length: 122.98
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2883584
                    Iteration time: 12.23s
                        Total time: 2560.01s
                               ETA: 1452006.8s

################################################################################
                    [1m Learning iteration 176/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.938s, learning 0.163s)
               Value function loss: 25.3153
                    Surrogate loss: -0.0183
             Mean action noise std: 0.80
                       Mean reward: 257.19
               Mean episode length: 122.83
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2899968
                    Iteration time: 12.10s
                        Total time: 2572.11s
                               ETA: 1450613.7s

################################################################################
                    [1m Learning iteration 177/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.824s, learning 0.164s)
               Value function loss: 25.5575
                    Surrogate loss: -0.0182
             Mean action noise std: 0.80
                       Mean reward: 263.53
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2916352
                    Iteration time: 11.99s
                        Total time: 2584.10s
                               ETA: 1449172.9s

################################################################################
                    [1m Learning iteration 178/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.748s, learning 0.166s)
               Value function loss: 22.9161
                    Surrogate loss: -0.0114
             Mean action noise std: 0.80
                       Mean reward: 262.44
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2932736
                    Iteration time: 11.91s
                        Total time: 2596.02s
                               ETA: 1447706.4s

################################################################################
                    [1m Learning iteration 179/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.433s, learning 0.206s)
               Value function loss: 16.7668
                    Surrogate loss: -0.0163
             Mean action noise std: 0.80
                       Mean reward: 259.98
               Mean episode length: 124.10
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2949120
                    Iteration time: 11.64s
                        Total time: 2607.65s
                               ETA: 1446103.9s

################################################################################
                    [1m Learning iteration 180/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.841s, learning 0.207s)
               Value function loss: 15.8188
                    Surrogate loss: -0.0164
             Mean action noise std: 0.80
                       Mean reward: 257.41
               Mean episode length: 123.36
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2965504
                    Iteration time: 12.05s
                        Total time: 2619.70s
                               ETA: 1444744.3s

################################################################################
                    [1m Learning iteration 181/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.951s, learning 0.226s)
               Value function loss: 13.9088
                    Surrogate loss: -0.0072
             Mean action noise std: 0.80
                       Mean reward: 260.92
               Mean episode length: 124.30
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2981888
                    Iteration time: 12.18s
                        Total time: 2631.88s
                               ETA: 1443470.1s

################################################################################
                    [1m Learning iteration 182/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.360s, learning 0.162s)
               Value function loss: 17.0514
                    Surrogate loss: -0.0132
             Mean action noise std: 0.80
                       Mean reward: 260.49
               Mean episode length: 124.28
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 2998272
                    Iteration time: 11.52s
                        Total time: 2643.40s
                               ETA: 1441852.6s

################################################################################
                    [1m Learning iteration 183/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.259s, learning 0.326s)
               Value function loss: 8.7236
                    Surrogate loss: -0.0118
             Mean action noise std: 0.80
                       Mean reward: 260.03
               Mean episode length: 123.84
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3014656
                    Iteration time: 12.58s
                        Total time: 2655.99s
                               ETA: 1440829.0s

################################################################################
                    [1m Learning iteration 184/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.098s, learning 0.292s)
               Value function loss: 10.9802
                    Surrogate loss: -0.0078
             Mean action noise std: 0.80
                       Mean reward: 259.90
               Mean episode length: 124.13
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3031040
                    Iteration time: 12.39s
                        Total time: 2668.38s
                               ETA: 1439711.5s

################################################################################
                    [1m Learning iteration 185/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.683s, learning 0.210s)
               Value function loss: 9.7489
                    Surrogate loss: -0.0046
             Mean action noise std: 0.80
                       Mean reward: 258.63
               Mean episode length: 124.13
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3047424
                    Iteration time: 11.89s
                        Total time: 2680.27s
                               ETA: 1438339.1s

################################################################################
                    [1m Learning iteration 186/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.754s, learning 0.171s)
               Value function loss: 6.0539
                    Surrogate loss: 0.0122
             Mean action noise std: 0.80
                       Mean reward: 255.52
               Mean episode length: 122.79
                  Mean reward/step: 2.12
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3063808
                    Iteration time: 11.92s
                        Total time: 2692.19s
                               ETA: 1436998.2s

################################################################################
                    [1m Learning iteration 187/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.754s, learning 0.229s)
               Value function loss: 69.6478
                    Surrogate loss: 0.0425
             Mean action noise std: 0.80
                       Mean reward: 261.99
               Mean episode length: 124.87
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3080192
                    Iteration time: 11.98s
                        Total time: 2704.18s
                               ETA: 1435702.4s

################################################################################
                    [1m Learning iteration 188/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.056s, learning 0.242s)
               Value function loss: 10.6036
                    Surrogate loss: -0.0094
             Mean action noise std: 0.80
                       Mean reward: 260.42
               Mean episode length: 124.77
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3096576
                    Iteration time: 12.30s
                        Total time: 2716.48s
                               ETA: 1434586.6s

################################################################################
                    [1m Learning iteration 189/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.856s, learning 0.161s)
               Value function loss: 197.0594
                    Surrogate loss: 0.0019
             Mean action noise std: 0.80
                       Mean reward: 262.92
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 6.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3112960
                    Iteration time: 12.02s
                        Total time: 2728.49s
                               ETA: 1433334.4s

################################################################################
                    [1m Learning iteration 190/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.019s, learning 0.268s)
               Value function loss: 33.4046
                    Surrogate loss: -0.0033
             Mean action noise std: 0.80
                       Mean reward: 261.65
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3129344
                    Iteration time: 12.29s
                        Total time: 2740.78s
                               ETA: 1432236.7s

################################################################################
                    [1m Learning iteration 191/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.169s)
               Value function loss: 20.2452
                    Surrogate loss: -0.0067
             Mean action noise std: 0.80
                       Mean reward: 260.85
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3145728
                    Iteration time: 12.21s
                        Total time: 2752.99s
                               ETA: 1431110.1s

################################################################################
                    [1m Learning iteration 192/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.899s, learning 0.236s)
               Value function loss: 21.3815
                    Surrogate loss: -0.0130
             Mean action noise std: 0.80
                       Mean reward: 260.75
               Mean episode length: 123.96
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3162112
                    Iteration time: 12.13s
                        Total time: 2765.12s
                               ETA: 1429956.0s

################################################################################
                    [1m Learning iteration 193/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.226s, learning 0.173s)
               Value function loss: 20.4301
                    Surrogate loss: -0.0143
             Mean action noise std: 0.80
                       Mean reward: 262.65
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3178496
                    Iteration time: 12.40s
                        Total time: 2777.52s
                               ETA: 1428949.8s

################################################################################
                    [1m Learning iteration 194/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.802s, learning 0.159s)
               Value function loss: 20.1052
                    Surrogate loss: -0.0116
             Mean action noise std: 0.80
                       Mean reward: 259.99
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3194880
                    Iteration time: 11.96s
                        Total time: 2789.48s
                               ETA: 1427729.6s

################################################################################
                    [1m Learning iteration 195/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.907s, learning 0.282s)
               Value function loss: 14.5356
                    Surrogate loss: -0.0130
             Mean action noise std: 0.80
                       Mean reward: 261.71
               Mean episode length: 125.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3211264
                    Iteration time: 12.19s
                        Total time: 2801.67s
                               ETA: 1426637.7s

################################################################################
                    [1m Learning iteration 196/100000 [0m                     

                       Computation: 1410 steps/s (collection: 11.441s, learning 0.173s)
               Value function loss: 14.7190
                    Surrogate loss: -0.0028
             Mean action noise std: 0.80
                       Mean reward: 260.93
               Mean episode length: 125.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3227648
                    Iteration time: 11.61s
                        Total time: 2813.29s
                               ETA: 1425265.8s

################################################################################
                    [1m Learning iteration 197/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.657s, learning 0.162s)
               Value function loss: 9.8993
                    Surrogate loss: -0.0066
             Mean action noise std: 0.80
                       Mean reward: 261.43
               Mean episode length: 125.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3244032
                    Iteration time: 11.82s
                        Total time: 2825.11s
                               ETA: 1424010.7s

################################################################################
                    [1m Learning iteration 198/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.855s, learning 0.257s)
               Value function loss: 11.2183
                    Surrogate loss: -0.0075
             Mean action noise std: 0.80
                       Mean reward: 261.79
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3260416
                    Iteration time: 12.11s
                        Total time: 2837.22s
                               ETA: 1422914.7s

################################################################################
                    [1m Learning iteration 199/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.624s, learning 0.269s)
               Value function loss: 9.5229
                    Surrogate loss: -0.0057
             Mean action noise std: 0.80
                       Mean reward: 261.32
               Mean episode length: 125.00
                  Mean reward/step: 2.11
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3276800
                    Iteration time: 11.89s
                        Total time: 2849.11s
                               ETA: 1421720.6s

################################################################################
                    [1m Learning iteration 200/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.804s, learning 0.171s)
               Value function loss: 8.4517
                    Surrogate loss: -0.0059
             Mean action noise std: 0.80
                       Mean reward: 262.06
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3293184
                    Iteration time: 11.97s
                        Total time: 2861.09s
                               ETA: 1420578.6s

################################################################################
                    [1m Learning iteration 201/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.783s, learning 0.307s)
               Value function loss: 8.3865
                    Surrogate loss: -0.0020
             Mean action noise std: 0.80
                       Mean reward: 262.86
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3309568
                    Iteration time: 12.09s
                        Total time: 2873.18s
                               ETA: 1419505.3s

################################################################################
                    [1m Learning iteration 202/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.698s, learning 0.192s)
               Value function loss: 87.0432
                    Surrogate loss: 0.0011
             Mean action noise std: 0.80
                       Mean reward: 257.44
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3325952
                    Iteration time: 11.89s
                        Total time: 2885.07s
                               ETA: 1418344.0s

################################################################################
                    [1m Learning iteration 203/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.640s, learning 0.177s)
               Value function loss: 12.7051
                    Surrogate loss: -0.0076
             Mean action noise std: 0.80
                       Mean reward: 259.14
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3342336
                    Iteration time: 11.82s
                        Total time: 2896.88s
                               ETA: 1417158.0s

################################################################################
                    [1m Learning iteration 204/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.922s, learning 0.180s)
               Value function loss: 10.7804
                    Surrogate loss: -0.0066
             Mean action noise std: 0.80
                       Mean reward: 261.40
               Mean episode length: 124.93
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3358720
                    Iteration time: 12.10s
                        Total time: 2908.99s
                               ETA: 1416122.4s

################################################################################
                    [1m Learning iteration 205/100000 [0m                     

                       Computation: 1426 steps/s (collection: 11.319s, learning 0.169s)
               Value function loss: 161.8174
                    Surrogate loss: 0.0078
             Mean action noise std: 0.79
                       Mean reward: 260.95
               Mean episode length: 124.87
                  Mean reward/step: 1.99
       Mean episode length/episode: 5.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3375104
                    Iteration time: 11.49s
                        Total time: 2920.47s
                               ETA: 1414799.2s

################################################################################
                    [1m Learning iteration 206/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.876s, learning 0.168s)
               Value function loss: 20.7204
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 262.59
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3391488
                    Iteration time: 12.04s
                        Total time: 2932.52s
                               ETA: 1413756.6s

################################################################################
                    [1m Learning iteration 207/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.943s, learning 0.159s)
               Value function loss: 23.7912
                    Surrogate loss: -0.0179
             Mean action noise std: 0.79
                       Mean reward: 261.43
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3407872
                    Iteration time: 12.10s
                        Total time: 2944.62s
                               ETA: 1412752.0s

################################################################################
                    [1m Learning iteration 208/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.491s, learning 0.164s)
               Value function loss: 25.3627
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 261.70
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3424256
                    Iteration time: 11.65s
                        Total time: 2956.27s
                               ETA: 1411543.3s

################################################################################
                    [1m Learning iteration 209/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.769s, learning 0.164s)
               Value function loss: 23.0594
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 261.19
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3440640
                    Iteration time: 11.93s
                        Total time: 2968.21s
                               ETA: 1410478.0s

################################################################################
                    [1m Learning iteration 210/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.626s, learning 0.194s)
               Value function loss: 14.0965
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 262.67
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3457024
                    Iteration time: 11.82s
                        Total time: 2980.03s
                               ETA: 1409369.1s

################################################################################
                    [1m Learning iteration 211/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.128s, learning 0.218s)
               Value function loss: 17.1057
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 260.72
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3473408
                    Iteration time: 12.35s
                        Total time: 2992.37s
                               ETA: 1408518.1s

################################################################################
                    [1m Learning iteration 212/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.728s, learning 0.318s)
               Value function loss: 11.4743
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 262.22
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3489792
                    Iteration time: 12.05s
                        Total time: 3004.42s
                               ETA: 1407534.6s

################################################################################
                    [1m Learning iteration 213/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.664s, learning 0.159s)
               Value function loss: 14.0157
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 260.52
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3506176
                    Iteration time: 11.82s
                        Total time: 3016.24s
                               ETA: 1406456.3s

################################################################################
                    [1m Learning iteration 214/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.953s, learning 0.169s)
               Value function loss: 8.8946
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 259.77
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3522560
                    Iteration time: 12.12s
                        Total time: 3028.36s
                               ETA: 1405526.4s

################################################################################
                    [1m Learning iteration 215/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.547s, learning 0.160s)
               Value function loss: 8.7140
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 259.81
               Mean episode length: 124.56
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3538944
                    Iteration time: 11.71s
                        Total time: 3040.07s
                               ETA: 1404413.5s

################################################################################
                    [1m Learning iteration 216/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.858s, learning 0.212s)
               Value function loss: 6.9518
                    Surrogate loss: 0.0044
             Mean action noise std: 0.79
                       Mean reward: 260.85
               Mean episode length: 124.56
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3555328
                    Iteration time: 12.07s
                        Total time: 3052.14s
                               ETA: 1403477.7s

################################################################################
                    [1m Learning iteration 217/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.569s, learning 0.192s)
               Value function loss: 8.3300
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 260.97
               Mean episode length: 124.56
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3571712
                    Iteration time: 11.76s
                        Total time: 3063.90s
                               ETA: 1402409.0s

################################################################################
                    [1m Learning iteration 218/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.939s, learning 0.157s)
               Value function loss: 73.6366
                    Surrogate loss: -0.0038
             Mean action noise std: 0.79
                       Mean reward: 261.11
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3588096
                    Iteration time: 12.10s
                        Total time: 3076.00s
                               ETA: 1401502.5s

################################################################################
                    [1m Learning iteration 219/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.782s, learning 0.270s)
               Value function loss: 11.9242
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 260.95
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3604480
                    Iteration time: 12.05s
                        Total time: 3088.05s
                               ETA: 1400584.3s

################################################################################
                    [1m Learning iteration 220/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.961s, learning 0.168s)
               Value function loss: 92.4188
                    Surrogate loss: 0.0021
             Mean action noise std: 0.79
                       Mean reward: 257.08
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3620864
                    Iteration time: 12.13s
                        Total time: 3100.18s
                               ETA: 1399708.8s

################################################################################
                    [1m Learning iteration 221/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.374s, learning 0.163s)
               Value function loss: 88.0153
                    Surrogate loss: 0.0138
             Mean action noise std: 0.79
                       Mean reward: 255.75
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3637248
                    Iteration time: 12.54s
                        Total time: 3112.71s
                               ETA: 1399024.6s

################################################################################
                    [1m Learning iteration 222/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.755s, learning 0.175s)
               Value function loss: 23.7067
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 258.01
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3653632
                    Iteration time: 11.93s
                        Total time: 3124.64s
                               ETA: 1398074.9s

################################################################################
                    [1m Learning iteration 223/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.016s, learning 0.242s)
               Value function loss: 27.1109
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 258.14
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3670016
                    Iteration time: 12.26s
                        Total time: 3136.90s
                               ETA: 1397279.6s

################################################################################
                    [1m Learning iteration 224/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.366s, learning 0.161s)
               Value function loss: 24.5836
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 259.02
               Mean episode length: 124.92
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3686400
                    Iteration time: 12.53s
                        Total time: 3149.43s
                               ETA: 1396610.3s

################################################################################
                    [1m Learning iteration 225/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.671s, learning 0.168s)
               Value function loss: 22.8771
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 259.41
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3702784
                    Iteration time: 11.84s
                        Total time: 3161.27s
                               ETA: 1395643.4s

################################################################################
                    [1m Learning iteration 226/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.692s, learning 0.156s)
               Value function loss: 15.9362
                    Surrogate loss: -0.0178
             Mean action noise std: 0.79
                       Mean reward: 258.42
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3719168
                    Iteration time: 11.85s
                        Total time: 3173.11s
                               ETA: 1394688.6s

################################################################################
                    [1m Learning iteration 227/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.859s, learning 0.172s)
               Value function loss: 14.3488
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 259.78
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3735552
                    Iteration time: 12.03s
                        Total time: 3185.15s
                               ETA: 1393822.6s

################################################################################
                    [1m Learning iteration 228/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.542s, learning 0.165s)
               Value function loss: 12.2235
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 258.30
               Mean episode length: 124.37
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3751936
                    Iteration time: 11.71s
                        Total time: 3196.85s
                               ETA: 1392822.6s

################################################################################
                    [1m Learning iteration 229/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.714s, learning 0.164s)
               Value function loss: 15.7680
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 257.60
               Mean episode length: 123.79
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3768320
                    Iteration time: 11.88s
                        Total time: 3208.73s
                               ETA: 1391905.7s

################################################################################
                    [1m Learning iteration 230/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.030s, learning 0.236s)
               Value function loss: 7.7091
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 253.46
               Mean episode length: 123.75
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3784704
                    Iteration time: 12.27s
                        Total time: 3221.00s
                               ETA: 1391163.9s

################################################################################
                    [1m Learning iteration 231/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.960s, learning 0.164s)
               Value function loss: 11.4289
                    Surrogate loss: -0.0023
             Mean action noise std: 0.79
                       Mean reward: 253.36
               Mean episode length: 124.33
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3801088
                    Iteration time: 12.12s
                        Total time: 3233.12s
                               ETA: 1390367.3s

################################################################################
                    [1m Learning iteration 232/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.516s, learning 0.161s)
               Value function loss: 8.3513
                    Surrogate loss: 0.0095
             Mean action noise std: 0.79
                       Mean reward: 252.91
               Mean episode length: 123.66
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3817472
                    Iteration time: 11.68s
                        Total time: 3244.80s
                               ETA: 1389386.2s

################################################################################
                    [1m Learning iteration 233/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.721s, learning 0.159s)
               Value function loss: 7.6953
                    Surrogate loss: 0.0145
             Mean action noise std: 0.79
                       Mean reward: 255.60
               Mean episode length: 123.95
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3833856
                    Iteration time: 11.88s
                        Total time: 3256.68s
                               ETA: 1388500.1s

################################################################################
                    [1m Learning iteration 234/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.998s, learning 0.208s)
               Value function loss: 49.4144
                    Surrogate loss: 0.0057
             Mean action noise std: 0.79
                       Mean reward: 255.70
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3850240
                    Iteration time: 12.21s
                        Total time: 3268.88s
                               ETA: 1387759.6s

################################################################################
                    [1m Learning iteration 235/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.143s, learning 0.192s)
               Value function loss: 10.9672
                    Surrogate loss: -0.0108
             Mean action noise std: 0.79
                       Mean reward: 255.59
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3866624
                    Iteration time: 12.34s
                        Total time: 3281.22s
                               ETA: 1387079.8s

################################################################################
                    [1m Learning iteration 236/100000 [0m                     

                       Computation: 1338 steps/s (collection: 11.960s, learning 0.281s)
               Value function loss: 189.2907
                    Surrogate loss: 0.0012
             Mean action noise std: 0.79
                       Mean reward: 253.96
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3883008
                    Iteration time: 12.24s
                        Total time: 3293.46s
                               ETA: 1386366.2s

################################################################################
                    [1m Learning iteration 237/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.211s, learning 0.166s)
               Value function loss: 27.0785
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 254.22
               Mean episode length: 124.77
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3899392
                    Iteration time: 12.38s
                        Total time: 3305.84s
                               ETA: 1385715.4s

################################################################################
                    [1m Learning iteration 238/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.023s, learning 0.273s)
               Value function loss: 19.8759
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 259.60
               Mean episode length: 124.69
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3915776
                    Iteration time: 12.30s
                        Total time: 3318.13s
                               ETA: 1385036.2s

################################################################################
                    [1m Learning iteration 239/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.054s, learning 0.155s)
               Value function loss: 22.3786
                    Surrogate loss: -0.0184
             Mean action noise std: 0.79
                       Mean reward: 254.01
               Mean episode length: 124.56
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3932160
                    Iteration time: 12.21s
                        Total time: 3330.34s
                               ETA: 1384326.3s

################################################################################
                    [1m Learning iteration 240/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.650s, learning 0.159s)
               Value function loss: 22.7668
                    Surrogate loss: -0.0188
             Mean action noise std: 0.79
                       Mean reward: 257.16
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3948544
                    Iteration time: 11.81s
                        Total time: 3342.15s
                               ETA: 1383456.8s

################################################################################
                    [1m Learning iteration 241/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.137s, learning 0.265s)
               Value function loss: 16.4940
                    Surrogate loss: -0.0193
             Mean action noise std: 0.79
                       Mean reward: 257.12
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3964928
                    Iteration time: 12.40s
                        Total time: 3354.55s
                               ETA: 1382838.7s

################################################################################
                    [1m Learning iteration 242/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.697s, learning 0.161s)
               Value function loss: 15.1893
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 258.15
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3981312
                    Iteration time: 11.86s
                        Total time: 3366.41s
                               ETA: 1382002.4s

################################################################################
                    [1m Learning iteration 243/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.229s, learning 0.176s)
               Value function loss: 13.1276
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 258.21
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 3997696
                    Iteration time: 12.40s
                        Total time: 3378.82s
                               ETA: 1381396.1s

################################################################################
                    [1m Learning iteration 244/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.930s, learning 0.196s)
               Value function loss: 13.9154
                    Surrogate loss: 0.0009
             Mean action noise std: 0.79
                       Mean reward: 258.02
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4014080
                    Iteration time: 12.13s
                        Total time: 3390.94s
                               ETA: 1380681.3s

################################################################################
                    [1m Learning iteration 245/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.637s, learning 0.180s)
               Value function loss: 11.0813
                    Surrogate loss: 0.0005
             Mean action noise std: 0.79
                       Mean reward: 252.88
               Mean episode length: 123.16
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4030464
                    Iteration time: 11.82s
                        Total time: 3402.76s
                               ETA: 1379846.9s

################################################################################
                    [1m Learning iteration 246/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.190s, learning 0.261s)
               Value function loss: 9.2629
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 254.39
               Mean episode length: 122.89
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4046848
                    Iteration time: 12.45s
                        Total time: 3415.21s
                               ETA: 1379275.2s

################################################################################
                    [1m Learning iteration 247/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.964s, learning 0.170s)
               Value function loss: 8.7426
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 256.43
               Mean episode length: 124.05
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4063232
                    Iteration time: 12.13s
                        Total time: 3427.35s
                               ETA: 1378580.5s

################################################################################
                    [1m Learning iteration 248/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.004s, learning 0.193s)
               Value function loss: 10.8967
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 257.51
               Mean episode length: 124.73
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4079616
                    Iteration time: 12.20s
                        Total time: 3439.54s
                               ETA: 1377916.6s

################################################################################
                    [1m Learning iteration 249/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.228s, learning 0.159s)
               Value function loss: 71.4396
                    Surrogate loss: 0.0179
             Mean action noise std: 0.79
                       Mean reward: 256.42
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4096000
                    Iteration time: 12.39s
                        Total time: 3451.93s
                               ETA: 1377333.7s

################################################################################
                    [1m Learning iteration 250/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.220s, learning 0.182s)
               Value function loss: 9.0196
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 256.77
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4112384
                    Iteration time: 12.40s
                        Total time: 3464.33s
                               ETA: 1376761.1s

################################################################################
                    [1m Learning iteration 251/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.699s, learning 0.158s)
               Value function loss: 15.8022
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 255.52
               Mean episode length: 124.95
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4128768
                    Iteration time: 11.86s
                        Total time: 3476.19s
                               ETA: 1375977.4s

################################################################################
                    [1m Learning iteration 252/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.881s, learning 0.200s)
               Value function loss: 123.8461
                    Surrogate loss: 0.0057
             Mean action noise std: 0.79
                       Mean reward: 257.66
               Mean episode length: 125.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 5.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4145152
                    Iteration time: 12.08s
                        Total time: 3488.27s
                               ETA: 1375288.4s

################################################################################
                    [1m Learning iteration 253/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.739s, learning 0.256s)
               Value function loss: 21.9242
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 259.05
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4161536
                    Iteration time: 12.00s
                        Total time: 3500.27s
                               ETA: 1374570.9s

################################################################################
                    [1m Learning iteration 254/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.945s, learning 0.215s)
               Value function loss: 23.1469
                    Surrogate loss: -0.0167
             Mean action noise std: 0.79
                       Mean reward: 257.78
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4177920
                    Iteration time: 12.16s
                        Total time: 3512.43s
                               ETA: 1373923.3s

################################################################################
                    [1m Learning iteration 255/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.852s, learning 0.209s)
               Value function loss: 28.7560
                    Surrogate loss: -0.0161
             Mean action noise std: 0.79
                       Mean reward: 255.81
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4194304
                    Iteration time: 12.06s
                        Total time: 3524.49s
                               ETA: 1373242.3s

################################################################################
                    [1m Learning iteration 256/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.529s, learning 0.180s)
               Value function loss: 24.2251
                    Surrogate loss: -0.0168
             Mean action noise std: 0.79
                       Mean reward: 257.29
               Mean episode length: 124.71
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4210688
                    Iteration time: 11.71s
                        Total time: 3536.20s
                               ETA: 1372429.7s

################################################################################
                    [1m Learning iteration 257/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.151s, learning 0.185s)
               Value function loss: 14.3000
                    Surrogate loss: -0.0177
             Mean action noise std: 0.79
                       Mean reward: 258.66
               Mean episode length: 124.71
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4227072
                    Iteration time: 12.34s
                        Total time: 3548.53s
                               ETA: 1371865.5s

################################################################################
                    [1m Learning iteration 258/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.569s, learning 0.196s)
               Value function loss: 16.1348
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 258.31
               Mean episode length: 124.91
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4243456
                    Iteration time: 11.77s
                        Total time: 3560.30s
                               ETA: 1371086.0s

################################################################################
                    [1m Learning iteration 259/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.908s, learning 0.191s)
               Value function loss: 12.4538
                    Surrogate loss: -0.0144
             Mean action noise std: 0.79
                       Mean reward: 257.44
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4259840
                    Iteration time: 12.10s
                        Total time: 3572.40s
                               ETA: 1370440.3s

################################################################################
                    [1m Learning iteration 260/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.635s, learning 0.161s)
               Value function loss: 17.8512
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 257.36
               Mean episode length: 124.41
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4276224
                    Iteration time: 11.80s
                        Total time: 3584.19s
                               ETA: 1369684.0s

################################################################################
                    [1m Learning iteration 261/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.691s, learning 0.170s)
               Value function loss: 10.1119
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 257.41
               Mean episode length: 124.41
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4292608
                    Iteration time: 11.86s
                        Total time: 3596.06s
                               ETA: 1368958.0s

################################################################################
                    [1m Learning iteration 262/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.128s, learning 0.261s)
               Value function loss: 8.1934
                    Surrogate loss: -0.0112
             Mean action noise std: 0.79
                       Mean reward: 258.01
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4308992
                    Iteration time: 12.39s
                        Total time: 3608.44s
                               ETA: 1368437.5s

################################################################################
                    [1m Learning iteration 263/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.147s, learning 0.171s)
               Value function loss: 9.9302
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 257.80
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4325376
                    Iteration time: 12.32s
                        Total time: 3620.76s
                               ETA: 1367894.2s

################################################################################
                    [1m Learning iteration 264/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.676s, learning 0.300s)
               Value function loss: 8.8034
                    Surrogate loss: -0.0075
             Mean action noise std: 0.79
                       Mean reward: 259.63
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4341760
                    Iteration time: 11.98s
                        Total time: 3632.74s
                               ETA: 1367225.7s

################################################################################
                    [1m Learning iteration 265/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.182s, learning 0.166s)
               Value function loss: 84.5238
                    Surrogate loss: 0.0155
             Mean action noise std: 0.79
                       Mean reward: 255.70
               Mean episode length: 124.80
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4358144
                    Iteration time: 12.35s
                        Total time: 3645.09s
                               ETA: 1366701.6s

################################################################################
                    [1m Learning iteration 266/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.209s, learning 0.203s)
               Value function loss: 10.3455
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 254.52
               Mean episode length: 124.52
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4374528
                    Iteration time: 12.41s
                        Total time: 3657.50s
                               ETA: 1366205.5s

################################################################################
                    [1m Learning iteration 267/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.847s, learning 0.217s)
               Value function loss: 177.6982
                    Surrogate loss: 0.0066
             Mean action noise std: 0.79
                       Mean reward: 257.83
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4390912
                    Iteration time: 12.06s
                        Total time: 3669.56s
                               ETA: 1365583.4s

################################################################################
                    [1m Learning iteration 268/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.020s, learning 0.173s)
               Value function loss: 58.2527
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 258.07
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4407296
                    Iteration time: 12.19s
                        Total time: 3681.75s
                               ETA: 1365013.9s

################################################################################
                    [1m Learning iteration 269/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.572s, learning 0.215s)
               Value function loss: 24.2384
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 256.96
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4423680
                    Iteration time: 11.79s
                        Total time: 3693.54s
                               ETA: 1364298.1s

################################################################################
                    [1m Learning iteration 270/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.717s, learning 0.169s)
               Value function loss: 28.3337
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 257.99
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4440064
                    Iteration time: 11.89s
                        Total time: 3705.43s
                               ETA: 1363624.1s

################################################################################
                    [1m Learning iteration 271/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.581s, learning 0.159s)
               Value function loss: 26.6194
                    Surrogate loss: -0.0159
             Mean action noise std: 0.79
                       Mean reward: 258.40
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4456448
                    Iteration time: 11.74s
                        Total time: 3717.17s
                               ETA: 1362901.7s

################################################################################
                    [1m Learning iteration 272/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.689s, learning 0.169s)
               Value function loss: 24.1573
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 258.71
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4472832
                    Iteration time: 11.86s
                        Total time: 3729.02s
                               ETA: 1362227.5s

################################################################################
                    [1m Learning iteration 273/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.754s, learning 0.162s)
               Value function loss: 18.5981
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 256.53
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4489216
                    Iteration time: 11.92s
                        Total time: 3740.94s
                               ETA: 1361579.2s

################################################################################
                    [1m Learning iteration 274/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.062s, learning 0.217s)
               Value function loss: 16.1716
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 255.82
               Mean episode length: 124.32
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4505600
                    Iteration time: 12.28s
                        Total time: 3753.22s
                               ETA: 1361067.1s

################################################################################
                    [1m Learning iteration 275/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.721s, learning 0.243s)
               Value function loss: 15.3478
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 255.58
               Mean episode length: 124.32
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4521984
                    Iteration time: 11.96s
                        Total time: 3765.18s
                               ETA: 1360445.2s

################################################################################
                    [1m Learning iteration 276/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.084s, learning 0.156s)
               Value function loss: 16.8210
                    Surrogate loss: -0.0145
             Mean action noise std: 0.79
                       Mean reward: 257.62
               Mean episode length: 123.91
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4538368
                    Iteration time: 12.24s
                        Total time: 3777.42s
                               ETA: 1359926.7s

################################################################################
                    [1m Learning iteration 277/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.669s, learning 0.167s)
               Value function loss: 10.3720
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 259.12
               Mean episode length: 123.91
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4554752
                    Iteration time: 11.84s
                        Total time: 3789.26s
                               ETA: 1359267.0s

################################################################################
                    [1m Learning iteration 278/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.761s, learning 0.200s)
               Value function loss: 11.8842
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 255.71
               Mean episode length: 123.51
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4571136
                    Iteration time: 11.96s
                        Total time: 3801.22s
                               ETA: 1358656.7s

################################################################################
                    [1m Learning iteration 279/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.854s, learning 0.195s)
               Value function loss: 11.5120
                    Surrogate loss: 0.0039
             Mean action noise std: 0.79
                       Mean reward: 256.39
               Mean episode length: 123.96
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4587520
                    Iteration time: 12.05s
                        Total time: 3813.27s
                               ETA: 1358082.0s

################################################################################
                    [1m Learning iteration 280/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.778s, learning 0.166s)
               Value function loss: 7.3168
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 255.00
               Mean episode length: 123.96
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4603904
                    Iteration time: 11.94s
                        Total time: 3825.21s
                               ETA: 1357474.1s

################################################################################
                    [1m Learning iteration 281/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.871s, learning 0.294s)
               Value function loss: 38.0576
                    Surrogate loss: 0.0202
             Mean action noise std: 0.79
                       Mean reward: 259.76
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.99
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4620288
                    Iteration time: 12.17s
                        Total time: 3837.38s
                               ETA: 1356948.6s

################################################################################
                    [1m Learning iteration 282/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.999s, learning 0.162s)
               Value function loss: 15.2305
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 258.43
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4636672
                    Iteration time: 12.16s
                        Total time: 3849.54s
                               ETA: 1356425.1s

################################################################################
                    [1m Learning iteration 283/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.951s, learning 0.222s)
               Value function loss: 319.3016
                    Surrogate loss: 0.0299
             Mean action noise std: 0.79
                       Mean reward: 258.59
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 5.96
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4653056
                    Iteration time: 12.17s
                        Total time: 3861.71s
                               ETA: 1355909.3s

################################################################################
                    [1m Learning iteration 284/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.057s, learning 0.172s)
               Value function loss: 28.2246
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 257.37
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4669440
                    Iteration time: 12.23s
                        Total time: 3873.94s
                               ETA: 1355416.8s

################################################################################
                    [1m Learning iteration 285/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.664s, learning 0.250s)
               Value function loss: 18.9764
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 258.37
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4685824
                    Iteration time: 11.91s
                        Total time: 3885.85s
                               ETA: 1354818.0s

################################################################################
                    [1m Learning iteration 286/100000 [0m                     

                       Computation: 1340 steps/s (collection: 11.989s, learning 0.237s)
               Value function loss: 23.4426
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 259.49
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4702208
                    Iteration time: 12.23s
                        Total time: 3898.08s
                               ETA: 1354331.5s

################################################################################
                    [1m Learning iteration 287/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.891s, learning 0.155s)
               Value function loss: 24.0501
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 257.24
               Mean episode length: 124.84
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4718592
                    Iteration time: 12.05s
                        Total time: 3910.13s
                               ETA: 1353786.2s

################################################################################
                    [1m Learning iteration 288/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.681s, learning 0.164s)
               Value function loss: 20.1910
                    Surrogate loss: -0.0084
             Mean action noise std: 0.79
                       Mean reward: 259.86
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4734976
                    Iteration time: 11.85s
                        Total time: 3921.97s
                               ETA: 1353175.2s

################################################################################
                    [1m Learning iteration 289/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.580s, learning 0.157s)
               Value function loss: 16.5685
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 257.89
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4751360
                    Iteration time: 11.74s
                        Total time: 3933.71s
                               ETA: 1352531.1s

################################################################################
                    [1m Learning iteration 290/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.764s, learning 0.156s)
               Value function loss: 14.1062
                    Surrogate loss: -0.0036
             Mean action noise std: 0.79
                       Mean reward: 258.60
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4767744
                    Iteration time: 11.92s
                        Total time: 3945.63s
                               ETA: 1351954.1s

################################################################################
                    [1m Learning iteration 291/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.006s, learning 0.194s)
               Value function loss: 17.0994
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 259.93
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4784128
                    Iteration time: 12.20s
                        Total time: 3957.83s
                               ETA: 1351476.7s

################################################################################
                    [1m Learning iteration 292/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.689s, learning 0.156s)
               Value function loss: 13.9694
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 257.57
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4800512
                    Iteration time: 11.85s
                        Total time: 3969.67s
                               ETA: 1350881.6s

################################################################################
                    [1m Learning iteration 293/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.743s, learning 0.163s)
               Value function loss: 17.3039
                    Surrogate loss: 0.0163
             Mean action noise std: 0.79
                       Mean reward: 259.74
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4816896
                    Iteration time: 11.91s
                        Total time: 3981.58s
                               ETA: 1350310.9s

################################################################################
                    [1m Learning iteration 294/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.042s, learning 0.187s)
               Value function loss: 9.6943
                    Surrogate loss: -0.0010
             Mean action noise std: 0.79
                       Mean reward: 260.53
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4833280
                    Iteration time: 12.23s
                        Total time: 3993.81s
                               ETA: 1349853.1s

################################################################################
                    [1m Learning iteration 295/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.163s)
               Value function loss: 11.1995
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 260.31
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4849664
                    Iteration time: 11.64s
                        Total time: 4005.44s
                               ETA: 1349198.6s

################################################################################
                    [1m Learning iteration 296/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.050s, learning 0.207s)
               Value function loss: 114.4205
                    Surrogate loss: -0.0004
             Mean action noise std: 0.79
                       Mean reward: 258.29
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4866048
                    Iteration time: 12.26s
                        Total time: 4017.70s
                               ETA: 1348757.0s

################################################################################
                    [1m Learning iteration 297/100000 [0m                     

                       Computation: 1283 steps/s (collection: 12.593s, learning 0.168s)
               Value function loss: 12.3873
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 257.53
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4882432
                    Iteration time: 12.76s
                        Total time: 4030.46s
                               ETA: 1348486.8s

################################################################################
                    [1m Learning iteration 298/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.720s, learning 0.160s)
               Value function loss: 67.3344
                    Surrogate loss: 0.0090
             Mean action noise std: 0.79
                       Mean reward: 260.02
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4898816
                    Iteration time: 11.88s
                        Total time: 4042.34s
                               ETA: 1347924.6s

################################################################################
                    [1m Learning iteration 299/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.886s, learning 0.188s)
               Value function loss: 140.8638
                    Surrogate loss: 0.0038
             Mean action noise std: 0.79
                       Mean reward: 260.93
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4915200
                    Iteration time: 12.07s
                        Total time: 4054.41s
                               ETA: 1347430.7s

################################################################################
                    [1m Learning iteration 300/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.773s, learning 0.182s)
               Value function loss: 26.9007
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 260.22
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4931584
                    Iteration time: 11.96s
                        Total time: 4066.37s
                               ETA: 1346900.6s

################################################################################
                    [1m Learning iteration 301/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.836s, learning 0.197s)
               Value function loss: 34.0282
                    Surrogate loss: -0.0091
             Mean action noise std: 0.79
                       Mean reward: 259.52
               Mean episode length: 123.89
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4947968
                    Iteration time: 12.03s
                        Total time: 4078.40s
                               ETA: 1346399.6s

################################################################################
                    [1m Learning iteration 302/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.730s, learning 0.165s)
               Value function loss: 33.0884
                    Surrogate loss: -0.0044
             Mean action noise std: 0.79
                       Mean reward: 259.06
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4964352
                    Iteration time: 11.89s
                        Total time: 4090.30s
                               ETA: 1345856.3s

################################################################################
                    [1m Learning iteration 303/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.959s, learning 0.171s)
               Value function loss: 29.1211
                    Surrogate loss: -0.0118
             Mean action noise std: 0.79
                       Mean reward: 260.88
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4980736
                    Iteration time: 12.13s
                        Total time: 4102.43s
                               ETA: 1345393.7s

################################################################################
                    [1m Learning iteration 304/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.953s, learning 0.247s)
               Value function loss: 21.9473
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 260.40
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 4997120
                    Iteration time: 12.20s
                        Total time: 4114.63s
                               ETA: 1344957.0s

################################################################################
                    [1m Learning iteration 305/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.741s, learning 0.234s)
               Value function loss: 22.2301
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 259.54
               Mean episode length: 124.13
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5013504
                    Iteration time: 11.97s
                        Total time: 4126.60s
                               ETA: 1344449.7s

################################################################################
                    [1m Learning iteration 306/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.807s, learning 0.158s)
               Value function loss: 18.6862
                    Surrogate loss: 0.0060
             Mean action noise std: 0.79
                       Mean reward: 259.93
               Mean episode length: 125.00
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5029888
                    Iteration time: 11.97s
                        Total time: 4138.57s
                               ETA: 1343942.4s

################################################################################
                    [1m Learning iteration 307/100000 [0m                     

                       Computation: 1322 steps/s (collection: 12.220s, learning 0.168s)
               Value function loss: 22.4052
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 258.32
               Mean episode length: 124.59
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5046272
                    Iteration time: 12.39s
                        Total time: 4150.96s
                               ETA: 1343575.2s

################################################################################
                    [1m Learning iteration 308/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.831s, learning 0.192s)
               Value function loss: 11.7956
                    Surrogate loss: -0.0055
             Mean action noise std: 0.79
                       Mean reward: 257.50
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5062656
                    Iteration time: 12.02s
                        Total time: 4162.98s
                               ETA: 1343092.6s

################################################################################
                    [1m Learning iteration 309/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.052s, learning 0.169s)
               Value function loss: 13.6931
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 256.80
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5079040
                    Iteration time: 12.22s
                        Total time: 4175.20s
                               ETA: 1342676.4s

################################################################################
                    [1m Learning iteration 310/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.548s, learning 0.287s)
               Value function loss: 11.3896
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 258.01
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5095424
                    Iteration time: 11.84s
                        Total time: 4187.03s
                               ETA: 1342139.4s

################################################################################
                    [1m Learning iteration 311/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.557s, learning 0.158s)
               Value function loss: 9.2539
                    Surrogate loss: -0.0050
             Mean action noise std: 0.79
                       Mean reward: 258.87
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5111808
                    Iteration time: 11.71s
                        Total time: 4198.75s
                               ETA: 1341567.2s

################################################################################
                    [1m Learning iteration 312/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.680s, learning 0.156s)
               Value function loss: 81.2875
                    Surrogate loss: 0.0178
             Mean action noise std: 0.79
                       Mean reward: 258.09
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5128192
                    Iteration time: 11.84s
                        Total time: 4210.58s
                               ETA: 1341037.3s

################################################################################
                    [1m Learning iteration 313/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.675s, learning 0.177s)
               Value function loss: 12.3167
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 257.72
               Mean episode length: 124.96
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5144576
                    Iteration time: 11.85s
                        Total time: 4222.44s
                               ETA: 1340515.7s

################################################################################
                    [1m Learning iteration 314/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.944s, learning 0.262s)
               Value function loss: 227.3618
                    Surrogate loss: 0.0339
             Mean action noise std: 0.79
                       Mean reward: 257.75
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5160960
                    Iteration time: 12.21s
                        Total time: 4234.64s
                               ETA: 1340109.4s

################################################################################
                    [1m Learning iteration 315/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.863s, learning 0.177s)
               Value function loss: 38.2879
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 256.31
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5177344
                    Iteration time: 12.04s
                        Total time: 4246.68s
                               ETA: 1339653.3s

################################################################################
                    [1m Learning iteration 316/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.063s, learning 0.167s)
               Value function loss: 29.0466
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 257.46
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5193728
                    Iteration time: 12.23s
                        Total time: 4258.91s
                               ETA: 1339259.5s

################################################################################
                    [1m Learning iteration 317/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.832s, learning 0.255s)
               Value function loss: 31.3410
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 258.24
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5210112
                    Iteration time: 12.09s
                        Total time: 4271.00s
                               ETA: 1338823.4s

################################################################################
                    [1m Learning iteration 318/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.633s, learning 0.259s)
               Value function loss: 26.7661
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 258.33
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5226496
                    Iteration time: 11.89s
                        Total time: 4282.89s
                               ETA: 1338329.1s

################################################################################
                    [1m Learning iteration 319/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.818s, learning 0.181s)
               Value function loss: 23.7751
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 259.08
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5242880
                    Iteration time: 12.00s
                        Total time: 4294.89s
                               ETA: 1337871.1s

################################################################################
                    [1m Learning iteration 320/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.836s, learning 0.159s)
               Value function loss: 18.4682
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 258.87
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5259264
                    Iteration time: 12.00s
                        Total time: 4306.88s
                               ETA: 1337414.8s

################################################################################
                    [1m Learning iteration 321/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.946s, learning 0.190s)
               Value function loss: 17.5152
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 257.01
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5275648
                    Iteration time: 12.14s
                        Total time: 4319.02s
                               ETA: 1337004.7s

################################################################################
                    [1m Learning iteration 322/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.058s, learning 0.204s)
               Value function loss: 14.4195
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 255.24
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5292032
                    Iteration time: 12.26s
                        Total time: 4331.28s
                               ETA: 1336636.0s

################################################################################
                    [1m Learning iteration 323/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.514s, learning 0.172s)
               Value function loss: 16.1160
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 255.75
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5308416
                    Iteration time: 11.69s
                        Total time: 4342.97s
                               ETA: 1336092.5s

################################################################################
                    [1m Learning iteration 324/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.526s, learning 0.163s)
               Value function loss: 9.0727
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 257.18
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5324800
                    Iteration time: 11.69s
                        Total time: 4354.66s
                               ETA: 1335552.9s

################################################################################
                    [1m Learning iteration 325/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.760s, learning 0.213s)
               Value function loss: 10.1459
                    Surrogate loss: -0.0027
             Mean action noise std: 0.79
                       Mean reward: 259.20
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5341184
                    Iteration time: 11.97s
                        Total time: 4366.63s
                               ETA: 1335103.7s

################################################################################
                    [1m Learning iteration 326/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.870s, learning 0.163s)
               Value function loss: 10.1562
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 257.77
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5357568
                    Iteration time: 12.03s
                        Total time: 4378.66s
                               ETA: 1334675.4s

################################################################################
                    [1m Learning iteration 327/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.573s, learning 0.172s)
               Value function loss: 104.7719
                    Surrogate loss: 0.0452
             Mean action noise std: 0.79
                       Mean reward: 256.24
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5373952
                    Iteration time: 11.75s
                        Total time: 4390.41s
                               ETA: 1334162.1s

################################################################################
                    [1m Learning iteration 328/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.777s, learning 0.218s)
               Value function loss: 13.9816
                    Surrogate loss: -0.0037
             Mean action noise std: 0.79
                       Mean reward: 255.85
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.93
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5390336
                    Iteration time: 11.99s
                        Total time: 4402.40s
                               ETA: 1333727.4s

################################################################################
                    [1m Learning iteration 329/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.757s, learning 0.170s)
               Value function loss: 12.3347
                    Surrogate loss: -0.0011
             Mean action noise std: 0.79
                       Mean reward: 255.32
               Mean episode length: 124.92
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5406720
                    Iteration time: 11.93s
                        Total time: 4414.33s
                               ETA: 1333274.9s

################################################################################
                    [1m Learning iteration 330/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.275s, learning 0.245s)
               Value function loss: 160.3342
                    Surrogate loss: 0.0155
             Mean action noise std: 0.79
                       Mean reward: 253.27
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 5.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5423104
                    Iteration time: 12.52s
                        Total time: 4426.85s
                               ETA: 1333003.5s

################################################################################
                    [1m Learning iteration 331/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.954s, learning 0.193s)
               Value function loss: 20.3266
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 257.10
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5439488
                    Iteration time: 12.15s
                        Total time: 4439.00s
                               ETA: 1332621.9s

################################################################################
                    [1m Learning iteration 332/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.996s, learning 0.163s)
               Value function loss: 20.0569
                    Surrogate loss: -0.0162
             Mean action noise std: 0.79
                       Mean reward: 254.71
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5455872
                    Iteration time: 12.16s
                        Total time: 4451.16s
                               ETA: 1332245.8s

################################################################################
                    [1m Learning iteration 333/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.919s, learning 0.161s)
               Value function loss: 22.2182
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 257.08
               Mean episode length: 123.94
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5472256
                    Iteration time: 12.08s
                        Total time: 4463.24s
                               ETA: 1331848.2s

################################################################################
                    [1m Learning iteration 334/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.681s, learning 0.215s)
               Value function loss: 20.2633
                    Surrogate loss: -0.0136
             Mean action noise std: 0.79
                       Mean reward: 256.10
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5488640
                    Iteration time: 11.90s
                        Total time: 4475.13s
                               ETA: 1331398.4s

################################################################################
                    [1m Learning iteration 335/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.688s, learning 0.207s)
               Value function loss: 13.7913
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 256.70
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5505024
                    Iteration time: 11.89s
                        Total time: 4487.03s
                               ETA: 1330950.9s

################################################################################
                    [1m Learning iteration 336/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.664s, learning 0.205s)
               Value function loss: 14.9379
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 255.02
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5521408
                    Iteration time: 11.87s
                        Total time: 4498.90s
                               ETA: 1330498.3s

################################################################################
                    [1m Learning iteration 337/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.097s, learning 0.187s)
               Value function loss: 10.9549
                    Surrogate loss: -0.0018
             Mean action noise std: 0.79
                       Mean reward: 256.15
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5537792
                    Iteration time: 12.28s
                        Total time: 4511.18s
                               ETA: 1330170.8s

################################################################################
                    [1m Learning iteration 338/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.407s, learning 0.173s)
               Value function loss: 12.4532
                    Surrogate loss: 0.0015
             Mean action noise std: 0.79
                       Mean reward: 252.23
               Mean episode length: 123.31
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5554176
                    Iteration time: 11.58s
                        Total time: 4522.76s
                               ETA: 1329638.3s

################################################################################
                    [1m Learning iteration 339/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.802s, learning 0.211s)
               Value function loss: 9.5032
                    Surrogate loss: -0.0022
             Mean action noise std: 0.79
                       Mean reward: 252.30
               Mean episode length: 122.80
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5570560
                    Iteration time: 12.01s
                        Total time: 4534.77s
                               ETA: 1329235.5s

################################################################################
                    [1m Learning iteration 340/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.048s, learning 0.186s)
               Value function loss: 8.7947
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 251.57
               Mean episode length: 122.73
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5586944
                    Iteration time: 12.23s
                        Total time: 4547.01s
                               ETA: 1328899.7s

################################################################################
                    [1m Learning iteration 341/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.682s, learning 0.192s)
               Value function loss: 7.9545
                    Surrogate loss: -0.0041
             Mean action noise std: 0.79
                       Mean reward: 251.07
               Mean episode length: 122.73
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5603328
                    Iteration time: 11.87s
                        Total time: 4558.88s
                               ETA: 1328460.7s

################################################################################
                    [1m Learning iteration 342/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.801s, learning 0.219s)
               Value function loss: 8.1243
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 250.72
               Mean episode length: 123.14
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5619712
                    Iteration time: 12.02s
                        Total time: 4570.90s
                               ETA: 1328066.6s

################################################################################
                    [1m Learning iteration 343/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.913s, learning 0.172s)
               Value function loss: 65.3467
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: 252.43
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5636096
                    Iteration time: 12.09s
                        Total time: 4582.99s
                               ETA: 1327693.8s

################################################################################
                    [1m Learning iteration 344/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.433s, learning 0.159s)
               Value function loss: 11.4902
                    Surrogate loss: 0.0140
             Mean action noise std: 0.79
                       Mean reward: 251.24
               Mean episode length: 124.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5652480
                    Iteration time: 11.59s
                        Total time: 4594.58s
                               ETA: 1327180.7s

################################################################################
                    [1m Learning iteration 345/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.868s, learning 0.171s)
               Value function loss: 63.3482
                    Surrogate loss: 0.0111
             Mean action noise std: 0.79
                       Mean reward: 257.51
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5668864
                    Iteration time: 12.04s
                        Total time: 4606.62s
                               ETA: 1326799.1s

################################################################################
                    [1m Learning iteration 346/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.834s, learning 0.171s)
               Value function loss: 67.1806
                    Surrogate loss: 0.0043
             Mean action noise std: 0.79
                       Mean reward: 254.63
               Mean episode length: 125.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 6.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5685248
                    Iteration time: 12.01s
                        Total time: 4618.62s
                               ETA: 1326409.9s

################################################################################
                    [1m Learning iteration 347/100000 [0m                     

                       Computation: 1338 steps/s (collection: 11.958s, learning 0.279s)
               Value function loss: 18.8728
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 256.36
               Mean episode length: 124.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5701632
                    Iteration time: 12.24s
                        Total time: 4630.86s
                               ETA: 1326089.3s

################################################################################
                    [1m Learning iteration 348/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.841s, learning 0.278s)
               Value function loss: 22.4821
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 257.33
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5718016
                    Iteration time: 12.12s
                        Total time: 4642.98s
                               ETA: 1325736.7s

################################################################################
                    [1m Learning iteration 349/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.143s, learning 0.163s)
               Value function loss: 21.5620
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 256.11
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5734400
                    Iteration time: 12.31s
                        Total time: 4655.28s
                               ETA: 1325439.4s

################################################################################
                    [1m Learning iteration 350/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.891s, learning 0.190s)
               Value function loss: 20.9802
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 258.20
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5750784
                    Iteration time: 12.08s
                        Total time: 4667.37s
                               ETA: 1325079.7s

################################################################################
                    [1m Learning iteration 351/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.786s, learning 0.175s)
               Value function loss: 14.7451
                    Surrogate loss: -0.0047
             Mean action noise std: 0.79
                       Mean reward: 257.13
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5767168
                    Iteration time: 11.96s
                        Total time: 4679.33s
                               ETA: 1324687.9s

################################################################################
                    [1m Learning iteration 352/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.724s, learning 0.222s)
               Value function loss: 12.3803
                    Surrogate loss: -0.0080
             Mean action noise std: 0.79
                       Mean reward: 256.24
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5783552
                    Iteration time: 11.95s
                        Total time: 4691.27s
                               ETA: 1324294.4s

################################################################################
                    [1m Learning iteration 353/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.565s, learning 0.171s)
               Value function loss: 9.8881
                    Surrogate loss: -0.0107
             Mean action noise std: 0.79
                       Mean reward: 258.36
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5799936
                    Iteration time: 11.74s
                        Total time: 4703.01s
                               ETA: 1323843.9s

################################################################################
                    [1m Learning iteration 354/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.904s, learning 0.205s)
               Value function loss: 13.5042
                    Surrogate loss: -0.0115
             Mean action noise std: 0.79
                       Mean reward: 252.68
               Mean episode length: 123.54
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5816320
                    Iteration time: 12.11s
                        Total time: 4715.12s
                               ETA: 1323500.4s

################################################################################
                    [1m Learning iteration 355/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.900s, learning 0.258s)
               Value function loss: 6.6744
                    Surrogate loss: -0.0103
             Mean action noise std: 0.79
                       Mean reward: 253.16
               Mean episode length: 123.54
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5832704
                    Iteration time: 12.16s
                        Total time: 4727.28s
                               ETA: 1323172.7s

################################################################################
                    [1m Learning iteration 356/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.665s, learning 0.204s)
               Value function loss: 9.0903
                    Surrogate loss: -0.0031
             Mean action noise std: 0.79
                       Mean reward: 256.02
               Mean episode length: 123.80
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5849088
                    Iteration time: 11.87s
                        Total time: 4739.15s
                               ETA: 1322766.0s

################################################################################
                    [1m Learning iteration 357/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.827s, learning 0.161s)
               Value function loss: 7.2077
                    Surrogate loss: -0.0000
             Mean action noise std: 0.79
                       Mean reward: 256.74
               Mean episode length: 124.65
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5865472
                    Iteration time: 11.99s
                        Total time: 4751.13s
                               ETA: 1322394.6s

################################################################################
                    [1m Learning iteration 358/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.860s, learning 0.162s)
               Value function loss: 6.8695
                    Surrogate loss: -0.0045
             Mean action noise std: 0.79
                       Mean reward: 253.79
               Mean episode length: 123.44
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5881856
                    Iteration time: 12.02s
                        Total time: 4763.16s
                               ETA: 1322034.5s

################################################################################
                    [1m Learning iteration 359/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.106s, learning 0.167s)
               Value function loss: 38.7946
                    Surrogate loss: 0.0270
             Mean action noise std: 0.79
                       Mean reward: 255.01
               Mean episode length: 124.86
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5898240
                    Iteration time: 12.27s
                        Total time: 4775.43s
                               ETA: 1321745.8s

################################################################################
                    [1m Learning iteration 360/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.795s, learning 0.195s)
               Value function loss: 10.7379
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 252.29
               Mean episode length: 124.43
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5914624
                    Iteration time: 11.99s
                        Total time: 4787.42s
                               ETA: 1321380.6s

################################################################################
                    [1m Learning iteration 361/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.568s, learning 0.160s)
               Value function loss: 141.4031
                    Surrogate loss: 0.0192
             Mean action noise std: 0.79
                       Mean reward: 255.39
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 6.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5931008
                    Iteration time: 11.73s
                        Total time: 4799.15s
                               ETA: 1320945.4s

################################################################################
                    [1m Learning iteration 362/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.860s, learning 0.242s)
               Value function loss: 25.8181
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 257.80
               Mean episode length: 124.88
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5947392
                    Iteration time: 12.10s
                        Total time: 4811.25s
                               ETA: 1320614.9s

################################################################################
                    [1m Learning iteration 363/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.815s, learning 0.161s)
               Value function loss: 21.4093
                    Surrogate loss: -0.0109
             Mean action noise std: 0.79
                       Mean reward: 256.86
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5963776
                    Iteration time: 11.98s
                        Total time: 4823.22s
                               ETA: 1320251.7s

################################################################################
                    [1m Learning iteration 364/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.660s, learning 0.162s)
               Value function loss: 23.8626
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 256.16
               Mean episode length: 124.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5980160
                    Iteration time: 11.82s
                        Total time: 4835.05s
                               ETA: 1319848.7s

################################################################################
                    [1m Learning iteration 365/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.841s, learning 0.168s)
               Value function loss: 21.5145
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 257.65
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 5996544
                    Iteration time: 12.01s
                        Total time: 4847.06s
                               ETA: 1319498.5s

################################################################################
                    [1m Learning iteration 366/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.089s, learning 0.255s)
               Value function loss: 15.4305
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 255.59
               Mean episode length: 124.99
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6012928
                    Iteration time: 12.34s
                        Total time: 4859.40s
                               ETA: 1319241.3s

################################################################################
                    [1m Learning iteration 367/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.107s, learning 0.170s)
               Value function loss: 13.9408
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 253.76
               Mean episode length: 124.13
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6029312
                    Iteration time: 12.28s
                        Total time: 4871.68s
                               ETA: 1318967.1s

################################################################################
                    [1m Learning iteration 368/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.213s, learning 0.197s)
               Value function loss: 11.9144
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 254.81
               Mean episode length: 124.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6045696
                    Iteration time: 12.41s
                        Total time: 4884.09s
                               ETA: 1318730.2s

################################################################################
                    [1m Learning iteration 369/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.520s, learning 0.176s)
               Value function loss: 11.0247
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 257.52
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6062080
                    Iteration time: 11.70s
                        Total time: 4895.78s
                               ETA: 1318302.3s

################################################################################
                    [1m Learning iteration 370/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.565s, learning 0.192s)
               Value function loss: 10.8242
                    Surrogate loss: -0.0124
             Mean action noise std: 0.79
                       Mean reward: 258.43
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6078464
                    Iteration time: 11.76s
                        Total time: 4907.54s
                               ETA: 1317893.0s

################################################################################
                    [1m Learning iteration 371/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.802s, learning 0.167s)
               Value function loss: 7.9486
                    Surrogate loss: -0.0076
             Mean action noise std: 0.79
                       Mean reward: 254.92
               Mean episode length: 124.58
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6094848
                    Iteration time: 11.97s
                        Total time: 4919.51s
                               ETA: 1317542.6s

################################################################################
                    [1m Learning iteration 372/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.173s)
               Value function loss: 6.8151
                    Surrogate loss: -0.0040
             Mean action noise std: 0.79
                       Mean reward: 255.55
               Mean episode length: 124.36
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6111232
                    Iteration time: 12.21s
                        Total time: 4931.72s
                               ETA: 1317259.5s

################################################################################
                    [1m Learning iteration 373/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.635s, learning 0.192s)
               Value function loss: 9.1550
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 254.31
               Mean episode length: 123.81
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6127616
                    Iteration time: 11.83s
                        Total time: 4943.55s
                               ETA: 1316874.7s

################################################################################
                    [1m Learning iteration 374/100000 [0m                     

                       Computation: 1281 steps/s (collection: 12.432s, learning 0.352s)
               Value function loss: 64.1215
                    Surrogate loss: 0.0355
             Mean action noise std: 0.79
                       Mean reward: 255.58
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6144000
                    Iteration time: 12.78s
                        Total time: 4956.33s
                               ETA: 1316746.0s

################################################################################
                    [1m Learning iteration 375/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.854s, learning 0.164s)
               Value function loss: 7.6397
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 254.69
               Mean episode length: 124.78
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6160384
                    Iteration time: 12.02s
                        Total time: 4968.35s
                               ETA: 1316415.4s

################################################################################
                    [1m Learning iteration 376/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.002s, learning 0.188s)
               Value function loss: 16.7081
                    Surrogate loss: -0.0090
             Mean action noise std: 0.79
                       Mean reward: 254.27
               Mean episode length: 124.74
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6176768
                    Iteration time: 12.19s
                        Total time: 4980.54s
                               ETA: 1316131.7s

################################################################################
                    [1m Learning iteration 377/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.251s, learning 0.164s)
               Value function loss: 114.4219
                    Surrogate loss: 0.0048
             Mean action noise std: 0.79
                       Mean reward: 257.00
               Mean episode length: 125.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 5.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6193152
                    Iteration time: 12.41s
                        Total time: 4992.96s
                               ETA: 1315908.7s

################################################################################
                    [1m Learning iteration 378/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.073s, learning 0.160s)
               Value function loss: 22.5996
                    Surrogate loss: 0.0080
             Mean action noise std: 0.79
                       Mean reward: 255.07
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6209536
                    Iteration time: 12.23s
                        Total time: 5005.19s
                               ETA: 1315639.2s

################################################################################
                    [1m Learning iteration 379/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.796s, learning 0.202s)
               Value function loss: 18.4698
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 255.51
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6225920
                    Iteration time: 12.00s
                        Total time: 5017.19s
                               ETA: 1315309.4s

################################################################################
                    [1m Learning iteration 380/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.855s, learning 0.173s)
               Value function loss: 21.9473
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 254.80
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6242304
                    Iteration time: 12.03s
                        Total time: 5029.22s
                               ETA: 1314989.1s

################################################################################
                    [1m Learning iteration 381/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.975s, learning 0.191s)
               Value function loss: 18.9451
                    Surrogate loss: -0.0146
             Mean action noise std: 0.79
                       Mean reward: 252.74
               Mean episode length: 123.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6258688
                    Iteration time: 12.17s
                        Total time: 5041.39s
                               ETA: 1314706.2s

################################################################################
                    [1m Learning iteration 382/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.878s, learning 0.192s)
               Value function loss: 12.9284
                    Surrogate loss: -0.0026
             Mean action noise std: 0.79
                       Mean reward: 253.48
               Mean episode length: 123.93
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6275072
                    Iteration time: 12.07s
                        Total time: 5053.46s
                               ETA: 1314400.0s

################################################################################
                    [1m Learning iteration 383/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.810s, learning 0.157s)
               Value function loss: 13.8906
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 253.60
               Mean episode length: 124.94
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6291456
                    Iteration time: 11.97s
                        Total time: 5065.42s
                               ETA: 1314068.5s

################################################################################
                    [1m Learning iteration 384/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.109s, learning 0.194s)
               Value function loss: 9.0210
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 252.27
               Mean episode length: 123.70
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6307840
                    Iteration time: 12.30s
                        Total time: 5077.73s
                               ETA: 1313825.5s

################################################################################
                    [1m Learning iteration 385/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.947s, learning 0.253s)
               Value function loss: 12.1233
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 253.28
               Mean episode length: 123.70
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6324224
                    Iteration time: 12.20s
                        Total time: 5089.93s
                               ETA: 1313557.1s

################################################################################
                    [1m Learning iteration 386/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.909s, learning 0.174s)
               Value function loss: 8.5886
                    Surrogate loss: -0.0034
             Mean action noise std: 0.79
                       Mean reward: 252.54
               Mean episode length: 123.31
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6340608
                    Iteration time: 12.08s
                        Total time: 5102.01s
                               ETA: 1313259.9s

################################################################################
                    [1m Learning iteration 387/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.168s, learning 0.214s)
               Value function loss: 7.4840
                    Surrogate loss: -0.0033
             Mean action noise std: 0.79
                       Mean reward: 250.89
               Mean episode length: 122.91
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6356992
                    Iteration time: 12.38s
                        Total time: 5114.39s
                               ETA: 1313040.9s

################################################################################
                    [1m Learning iteration 388/100000 [0m                     

                       Computation: 1304 steps/s (collection: 12.345s, learning 0.212s)
               Value function loss: 8.1531
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 248.79
               Mean episode length: 122.27
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6373376
                    Iteration time: 12.56s
                        Total time: 5126.95s
                               ETA: 1312867.9s

################################################################################
                    [1m Learning iteration 389/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.696s, learning 0.184s)
               Value function loss: 8.2222
                    Surrogate loss: 0.0023
             Mean action noise std: 0.79
                       Mean reward: 248.67
               Mean episode length: 122.72
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6389760
                    Iteration time: 11.88s
                        Total time: 5138.83s
                               ETA: 1312522.7s

################################################################################
                    [1m Learning iteration 390/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.420s, learning 0.167s)
               Value function loss: 62.3262
                    Surrogate loss: 0.0149
             Mean action noise std: 0.79
                       Mean reward: 253.28
               Mean episode length: 124.07
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6406144
                    Iteration time: 11.59s
                        Total time: 5150.42s
                               ETA: 1312104.7s

################################################################################
                    [1m Learning iteration 391/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.837s, learning 0.168s)
               Value function loss: 9.9338
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 253.37
               Mean episode length: 124.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6422528
                    Iteration time: 12.01s
                        Total time: 5162.42s
                               ETA: 1311795.0s

################################################################################
                    [1m Learning iteration 392/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.216s, learning 0.157s)
               Value function loss: 106.0371
                    Surrogate loss: 0.0022
             Mean action noise std: 0.79
                       Mean reward: 254.88
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.01
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6438912
                    Iteration time: 12.37s
                        Total time: 5174.79s
                               ETA: 1311579.9s

################################################################################
                    [1m Learning iteration 393/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.693s, learning 0.180s)
               Value function loss: 56.4817
                    Surrogate loss: -0.0164
             Mean action noise std: 0.79
                       Mean reward: 256.72
               Mean episode length: 124.80
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6455296
                    Iteration time: 11.87s
                        Total time: 5186.67s
                               ETA: 1311239.5s

################################################################################
                    [1m Learning iteration 394/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.849s, learning 0.174s)
               Value function loss: 38.7620
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 253.17
               Mean episode length: 123.38
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6471680
                    Iteration time: 12.02s
                        Total time: 5198.69s
                               ETA: 1310938.7s

################################################################################
                    [1m Learning iteration 395/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.852s, learning 0.242s)
               Value function loss: 21.1824
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 256.02
               Mean episode length: 124.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6488064
                    Iteration time: 12.09s
                        Total time: 5210.79s
                               ETA: 1310657.3s

################################################################################
                    [1m Learning iteration 396/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.372s, learning 0.166s)
               Value function loss: 17.4519
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 255.64
               Mean episode length: 124.04
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6504448
                    Iteration time: 12.54s
                        Total time: 5223.32s
                               ETA: 1310488.6s

################################################################################
                    [1m Learning iteration 397/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.930s, learning 0.201s)
               Value function loss: 15.4863
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 254.24
               Mean episode length: 123.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6520832
                    Iteration time: 12.13s
                        Total time: 5235.46s
                               ETA: 1310218.7s

################################################################################
                    [1m Learning iteration 398/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.015s, learning 0.169s)
               Value function loss: 12.8660
                    Surrogate loss: -0.0120
             Mean action noise std: 0.79
                       Mean reward: 254.54
               Mean episode length: 122.95
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6537216
                    Iteration time: 12.18s
                        Total time: 5247.64s
                               ETA: 1309963.3s

################################################################################
                    [1m Learning iteration 399/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.969s, learning 0.188s)
               Value function loss: 12.2471
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 256.32
               Mean episode length: 124.22
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6553600
                    Iteration time: 12.16s
                        Total time: 5259.80s
                               ETA: 1309702.2s

################################################################################
                    [1m Learning iteration 400/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.805s, learning 0.172s)
               Value function loss: 10.3528
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 254.96
               Mean episode length: 124.20
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6569984
                    Iteration time: 11.98s
                        Total time: 5271.77s
                               ETA: 1309397.9s

################################################################################
                    [1m Learning iteration 401/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.649s, learning 0.171s)
               Value function loss: 13.6810
                    Surrogate loss: -0.0129
             Mean action noise std: 0.79
                       Mean reward: 257.14
               Mean episode length: 124.47
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6586368
                    Iteration time: 11.82s
                        Total time: 5283.59s
                               ETA: 1309056.1s

################################################################################
                    [1m Learning iteration 402/100000 [0m                     

                       Computation: 1338 steps/s (collection: 11.972s, learning 0.270s)
               Value function loss: 9.5249
                    Surrogate loss: -0.0077
             Mean action noise std: 0.79
                       Mean reward: 257.35
               Mean episode length: 124.47
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6602752
                    Iteration time: 12.24s
                        Total time: 5295.83s
                               ETA: 1308820.1s

################################################################################
                    [1m Learning iteration 403/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.092s, learning 0.163s)
               Value function loss: 10.0745
                    Surrogate loss: -0.0099
             Mean action noise std: 0.79
                       Mean reward: 257.62
               Mean episode length: 124.47
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6619136
                    Iteration time: 12.26s
                        Total time: 5308.09s
                               ETA: 1308588.6s

################################################################################
                    [1m Learning iteration 404/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.718s, learning 0.174s)
               Value function loss: 9.5120
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 257.01
               Mean episode length: 124.55
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6635520
                    Iteration time: 11.89s
                        Total time: 5319.98s
                               ETA: 1308269.0s

################################################################################
                    [1m Learning iteration 405/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.847s, learning 0.165s)
               Value function loss: 7.8393
                    Surrogate loss: 0.0075
             Mean action noise std: 0.79
                       Mean reward: 257.04
               Mean episode length: 124.55
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6651904
                    Iteration time: 12.01s
                        Total time: 5331.99s
                               ETA: 1307980.1s

################################################################################
                    [1m Learning iteration 406/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.934s, learning 0.184s)
               Value function loss: 29.9338
                    Surrogate loss: 0.0050
             Mean action noise std: 0.79
                       Mean reward: 254.42
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6668288
                    Iteration time: 12.12s
                        Total time: 5344.11s
                               ETA: 1307718.7s

################################################################################
                    [1m Learning iteration 407/100000 [0m                     

                       Computation: 1346 steps/s (collection: 11.931s, learning 0.237s)
               Value function loss: 14.6613
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 254.49
               Mean episode length: 123.85
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6684672
                    Iteration time: 12.17s
                        Total time: 5356.28s
                               ETA: 1307470.6s

################################################################################
                    [1m Learning iteration 408/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.049s, learning 0.206s)
               Value function loss: 184.7565
                    Surrogate loss: 0.0182
             Mean action noise std: 0.79
                       Mean reward: 256.76
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 6.00
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6701056
                    Iteration time: 12.26s
                        Total time: 5368.54s
                               ETA: 1307244.9s

################################################################################
                    [1m Learning iteration 409/100000 [0m                     

                       Computation: 1412 steps/s (collection: 11.391s, learning 0.206s)
               Value function loss: 27.7439
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 254.39
               Mean episode length: 124.52
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6717440
                    Iteration time: 11.60s
                        Total time: 5380.13s
                               ETA: 1306860.4s

################################################################################
                    [1m Learning iteration 410/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.791s, learning 0.185s)
               Value function loss: 28.3831
                    Surrogate loss: 0.0025
             Mean action noise std: 0.79
                       Mean reward: 255.93
               Mean episode length: 123.53
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6733824
                    Iteration time: 11.98s
                        Total time: 5392.11s
                               ETA: 1306569.5s

################################################################################
                    [1m Learning iteration 411/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.502s, learning 0.269s)
               Value function loss: 30.8900
                    Surrogate loss: -0.0071
             Mean action noise std: 0.79
                       Mean reward: 256.22
               Mean episode length: 124.52
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6750208
                    Iteration time: 11.77s
                        Total time: 5403.88s
                               ETA: 1306230.5s

################################################################################
                    [1m Learning iteration 412/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.038s, learning 0.271s)
               Value function loss: 27.0583
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 258.24
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6766592
                    Iteration time: 12.31s
                        Total time: 5416.19s
                               ETA: 1306022.7s

################################################################################
                    [1m Learning iteration 413/100000 [0m                     

                       Computation: 1277 steps/s (collection: 12.515s, learning 0.307s)
               Value function loss: 22.2934
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 252.14
               Mean episode length: 122.13
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6782976
                    Iteration time: 12.82s
                        Total time: 5429.01s
                               ETA: 1305939.2s

################################################################################
                    [1m Learning iteration 414/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.847s, learning 0.156s)
               Value function loss: 28.1097
                    Surrogate loss: -0.0104
             Mean action noise std: 0.79
                       Mean reward: 251.89
               Mean episode length: 122.61
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6799360
                    Iteration time: 12.00s
                        Total time: 5441.01s
                               ETA: 1305659.7s

################################################################################
                    [1m Learning iteration 415/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.754s, learning 0.169s)
               Value function loss: 15.5634
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 256.51
               Mean episode length: 124.26
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6815744
                    Iteration time: 11.92s
                        Total time: 5452.94s
                               ETA: 1305362.3s

################################################################################
                    [1m Learning iteration 416/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.600s, learning 0.175s)
               Value function loss: 17.0787
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 250.48
               Mean episode length: 121.76
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6832128
                    Iteration time: 11.78s
                        Total time: 5464.71s
                               ETA: 1305030.9s

################################################################################
                    [1m Learning iteration 417/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.605s, learning 0.160s)
               Value function loss: 13.5080
                    Surrogate loss: -0.0048
             Mean action noise std: 0.79
                       Mean reward: 249.50
               Mean episode length: 121.82
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6848512
                    Iteration time: 11.77s
                        Total time: 5476.48s
                               ETA: 1304698.6s

################################################################################
                    [1m Learning iteration 418/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.734s, learning 0.186s)
               Value function loss: 12.2805
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 247.48
               Mean episode length: 121.71
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6864896
                    Iteration time: 11.92s
                        Total time: 5488.40s
                               ETA: 1304404.8s

################################################################################
                    [1m Learning iteration 419/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.107s, learning 0.180s)
               Value function loss: 9.4503
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 250.68
               Mean episode length: 122.38
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6881280
                    Iteration time: 12.29s
                        Total time: 5500.69s
                               ETA: 1304199.4s

################################################################################
                    [1m Learning iteration 420/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.075s, learning 0.187s)
               Value function loss: 12.6143
                    Surrogate loss: 0.0011
             Mean action noise std: 0.79
                       Mean reward: 254.20
               Mean episode length: 123.27
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6897664
                    Iteration time: 12.26s
                        Total time: 5512.95s
                               ETA: 1303988.9s

################################################################################
                    [1m Learning iteration 421/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.294s, learning 0.163s)
               Value function loss: 100.3812
                    Surrogate loss: 0.0192
             Mean action noise std: 0.79
                       Mean reward: 251.13
               Mean episode length: 124.48
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6914048
                    Iteration time: 12.46s
                        Total time: 5525.40s
                               ETA: 1303825.2s

################################################################################
                    [1m Learning iteration 422/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.685s, learning 0.165s)
               Value function loss: 12.6775
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 250.48
               Mean episode length: 124.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6930432
                    Iteration time: 11.85s
                        Total time: 5537.25s
                               ETA: 1303519.4s

################################################################################
                    [1m Learning iteration 423/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.725s, learning 0.170s)
               Value function loss: 59.9538
                    Surrogate loss: 0.0074
             Mean action noise std: 0.79
                       Mean reward: 258.87
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6946816
                    Iteration time: 11.90s
                        Total time: 5549.15s
                               ETA: 1303225.7s

################################################################################
                    [1m Learning iteration 424/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.788s, learning 0.199s)
               Value function loss: 122.0235
                    Surrogate loss: 0.0040
             Mean action noise std: 0.79
                       Mean reward: 259.14
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6963200
                    Iteration time: 11.99s
                        Total time: 5561.14s
                               ETA: 1302954.6s

################################################################################
                    [1m Learning iteration 425/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.925s, learning 0.199s)
               Value function loss: 27.8143
                    Surrogate loss: -0.0114
             Mean action noise std: 0.79
                       Mean reward: 257.27
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6979584
                    Iteration time: 12.12s
                        Total time: 5573.26s
                               ETA: 1302717.0s

################################################################################
                    [1m Learning iteration 426/100000 [0m                     

                       Computation: 1395 steps/s (collection: 11.583s, learning 0.159s)
               Value function loss: 30.4844
                    Surrogate loss: -0.0153
             Mean action noise std: 0.79
                       Mean reward: 254.00
               Mean episode length: 122.88
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 6995968
                    Iteration time: 11.74s
                        Total time: 5585.00s
                               ETA: 1302391.4s

################################################################################
                    [1m Learning iteration 427/100000 [0m                     

                       Computation: 1307 steps/s (collection: 12.327s, learning 0.203s)
               Value function loss: 29.2895
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 255.56
               Mean episode length: 124.21
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7012352
                    Iteration time: 12.53s
                        Total time: 5597.53s
                               ETA: 1302250.2s

################################################################################
                    [1m Learning iteration 428/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.136s, learning 0.160s)
               Value function loss: 29.3907
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 255.68
               Mean episode length: 123.65
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7028736
                    Iteration time: 12.30s
                        Total time: 5609.83s
                               ETA: 1302055.4s

################################################################################
                    [1m Learning iteration 429/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.620s, learning 0.165s)
               Value function loss: 20.7839
                    Surrogate loss: -0.0160
             Mean action noise std: 0.79
                       Mean reward: 253.62
               Mean episode length: 122.63
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7045120
                    Iteration time: 11.79s
                        Total time: 5621.61s
                               ETA: 1301743.3s

################################################################################
                    [1m Learning iteration 430/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.049s, learning 0.192s)
               Value function loss: 17.6808
                    Surrogate loss: -0.0165
             Mean action noise std: 0.79
                       Mean reward: 255.97
               Mean episode length: 124.08
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7061504
                    Iteration time: 12.24s
                        Total time: 5633.85s
                               ETA: 1301537.9s

################################################################################
                    [1m Learning iteration 431/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.633s, learning 0.246s)
               Value function loss: 13.8687
                    Surrogate loss: -0.0166
             Mean action noise std: 0.79
                       Mean reward: 255.55
               Mean episode length: 124.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7077888
                    Iteration time: 11.88s
                        Total time: 5645.73s
                               ETA: 1301250.0s

################################################################################
                    [1m Learning iteration 432/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.683s, learning 0.160s)
               Value function loss: 19.4593
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 258.45
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7094272
                    Iteration time: 11.84s
                        Total time: 5657.58s
                               ETA: 1300955.0s

################################################################################
                    [1m Learning iteration 433/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.529s, learning 0.161s)
               Value function loss: 10.9263
                    Surrogate loss: 0.0007
             Mean action noise std: 0.79
                       Mean reward: 258.26
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7110656
                    Iteration time: 11.69s
                        Total time: 5669.27s
                               ETA: 1300626.4s

################################################################################
                    [1m Learning iteration 434/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.746s, learning 0.213s)
               Value function loss: 11.7344
                    Surrogate loss: 0.0035
             Mean action noise std: 0.79
                       Mean reward: 257.84
               Mean episode length: 124.56
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7127040
                    Iteration time: 11.96s
                        Total time: 5681.23s
                               ETA: 1300360.7s

################################################################################
                    [1m Learning iteration 435/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.643s, learning 0.158s)
               Value function loss: 11.3542
                    Surrogate loss: -0.0082
             Mean action noise std: 0.79
                       Mean reward: 253.97
               Mean episode length: 123.08
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7143424
                    Iteration time: 11.80s
                        Total time: 5693.03s
                               ETA: 1300059.9s

################################################################################
                    [1m Learning iteration 436/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.082s, learning 0.281s)
               Value function loss: 9.8267
                    Surrogate loss: -0.0057
             Mean action noise std: 0.79
                       Mean reward: 253.63
               Mean episode length: 123.52
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7159808
                    Iteration time: 12.36s
                        Total time: 5705.39s
                               ETA: 1299888.6s

################################################################################
                    [1m Learning iteration 437/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.763s, learning 0.160s)
               Value function loss: 55.1236
                    Surrogate loss: 0.0090
             Mean action noise std: 0.79
                       Mean reward: 256.66
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7176192
                    Iteration time: 11.92s
                        Total time: 5717.31s
                               ETA: 1299618.0s

################################################################################
                    [1m Learning iteration 438/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.560s, learning 0.164s)
               Value function loss: 13.4962
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 258.11
               Mean episode length: 125.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7192576
                    Iteration time: 11.72s
                        Total time: 5729.04s
                               ETA: 1299303.5s

################################################################################
                    [1m Learning iteration 439/100000 [0m                     

                       Computation: 1376 steps/s (collection: 11.714s, learning 0.191s)
               Value function loss: 142.1998
                    Surrogate loss: 0.0096
             Mean action noise std: 0.79
                       Mean reward: 258.20
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 6.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7208960
                    Iteration time: 11.91s
                        Total time: 5740.94s
                               ETA: 1299031.3s

################################################################################
                    [1m Learning iteration 440/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.846s, learning 0.230s)
               Value function loss: 34.2501
                    Surrogate loss: -0.0116
             Mean action noise std: 0.79
                       Mean reward: 258.82
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7225344
                    Iteration time: 12.08s
                        Total time: 5753.02s
                               ETA: 1298799.1s

################################################################################
                    [1m Learning iteration 441/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.761s, learning 0.163s)
               Value function loss: 24.1200
                    Surrogate loss: -0.0173
             Mean action noise std: 0.79
                       Mean reward: 257.40
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7241728
                    Iteration time: 11.92s
                        Total time: 5764.94s
                               ETA: 1298533.3s

################################################################################
                    [1m Learning iteration 442/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.464s, learning 0.168s)
               Value function loss: 26.4233
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 259.23
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7258112
                    Iteration time: 11.63s
                        Total time: 5776.57s
                               ETA: 1298203.1s

################################################################################
                    [1m Learning iteration 443/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.556s, learning 0.223s)
               Value function loss: 22.4458
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 258.76
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7274496
                    Iteration time: 11.78s
                        Total time: 5788.35s
                               ETA: 1297907.5s

################################################################################
                    [1m Learning iteration 444/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.926s, learning 0.163s)
               Value function loss: 20.9557
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 256.53
               Mean episode length: 123.15
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7290880
                    Iteration time: 12.09s
                        Total time: 5800.44s
                               ETA: 1297682.4s

################################################################################
                    [1m Learning iteration 445/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.221s, learning 0.193s)
               Value function loss: 18.3184
                    Surrogate loss: -0.0155
             Mean action noise std: 0.79
                       Mean reward: 258.67
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7307264
                    Iteration time: 12.41s
                        Total time: 5812.85s
                               ETA: 1297530.8s

################################################################################
                    [1m Learning iteration 446/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.627s, learning 0.162s)
               Value function loss: 16.0082
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 257.34
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7323648
                    Iteration time: 11.79s
                        Total time: 5824.64s
                               ETA: 1297240.5s

################################################################################
                    [1m Learning iteration 447/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.755s, learning 0.160s)
               Value function loss: 14.9636
                    Surrogate loss: -0.0111
             Mean action noise std: 0.79
                       Mean reward: 256.95
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7340032
                    Iteration time: 11.92s
                        Total time: 5836.56s
                               ETA: 1296979.6s

################################################################################
                    [1m Learning iteration 448/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.596s, learning 0.162s)
               Value function loss: 16.6581
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 256.42
               Mean episode length: 124.67
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7356416
                    Iteration time: 11.76s
                        Total time: 5848.32s
                               ETA: 1296685.1s

################################################################################
                    [1m Learning iteration 449/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.879s, learning 0.159s)
               Value function loss: 10.3929
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 257.09
               Mean episode length: 124.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7372800
                    Iteration time: 12.04s
                        Total time: 5860.36s
                               ETA: 1296453.9s

################################################################################
                    [1m Learning iteration 450/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.184s, learning 0.185s)
               Value function loss: 10.7443
                    Surrogate loss: -0.0049
             Mean action noise std: 0.79
                       Mean reward: 258.10
               Mean episode length: 124.67
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7389184
                    Iteration time: 12.37s
                        Total time: 5872.72s
                               ETA: 1296296.6s

################################################################################
                    [1m Learning iteration 451/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.561s, learning 0.157s)
               Value function loss: 12.4930
                    Surrogate loss: -0.0035
             Mean action noise std: 0.79
                       Mean reward: 259.37
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7405568
                    Iteration time: 11.72s
                        Total time: 5884.44s
                               ETA: 1295996.4s

################################################################################
                    [1m Learning iteration 452/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.813s, learning 0.160s)
               Value function loss: 98.0989
                    Surrogate loss: 0.0152
             Mean action noise std: 0.79
                       Mean reward: 256.76
               Mean episode length: 125.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7421952
                    Iteration time: 11.97s
                        Total time: 5896.42s
                               ETA: 1295753.7s

################################################################################
                    [1m Learning iteration 453/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.622s, learning 0.194s)
               Value function loss: 12.3192
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 257.36
               Mean episode length: 124.83
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.92
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7438336
                    Iteration time: 11.82s
                        Total time: 5908.23s
                               ETA: 1295477.4s

################################################################################
                    [1m Learning iteration 454/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.560s, learning 0.160s)
               Value function loss: 15.6337
                    Surrogate loss: -0.0004
             Mean action noise std: 0.79
                       Mean reward: 258.92
               Mean episode length: 124.83
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7454720
                    Iteration time: 11.72s
                        Total time: 5919.95s
                               ETA: 1295181.3s

################################################################################
                    [1m Learning iteration 455/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.791s, learning 0.298s)
               Value function loss: 159.5039
                    Surrogate loss: 0.0264
             Mean action noise std: 0.79
                       Mean reward: 258.19
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 5.97
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7471104
                    Iteration time: 12.09s
                        Total time: 5932.04s
                               ETA: 1294967.0s

################################################################################
                    [1m Learning iteration 456/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.736s, learning 0.178s)
               Value function loss: 19.8783
                    Surrogate loss: -0.0135
             Mean action noise std: 0.79
                       Mean reward: 255.64
               Mean episode length: 123.88
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7487488
                    Iteration time: 11.91s
                        Total time: 5943.95s
                               ETA: 1294715.4s

################################################################################
                    [1m Learning iteration 457/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.899s, learning 0.216s)
               Value function loss: 21.9918
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 255.32
               Mean episode length: 124.62
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7503872
                    Iteration time: 12.12s
                        Total time: 5956.07s
                               ETA: 1294508.7s

################################################################################
                    [1m Learning iteration 458/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.022s, learning 0.204s)
               Value function loss: 23.8082
                    Surrogate loss: -0.0121
             Mean action noise std: 0.79
                       Mean reward: 255.82
               Mean episode length: 124.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7520256
                    Iteration time: 12.23s
                        Total time: 5968.29s
                               ETA: 1294326.8s

################################################################################
                    [1m Learning iteration 459/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.785s, learning 0.322s)
               Value function loss: 23.9444
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 255.85
               Mean episode length: 124.08
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7536640
                    Iteration time: 12.11s
                        Total time: 5980.40s
                               ETA: 1294120.0s

################################################################################
                    [1m Learning iteration 460/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.792s, learning 0.166s)
               Value function loss: 16.9876
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 259.31
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7553024
                    Iteration time: 11.96s
                        Total time: 5992.36s
                               ETA: 1293881.8s

################################################################################
                    [1m Learning iteration 461/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.159s, learning 0.213s)
               Value function loss: 19.0309
                    Surrogate loss: -0.0138
             Mean action noise std: 0.79
                       Mean reward: 255.90
               Mean episode length: 123.92
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7569408
                    Iteration time: 12.37s
                        Total time: 6004.73s
                               ETA: 1293733.7s

################################################################################
                    [1m Learning iteration 462/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.812s, learning 0.176s)
               Value function loss: 12.1520
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 253.09
               Mean episode length: 123.25
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7585792
                    Iteration time: 11.99s
                        Total time: 6016.72s
                               ETA: 1293503.6s

################################################################################
                    [1m Learning iteration 463/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.553s, learning 0.157s)
               Value function loss: 17.3306
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 255.03
               Mean episode length: 123.27
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7602176
                    Iteration time: 11.71s
                        Total time: 6028.43s
                               ETA: 1293214.9s

################################################################################
                    [1m Learning iteration 464/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.495s, learning 0.166s)
               Value function loss: 12.3411
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 256.54
               Mean episode length: 123.94
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7618560
                    Iteration time: 11.66s
                        Total time: 6040.09s
                               ETA: 1292916.9s

################################################################################
                    [1m Learning iteration 465/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.850s, learning 0.197s)
               Value function loss: 11.4298
                    Surrogate loss: -0.0078
             Mean action noise std: 0.79
                       Mean reward: 256.97
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7634944
                    Iteration time: 12.05s
                        Total time: 6052.14s
                               ETA: 1292702.6s

################################################################################
                    [1m Learning iteration 466/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.574s, learning 0.179s)
               Value function loss: 9.6732
                    Surrogate loss: 0.0170
             Mean action noise std: 0.79
                       Mean reward: 257.29
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7651328
                    Iteration time: 11.75s
                        Total time: 6063.89s
                               ETA: 1292426.4s

################################################################################
                    [1m Learning iteration 467/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.678s, learning 0.198s)
               Value function loss: 10.0433
                    Surrogate loss: -0.0028
             Mean action noise std: 0.79
                       Mean reward: 256.88
               Mean episode length: 124.71
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7667712
                    Iteration time: 11.88s
                        Total time: 6075.76s
                               ETA: 1292177.5s

################################################################################
                    [1m Learning iteration 468/100000 [0m                     

                       Computation: 912 steps/s (collection: 17.747s, learning 0.200s)
               Value function loss: 80.5284
                    Surrogate loss: 0.0008
             Mean action noise std: 0.79
                       Mean reward: 257.87
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7684096
                    Iteration time: 17.95s
                        Total time: 6093.71s
                               ETA: 1293218.2s

################################################################################
                    [1m Learning iteration 469/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.732s, learning 0.176s)
               Value function loss: 11.9975
                    Surrogate loss: -0.0069
             Mean action noise std: 0.79
                       Mean reward: 257.99
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7700480
                    Iteration time: 23.91s
                        Total time: 6117.62s
                               ETA: 1295516.7s

################################################################################
                    [1m Learning iteration 470/100000 [0m                     

                       Computation: 718 steps/s (collection: 22.636s, learning 0.172s)
               Value function loss: 76.4231
                    Surrogate loss: -0.0061
             Mean action noise std: 0.79
                       Mean reward: 256.76
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7716864
                    Iteration time: 22.81s
                        Total time: 6140.43s
                               ETA: 1297572.8s

################################################################################
                    [1m Learning iteration 471/100000 [0m                     

                       Computation: 705 steps/s (collection: 23.004s, learning 0.231s)
               Value function loss: 70.0669
                    Surrogate loss: 0.0116
             Mean action noise std: 0.79
                       Mean reward: 257.13
               Mean episode length: 125.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 6.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7733248
                    Iteration time: 23.24s
                        Total time: 6163.66s
                               ETA: 1299710.2s

################################################################################
                    [1m Learning iteration 472/100000 [0m                     

                       Computation: 714 steps/s (collection: 22.729s, learning 0.205s)
               Value function loss: 23.3674
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 254.43
               Mean episode length: 123.89
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7749632
                    Iteration time: 22.93s
                        Total time: 6186.60s
                               ETA: 1301775.0s

################################################################################
                    [1m Learning iteration 473/100000 [0m                     

                       Computation: 714 steps/s (collection: 22.776s, learning 0.162s)
               Value function loss: 25.8874
                    Surrogate loss: -0.0088
             Mean action noise std: 0.79
                       Mean reward: 255.02
               Mean episode length: 123.95
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7766016
                    Iteration time: 22.94s
                        Total time: 6209.53s
                               ETA: 1303831.9s

################################################################################
                    [1m Learning iteration 474/100000 [0m                     

                       Computation: 708 steps/s (collection: 22.951s, learning 0.158s)
               Value function loss: 23.4807
                    Surrogate loss: -0.0093
             Mean action noise std: 0.79
                       Mean reward: 255.62
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7782400
                    Iteration time: 23.11s
                        Total time: 6232.64s
                               ETA: 1305916.0s

################################################################################
                    [1m Learning iteration 475/100000 [0m                     

                       Computation: 709 steps/s (collection: 22.908s, learning 0.187s)
               Value function loss: 24.1219
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 255.33
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7798784
                    Iteration time: 23.10s
                        Total time: 6255.74s
                               ETA: 1307988.3s

################################################################################
                    [1m Learning iteration 476/100000 [0m                     

                       Computation: 686 steps/s (collection: 23.686s, learning 0.192s)
               Value function loss: 18.6101
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 254.54
               Mean episode length: 124.15
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7815168
                    Iteration time: 23.88s
                        Total time: 6279.62s
                               ETA: 1310215.0s

################################################################################
                    [1m Learning iteration 477/100000 [0m                     

                       Computation: 703 steps/s (collection: 23.038s, learning 0.247s)
               Value function loss: 15.6193
                    Surrogate loss: -0.0062
             Mean action noise std: 0.79
                       Mean reward: 254.06
               Mean episode length: 124.33
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7831552
                    Iteration time: 23.29s
                        Total time: 6302.90s
                               ETA: 1312309.0s

################################################################################
                    [1m Learning iteration 478/100000 [0m                     

                       Computation: 685 steps/s (collection: 23.681s, learning 0.225s)
               Value function loss: 13.8019
                    Surrogate loss: 0.0028
             Mean action noise std: 0.79
                       Mean reward: 254.88
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7847936
                    Iteration time: 23.91s
                        Total time: 6326.81s
                               ETA: 1314523.0s

################################################################################
                    [1m Learning iteration 479/100000 [0m                     

                       Computation: 682 steps/s (collection: 23.772s, learning 0.226s)
               Value function loss: 19.6889
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 251.61
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7864320
                    Iteration time: 24.00s
                        Total time: 6350.81s
                               ETA: 1316746.8s

################################################################################
                    [1m Learning iteration 480/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.271s, learning 0.315s)
               Value function loss: 9.0181
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 248.83
               Mean episode length: 124.13
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7880704
                    Iteration time: 23.59s
                        Total time: 6374.39s
                               ETA: 1318876.1s

################################################################################
                    [1m Learning iteration 481/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.424s, learning 0.165s)
               Value function loss: 12.5197
                    Surrogate loss: -0.0054
             Mean action noise std: 0.79
                       Mean reward: 248.41
               Mean episode length: 123.73
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7897088
                    Iteration time: 23.59s
                        Total time: 6397.98s
                               ETA: 1320997.1s

################################################################################
                    [1m Learning iteration 482/100000 [0m                     

                       Computation: 708 steps/s (collection: 22.947s, learning 0.174s)
               Value function loss: 11.1284
                    Surrogate loss: -0.0081
             Mean action noise std: 0.79
                       Mean reward: 251.54
               Mean episode length: 124.60
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7913472
                    Iteration time: 23.12s
                        Total time: 6421.10s
                               ETA: 1323012.7s

################################################################################
                    [1m Learning iteration 483/100000 [0m                     

                       Computation: 699 steps/s (collection: 23.274s, learning 0.162s)
               Value function loss: 10.0762
                    Surrogate loss: 0.0017
             Mean action noise std: 0.79
                       Mean reward: 246.92
               Mean episode length: 122.63
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7929856
                    Iteration time: 23.44s
                        Total time: 6444.54s
                               ETA: 1325084.8s

################################################################################
                    [1m Learning iteration 484/100000 [0m                     

                       Computation: 670 steps/s (collection: 24.260s, learning 0.170s)
               Value function loss: 46.6942
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 251.96
               Mean episode length: 124.92
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7946240
                    Iteration time: 24.43s
                        Total time: 6468.97s
                               ETA: 1327352.1s

################################################################################
                    [1m Learning iteration 485/100000 [0m                     

                       Computation: 705 steps/s (collection: 23.041s, learning 0.174s)
               Value function loss: 12.9062
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 252.47
               Mean episode length: 124.92
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7962624
                    Iteration time: 23.22s
                        Total time: 6492.18s
                               ETA: 1329361.3s

################################################################################
                    [1m Learning iteration 486/100000 [0m                     

                       Computation: 668 steps/s (collection: 24.267s, learning 0.226s)
               Value function loss: 179.2478
                    Surrogate loss: 0.0391
             Mean action noise std: 0.79
                       Mean reward: 247.53
               Mean episode length: 124.15
                  Mean reward/step: 1.93
       Mean episode length/episode: 6.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7979008
                    Iteration time: 24.49s
                        Total time: 6516.68s
                               ETA: 1331623.2s

################################################################################
                    [1m Learning iteration 487/100000 [0m                     

                       Computation: 681 steps/s (collection: 23.797s, learning 0.234s)
               Value function loss: 24.0768
                    Surrogate loss: -0.0125
             Mean action noise std: 0.79
                       Mean reward: 250.50
               Mean episode length: 125.00
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 7995392
                    Iteration time: 24.03s
                        Total time: 6540.71s
                               ETA: 1333781.6s

################################################################################
                    [1m Learning iteration 488/100000 [0m                     

                       Computation: 696 steps/s (collection: 23.359s, learning 0.176s)
               Value function loss: 24.9085
                    Surrogate loss: -0.0156
             Mean action noise std: 0.79
                       Mean reward: 245.52
               Mean episode length: 122.02
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8011776
                    Iteration time: 23.54s
                        Total time: 6564.24s
                               ETA: 1335830.2s

################################################################################
                    [1m Learning iteration 489/100000 [0m                     

                       Computation: 686 steps/s (collection: 23.612s, learning 0.256s)
               Value function loss: 22.7853
                    Surrogate loss: -0.0105
             Mean action noise std: 0.79
                       Mean reward: 250.90
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8028160
                    Iteration time: 23.87s
                        Total time: 6588.11s
                               ETA: 1337938.0s

################################################################################
                    [1m Learning iteration 490/100000 [0m                     

                       Computation: 688 steps/s (collection: 23.575s, learning 0.212s)
               Value function loss: 21.8691
                    Surrogate loss: -0.0128
             Mean action noise std: 0.79
                       Mean reward: 250.01
               Mean episode length: 124.39
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8044544
                    Iteration time: 23.79s
                        Total time: 6611.90s
                               ETA: 1340020.6s

################################################################################
                    [1m Learning iteration 491/100000 [0m                     

                       Computation: 702 steps/s (collection: 23.143s, learning 0.167s)
               Value function loss: 18.3901
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 248.38
               Mean episode length: 124.19
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8060928
                    Iteration time: 23.31s
                        Total time: 6635.21s
                               ETA: 1341998.1s

################################################################################
                    [1m Learning iteration 492/100000 [0m                     

                       Computation: 712 steps/s (collection: 22.771s, learning 0.226s)
               Value function loss: 16.9531
                    Surrogate loss: -0.0149
             Mean action noise std: 0.79
                       Mean reward: 247.07
               Mean episode length: 124.19
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8077312
                    Iteration time: 23.00s
                        Total time: 6658.21s
                               ETA: 1343904.3s

################################################################################
                    [1m Learning iteration 493/100000 [0m                     

                       Computation: 686 steps/s (collection: 23.698s, learning 0.160s)
               Value function loss: 14.2553
                    Surrogate loss: -0.0063
             Mean action noise std: 0.79
                       Mean reward: 245.68
               Mean episode length: 124.31
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8093696
                    Iteration time: 23.86s
                        Total time: 6682.06s
                               ETA: 1345976.1s

################################################################################
                    [1m Learning iteration 494/100000 [0m                     

                       Computation: 695 steps/s (collection: 23.409s, learning 0.161s)
               Value function loss: 16.2749
                    Surrogate loss: -0.0148
             Mean action noise std: 0.79
                       Mean reward: 244.10
               Mean episode length: 123.84
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8110080
                    Iteration time: 23.57s
                        Total time: 6705.63s
                               ETA: 1347981.6s

################################################################################
                    [1m Learning iteration 495/100000 [0m                     

                       Computation: 698 steps/s (collection: 23.317s, learning 0.155s)
               Value function loss: 14.0228
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 242.84
               Mean episode length: 123.36
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8126464
                    Iteration time: 23.47s
                        Total time: 6729.11s
                               ETA: 1349959.3s

################################################################################
                    [1m Learning iteration 496/100000 [0m                     

                       Computation: 692 steps/s (collection: 23.463s, learning 0.189s)
               Value function loss: 11.7685
                    Surrogate loss: -0.0066
             Mean action noise std: 0.79
                       Mean reward: 243.39
               Mean episode length: 123.46
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8142848
                    Iteration time: 23.65s
                        Total time: 6752.76s
                               ETA: 1351964.9s

################################################################################
                    [1m Learning iteration 497/100000 [0m                     

                       Computation: 676 steps/s (collection: 23.939s, learning 0.283s)
               Value function loss: 10.5304
                    Surrogate loss: -0.0020
             Mean action noise std: 0.79
                       Mean reward: 238.36
               Mean episode length: 121.80
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8159232
                    Iteration time: 24.22s
                        Total time: 6776.98s
                               ETA: 1354076.3s

################################################################################
                    [1m Learning iteration 498/100000 [0m                     

                       Computation: 701 steps/s (collection: 23.112s, learning 0.238s)
               Value function loss: 12.6258
                    Surrogate loss: -0.0034
             Mean action noise std: 0.79
                       Mean reward: 238.82
               Mean episode length: 121.99
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8175616
                    Iteration time: 23.35s
                        Total time: 6800.33s
                               ETA: 1356005.1s

################################################################################
                    [1m Learning iteration 499/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.645s, learning 0.327s)
               Value function loss: 83.5624
                    Surrogate loss: -0.0046
             Mean action noise std: 0.79
                       Mean reward: 245.26
               Mean episode length: 125.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8192000
                    Iteration time: 23.97s
                        Total time: 6824.30s
                               ETA: 1358049.8s

################################################################################
                    [1m Learning iteration 500/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.319s, learning 0.170s)
               Value function loss: 14.1032
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 244.34
               Mean episode length: 124.67
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8208384
                    Iteration time: 23.49s
                        Total time: 6847.79s
                               ETA: 1359990.6s

################################################################################
                    [1m Learning iteration 501/100000 [0m                     

                       Computation: 695 steps/s (collection: 23.373s, learning 0.172s)
               Value function loss: 20.5598
                    Surrogate loss: -0.0012
             Mean action noise std: 0.79
                       Mean reward: 245.38
               Mean episode length: 124.67
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8224768
                    Iteration time: 23.55s
                        Total time: 6871.34s
                               ETA: 1361934.7s

################################################################################
                    [1m Learning iteration 502/100000 [0m                     

                       Computation: 693 steps/s (collection: 23.432s, learning 0.192s)
               Value function loss: 128.4410
                    Surrogate loss: 0.0155
             Mean action noise std: 0.79
                       Mean reward: 247.97
               Mean episode length: 123.91
                  Mean reward/step: 1.92
       Mean episode length/episode: 6.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8241152
                    Iteration time: 23.62s
                        Total time: 6894.96s
                               ETA: 1363886.4s

################################################################################
                    [1m Learning iteration 503/100000 [0m                     

                       Computation: 698 steps/s (collection: 23.250s, learning 0.198s)
               Value function loss: 21.7073
                    Surrogate loss: -0.0154
             Mean action noise std: 0.79
                       Mean reward: 249.28
               Mean episode length: 124.66
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8257536
                    Iteration time: 23.45s
                        Total time: 6918.41s
                               ETA: 1365795.6s

################################################################################
                    [1m Learning iteration 504/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.295s, learning 0.184s)
               Value function loss: 23.3835
                    Surrogate loss: -0.0157
             Mean action noise std: 0.79
                       Mean reward: 254.23
               Mean episode length: 124.94
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8273920
                    Iteration time: 23.48s
                        Total time: 6941.89s
                               ETA: 1367703.2s

################################################################################
                    [1m Learning iteration 505/100000 [0m                     

                       Computation: 682 steps/s (collection: 23.721s, learning 0.282s)
               Value function loss: 25.0310
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 247.38
               Mean episode length: 123.90
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8290304
                    Iteration time: 24.00s
                        Total time: 6965.89s
                               ETA: 1369706.3s

################################################################################
                    [1m Learning iteration 506/100000 [0m                     

                       Computation: 1177 steps/s (collection: 13.746s, learning 0.167s)
               Value function loss: 21.4780
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 248.75
               Mean episode length: 123.23
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8306688
                    Iteration time: 13.91s
                        Total time: 6979.80s
                               ETA: 1369721.3s

################################################################################
                    [1m Learning iteration 507/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.745s, learning 0.229s)
               Value function loss: 15.4552
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 253.43
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8323072
                    Iteration time: 11.97s
                        Total time: 6991.78s
                               ETA: 1369356.3s

################################################################################
                    [1m Learning iteration 508/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.991s, learning 0.172s)
               Value function loss: 15.0685
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 252.10
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8339456
                    Iteration time: 12.16s
                        Total time: 7003.94s
                               ETA: 1369029.7s

################################################################################
                    [1m Learning iteration 509/100000 [0m                     

                       Computation: 1289 steps/s (collection: 12.541s, learning 0.165s)
               Value function loss: 11.8524
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 250.36
               Mean episode length: 124.25
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8355840
                    Iteration time: 12.71s
                        Total time: 7016.65s
                               ETA: 1368810.2s

################################################################################
                    [1m Learning iteration 510/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.990s, learning 0.170s)
               Value function loss: 15.7051
                    Surrogate loss: -0.0113
             Mean action noise std: 0.79
                       Mean reward: 248.84
               Mean episode length: 122.80
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8372224
                    Iteration time: 12.16s
                        Total time: 7028.81s
                               ETA: 1368485.5s

################################################################################
                    [1m Learning iteration 511/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.132s, learning 0.172s)
               Value function loss: 11.7192
                    Surrogate loss: 0.0110
             Mean action noise std: 0.79
                       Mean reward: 249.88
               Mean episode length: 123.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8388608
                    Iteration time: 12.30s
                        Total time: 7041.11s
                               ETA: 1368189.7s

################################################################################
                    [1m Learning iteration 512/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.152s, learning 0.164s)
               Value function loss: 9.7663
                    Surrogate loss: -0.0032
             Mean action noise std: 0.79
                       Mean reward: 249.41
               Mean episode length: 123.20
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8404992
                    Iteration time: 12.32s
                        Total time: 7053.43s
                               ETA: 1367897.4s

################################################################################
                    [1m Learning iteration 513/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.014s, learning 0.191s)
               Value function loss: 12.1747
                    Surrogate loss: 0.0116
             Mean action noise std: 0.79
                       Mean reward: 243.92
               Mean episode length: 120.69
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8421376
                    Iteration time: 12.20s
                        Total time: 7065.63s
                               ETA: 1367584.7s

################################################################################
                    [1m Learning iteration 514/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.137s, learning 0.216s)
               Value function loss: 9.7603
                    Surrogate loss: 0.0101
             Mean action noise std: 0.79
                       Mean reward: 249.70
               Mean episode length: 122.49
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8437760
                    Iteration time: 12.35s
                        Total time: 7077.98s
                               ETA: 1367301.7s

################################################################################
                    [1m Learning iteration 515/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.930s, learning 0.277s)
               Value function loss: 61.8811
                    Surrogate loss: 0.0191
             Mean action noise std: 0.79
                       Mean reward: 252.22
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8454144
                    Iteration time: 12.21s
                        Total time: 7090.19s
                               ETA: 1366991.6s

################################################################################
                    [1m Learning iteration 516/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.120s, learning 0.169s)
               Value function loss: 10.2783
                    Surrogate loss: -0.0021
             Mean action noise std: 0.79
                       Mean reward: 253.21
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8470528
                    Iteration time: 12.29s
                        Total time: 7102.48s
                               ETA: 1366698.6s

################################################################################
                    [1m Learning iteration 517/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.072s, learning 0.160s)
               Value function loss: 131.4985
                    Surrogate loss: 0.0218
             Mean action noise std: 0.79
                       Mean reward: 254.33
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8486912
                    Iteration time: 12.23s
                        Total time: 7114.71s
                               ETA: 1366395.8s

################################################################################
                    [1m Learning iteration 518/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.765s, learning 0.248s)
               Value function loss: 50.7556
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 255.75
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8503296
                    Iteration time: 12.01s
                        Total time: 7126.73s
                               ETA: 1366052.0s

################################################################################
                    [1m Learning iteration 519/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.096s, learning 0.179s)
               Value function loss: 27.7125
                    Surrogate loss: -0.0083
             Mean action noise std: 0.79
                       Mean reward: 248.35
               Mean episode length: 122.59
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8519680
                    Iteration time: 12.27s
                        Total time: 7139.00s
                               ETA: 1365759.5s

################################################################################
                    [1m Learning iteration 520/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.266s, learning 0.162s)
               Value function loss: 28.3247
                    Surrogate loss: -0.0073
             Mean action noise std: 0.79
                       Mean reward: 252.66
               Mean episode length: 123.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8536064
                    Iteration time: 12.43s
                        Total time: 7151.43s
                               ETA: 1365497.3s

################################################################################
                    [1m Learning iteration 521/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.940s, learning 0.187s)
               Value function loss: 24.1363
                    Surrogate loss: -0.0102
             Mean action noise std: 0.79
                       Mean reward: 255.16
               Mean episode length: 124.58
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8552448
                    Iteration time: 12.13s
                        Total time: 7163.56s
                               ETA: 1365178.9s

################################################################################
                    [1m Learning iteration 522/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.856s, learning 0.186s)
               Value function loss: 24.0133
                    Surrogate loss: -0.0016
             Mean action noise std: 0.79
                       Mean reward: 253.56
               Mean episode length: 123.06
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8568832
                    Iteration time: 12.04s
                        Total time: 7175.60s
                               ETA: 1364845.4s

################################################################################
                    [1m Learning iteration 523/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.998s, learning 0.176s)
               Value function loss: 17.9517
                    Surrogate loss: -0.0143
             Mean action noise std: 0.79
                       Mean reward: 255.24
               Mean episode length: 123.99
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8585216
                    Iteration time: 12.17s
                        Total time: 7187.77s
                               ETA: 1364538.1s

################################################################################
                    [1m Learning iteration 524/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.733s, learning 0.204s)
               Value function loss: 15.0840
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 250.12
               Mean episode length: 122.67
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8601600
                    Iteration time: 11.94s
                        Total time: 7199.71s
                               ETA: 1364187.0s

################################################################################
                    [1m Learning iteration 525/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.850s, learning 0.266s)
               Value function loss: 13.2951
                    Surrogate loss: -0.0123
             Mean action noise std: 0.79
                       Mean reward: 252.41
               Mean episode length: 123.54
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8617984
                    Iteration time: 12.12s
                        Total time: 7211.82s
                               ETA: 1363871.2s

################################################################################
                    [1m Learning iteration 526/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.888s, learning 0.161s)
               Value function loss: 17.7579
                    Surrogate loss: -0.0043
             Mean action noise std: 0.79
                       Mean reward: 254.59
               Mean episode length: 124.15
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8634368
                    Iteration time: 12.05s
                        Total time: 7223.87s
                               ETA: 1363543.9s

################################################################################
                    [1m Learning iteration 527/100000 [0m                     

                       Computation: 1308 steps/s (collection: 12.319s, learning 0.203s)
               Value function loss: 9.9984
                    Surrogate loss: -0.0051
             Mean action noise std: 0.79
                       Mean reward: 252.17
               Mean episode length: 123.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8650752
                    Iteration time: 12.52s
                        Total time: 7236.40s
                               ETA: 1363306.8s

################################################################################
                    [1m Learning iteration 528/100000 [0m                     

                       Computation: 1306 steps/s (collection: 12.236s, learning 0.302s)
               Value function loss: 13.6360
                    Surrogate loss: -0.0074
             Mean action noise std: 0.79
                       Mean reward: 249.87
               Mean episode length: 122.66
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8667136
                    Iteration time: 12.54s
                        Total time: 7248.93s
                               ETA: 1363073.6s

################################################################################
                    [1m Learning iteration 529/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.704s, learning 0.191s)
               Value function loss: 15.4781
                    Surrogate loss: -0.0059
             Mean action noise std: 0.79
                       Mean reward: 253.02
               Mean episode length: 123.22
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8683520
                    Iteration time: 11.90s
                        Total time: 7260.83s
                               ETA: 1362720.7s

################################################################################
                    [1m Learning iteration 530/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.758s, learning 0.167s)
               Value function loss: 9.1065
                    Surrogate loss: -0.0017
             Mean action noise std: 0.79
                       Mean reward: 254.59
               Mean episode length: 124.12
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8699904
                    Iteration time: 11.93s
                        Total time: 7272.75s
                               ETA: 1362374.6s

################################################################################
                    [1m Learning iteration 531/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.128s, learning 0.160s)
               Value function loss: 32.2608
                    Surrogate loss: 0.0028
             Mean action noise std: 0.79
                       Mean reward: 253.96
               Mean episode length: 123.97
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8716288
                    Iteration time: 12.29s
                        Total time: 7285.04s
                               ETA: 1362097.5s

################################################################################
                    [1m Learning iteration 532/100000 [0m                     

                       Computation: 1305 steps/s (collection: 12.389s, learning 0.164s)
               Value function loss: 13.3478
                    Surrogate loss: -0.0119
             Mean action noise std: 0.79
                       Mean reward: 252.92
               Mean episode length: 123.56
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8732672
                    Iteration time: 12.55s
                        Total time: 7297.60s
                               ETA: 1361871.0s

################################################################################
                    [1m Learning iteration 533/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.123s, learning 0.281s)
               Value function loss: 177.8322
                    Surrogate loss: 0.0260
             Mean action noise std: 0.79
                       Mean reward: 257.61
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 6.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8749056
                    Iteration time: 12.40s
                        Total time: 7310.00s
                               ETA: 1361617.6s

################################################################################
                    [1m Learning iteration 534/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.793s, learning 0.198s)
               Value function loss: 25.5967
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 256.87
               Mean episode length: 124.35
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8765440
                    Iteration time: 11.99s
                        Total time: 7321.99s
                               ETA: 1361288.2s

################################################################################
                    [1m Learning iteration 535/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.181s, learning 0.231s)
               Value function loss: 27.6380
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 256.89
               Mean episode length: 123.89
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8781824
                    Iteration time: 12.41s
                        Total time: 7334.40s
                               ETA: 1361038.0s

################################################################################
                    [1m Learning iteration 536/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.668s, learning 0.159s)
               Value function loss: 29.4063
                    Surrogate loss: -0.0122
             Mean action noise std: 0.79
                       Mean reward: 256.13
               Mean episode length: 124.90
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8798208
                    Iteration time: 11.83s
                        Total time: 7346.23s
                               ETA: 1360680.4s

################################################################################
                    [1m Learning iteration 537/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.682s, learning 0.169s)
               Value function loss: 24.5327
                    Surrogate loss: -0.0150
             Mean action noise std: 0.79
                       Mean reward: 255.65
               Mean episode length: 124.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8814592
                    Iteration time: 11.85s
                        Total time: 7358.08s
                               ETA: 1360328.6s

################################################################################
                    [1m Learning iteration 538/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.840s, learning 0.161s)
               Value function loss: 17.8712
                    Surrogate loss: -0.0013
             Mean action noise std: 0.79
                       Mean reward: 256.37
               Mean episode length: 124.18
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8830976
                    Iteration time: 12.00s
                        Total time: 7370.08s
                               ETA: 1360005.6s

################################################################################
                    [1m Learning iteration 539/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.843s, learning 0.174s)
               Value function loss: 18.8244
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 252.68
               Mean episode length: 123.46
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8847360
                    Iteration time: 12.02s
                        Total time: 7382.10s
                               ETA: 1359686.7s

################################################################################
                    [1m Learning iteration 540/100000 [0m                     

                       Computation: 1260 steps/s (collection: 12.780s, learning 0.213s)
               Value function loss: 15.0011
                    Surrogate loss: -0.0139
             Mean action noise std: 0.79
                       Mean reward: 249.60
               Mean episode length: 121.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8863744
                    Iteration time: 12.99s
                        Total time: 7395.09s
                               ETA: 1359548.5s

################################################################################
                    [1m Learning iteration 541/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.271s, learning 0.164s)
               Value function loss: 18.4393
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 249.56
               Mean episode length: 122.11
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8880128
                    Iteration time: 12.43s
                        Total time: 7407.53s
                               ETA: 1359308.3s

################################################################################
                    [1m Learning iteration 542/100000 [0m                     

                       Computation: 1294 steps/s (collection: 12.483s, learning 0.172s)
               Value function loss: 15.0964
                    Surrogate loss: -0.0065
             Mean action noise std: 0.79
                       Mean reward: 249.06
               Mean episode length: 122.13
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8896512
                    Iteration time: 12.66s
                        Total time: 7420.18s
                               ETA: 1359109.4s

################################################################################
                    [1m Learning iteration 543/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.702s, learning 0.174s)
               Value function loss: 13.9344
                    Surrogate loss: -0.0087
             Mean action noise std: 0.79
                       Mean reward: 250.11
               Mean episode length: 122.34
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8912896
                    Iteration time: 11.88s
                        Total time: 7432.06s
                               ETA: 1358768.6s

################################################################################
                    [1m Learning iteration 544/100000 [0m                     

                       Computation: 1439 steps/s (collection: 11.218s, learning 0.161s)
               Value function loss: 11.4953
                    Surrogate loss: -0.0060
             Mean action noise std: 0.79
                       Mean reward: 252.75
               Mean episode length: 122.98
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8929280
                    Iteration time: 11.38s
                        Total time: 7443.44s
                               ETA: 1358338.2s

################################################################################
                    [1m Learning iteration 545/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.881s, learning 0.174s)
               Value function loss: 11.3986
                    Surrogate loss: -0.0068
             Mean action noise std: 0.79
                       Mean reward: 256.49
               Mean episode length: 124.44
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8945664
                    Iteration time: 12.05s
                        Total time: 7455.49s
                               ETA: 1358032.6s

################################################################################
                    [1m Learning iteration 546/100000 [0m                     

                       Computation: 1316 steps/s (collection: 12.231s, learning 0.212s)
               Value function loss: 79.3017
                    Surrogate loss: 0.0190
             Mean action noise std: 0.79
                       Mean reward: 255.85
               Mean episode length: 124.79
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.05
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8962048
                    Iteration time: 12.44s
                        Total time: 7467.93s
                               ETA: 1357798.6s

################################################################################
                    [1m Learning iteration 547/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.974s, learning 0.168s)
               Value function loss: 12.1050
                    Surrogate loss: -0.0086
             Mean action noise std: 0.79
                       Mean reward: 251.71
               Mean episode length: 123.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8978432
                    Iteration time: 12.14s
                        Total time: 7480.08s
                               ETA: 1357510.8s

################################################################################
                    [1m Learning iteration 548/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.816s, learning 0.166s)
               Value function loss: 44.1865
                    Surrogate loss: 0.0023
             Mean action noise std: 0.79
                       Mean reward: 257.10
               Mean episode length: 124.77
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 8994816
                    Iteration time: 11.98s
                        Total time: 7492.06s
                               ETA: 1357195.0s

################################################################################
                    [1m Learning iteration 549/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.240s, learning 0.221s)
               Value function loss: 95.3599
                    Surrogate loss: 0.0055
             Mean action noise std: 0.79
                       Mean reward: 257.58
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.23
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9011200
                    Iteration time: 12.46s
                        Total time: 7504.52s
                               ETA: 1356967.1s

################################################################################
                    [1m Learning iteration 550/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.632s, learning 0.168s)
               Value function loss: 23.9006
                    Surrogate loss: -0.0089
             Mean action noise std: 0.79
                       Mean reward: 256.41
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9027584
                    Iteration time: 11.80s
                        Total time: 7516.32s
                               ETA: 1356620.7s

################################################################################
                    [1m Learning iteration 551/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.570s, learning 0.305s)
               Value function loss: 28.9577
                    Surrogate loss: -0.0110
             Mean action noise std: 0.79
                       Mean reward: 255.44
               Mean episode length: 123.93
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9043968
                    Iteration time: 11.88s
                        Total time: 7528.20s
                               ETA: 1356288.9s

################################################################################
                    [1m Learning iteration 552/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.755s, learning 0.204s)
               Value function loss: 25.5886
                    Surrogate loss: -0.0097
             Mean action noise std: 0.79
                       Mean reward: 254.00
               Mean episode length: 124.09
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9060352
                    Iteration time: 11.96s
                        Total time: 7540.15s
                               ETA: 1355973.2s

################################################################################
                    [1m Learning iteration 553/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.112s, learning 0.165s)
               Value function loss: 22.8785
                    Surrogate loss: -0.0127
             Mean action noise std: 0.79
                       Mean reward: 256.30
               Mean episode length: 124.82
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9076736
                    Iteration time: 12.28s
                        Total time: 7552.43s
                               ETA: 1355715.8s

################################################################################
                    [1m Learning iteration 554/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.859s, learning 0.198s)
               Value function loss: 16.9800
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 256.19
               Mean episode length: 124.69
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9093120
                    Iteration time: 12.06s
                        Total time: 7564.49s
                               ETA: 1355419.9s

################################################################################
                    [1m Learning iteration 555/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.928s, learning 0.173s)
               Value function loss: 16.6364
                    Surrogate loss: -0.0175
             Mean action noise std: 0.79
                       Mean reward: 248.76
               Mean episode length: 121.34
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9109504
                    Iteration time: 12.10s
                        Total time: 7576.59s
                               ETA: 1355132.8s

################################################################################
                    [1m Learning iteration 556/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.469s, learning 0.233s)
               Value function loss: 13.9833
                    Surrogate loss: -0.0147
             Mean action noise std: 0.79
                       Mean reward: 246.21
               Mean episode length: 120.99
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9125888
                    Iteration time: 11.70s
                        Total time: 7588.29s
                               ETA: 1354775.4s

################################################################################
                    [1m Learning iteration 557/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.115s, learning 0.200s)
               Value function loss: 17.5894
                    Surrogate loss: -0.0137
             Mean action noise std: 0.79
                       Mean reward: 253.40
               Mean episode length: 123.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9142272
                    Iteration time: 12.32s
                        Total time: 7600.61s
                               ETA: 1354528.6s

################################################################################
                    [1m Learning iteration 558/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.981s, learning 0.162s)
               Value function loss: 12.5791
                    Surrogate loss: -0.0067
             Mean action noise std: 0.79
                       Mean reward: 243.56
               Mean episode length: 120.41
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9158656
                    Iteration time: 12.14s
                        Total time: 7612.75s
                               ETA: 1354252.0s

################################################################################
                    [1m Learning iteration 559/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.904s, learning 0.288s)
               Value function loss: 12.0218
                    Surrogate loss: -0.0005
             Mean action noise std: 0.79
                       Mean reward: 241.49
               Mean episode length: 119.20
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9175040
                    Iteration time: 12.19s
                        Total time: 7624.94s
                               ETA: 1353984.9s

################################################################################
                    [1m Learning iteration 560/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.708s, learning 0.167s)
               Value function loss: 10.8956
                    Surrogate loss: -0.0098
             Mean action noise std: 0.79
                       Mean reward: 251.75
               Mean episode length: 122.98
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9191424
                    Iteration time: 11.88s
                        Total time: 7636.81s
                               ETA: 1353662.8s

################################################################################
                    [1m Learning iteration 561/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.710s, learning 0.208s)
               Value function loss: 10.7601
                    Surrogate loss: 0.0174
             Mean action noise std: 0.79
                       Mean reward: 250.70
               Mean episode length: 123.32
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9207808
                    Iteration time: 11.92s
                        Total time: 7648.73s
                               ETA: 1353349.3s

################################################################################
                    [1m Learning iteration 562/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.927s, learning 0.165s)
               Value function loss: 48.2851
                    Surrogate loss: 0.0129
             Mean action noise std: 0.79
                       Mean reward: 256.20
               Mean episode length: 124.81
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9224192
                    Iteration time: 12.09s
                        Total time: 7660.82s
                               ETA: 1353067.7s

################################################################################
                    [1m Learning iteration 563/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.012s, learning 0.173s)
               Value function loss: 12.3734
                    Surrogate loss: -0.0072
             Mean action noise std: 0.79
                       Mean reward: 252.84
               Mean episode length: 124.01
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.87
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9240576
                    Iteration time: 12.19s
                        Total time: 7673.01s
                               ETA: 1352803.3s

################################################################################
                    [1m Learning iteration 564/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.616s, learning 0.204s)
               Value function loss: 109.2517
                    Surrogate loss: 0.0279
             Mean action noise std: 0.79
                       Mean reward: 253.55
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 6.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9256960
                    Iteration time: 11.82s
                        Total time: 7684.83s
                               ETA: 1352475.6s

################################################################################
                    [1m Learning iteration 565/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.955s, learning 0.164s)
               Value function loss: 31.6817
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 253.05
               Mean episode length: 123.83
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9273344
                    Iteration time: 12.12s
                        Total time: 7696.95s
                               ETA: 1352201.6s

################################################################################
                    [1m Learning iteration 566/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.642s, learning 0.160s)
               Value function loss: 21.9902
                    Surrogate loss: -0.0101
             Mean action noise std: 0.79
                       Mean reward: 252.53
               Mean episode length: 123.85
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9289728
                    Iteration time: 11.80s
                        Total time: 7708.75s
                               ETA: 1351872.8s

################################################################################
                    [1m Learning iteration 567/100000 [0m                     

                       Computation: 1301 steps/s (collection: 12.417s, learning 0.172s)
               Value function loss: 22.8629
                    Surrogate loss: -0.0126
             Mean action noise std: 0.79
                       Mean reward: 253.83
               Mean episode length: 124.31
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9306112
                    Iteration time: 12.59s
                        Total time: 7721.34s
                               ETA: 1351683.0s

################################################################################
                    [1m Learning iteration 568/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.867s, learning 0.206s)
               Value function loss: 19.8565
                    Surrogate loss: -0.0133
             Mean action noise std: 0.79
                       Mean reward: 253.15
               Mean episode length: 124.22
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9322496
                    Iteration time: 12.07s
                        Total time: 7733.41s
                               ETA: 1351403.6s

################################################################################
                    [1m Learning iteration 569/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.934s, learning 0.158s)
               Value function loss: 17.5586
                    Surrogate loss: -0.0131
             Mean action noise std: 0.79
                       Mean reward: 252.51
               Mean episode length: 123.32
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9338880
                    Iteration time: 12.09s
                        Total time: 7745.51s
                               ETA: 1351128.6s

################################################################################
                    [1m Learning iteration 570/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.308s, learning 0.167s)
               Value function loss: 15.2666
                    Surrogate loss: -0.0094
             Mean action noise std: 0.79
                       Mean reward: 254.20
               Mean episode length: 124.24
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9355264
                    Iteration time: 11.48s
                        Total time: 7756.98s
                               ETA: 1350747.0s

################################################################################
                    [1m Learning iteration 571/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.503s, learning 0.195s)
               Value function loss: 15.0065
                    Surrogate loss: -0.0096
             Mean action noise std: 0.79
                       Mean reward: 246.98
               Mean episode length: 121.89
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9371648
                    Iteration time: 11.70s
                        Total time: 7768.68s
                               ETA: 1350405.4s

################################################################################
                    [1m Learning iteration 572/100000 [0m                     

                       Computation: 1339 steps/s (collection: 11.997s, learning 0.239s)
               Value function loss: 13.7049
                    Surrogate loss: -0.0092
             Mean action noise std: 0.79
                       Mean reward: 245.79
               Mean episode length: 122.27
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9388032
                    Iteration time: 12.24s
                        Total time: 7780.91s
                               ETA: 1350158.3s

################################################################################
                    [1m Learning iteration 573/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.079s, learning 0.189s)
               Value function loss: 14.5819
                    Surrogate loss: -0.0142
             Mean action noise std: 0.79
                       Mean reward: 251.44
               Mean episode length: 123.82
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9404416
                    Iteration time: 12.27s
                        Total time: 7793.18s
                               ETA: 1349917.6s

################################################################################
                    [1m Learning iteration 574/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.642s, learning 0.160s)
               Value function loss: 11.1716
                    Surrogate loss: -0.0085
             Mean action noise std: 0.79
                       Mean reward: 249.39
               Mean episode length: 122.87
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9420800
                    Iteration time: 11.80s
                        Total time: 7804.98s
                               ETA: 1349597.2s

################################################################################
                    [1m Learning iteration 575/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.284s, learning 0.218s)
               Value function loss: 11.4643
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 245.94
               Mean episode length: 122.21
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9437184
                    Iteration time: 12.50s
                        Total time: 7817.49s
                               ETA: 1349398.6s

################################################################################
                    [1m Learning iteration 576/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.904s, learning 0.165s)
               Value function loss: 18.7663
                    Surrogate loss: -0.0079
             Mean action noise std: 0.79
                       Mean reward: 246.62
               Mean episode length: 122.50
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9453568
                    Iteration time: 12.07s
                        Total time: 7829.56s
                               ETA: 1349126.0s

################################################################################
                    [1m Learning iteration 577/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.161s)
               Value function loss: 69.7575
                    Surrogate loss: 0.0204
             Mean action noise std: 0.79
                       Mean reward: 252.02
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.90
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9469952
                    Iteration time: 12.07s
                        Total time: 7841.62s
                               ETA: 1348854.0s

################################################################################
                    [1m Learning iteration 578/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.216s, learning 0.167s)
               Value function loss: 19.2484
                    Surrogate loss: 0.0039
             Mean action noise std: 0.79
                       Mean reward: 249.86
               Mean episode length: 123.85
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9486336
                    Iteration time: 12.38s
                        Total time: 7854.00s
                               ETA: 1348637.1s

################################################################################
                    [1m Learning iteration 579/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.611s, learning 0.174s)
               Value function loss: 15.4751
                    Surrogate loss: -0.0106
             Mean action noise std: 0.79
                       Mean reward: 246.36
               Mean episode length: 121.83
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9502720
                    Iteration time: 11.79s
                        Total time: 7865.79s
                               ETA: 1348318.5s

################################################################################
                    [1m Learning iteration 580/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.640s, learning 0.158s)
               Value function loss: 111.8992
                    Surrogate loss: 0.0184
             Mean action noise std: 0.79
                       Mean reward: 252.90
               Mean episode length: 123.84
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9519104
                    Iteration time: 11.80s
                        Total time: 7877.59s
                               ETA: 1348003.2s

################################################################################
                    [1m Learning iteration 581/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.661s, learning 0.166s)
               Value function loss: 21.1368
                    Surrogate loss: -0.0130
             Mean action noise std: 0.79
                       Mean reward: 251.40
               Mean episode length: 124.64
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9535488
                    Iteration time: 11.83s
                        Total time: 7889.42s
                               ETA: 1347693.8s

################################################################################
                    [1m Learning iteration 582/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.875s, learning 0.252s)
               Value function loss: 25.8058
                    Surrogate loss: -0.0132
             Mean action noise std: 0.79
                       Mean reward: 253.52
               Mean episode length: 124.84
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9551872
                    Iteration time: 12.13s
                        Total time: 7901.54s
                               ETA: 1347436.7s

################################################################################
                    [1m Learning iteration 583/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.109s, learning 0.168s)
               Value function loss: 29.8339
                    Surrogate loss: -0.0134
             Mean action noise std: 0.79
                       Mean reward: 251.72
               Mean episode length: 124.21
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9568256
                    Iteration time: 12.28s
                        Total time: 7913.82s
                               ETA: 1347205.7s

################################################################################
                    [1m Learning iteration 584/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.599s, learning 0.162s)
               Value function loss: 26.6257
                    Surrogate loss: -0.0151
             Mean action noise std: 0.79
                       Mean reward: 252.15
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9584640
                    Iteration time: 11.76s
                        Total time: 7925.58s
                               ETA: 1346887.9s

################################################################################
                    [1m Learning iteration 585/100000 [0m                     

                       Computation: 1311 steps/s (collection: 12.335s, learning 0.158s)
               Value function loss: 18.9530
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 249.56
               Mean episode length: 123.23
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9601024
                    Iteration time: 12.49s
                        Total time: 7938.07s
                               ETA: 1346695.3s

################################################################################
                    [1m Learning iteration 586/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.661s, learning 0.222s)
               Value function loss: 19.7400
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 249.74
               Mean episode length: 123.62
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9617408
                    Iteration time: 11.88s
                        Total time: 7949.96s
                               ETA: 1346400.1s

################################################################################
                    [1m Learning iteration 587/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.217s, learning 0.177s)
               Value function loss: 14.4282
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 247.67
               Mean episode length: 122.77
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9633792
                    Iteration time: 12.39s
                        Total time: 7962.35s
                               ETA: 1346192.2s

################################################################################
                    [1m Learning iteration 588/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.050s, learning 0.162s)
               Value function loss: 16.9137
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 246.62
               Mean episode length: 123.33
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9650176
                    Iteration time: 12.21s
                        Total time: 7974.56s
                               ETA: 1345954.3s

################################################################################
                    [1m Learning iteration 589/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.261s, learning 0.191s)
               Value function loss: 14.7158
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 244.09
               Mean episode length: 122.12
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9666560
                    Iteration time: 12.45s
                        Total time: 7987.01s
                               ETA: 1345757.6s

################################################################################
                    [1m Learning iteration 590/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.876s, learning 0.163s)
               Value function loss: 12.8286
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 239.01
               Mean episode length: 120.62
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9682944
                    Iteration time: 12.04s
                        Total time: 7999.05s
                               ETA: 1345492.1s

################################################################################
                    [1m Learning iteration 591/100000 [0m                     

                       Computation: 1298 steps/s (collection: 12.427s, learning 0.186s)
               Value function loss: 11.1872
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 237.76
               Mean episode length: 119.39
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9699328
                    Iteration time: 12.61s
                        Total time: 8011.67s
                               ETA: 1345323.8s

################################################################################
                    [1m Learning iteration 592/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.976s, learning 0.159s)
               Value function loss: 9.8963
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 240.98
               Mean episode length: 120.73
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9715712
                    Iteration time: 12.14s
                        Total time: 8023.80s
                               ETA: 1345076.0s

################################################################################
                    [1m Learning iteration 593/100000 [0m                     

                       Computation: 1323 steps/s (collection: 12.129s, learning 0.248s)
               Value function loss: 64.4629
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 249.59
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.07
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9732096
                    Iteration time: 12.38s
                        Total time: 8036.18s
                               ETA: 1344869.4s

################################################################################
                    [1m Learning iteration 594/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.698s, learning 0.193s)
               Value function loss: 16.9048
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 248.21
               Mean episode length: 123.90
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9748480
                    Iteration time: 11.89s
                        Total time: 8048.07s
                               ETA: 1344582.2s

################################################################################
                    [1m Learning iteration 595/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.851s, learning 0.194s)
               Value function loss: 67.9533
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 251.12
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9764864
                    Iteration time: 12.04s
                        Total time: 8060.11s
                               ETA: 1344321.6s

################################################################################
                    [1m Learning iteration 596/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.845s, learning 0.158s)
               Value function loss: 63.5603
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 250.03
               Mean episode length: 123.90
                  Mean reward/step: 1.94
       Mean episode length/episode: 6.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9781248
                    Iteration time: 12.00s
                        Total time: 8072.12s
                               ETA: 1344054.8s

################################################################################
                    [1m Learning iteration 597/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.583s, learning 0.164s)
               Value function loss: 23.3485
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 246.98
               Mean episode length: 123.54
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9797632
                    Iteration time: 11.75s
                        Total time: 8083.86s
                               ETA: 1343746.3s

################################################################################
                    [1m Learning iteration 598/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.779s, learning 0.155s)
               Value function loss: 23.9549
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 249.94
               Mean episode length: 124.13
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9814016
                    Iteration time: 11.93s
                        Total time: 8095.80s
                               ETA: 1343470.0s

################################################################################
                    [1m Learning iteration 599/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.100s, learning 0.172s)
               Value function loss: 19.4901
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 250.42
               Mean episode length: 124.02
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9830400
                    Iteration time: 12.27s
                        Total time: 8108.07s
                               ETA: 1343250.5s

################################################################################
                    [1m Learning iteration 600/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.106s, learning 0.195s)
               Value function loss: 19.1785
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 244.87
               Mean episode length: 121.50
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9846784
                    Iteration time: 12.30s
                        Total time: 8120.37s
                               ETA: 1343036.4s

################################################################################
                    [1m Learning iteration 601/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.006s, learning 0.167s)
               Value function loss: 15.9382
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 246.51
               Mean episode length: 122.16
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9863168
                    Iteration time: 12.17s
                        Total time: 8132.54s
                               ETA: 1342801.9s

################################################################################
                    [1m Learning iteration 602/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.646s, learning 0.163s)
               Value function loss: 14.1431
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 243.56
               Mean episode length: 121.95
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9879552
                    Iteration time: 11.81s
                        Total time: 8144.35s
                               ETA: 1342508.0s

################################################################################
                    [1m Learning iteration 603/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.874s, learning 0.306s)
               Value function loss: 13.4348
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 240.69
               Mean episode length: 120.72
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9895936
                    Iteration time: 12.18s
                        Total time: 8156.53s
                               ETA: 1342276.2s

################################################################################
                    [1m Learning iteration 604/100000 [0m                     

                       Computation: 1291 steps/s (collection: 12.528s, learning 0.161s)
               Value function loss: 15.2840
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 248.67
               Mean episode length: 123.28
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9912320
                    Iteration time: 12.69s
                        Total time: 8169.22s
                               ETA: 1342128.8s

################################################################################
                    [1m Learning iteration 605/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.882s, learning 0.169s)
               Value function loss: 11.6436
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 249.32
               Mean episode length: 123.14
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9928704
                    Iteration time: 12.05s
                        Total time: 8181.27s
                               ETA: 1341877.2s

################################################################################
                    [1m Learning iteration 606/100000 [0m                     

                       Computation: 1341 steps/s (collection: 12.050s, learning 0.167s)
               Value function loss: 13.5566
                    Surrogate loss: 0.0024
             Mean action noise std: 0.78
                       Mean reward: 247.24
               Mean episode length: 122.38
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9945088
                    Iteration time: 12.22s
                        Total time: 8193.49s
                               ETA: 1341653.6s

################################################################################
                    [1m Learning iteration 607/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.902s, learning 0.161s)
               Value function loss: 14.8857
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 243.09
               Mean episode length: 120.83
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9961472
                    Iteration time: 12.06s
                        Total time: 8205.55s
                               ETA: 1341405.5s

################################################################################
                    [1m Learning iteration 608/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.865s, learning 0.154s)
               Value function loss: 10.4797
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 245.07
               Mean episode length: 121.70
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9977856
                    Iteration time: 12.02s
                        Total time: 8217.57s
                               ETA: 1341151.0s

################################################################################
                    [1m Learning iteration 609/100000 [0m                     

                       Computation: 1340 steps/s (collection: 11.982s, learning 0.236s)
               Value function loss: 57.3262
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 250.25
               Mean episode length: 124.13
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 9994240
                    Iteration time: 12.22s
                        Total time: 8229.79s
                               ETA: 1340929.7s

################################################################################
                    [1m Learning iteration 610/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.855s, learning 0.165s)
               Value function loss: 13.4445
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 249.02
               Mean episode length: 123.38
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10010624
                    Iteration time: 12.02s
                        Total time: 8241.81s
                               ETA: 1340676.8s

################################################################################
                    [1m Learning iteration 611/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.858s, learning 0.245s)
               Value function loss: 133.1460
                    Surrogate loss: 0.0255
             Mean action noise std: 0.78
                       Mean reward: 254.03
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 6.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10027008
                    Iteration time: 12.10s
                        Total time: 8253.91s
                               ETA: 1340438.3s

################################################################################
                    [1m Learning iteration 612/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.955s, learning 0.160s)
               Value function loss: 23.7809
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 253.81
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10043392
                    Iteration time: 12.12s
                        Total time: 8266.03s
                               ETA: 1340202.4s

################################################################################
                    [1m Learning iteration 613/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.988s, learning 0.167s)
               Value function loss: 24.5662
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 246.47
               Mean episode length: 122.12
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10059776
                    Iteration time: 12.15s
                        Total time: 8278.18s
                               ETA: 1339973.7s

################################################################################
                    [1m Learning iteration 614/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.772s, learning 0.195s)
               Value function loss: 24.9218
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 245.68
               Mean episode length: 122.56
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10076160
                    Iteration time: 11.97s
                        Total time: 8290.15s
                               ETA: 1339715.3s

################################################################################
                    [1m Learning iteration 615/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.687s, learning 0.173s)
               Value function loss: 21.1168
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 254.33
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10092544
                    Iteration time: 11.86s
                        Total time: 8302.01s
                               ETA: 1339440.3s

################################################################################
                    [1m Learning iteration 616/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.794s, learning 0.207s)
               Value function loss: 20.0419
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 248.15
               Mean episode length: 122.22
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10108928
                    Iteration time: 12.00s
                        Total time: 8314.01s
                               ETA: 1339189.0s

################################################################################
                    [1m Learning iteration 617/100000 [0m                     

                       Computation: 1333 steps/s (collection: 12.004s, learning 0.282s)
               Value function loss: 17.8562
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 241.12
               Mean episode length: 119.72
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10125312
                    Iteration time: 12.29s
                        Total time: 8326.30s
                               ETA: 1338984.4s

################################################################################
                    [1m Learning iteration 618/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.721s, learning 0.202s)
               Value function loss: 15.7607
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 241.42
               Mean episode length: 119.68
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10141696
                    Iteration time: 11.92s
                        Total time: 8338.22s
                               ETA: 1338722.1s

################################################################################
                    [1m Learning iteration 619/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.858s, learning 0.164s)
               Value function loss: 16.8250
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 252.33
               Mean episode length: 123.92
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10158080
                    Iteration time: 12.02s
                        Total time: 8350.24s
                               ETA: 1338476.5s

################################################################################
                    [1m Learning iteration 620/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.312s, learning 0.185s)
               Value function loss: 15.1969
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 251.08
               Mean episode length: 123.37
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10174464
                    Iteration time: 11.50s
                        Total time: 8361.74s
                               ETA: 1338147.6s

################################################################################
                    [1m Learning iteration 621/100000 [0m                     

                       Computation: 1332 steps/s (collection: 12.110s, learning 0.189s)
               Value function loss: 13.8782
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 250.48
               Mean episode length: 123.76
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10190848
                    Iteration time: 12.30s
                        Total time: 8374.04s
                               ETA: 1337947.8s

################################################################################
                    [1m Learning iteration 622/100000 [0m                     

                       Computation: 1334 steps/s (collection: 11.965s, learning 0.313s)
               Value function loss: 12.4500
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 245.11
               Mean episode length: 121.55
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10207232
                    Iteration time: 12.28s
                        Total time: 8386.32s
                               ETA: 1337745.4s

################################################################################
                    [1m Learning iteration 623/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.904s, learning 0.171s)
               Value function loss: 13.0405
                    Surrogate loss: -0.0009
             Mean action noise std: 0.78
                       Mean reward: 248.30
               Mean episode length: 122.09
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10223616
                    Iteration time: 12.07s
                        Total time: 8398.39s
                               ETA: 1337511.2s

################################################################################
                    [1m Learning iteration 624/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.127s, learning 0.280s)
               Value function loss: 66.1286
                    Surrogate loss: 0.0010
             Mean action noise std: 0.78
                       Mean reward: 252.99
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10240000
                    Iteration time: 12.41s
                        Total time: 8410.80s
                               ETA: 1337330.4s

################################################################################
                    [1m Learning iteration 625/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.087s, learning 0.157s)
               Value function loss: 18.2079
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: 250.10
               Mean episode length: 124.01
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10256384
                    Iteration time: 12.24s
                        Total time: 8423.04s
                               ETA: 1337124.4s

################################################################################
                    [1m Learning iteration 626/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.776s, learning 0.202s)
               Value function loss: 22.9342
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 251.33
               Mean episode length: 124.31
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10272768
                    Iteration time: 11.98s
                        Total time: 8435.02s
                               ETA: 1336876.8s

################################################################################
                    [1m Learning iteration 627/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.275s, learning 0.192s)
               Value function loss: 97.0210
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 254.64
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10289152
                    Iteration time: 12.47s
                        Total time: 8447.49s
                               ETA: 1336707.3s

################################################################################
                    [1m Learning iteration 628/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.807s, learning 0.167s)
               Value function loss: 18.7148
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 256.07
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10305536
                    Iteration time: 11.97s
                        Total time: 8459.46s
                               ETA: 1336460.4s

################################################################################
                    [1m Learning iteration 629/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.819s, learning 0.159s)
               Value function loss: 22.9903
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 252.69
               Mean episode length: 124.02
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10321920
                    Iteration time: 11.98s
                        Total time: 8471.44s
                               ETA: 1336215.0s

################################################################################
                    [1m Learning iteration 630/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.149s, learning 0.157s)
               Value function loss: 24.4638
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 248.87
               Mean episode length: 123.79
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10338304
                    Iteration time: 12.31s
                        Total time: 8483.75s
                               ETA: 1336022.0s

################################################################################
                    [1m Learning iteration 631/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.342s, learning 0.157s)
               Value function loss: 23.5660
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 254.52
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10354688
                    Iteration time: 11.50s
                        Total time: 8495.24s
                               ETA: 1335702.5s

################################################################################
                    [1m Learning iteration 632/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.813s, learning 0.184s)
               Value function loss: 16.9554
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 254.27
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10371072
                    Iteration time: 12.00s
                        Total time: 8507.24s
                               ETA: 1335462.2s

################################################################################
                    [1m Learning iteration 633/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.061s, learning 0.196s)
               Value function loss: 19.1928
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 250.48
               Mean episode length: 123.25
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10387456
                    Iteration time: 12.26s
                        Total time: 8519.50s
                               ETA: 1335263.4s

################################################################################
                    [1m Learning iteration 634/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.194s, learning 0.162s)
               Value function loss: 15.7806
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 249.32
               Mean episode length: 123.28
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10403840
                    Iteration time: 12.36s
                        Total time: 8531.85s
                               ETA: 1335080.8s

################################################################################
                    [1m Learning iteration 635/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.670s, learning 0.204s)
               Value function loss: 18.5227
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 248.69
               Mean episode length: 123.37
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10420224
                    Iteration time: 11.87s
                        Total time: 8543.73s
                               ETA: 1334823.3s

################################################################################
                    [1m Learning iteration 636/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.165s)
               Value function loss: 14.8262
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 244.69
               Mean episode length: 121.67
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10436608
                    Iteration time: 12.07s
                        Total time: 8555.80s
                               ETA: 1334597.3s

################################################################################
                    [1m Learning iteration 637/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.508s, learning 0.178s)
               Value function loss: 14.9656
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 243.91
               Mean episode length: 121.21
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10452992
                    Iteration time: 11.69s
                        Total time: 8567.49s
                               ETA: 1334312.1s

################################################################################
                    [1m Learning iteration 638/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.810s, learning 0.173s)
               Value function loss: 13.2395
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 246.08
               Mean episode length: 122.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10469376
                    Iteration time: 11.98s
                        Total time: 8579.47s
                               ETA: 1334073.9s

################################################################################
                    [1m Learning iteration 639/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.584s, learning 0.162s)
               Value function loss: 13.4050
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 247.29
               Mean episode length: 123.57
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10485760
                    Iteration time: 11.75s
                        Total time: 8591.22s
                               ETA: 1333799.7s

################################################################################
                    [1m Learning iteration 640/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.145s, learning 0.172s)
               Value function loss: 80.1133
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 251.99
               Mean episode length: 124.83
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.09
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10502144
                    Iteration time: 12.32s
                        Total time: 8603.53s
                               ETA: 1333614.8s

################################################################################
                    [1m Learning iteration 641/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.777s, learning 0.203s)
               Value function loss: 13.6139
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 246.82
               Mean episode length: 122.77
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10518528
                    Iteration time: 11.98s
                        Total time: 8615.51s
                               ETA: 1333378.3s

################################################################################
                    [1m Learning iteration 642/100000 [0m                     

                       Computation: 1392 steps/s (collection: 11.595s, learning 0.169s)
               Value function loss: 101.8467
                    Surrogate loss: 0.0021
             Mean action noise std: 0.78
                       Mean reward: 249.84
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10534912
                    Iteration time: 11.76s
                        Total time: 8627.28s
                               ETA: 1333109.1s

################################################################################
                    [1m Learning iteration 643/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.480s, learning 0.174s)
               Value function loss: 40.2106
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 252.49
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10551296
                    Iteration time: 11.65s
                        Total time: 8638.93s
                               ETA: 1332823.6s

################################################################################
                    [1m Learning iteration 644/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.643s, learning 0.158s)
               Value function loss: 20.5426
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 245.19
               Mean episode length: 122.27
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10567680
                    Iteration time: 11.80s
                        Total time: 8650.73s
                               ETA: 1332561.6s

################################################################################
                    [1m Learning iteration 645/100000 [0m                     

                       Computation: 1319 steps/s (collection: 12.121s, learning 0.294s)
               Value function loss: 22.6404
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 252.70
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10584064
                    Iteration time: 12.41s
                        Total time: 8663.15s
                               ETA: 1332394.8s

################################################################################
                    [1m Learning iteration 646/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.953s, learning 0.163s)
               Value function loss: 19.6819
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 253.16
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10600448
                    Iteration time: 12.12s
                        Total time: 8675.26s
                               ETA: 1332182.6s

################################################################################
                    [1m Learning iteration 647/100000 [0m                     

                       Computation: 1343 steps/s (collection: 11.995s, learning 0.199s)
               Value function loss: 18.3033
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 246.02
               Mean episode length: 122.54
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10616832
                    Iteration time: 12.19s
                        Total time: 8687.46s
                               ETA: 1331982.9s

################################################################################
                    [1m Learning iteration 648/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.621s, learning 0.177s)
               Value function loss: 16.2807
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 249.69
               Mean episode length: 124.20
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10633216
                    Iteration time: 11.80s
                        Total time: 8699.26s
                               ETA: 1331723.4s

################################################################################
                    [1m Learning iteration 649/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.709s, learning 0.179s)
               Value function loss: 16.3841
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 251.50
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10649600
                    Iteration time: 11.89s
                        Total time: 8711.14s
                               ETA: 1331478.1s

################################################################################
                    [1m Learning iteration 650/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.758s, learning 0.171s)
               Value function loss: 15.8903
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 249.90
               Mean episode length: 123.98
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10665984
                    Iteration time: 11.93s
                        Total time: 8723.07s
                               ETA: 1331239.9s

################################################################################
                    [1m Learning iteration 651/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.153s, learning 0.167s)
               Value function loss: 17.4850
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 242.78
               Mean episode length: 121.18
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10682368
                    Iteration time: 12.32s
                        Total time: 8735.39s
                               ETA: 1331062.1s

################################################################################
                    [1m Learning iteration 652/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.730s, learning 0.232s)
               Value function loss: 13.0861
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 245.08
               Mean episode length: 122.98
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10698752
                    Iteration time: 11.96s
                        Total time: 8747.35s
                               ETA: 1330830.2s

################################################################################
                    [1m Learning iteration 653/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.908s, learning 0.229s)
               Value function loss: 17.5233
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 242.07
               Mean episode length: 121.68
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10715136
                    Iteration time: 12.14s
                        Total time: 8759.49s
                               ETA: 1330625.7s

################################################################################
                    [1m Learning iteration 654/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.792s, learning 0.215s)
               Value function loss: 14.1962
                    Surrogate loss: -0.0019
             Mean action noise std: 0.78
                       Mean reward: 245.07
               Mean episode length: 122.54
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10731520
                    Iteration time: 12.01s
                        Total time: 8771.50s
                               ETA: 1330401.9s

################################################################################
                    [1m Learning iteration 655/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.911s, learning 0.166s)
               Value function loss: 12.5550
                    Surrogate loss: 0.0032
             Mean action noise std: 0.78
                       Mean reward: 243.41
               Mean episode length: 122.51
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10747904
                    Iteration time: 12.08s
                        Total time: 8783.58s
                               ETA: 1330189.5s

################################################################################
                    [1m Learning iteration 656/100000 [0m                     

                       Computation: 1290 steps/s (collection: 12.545s, learning 0.155s)
               Value function loss: 27.3958
                    Surrogate loss: 0.0018
             Mean action noise std: 0.78
                       Mean reward: 248.58
               Mean episode length: 124.55
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10764288
                    Iteration time: 12.70s
                        Total time: 8796.28s
                               ETA: 1330071.9s

################################################################################
                    [1m Learning iteration 657/100000 [0m                     

                       Computation: 1355 steps/s (collection: 11.914s, learning 0.170s)
               Value function loss: 13.5102
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 242.24
               Mean episode length: 122.25
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10780672
                    Iteration time: 12.08s
                        Total time: 8808.36s
                               ETA: 1329861.5s

################################################################################
                    [1m Learning iteration 658/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.086s, learning 0.170s)
               Value function loss: 153.7255
                    Surrogate loss: 0.0085
             Mean action noise std: 0.78
                       Mean reward: 253.07
               Mean episode length: 124.83
                  Mean reward/step: 1.97
       Mean episode length/episode: 6.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10797056
                    Iteration time: 12.26s
                        Total time: 8820.62s
                               ETA: 1329677.8s

################################################################################
                    [1m Learning iteration 659/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.514s, learning 0.162s)
               Value function loss: 38.9848
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 250.16
               Mean episode length: 123.88
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10813440
                    Iteration time: 11.68s
                        Total time: 8832.29s
                               ETA: 1329407.2s

################################################################################
                    [1m Learning iteration 660/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.896s, learning 0.176s)
               Value function loss: 38.6218
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 234.84
               Mean episode length: 117.83
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10829824
                    Iteration time: 12.07s
                        Total time: 8844.36s
                               ETA: 1329197.0s

################################################################################
                    [1m Learning iteration 661/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.999s, learning 0.158s)
               Value function loss: 31.0839
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 249.22
               Mean episode length: 122.33
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10846208
                    Iteration time: 12.16s
                        Total time: 8856.52s
                               ETA: 1329000.1s

################################################################################
                    [1m Learning iteration 662/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.791s, learning 0.156s)
               Value function loss: 23.1052
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 250.24
               Mean episode length: 123.68
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10862592
                    Iteration time: 11.95s
                        Total time: 8868.47s
                               ETA: 1328772.3s

################################################################################
                    [1m Learning iteration 663/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.501s, learning 0.196s)
               Value function loss: 21.3705
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 250.61
               Mean episode length: 123.68
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10878976
                    Iteration time: 11.70s
                        Total time: 8880.17s
                               ETA: 1328507.7s

################################################################################
                    [1m Learning iteration 664/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.039s, learning 0.162s)
               Value function loss: 19.4565
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 246.29
               Mean episode length: 122.55
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10895360
                    Iteration time: 12.20s
                        Total time: 8892.37s
                               ETA: 1328319.2s

################################################################################
                    [1m Learning iteration 665/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.850s, learning 0.311s)
               Value function loss: 15.8808
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 251.68
               Mean episode length: 123.88
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10911744
                    Iteration time: 12.16s
                        Total time: 8904.53s
                               ETA: 1328125.2s

################################################################################
                    [1m Learning iteration 666/100000 [0m                     

                       Computation: 1282 steps/s (collection: 12.512s, learning 0.259s)
               Value function loss: 18.5359
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 249.43
               Mean episode length: 122.98
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10928128
                    Iteration time: 12.77s
                        Total time: 8917.30s
                               ETA: 1328022.6s

################################################################################
                    [1m Learning iteration 667/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.044s, learning 0.157s)
               Value function loss: 16.8692
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 246.81
               Mean episode length: 122.52
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10944512
                    Iteration time: 12.20s
                        Total time: 8929.50s
                               ETA: 1327835.4s

################################################################################
                    [1m Learning iteration 668/100000 [0m                     

                       Computation: 1325 steps/s (collection: 12.198s, learning 0.165s)
               Value function loss: 15.2289
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 242.87
               Mean episode length: 121.91
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10960896
                    Iteration time: 12.36s
                        Total time: 8941.86s
                               ETA: 1327672.9s

################################################################################
                    [1m Learning iteration 669/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.656s, learning 0.242s)
               Value function loss: 13.1801
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 239.57
               Mean episode length: 120.23
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10977280
                    Iteration time: 11.90s
                        Total time: 8953.76s
                               ETA: 1327441.8s

################################################################################
                    [1m Learning iteration 670/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.496s, learning 0.160s)
               Value function loss: 13.4854
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 245.24
               Mean episode length: 121.86
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 10993664
                    Iteration time: 11.66s
                        Total time: 8965.42s
                               ETA: 1327175.7s

################################################################################
                    [1m Learning iteration 671/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.428s, learning 0.166s)
               Value function loss: 80.0210
                    Surrogate loss: 0.0154
             Mean action noise std: 0.78
                       Mean reward: 251.44
               Mean episode length: 124.09
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11010048
                    Iteration time: 11.59s
                        Total time: 8977.01s
                               ETA: 1326901.2s

################################################################################
                    [1m Learning iteration 672/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.848s, learning 0.168s)
               Value function loss: 15.0214
                    Surrogate loss: -0.0014
             Mean action noise std: 0.78
                       Mean reward: 248.07
               Mean episode length: 123.91
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11026432
                    Iteration time: 12.02s
                        Total time: 8989.03s
                               ETA: 1326689.6s

################################################################################
                    [1m Learning iteration 673/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.869s, learning 0.243s)
               Value function loss: 44.1956
                    Surrogate loss: 0.0088
             Mean action noise std: 0.78
                       Mean reward: 252.41
               Mean episode length: 124.19
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11042816
                    Iteration time: 12.11s
                        Total time: 9001.14s
                               ETA: 1326492.9s

################################################################################
                    [1m Learning iteration 674/100000 [0m                     

                       Computation: 1327 steps/s (collection: 12.182s, learning 0.162s)
               Value function loss: 82.0971
                    Surrogate loss: 0.0044
             Mean action noise std: 0.78
                       Mean reward: 253.97
               Mean episode length: 124.86
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11059200
                    Iteration time: 12.34s
                        Total time: 9013.48s
                               ETA: 1326330.8s

################################################################################
                    [1m Learning iteration 675/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.997s, learning 0.162s)
               Value function loss: 25.8558
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 250.24
               Mean episode length: 123.13
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11075584
                    Iteration time: 12.16s
                        Total time: 9025.64s
                               ETA: 1326142.0s

################################################################################
                    [1m Learning iteration 676/100000 [0m                     

                       Computation: 1346 steps/s (collection: 12.001s, learning 0.164s)
               Value function loss: 30.2977
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 253.93
               Mean episode length: 123.92
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11091968
                    Iteration time: 12.16s
                        Total time: 9037.81s
                               ETA: 1325954.6s

################################################################################
                    [1m Learning iteration 677/100000 [0m                     

                       Computation: 1343 steps/s (collection: 12.041s, learning 0.154s)
               Value function loss: 28.4046
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 255.82
               Mean episode length: 124.72
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11108352
                    Iteration time: 12.20s
                        Total time: 9050.00s
                               ETA: 1325772.0s

################################################################################
                    [1m Learning iteration 678/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.742s, learning 0.174s)
               Value function loss: 25.9956
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 250.07
               Mean episode length: 123.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11124736
                    Iteration time: 11.92s
                        Total time: 9061.92s
                               ETA: 1325549.2s

################################################################################
                    [1m Learning iteration 679/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.850s, learning 0.200s)
               Value function loss: 17.9609
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 250.41
               Mean episode length: 122.67
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11141120
                    Iteration time: 12.05s
                        Total time: 9073.97s
                               ETA: 1325346.6s

################################################################################
                    [1m Learning iteration 680/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.722s, learning 0.196s)
               Value function loss: 18.0887
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 256.93
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11157504
                    Iteration time: 11.92s
                        Total time: 9085.89s
                               ETA: 1325125.2s

################################################################################
                    [1m Learning iteration 681/100000 [0m                     

                       Computation: 1342 steps/s (collection: 12.039s, learning 0.163s)
               Value function loss: 17.2639
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 250.65
               Mean episode length: 123.27
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11173888
                    Iteration time: 12.20s
                        Total time: 9098.09s
                               ETA: 1324945.7s

################################################################################
                    [1m Learning iteration 682/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.731s, learning 0.159s)
               Value function loss: 20.3194
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 251.51
               Mean episode length: 123.83
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11190272
                    Iteration time: 11.89s
                        Total time: 9109.98s
                               ETA: 1324721.5s

################################################################################
                    [1m Learning iteration 683/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.085s, learning 0.160s)
               Value function loss: 16.8643
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 245.48
               Mean episode length: 122.12
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11206656
                    Iteration time: 12.25s
                        Total time: 9122.22s
                               ETA: 1324549.5s

################################################################################
                    [1m Learning iteration 684/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.360s, learning 0.161s)
               Value function loss: 15.6898
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 253.01
               Mean episode length: 123.44
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11223040
                    Iteration time: 11.52s
                        Total time: 9133.74s
                               ETA: 1324272.9s

################################################################################
                    [1m Learning iteration 685/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.807s, learning 0.195s)
               Value function loss: 16.3419
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 251.49
               Mean episode length: 123.12
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11239424
                    Iteration time: 12.00s
                        Total time: 9145.75s
                               ETA: 1324066.7s

################################################################################
                    [1m Learning iteration 686/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.826s, learning 0.157s)
               Value function loss: 13.1989
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 250.13
               Mean episode length: 122.90
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11255808
                    Iteration time: 11.98s
                        Total time: 9157.73s
                               ETA: 1323858.3s

################################################################################
                    [1m Learning iteration 687/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.383s, learning 0.194s)
               Value function loss: 53.1875
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 256.26
               Mean episode length: 124.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11272192
                    Iteration time: 11.58s
                        Total time: 9169.31s
                               ETA: 1323591.9s

################################################################################
                    [1m Learning iteration 688/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.900s, learning 0.261s)
               Value function loss: 18.5909
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 255.34
               Mean episode length: 124.45
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11288576
                    Iteration time: 12.16s
                        Total time: 9181.47s
                               ETA: 1323410.4s

################################################################################
                    [1m Learning iteration 689/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.722s, learning 0.193s)
               Value function loss: 138.6488
                    Surrogate loss: 0.0070
             Mean action noise std: 0.78
                       Mean reward: 256.30
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11304960
                    Iteration time: 11.92s
                        Total time: 9193.38s
                               ETA: 1323194.0s

################################################################################
                    [1m Learning iteration 690/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.436s, learning 0.159s)
               Value function loss: 29.8095
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 256.23
               Mean episode length: 124.68
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11321344
                    Iteration time: 11.60s
                        Total time: 9204.98s
                               ETA: 1322932.3s

################################################################################
                    [1m Learning iteration 691/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.687s, learning 0.207s)
               Value function loss: 22.9062
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 256.37
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11337728
                    Iteration time: 11.89s
                        Total time: 9216.87s
                               ETA: 1322714.2s

################################################################################
                    [1m Learning iteration 692/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.891s, learning 0.187s)
               Value function loss: 25.4393
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 248.31
               Mean episode length: 123.07
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11354112
                    Iteration time: 12.08s
                        Total time: 9228.95s
                               ETA: 1322523.0s

################################################################################
                    [1m Learning iteration 693/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.161s)
               Value function loss: 23.0528
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 251.77
               Mean episode length: 123.19
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11370496
                    Iteration time: 11.55s
                        Total time: 9240.50s
                               ETA: 1322256.3s

################################################################################
                    [1m Learning iteration 694/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.599s, learning 0.206s)
               Value function loss: 21.4468
                    Surrogate loss: -0.0178
             Mean action noise std: 0.78
                       Mean reward: 250.92
               Mean episode length: 122.90
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11386880
                    Iteration time: 11.80s
                        Total time: 9252.30s
                               ETA: 1322027.2s

################################################################################
                    [1m Learning iteration 695/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.727s, learning 0.161s)
               Value function loss: 17.9525
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 248.44
               Mean episode length: 121.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11403264
                    Iteration time: 11.89s
                        Total time: 9264.19s
                               ETA: 1321810.7s

################################################################################
                    [1m Learning iteration 696/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.558s, learning 0.172s)
               Value function loss: 17.8645
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 255.45
               Mean episode length: 124.05
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11419648
                    Iteration time: 11.73s
                        Total time: 9275.92s
                               ETA: 1321572.1s

################################################################################
                    [1m Learning iteration 697/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.650s, learning 0.163s)
               Value function loss: 17.7841
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 242.67
               Mean episode length: 119.61
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11436032
                    Iteration time: 11.81s
                        Total time: 9287.73s
                               ETA: 1321346.0s

################################################################################
                    [1m Learning iteration 698/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.196s, learning 0.207s)
               Value function loss: 20.5482
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 249.32
               Mean episode length: 121.99
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11452416
                    Iteration time: 12.40s
                        Total time: 9300.13s
                               ETA: 1321204.3s

################################################################################
                    [1m Learning iteration 699/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.073s, learning 0.281s)
               Value function loss: 13.9697
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 247.93
               Mean episode length: 121.67
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11468800
                    Iteration time: 12.35s
                        Total time: 9312.49s
                               ETA: 1321056.0s

################################################################################
                    [1m Learning iteration 700/100000 [0m                     

                       Computation: 1320 steps/s (collection: 12.236s, learning 0.173s)
               Value function loss: 16.7575
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 250.62
               Mean episode length: 122.06
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11485184
                    Iteration time: 12.41s
                        Total time: 9324.90s
                               ETA: 1320916.0s

################################################################################
                    [1m Learning iteration 701/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.823s, learning 0.164s)
               Value function loss: 16.4874
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 252.89
               Mean episode length: 123.37
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11501568
                    Iteration time: 11.99s
                        Total time: 9336.88s
                               ETA: 1320716.6s

################################################################################
                    [1m Learning iteration 702/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.940s, learning 0.192s)
               Value function loss: 71.9739
                    Surrogate loss: 0.0122
             Mean action noise std: 0.78
                       Mean reward: 255.78
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11517952
                    Iteration time: 12.13s
                        Total time: 9349.01s
                               ETA: 1320538.3s

################################################################################
                    [1m Learning iteration 703/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.568s, learning 0.166s)
               Value function loss: 17.7658
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 255.38
               Mean episode length: 124.29
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11534336
                    Iteration time: 11.73s
                        Total time: 9360.75s
                               ETA: 1320304.3s

################################################################################
                    [1m Learning iteration 704/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.913s, learning 0.157s)
               Value function loss: 18.7568
                    Surrogate loss: -0.0030
             Mean action noise std: 0.78
                       Mean reward: 252.92
               Mean episode length: 123.45
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11550720
                    Iteration time: 12.07s
                        Total time: 9372.82s
                               ETA: 1320118.3s

################################################################################
                    [1m Learning iteration 705/100000 [0m                     

                       Computation: 1420 steps/s (collection: 11.368s, learning 0.167s)
               Value function loss: 121.4344
                    Surrogate loss: 0.0129
             Mean action noise std: 0.78
                       Mean reward: 256.41
               Mean episode length: 124.83
                  Mean reward/step: 1.98
       Mean episode length/episode: 6.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11567104
                    Iteration time: 11.54s
                        Total time: 9384.35s
                               ETA: 1319857.6s

################################################################################
                    [1m Learning iteration 706/100000 [0m                     

                       Computation: 1385 steps/s (collection: 11.631s, learning 0.198s)
               Value function loss: 23.4757
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 248.36
               Mean episode length: 121.65
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11583488
                    Iteration time: 11.83s
                        Total time: 9396.18s
                               ETA: 1319638.8s

################################################################################
                    [1m Learning iteration 707/100000 [0m                     

                       Computation: 1340 steps/s (collection: 12.059s, learning 0.159s)
               Value function loss: 29.6106
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 251.28
               Mean episode length: 122.29
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11599872
                    Iteration time: 12.22s
                        Total time: 9408.40s
                               ETA: 1319475.1s

################################################################################
                    [1m Learning iteration 708/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.589s, learning 0.248s)
               Value function loss: 31.2060
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 252.30
               Mean episode length: 123.44
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11616256
                    Iteration time: 11.84s
                        Total time: 9420.24s
                               ETA: 1319258.6s

################################################################################
                    [1m Learning iteration 709/100000 [0m                     

                       Computation: 1326 steps/s (collection: 12.166s, learning 0.185s)
               Value function loss: 27.4882
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 257.59
               Mean episode length: 125.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11632640
                    Iteration time: 12.35s
                        Total time: 9432.59s
                               ETA: 1319114.4s

################################################################################
                    [1m Learning iteration 710/100000 [0m                     

                       Computation: 1431 steps/s (collection: 11.241s, learning 0.205s)
               Value function loss: 21.8568
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 252.39
               Mean episode length: 122.78
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11649024
                    Iteration time: 11.45s
                        Total time: 9444.04s
                               ETA: 1318844.3s

################################################################################
                    [1m Learning iteration 711/100000 [0m                     

                       Computation: 1330 steps/s (collection: 12.042s, learning 0.268s)
               Value function loss: 22.4409
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 253.91
               Mean episode length: 123.40
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11665408
                    Iteration time: 12.31s
                        Total time: 9456.35s
                               ETA: 1318695.3s

################################################################################
                    [1m Learning iteration 712/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.679s, learning 0.247s)
               Value function loss: 18.6067
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 248.08
               Mean episode length: 121.76
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11681792
                    Iteration time: 11.93s
                        Total time: 9468.27s
                               ETA: 1318493.3s

################################################################################
                    [1m Learning iteration 713/100000 [0m                     

                       Computation: 1350 steps/s (collection: 11.927s, learning 0.207s)
               Value function loss: 20.2140
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 250.75
               Mean episode length: 122.56
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11698176
                    Iteration time: 12.13s
                        Total time: 9480.41s
                               ETA: 1318320.7s

################################################################################
                    [1m Learning iteration 714/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.017s, learning 0.304s)
               Value function loss: 18.8866
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 255.01
               Mean episode length: 123.99
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11714560
                    Iteration time: 12.32s
                        Total time: 9492.73s
                               ETA: 1318174.5s

################################################################################
                    [1m Learning iteration 715/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.797s, learning 0.203s)
               Value function loss: 20.1186
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 254.78
               Mean episode length: 124.14
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11730944
                    Iteration time: 12.00s
                        Total time: 9504.73s
                               ETA: 1317984.3s

################################################################################
                    [1m Learning iteration 716/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.536s, learning 0.163s)
               Value function loss: 17.1940
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 251.50
               Mean episode length: 123.12
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11747328
                    Iteration time: 11.70s
                        Total time: 9516.43s
                               ETA: 1317752.9s

################################################################################
                    [1m Learning iteration 717/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.804s, learning 0.240s)
               Value function loss: 13.9617
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 250.16
               Mean episode length: 122.89
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11763712
                    Iteration time: 12.04s
                        Total time: 9528.47s
                               ETA: 1317569.9s

################################################################################
                    [1m Learning iteration 718/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.117s, learning 0.161s)
               Value function loss: 74.5481
                    Surrogate loss: 0.0024
             Mean action noise std: 0.78
                       Mean reward: 256.39
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11780096
                    Iteration time: 12.28s
                        Total time: 9540.75s
                               ETA: 1317419.5s

################################################################################
                    [1m Learning iteration 719/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.486s, learning 0.172s)
               Value function loss: 15.3110
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 247.93
               Mean episode length: 121.89
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11796480
                    Iteration time: 11.66s
                        Total time: 9552.41s
                               ETA: 1317184.0s

################################################################################
                    [1m Learning iteration 720/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.983s, learning 0.168s)
               Value function loss: 68.9830
                    Surrogate loss: 0.0159
             Mean action noise std: 0.78
                       Mean reward: 256.81
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11812864
                    Iteration time: 12.15s
                        Total time: 9564.56s
                               ETA: 1317016.9s

################################################################################
                    [1m Learning iteration 721/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.694s, learning 0.158s)
               Value function loss: 65.1806
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: 253.27
               Mean episode length: 123.46
                  Mean reward/step: 1.94
       Mean episode length/episode: 6.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11829248
                    Iteration time: 11.85s
                        Total time: 9576.41s
                               ETA: 1316809.3s

################################################################################
                    [1m Learning iteration 722/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.022s, learning 0.207s)
               Value function loss: 29.8496
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 252.96
               Mean episode length: 124.46
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11845632
                    Iteration time: 12.23s
                        Total time: 9588.64s
                               ETA: 1316653.9s

################################################################################
                    [1m Learning iteration 723/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.827s, learning 0.163s)
               Value function loss: 37.0650
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 253.70
               Mean episode length: 124.91
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11862016
                    Iteration time: 11.99s
                        Total time: 9600.63s
                               ETA: 1316466.3s

################################################################################
                    [1m Learning iteration 724/100000 [0m                     

                       Computation: 1345 steps/s (collection: 11.921s, learning 0.252s)
               Value function loss: 33.1658
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 254.67
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11878400
                    Iteration time: 12.17s
                        Total time: 9612.80s
                               ETA: 1316304.1s

################################################################################
                    [1m Learning iteration 725/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.592s, learning 0.163s)
               Value function loss: 36.8346
                    Surrogate loss: 0.0072
             Mean action noise std: 0.78
                       Mean reward: 253.46
               Mean episode length: 124.33
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11894784
                    Iteration time: 11.75s
                        Total time: 9624.56s
                               ETA: 1316085.2s

################################################################################
                    [1m Learning iteration 726/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.826s, learning 0.164s)
               Value function loss: 28.2840
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 248.17
               Mean episode length: 124.01
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11911168
                    Iteration time: 11.99s
                        Total time: 9636.55s
                               ETA: 1315898.9s

################################################################################
                    [1m Learning iteration 727/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.931s, learning 0.169s)
               Value function loss: 27.5948
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 244.88
               Mean episode length: 121.52
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11927552
                    Iteration time: 12.10s
                        Total time: 9648.65s
                               ETA: 1315728.0s

################################################################################
                    [1m Learning iteration 728/100000 [0m                     

                       Computation: 1416 steps/s (collection: 11.406s, learning 0.161s)
               Value function loss: 26.3560
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 249.23
               Mean episode length: 123.20
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11943936
                    Iteration time: 11.57s
                        Total time: 9660.21s
                               ETA: 1315485.1s

################################################################################
                    [1m Learning iteration 729/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.722s, learning 0.161s)
               Value function loss: 31.2155
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 248.20
               Mean episode length: 124.19
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11960320
                    Iteration time: 11.88s
                        Total time: 9672.10s
                               ETA: 1315285.8s

################################################################################
                    [1m Learning iteration 730/100000 [0m                     

                       Computation: 1423 steps/s (collection: 11.342s, learning 0.168s)
               Value function loss: 21.6051
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 248.40
               Mean episode length: 124.19
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11976704
                    Iteration time: 11.51s
                        Total time: 9683.61s
                               ETA: 1315036.3s

################################################################################
                    [1m Learning iteration 731/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.567s, learning 0.168s)
               Value function loss: 28.1394
                    Surrogate loss: -0.0008
             Mean action noise std: 0.78
                       Mean reward: 247.31
               Mean episode length: 123.09
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 11993088
                    Iteration time: 11.74s
                        Total time: 9695.34s
                               ETA: 1314818.1s

################################################################################
                    [1m Learning iteration 732/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.615s, learning 0.166s)
               Value function loss: 21.9573
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 242.22
               Mean episode length: 120.85
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12009472
                    Iteration time: 11.78s
                        Total time: 9707.12s
                               ETA: 1314606.6s

################################################################################
                    [1m Learning iteration 733/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.682s, learning 0.164s)
               Value function loss: 17.8935
                    Surrogate loss: -0.0020
             Mean action noise std: 0.78
                       Mean reward: 243.28
               Mean episode length: 120.53
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12025856
                    Iteration time: 11.85s
                        Total time: 9718.97s
                               ETA: 1314404.4s

################################################################################
                    [1m Learning iteration 734/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.695s, learning 0.180s)
               Value function loss: 53.4810
                    Surrogate loss: 0.0080
             Mean action noise std: 0.78
                       Mean reward: 253.29
               Mean episode length: 125.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12042240
                    Iteration time: 11.88s
                        Total time: 9730.84s
                               ETA: 1314206.8s

################################################################################
                    [1m Learning iteration 735/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.773s, learning 0.158s)
               Value function loss: 19.6277
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 248.50
               Mean episode length: 123.54
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12058624
                    Iteration time: 11.93s
                        Total time: 9742.78s
                               ETA: 1314017.1s

################################################################################
                    [1m Learning iteration 736/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.851s, learning 0.194s)
               Value function loss: 175.4213
                    Surrogate loss: 0.0244
             Mean action noise std: 0.78
                       Mean reward: 252.16
               Mean episode length: 124.85
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12075008
                    Iteration time: 12.05s
                        Total time: 9754.82s
                               ETA: 1313843.4s

################################################################################
                    [1m Learning iteration 737/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.104s, learning 0.160s)
               Value function loss: 32.4882
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 249.49
               Mean episode length: 124.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12091392
                    Iteration time: 12.26s
                        Total time: 9767.09s
                               ETA: 1313699.4s

################################################################################
                    [1m Learning iteration 738/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.804s, learning 0.208s)
               Value function loss: 30.7832
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 252.18
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12107776
                    Iteration time: 12.01s
                        Total time: 9779.10s
                               ETA: 1313522.0s

################################################################################
                    [1m Learning iteration 739/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.153s, learning 0.166s)
               Value function loss: 30.6990
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 249.18
               Mean episode length: 124.45
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12124160
                    Iteration time: 12.32s
                        Total time: 9791.42s
                               ETA: 1313386.2s

################################################################################
                    [1m Learning iteration 740/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.688s, learning 0.164s)
               Value function loss: 27.7693
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 247.10
               Mean episode length: 123.86
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12140544
                    Iteration time: 11.85s
                        Total time: 9803.27s
                               ETA: 1313188.2s

################################################################################
                    [1m Learning iteration 741/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.772s, learning 0.167s)
               Value function loss: 24.0977
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 247.56
               Mean episode length: 123.58
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12156928
                    Iteration time: 11.94s
                        Total time: 9815.21s
                               ETA: 1313002.4s

################################################################################
                    [1m Learning iteration 742/100000 [0m                     

                       Computation: 1315 steps/s (collection: 12.211s, learning 0.240s)
               Value function loss: 25.3944
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 242.64
               Mean episode length: 121.97
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12173312
                    Iteration time: 12.45s
                        Total time: 9827.66s
                               ETA: 1312885.3s

################################################################################
                    [1m Learning iteration 743/100000 [0m                     

                       Computation: 1388 steps/s (collection: 11.547s, learning 0.257s)
               Value function loss: 20.3905
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 248.72
               Mean episode length: 123.55
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12189696
                    Iteration time: 11.80s
                        Total time: 9839.46s
                               ETA: 1312682.1s

################################################################################
                    [1m Learning iteration 744/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.821s, learning 0.161s)
               Value function loss: 23.0125
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 249.00
               Mean episode length: 124.64
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12206080
                    Iteration time: 11.98s
                        Total time: 9851.44s
                               ETA: 1312503.3s

################################################################################
                    [1m Learning iteration 745/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.653s, learning 0.210s)
               Value function loss: 22.9936
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 248.04
               Mean episode length: 123.20
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12222464
                    Iteration time: 11.86s
                        Total time: 9863.31s
                               ETA: 1312309.0s

################################################################################
                    [1m Learning iteration 746/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.669s, learning 0.165s)
               Value function loss: 18.4907
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 250.94
               Mean episode length: 122.91
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12238848
                    Iteration time: 11.83s
                        Total time: 9875.14s
                               ETA: 1312111.4s

################################################################################
                    [1m Learning iteration 747/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.665s, learning 0.196s)
               Value function loss: 22.9462
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 247.31
               Mean episode length: 122.70
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12255232
                    Iteration time: 11.86s
                        Total time: 9887.00s
                               ETA: 1311917.9s

################################################################################
                    [1m Learning iteration 748/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.789s, learning 0.165s)
               Value function loss: 15.4617
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 249.91
               Mean episode length: 123.62
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.80
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12271616
                    Iteration time: 11.95s
                        Total time: 9898.96s
                               ETA: 1311737.1s

################################################################################
                    [1m Learning iteration 749/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.748s, learning 0.160s)
               Value function loss: 86.9065
                    Surrogate loss: 0.0284
             Mean action noise std: 0.78
                       Mean reward: 254.08
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.15
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12288000
                    Iteration time: 11.91s
                        Total time: 9910.86s
                               ETA: 1311550.8s

################################################################################
                    [1m Learning iteration 750/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.905s, learning 0.163s)
               Value function loss: 16.7054
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 250.58
               Mean episode length: 123.92
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12304384
                    Iteration time: 12.07s
                        Total time: 9922.93s
                               ETA: 1311386.0s

################################################################################
                    [1m Learning iteration 751/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.641s, learning 0.312s)
               Value function loss: 30.0640
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 252.19
               Mean episode length: 123.38
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12320768
                    Iteration time: 11.95s
                        Total time: 9934.88s
                               ETA: 1311206.5s

################################################################################
                    [1m Learning iteration 752/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.840s, learning 0.200s)
               Value function loss: 121.6513
                    Surrogate loss: 0.0050
             Mean action noise std: 0.78
                       Mean reward: 253.51
               Mean episode length: 124.33
                  Mean reward/step: 1.96
       Mean episode length/episode: 6.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12337152
                    Iteration time: 12.04s
                        Total time: 9946.92s
                               ETA: 1311038.9s

################################################################################
                    [1m Learning iteration 753/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.583s, learning 0.292s)
               Value function loss: 26.5589
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 254.05
               Mean episode length: 124.63
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12353536
                    Iteration time: 11.88s
                        Total time: 9958.80s
                               ETA: 1310850.0s

################################################################################
                    [1m Learning iteration 754/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.764s, learning 0.178s)
               Value function loss: 29.9592
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 254.45
               Mean episode length: 124.08
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12369920
                    Iteration time: 11.94s
                        Total time: 9970.74s
                               ETA: 1310670.4s

################################################################################
                    [1m Learning iteration 755/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.398s, learning 0.188s)
               Value function loss: 32.4851
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 253.32
               Mean episode length: 124.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12386304
                    Iteration time: 11.59s
                        Total time: 9982.33s
                               ETA: 1310444.5s

################################################################################
                    [1m Learning iteration 756/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.605s, learning 0.167s)
               Value function loss: 28.8724
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 252.62
               Mean episode length: 122.93
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12402688
                    Iteration time: 11.77s
                        Total time: 9994.10s
                               ETA: 1310243.5s

################################################################################
                    [1m Learning iteration 757/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.710s, learning 0.161s)
               Value function loss: 28.3585
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 251.73
               Mean episode length: 123.30
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12419072
                    Iteration time: 11.87s
                        Total time: 10005.97s
                               ETA: 1310056.0s

################################################################################
                    [1m Learning iteration 758/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.157s, learning 0.164s)
               Value function loss: 22.8087
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 255.85
               Mean episode length: 124.85
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12435456
                    Iteration time: 12.32s
                        Total time: 10018.29s
                               ETA: 1309927.7s

################################################################################
                    [1m Learning iteration 759/100000 [0m                     

                       Computation: 1397 steps/s (collection: 11.562s, learning 0.161s)
               Value function loss: 16.3840
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 254.21
               Mean episode length: 124.17
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12451840
                    Iteration time: 11.72s
                        Total time: 10030.01s
                               ETA: 1309721.8s

################################################################################
                    [1m Learning iteration 760/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.676s, learning 0.211s)
               Value function loss: 21.8470
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 252.53
               Mean episode length: 123.21
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12468224
                    Iteration time: 11.89s
                        Total time: 10041.90s
                               ETA: 1309537.7s

################################################################################
                    [1m Learning iteration 761/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.564s, learning 0.189s)
               Value function loss: 18.4750
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 250.01
               Mean episode length: 124.15
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12484608
                    Iteration time: 11.75s
                        Total time: 10053.65s
                               ETA: 1309336.7s

################################################################################
                    [1m Learning iteration 762/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.735s, learning 0.206s)
               Value function loss: 17.2181
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 253.36
               Mean episode length: 124.69
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12500992
                    Iteration time: 11.94s
                        Total time: 10065.59s
                               ETA: 1309160.5s

################################################################################
                    [1m Learning iteration 763/100000 [0m                     

                       Computation: 1344 steps/s (collection: 12.026s, learning 0.159s)
               Value function loss: 18.9689
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 254.43
               Mean episode length: 123.69
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12517376
                    Iteration time: 12.19s
                        Total time: 10077.78s
                               ETA: 1309016.5s

################################################################################
                    [1m Learning iteration 764/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.941s, learning 0.166s)
               Value function loss: 12.9246
                    Surrogate loss: -0.0004
             Mean action noise std: 0.78
                       Mean reward: 252.22
               Mean episode length: 123.58
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12533760
                    Iteration time: 12.11s
                        Total time: 10089.89s
                               ETA: 1308862.7s

################################################################################
                    [1m Learning iteration 765/100000 [0m                     

                       Computation: 1418 steps/s (collection: 11.386s, learning 0.168s)
               Value function loss: 58.6171
                    Surrogate loss: 0.0042
             Mean action noise std: 0.78
                       Mean reward: 253.88
               Mean episode length: 124.56
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12550144
                    Iteration time: 11.55s
                        Total time: 10101.44s
                               ETA: 1308637.6s

################################################################################
                    [1m Learning iteration 766/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.349s, learning 0.189s)
               Value function loss: 17.9189
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 255.02
               Mean episode length: 124.56
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12566528
                    Iteration time: 11.54s
                        Total time: 10112.98s
                               ETA: 1308411.1s

################################################################################
                    [1m Learning iteration 767/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.725s, learning 0.166s)
               Value function loss: 113.8621
                    Surrogate loss: 0.0036
             Mean action noise std: 0.78
                       Mean reward: 252.72
               Mean episode length: 124.36
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.21
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12582912
                    Iteration time: 11.89s
                        Total time: 10124.87s
                               ETA: 1308230.7s

################################################################################
                    [1m Learning iteration 768/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.589s, learning 0.162s)
               Value function loss: 40.4117
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 255.68
               Mean episode length: 124.59
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.11
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12599296
                    Iteration time: 11.75s
                        Total time: 10136.62s
                               ETA: 1308032.7s

################################################################################
                    [1m Learning iteration 769/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.849s, learning 0.242s)
               Value function loss: 22.3985
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 256.17
               Mean episode length: 124.85
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12615680
                    Iteration time: 12.09s
                        Total time: 10148.71s
                               ETA: 1307879.1s

################################################################################
                    [1m Learning iteration 770/100000 [0m                     

                       Computation: 1339 steps/s (collection: 11.968s, learning 0.259s)
               Value function loss: 25.2988
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 253.87
               Mean episode length: 123.53
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12632064
                    Iteration time: 12.23s
                        Total time: 10160.94s
                               ETA: 1307743.3s

################################################################################
                    [1m Learning iteration 771/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.987s, learning 0.199s)
               Value function loss: 18.8351
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 252.08
               Mean episode length: 124.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12648448
                    Iteration time: 12.19s
                        Total time: 10173.13s
                               ETA: 1307602.4s

################################################################################
                    [1m Learning iteration 772/100000 [0m                     

                       Computation: 1331 steps/s (collection: 12.000s, learning 0.306s)
               Value function loss: 21.4083
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 255.17
               Mean episode length: 124.10
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12664832
                    Iteration time: 12.31s
                        Total time: 10185.43s
                               ETA: 1307477.4s

################################################################################
                    [1m Learning iteration 773/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.932s, learning 0.160s)
               Value function loss: 20.2241
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 252.14
               Mean episode length: 122.28
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12681216
                    Iteration time: 12.09s
                        Total time: 10197.52s
                               ETA: 1307325.2s

################################################################################
                    [1m Learning iteration 774/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.800s, learning 0.191s)
               Value function loss: 18.5108
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 253.74
               Mean episode length: 124.26
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12697600
                    Iteration time: 11.99s
                        Total time: 10209.52s
                               ETA: 1307160.5s

################################################################################
                    [1m Learning iteration 775/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.723s, learning 0.171s)
               Value function loss: 17.9425
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 251.84
               Mean episode length: 123.64
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12713984
                    Iteration time: 11.89s
                        Total time: 10221.41s
                               ETA: 1306983.7s

################################################################################
                    [1m Learning iteration 776/100000 [0m                     

                       Computation: 1427 steps/s (collection: 11.288s, learning 0.193s)
               Value function loss: 21.9410
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 252.74
               Mean episode length: 123.78
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12730368
                    Iteration time: 11.48s
                        Total time: 10232.89s
                               ETA: 1306754.6s

################################################################################
                    [1m Learning iteration 777/100000 [0m                     

                       Computation: 1430 steps/s (collection: 11.254s, learning 0.197s)
               Value function loss: 13.6767
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 248.53
               Mean episode length: 122.86
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12746752
                    Iteration time: 11.45s
                        Total time: 10244.34s
                               ETA: 1306522.2s

################################################################################
                    [1m Learning iteration 778/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.773s, learning 0.169s)
               Value function loss: 20.1560
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 246.55
               Mean episode length: 123.18
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12763136
                    Iteration time: 11.94s
                        Total time: 10256.28s
                               ETA: 1306353.0s

################################################################################
                    [1m Learning iteration 779/100000 [0m                     

                       Computation: 1414 steps/s (collection: 11.397s, learning 0.185s)
               Value function loss: 16.8879
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 243.95
               Mean episode length: 123.27
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12779520
                    Iteration time: 11.58s
                        Total time: 10267.87s
                               ETA: 1306138.4s

################################################################################
                    [1m Learning iteration 780/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.977s, learning 0.162s)
               Value function loss: 15.5200
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 249.16
               Mean episode length: 123.49
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12795904
                    Iteration time: 12.14s
                        Total time: 10280.00s
                               ETA: 1305995.0s

################################################################################
                    [1m Learning iteration 781/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.554s, learning 0.155s)
               Value function loss: 29.9237
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 256.31
               Mean episode length: 124.66
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12812288
                    Iteration time: 11.71s
                        Total time: 10291.71s
                               ETA: 1305797.3s

################################################################################
                    [1m Learning iteration 782/100000 [0m                     

                       Computation: 1346 steps/s (collection: 12.009s, learning 0.162s)
               Value function loss: 17.6630
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 254.29
               Mean episode length: 124.44
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12828672
                    Iteration time: 12.17s
                        Total time: 10303.88s
                               ETA: 1305658.8s

################################################################################
                    [1m Learning iteration 783/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.854s, learning 0.204s)
               Value function loss: 127.9922
                    Surrogate loss: 0.0105
             Mean action noise std: 0.78
                       Mean reward: 256.23
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 6.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12845056
                    Iteration time: 12.06s
                        Total time: 10315.94s
                               ETA: 1305506.2s

################################################################################
                    [1m Learning iteration 784/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.096s, learning 0.163s)
               Value function loss: 21.4795
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 256.18
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12861440
                    Iteration time: 12.26s
                        Total time: 10328.20s
                               ETA: 1305379.5s

################################################################################
                    [1m Learning iteration 785/100000 [0m                     

                       Computation: 1342 steps/s (collection: 11.996s, learning 0.211s)
               Value function loss: 25.2023
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 254.34
               Mean episode length: 124.44
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12877824
                    Iteration time: 12.21s
                        Total time: 10340.41s
                               ETA: 1305246.5s

################################################################################
                    [1m Learning iteration 786/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.303s, learning 0.161s)
               Value function loss: 28.3671
                    Surrogate loss: 0.0005
             Mean action noise std: 0.78
                       Mean reward: 249.69
               Mean episode length: 122.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12894208
                    Iteration time: 12.46s
                        Total time: 10352.87s
                               ETA: 1305146.0s

################################################################################
                    [1m Learning iteration 787/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.665s, learning 0.249s)
               Value function loss: 22.7209
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 253.15
               Mean episode length: 124.30
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12910592
                    Iteration time: 11.91s
                        Total time: 10364.79s
                               ETA: 1304976.7s

################################################################################
                    [1m Learning iteration 788/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.809s, learning 0.215s)
               Value function loss: 20.1821
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 249.11
               Mean episode length: 123.61
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12926976
                    Iteration time: 12.02s
                        Total time: 10376.81s
                               ETA: 1304821.5s

################################################################################
                    [1m Learning iteration 789/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.778s, learning 0.271s)
               Value function loss: 20.3560
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 248.86
               Mean episode length: 123.48
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12943360
                    Iteration time: 12.05s
                        Total time: 10388.86s
                               ETA: 1304669.9s

################################################################################
                    [1m Learning iteration 790/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.751s, learning 0.191s)
               Value function loss: 15.2353
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 251.31
               Mean episode length: 124.20
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12959744
                    Iteration time: 11.94s
                        Total time: 10400.80s
                               ETA: 1304505.2s

################################################################################
                    [1m Learning iteration 791/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.618s, learning 0.172s)
               Value function loss: 18.9289
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 242.52
               Mean episode length: 121.25
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12976128
                    Iteration time: 11.79s
                        Total time: 10412.59s
                               ETA: 1304321.8s

################################################################################
                    [1m Learning iteration 792/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.338s, learning 0.155s)
               Value function loss: 18.8622
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 247.68
               Mean episode length: 122.77
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 12992512
                    Iteration time: 11.49s
                        Total time: 10424.08s
                               ETA: 1304101.7s

################################################################################
                    [1m Learning iteration 793/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.820s, learning 0.173s)
               Value function loss: 16.3805
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 253.44
               Mean episode length: 124.23
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13008896
                    Iteration time: 11.99s
                        Total time: 10436.08s
                               ETA: 1303944.7s

################################################################################
                    [1m Learning iteration 794/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.109s, learning 0.217s)
               Value function loss: 17.0112
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 253.02
               Mean episode length: 124.10
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13025280
                    Iteration time: 12.33s
                        Total time: 10448.41s
                               ETA: 1303829.5s

################################################################################
                    [1m Learning iteration 795/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.859s, learning 0.158s)
               Value function loss: 11.8330
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 253.18
               Mean episode length: 124.72
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13041664
                    Iteration time: 12.02s
                        Total time: 10460.42s
                               ETA: 1303676.1s

################################################################################
                    [1m Learning iteration 796/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.982s, learning 0.208s)
               Value function loss: 56.2780
                    Surrogate loss: 0.0188
             Mean action noise std: 0.78
                       Mean reward: 252.65
               Mean episode length: 124.81
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.16
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13058048
                    Iteration time: 12.19s
                        Total time: 10472.61s
                               ETA: 1303544.6s

################################################################################
                    [1m Learning iteration 797/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.654s, learning 0.200s)
               Value function loss: 13.5620
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 250.55
               Mean episode length: 124.18
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13074432
                    Iteration time: 11.85s
                        Total time: 10484.47s
                               ETA: 1303371.5s

################################################################################
                    [1m Learning iteration 798/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.677s, learning 0.157s)
               Value function loss: 30.8712
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 254.49
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13090816
                    Iteration time: 11.83s
                        Total time: 10496.30s
                               ETA: 1303196.4s

################################################################################
                    [1m Learning iteration 799/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.936s, learning 0.164s)
               Value function loss: 66.9829
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 249.23
               Mean episode length: 124.89
                  Mean reward/step: 1.92
       Mean episode length/episode: 6.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13107200
                    Iteration time: 12.10s
                        Total time: 10508.40s
                               ETA: 1303054.7s

################################################################################
                    [1m Learning iteration 800/100000 [0m                     

                       Computation: 1402 steps/s (collection: 11.517s, learning 0.165s)
               Value function loss: 26.2511
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 248.90
               Mean episode length: 124.12
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13123584
                    Iteration time: 11.68s
                        Total time: 10520.08s
                               ETA: 1302861.7s

################################################################################
                    [1m Learning iteration 801/100000 [0m                     

                       Computation: 1321 steps/s (collection: 12.097s, learning 0.299s)
               Value function loss: 26.4204
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 247.80
               Mean episode length: 124.17
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13139968
                    Iteration time: 12.40s
                        Total time: 10532.48s
                               ETA: 1302757.3s

################################################################################
                    [1m Learning iteration 802/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.025s, learning 0.303s)
               Value function loss: 20.4226
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 247.68
               Mean episode length: 123.27
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13156352
                    Iteration time: 12.33s
                        Total time: 10544.81s
                               ETA: 1302644.8s

################################################################################
                    [1m Learning iteration 803/100000 [0m                     

                       Computation: 1324 steps/s (collection: 12.146s, learning 0.224s)
               Value function loss: 26.5085
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 245.24
               Mean episode length: 123.46
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13172736
                    Iteration time: 12.37s
                        Total time: 10557.18s
                               ETA: 1302537.8s

################################################################################
                    [1m Learning iteration 804/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.135s, learning 0.199s)
               Value function loss: 19.5174
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 244.00
               Mean episode length: 123.93
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13189120
                    Iteration time: 12.33s
                        Total time: 10569.51s
                               ETA: 1302426.4s

################################################################################
                    [1m Learning iteration 805/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.910s, learning 0.160s)
               Value function loss: 18.2573
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 244.36
               Mean episode length: 122.64
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13205504
                    Iteration time: 12.07s
                        Total time: 10581.58s
                               ETA: 1302282.8s

################################################################################
                    [1m Learning iteration 806/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.489s, learning 0.188s)
               Value function loss: 14.4125
                    Surrogate loss: 0.0013
             Mean action noise std: 0.78
                       Mean reward: 239.79
               Mean episode length: 122.94
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13221888
                    Iteration time: 11.68s
                        Total time: 10593.26s
                               ETA: 1302091.2s

################################################################################
                    [1m Learning iteration 807/100000 [0m                     

                       Computation: 1433 steps/s (collection: 11.275s, learning 0.157s)
               Value function loss: 19.6532
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 242.74
               Mean episode length: 124.05
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13238272
                    Iteration time: 11.43s
                        Total time: 10604.69s
                               ETA: 1301870.1s

################################################################################
                    [1m Learning iteration 808/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.802s, learning 0.251s)
               Value function loss: 13.2891
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 243.16
               Mean episode length: 123.30
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13254656
                    Iteration time: 12.05s
                        Total time: 10616.74s
                               ETA: 1301725.5s

################################################################################
                    [1m Learning iteration 809/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.077s, learning 0.176s)
               Value function loss: 20.2136
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 243.47
               Mean episode length: 122.92
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13271040
                    Iteration time: 12.25s
                        Total time: 10629.00s
                               ETA: 1301605.8s

################################################################################
                    [1m Learning iteration 810/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.778s, learning 0.167s)
               Value function loss: 14.6039
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 238.63
               Mean episode length: 123.31
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13287424
                    Iteration time: 11.94s
                        Total time: 10640.94s
                               ETA: 1301448.6s

################################################################################
                    [1m Learning iteration 811/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.704s, learning 0.251s)
               Value function loss: 11.0342
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 240.78
               Mean episode length: 123.48
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.85
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13303808
                    Iteration time: 11.95s
                        Total time: 10652.89s
                               ETA: 1301293.1s

################################################################################
                    [1m Learning iteration 812/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.042s, learning 0.211s)
               Value function loss: 38.4765
                    Surrogate loss: 0.0046
             Mean action noise std: 0.78
                       Mean reward: 244.57
               Mean episode length: 124.13
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.17
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13320192
                    Iteration time: 12.25s
                        Total time: 10665.15s
                               ETA: 1301174.2s

################################################################################
                    [1m Learning iteration 813/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.840s, learning 0.205s)
               Value function loss: 12.1637
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 237.97
               Mean episode length: 123.06
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13336576
                    Iteration time: 12.04s
                        Total time: 10677.19s
                               ETA: 1301030.3s

################################################################################
                    [1m Learning iteration 814/100000 [0m                     

                       Computation: 1351 steps/s (collection: 11.902s, learning 0.218s)
               Value function loss: 88.5888
                    Surrogate loss: 0.0226
             Mean action noise std: 0.78
                       Mean reward: 247.22
               Mean episode length: 125.00
                  Mean reward/step: 1.89
       Mean episode length/episode: 6.89
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13352960
                    Iteration time: 12.12s
                        Total time: 10689.31s
                               ETA: 1300895.9s

################################################################################
                    [1m Learning iteration 815/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.946s, learning 0.168s)
               Value function loss: 27.6937
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 246.88
               Mean episode length: 124.55
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13369344
                    Iteration time: 12.11s
                        Total time: 10701.43s
                               ETA: 1300761.0s

################################################################################
                    [1m Learning iteration 816/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.332s, learning 0.207s)
               Value function loss: 26.0685
                    Surrogate loss: 0.0061
             Mean action noise std: 0.78
                       Mean reward: 236.85
               Mean episode length: 122.85
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13385728
                    Iteration time: 11.54s
                        Total time: 10712.96s
                               ETA: 1300556.5s

################################################################################
                    [1m Learning iteration 817/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.469s, learning 0.169s)
               Value function loss: 26.5769
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 244.73
               Mean episode length: 124.81
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13402112
                    Iteration time: 11.64s
                        Total time: 10724.60s
                               ETA: 1300364.6s

################################################################################
                    [1m Learning iteration 818/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.453s, learning 0.174s)
               Value function loss: 21.1066
                    Surrogate loss: 0.0008
             Mean action noise std: 0.78
                       Mean reward: 237.81
               Mean episode length: 123.07
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13418496
                    Iteration time: 11.63s
                        Total time: 10736.23s
                               ETA: 1300171.8s

################################################################################
                    [1m Learning iteration 819/100000 [0m                     

                       Computation: 1422 steps/s (collection: 11.287s, learning 0.235s)
               Value function loss: 21.5596
                    Surrogate loss: 0.0001
             Mean action noise std: 0.78
                       Mean reward: 238.89
               Mean episode length: 124.71
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13434880
                    Iteration time: 11.52s
                        Total time: 10747.75s
                               ETA: 1299966.7s

################################################################################
                    [1m Learning iteration 820/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.448s, learning 0.187s)
               Value function loss: 18.8768
                    Surrogate loss: 0.0006
             Mean action noise std: 0.78
                       Mean reward: 238.77
               Mean episode length: 124.91
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13451264
                    Iteration time: 11.63s
                        Total time: 10759.39s
                               ETA: 1299775.7s

################################################################################
                    [1m Learning iteration 821/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.170s, learning 0.166s)
               Value function loss: 17.0713
                    Surrogate loss: -0.0023
             Mean action noise std: 0.78
                       Mean reward: 232.42
               Mean episode length: 122.70
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13467648
                    Iteration time: 12.34s
                        Total time: 10771.72s
                               ETA: 1299669.7s

################################################################################
                    [1m Learning iteration 822/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.829s, learning 0.162s)
               Value function loss: 17.7414
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 234.38
               Mean episode length: 123.53
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13484032
                    Iteration time: 11.99s
                        Total time: 10783.71s
                               ETA: 1299522.4s

################################################################################
                    [1m Learning iteration 823/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.701s, learning 0.159s)
               Value function loss: 20.8562
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 232.01
               Mean episode length: 122.86
                  Mean reward/step: 1.86
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13500416
                    Iteration time: 11.86s
                        Total time: 10795.57s
                               ETA: 1299359.7s

################################################################################
                    [1m Learning iteration 824/100000 [0m                     

                       Computation: 1405 steps/s (collection: 11.485s, learning 0.169s)
               Value function loss: 13.7393
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 230.32
               Mean episode length: 122.47
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13516800
                    Iteration time: 11.65s
                        Total time: 10807.23s
                               ETA: 1299172.6s

################################################################################
                    [1m Learning iteration 825/100000 [0m                     

                       Computation: 1401 steps/s (collection: 11.515s, learning 0.172s)
               Value function loss: 22.5656
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 224.42
               Mean episode length: 120.56
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13533184
                    Iteration time: 11.69s
                        Total time: 10818.91s
                               ETA: 1298989.9s

################################################################################
                    [1m Learning iteration 826/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.502s, learning 0.194s)
               Value function loss: 17.1325
                    Surrogate loss: 0.0011
             Mean action noise std: 0.78
                       Mean reward: 225.09
               Mean episode length: 121.36
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13549568
                    Iteration time: 11.70s
                        Total time: 10830.61s
                               ETA: 1298808.7s

################################################################################
                    [1m Learning iteration 827/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.196s, learning 0.243s)
               Value function loss: 56.8595
                    Surrogate loss: 0.0078
             Mean action noise std: 0.78
                       Mean reward: 232.14
               Mean episode length: 124.80
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13565952
                    Iteration time: 12.44s
                        Total time: 10843.05s
                               ETA: 1298717.0s

################################################################################
                    [1m Learning iteration 828/100000 [0m                     

                       Computation: 1425 steps/s (collection: 11.307s, learning 0.183s)
               Value function loss: 24.9577
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 229.30
               Mean episode length: 124.03
                  Mean reward/step: 1.78
       Mean episode length/episode: 7.84
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13582336
                    Iteration time: 11.49s
                        Total time: 10854.54s
                               ETA: 1298511.8s

################################################################################
                    [1m Learning iteration 829/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.983s, learning 0.167s)
               Value function loss: 20.2550
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 227.78
               Mean episode length: 122.49
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13598720
                    Iteration time: 12.15s
                        Total time: 10866.69s
                               ETA: 1298385.9s

################################################################################
                    [1m Learning iteration 830/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.784s, learning 0.185s)
               Value function loss: 87.4774
                    Surrogate loss: -0.0042
             Mean action noise std: 0.78
                       Mean reward: 229.20
               Mean episode length: 124.07
                  Mean reward/step: 1.76
       Mean episode length/episode: 6.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13615104
                    Iteration time: 11.97s
                        Total time: 10878.66s
                               ETA: 1298238.7s

################################################################################
                    [1m Learning iteration 831/100000 [0m                     

                       Computation: 1317 steps/s (collection: 12.168s, learning 0.263s)
               Value function loss: 20.9289
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 228.69
               Mean episode length: 123.45
                  Mean reward/step: 1.83
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13631488
                    Iteration time: 12.43s
                        Total time: 10891.09s
                               ETA: 1298147.0s

################################################################################
                    [1m Learning iteration 832/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.852s, learning 0.162s)
               Value function loss: 22.8249
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 224.90
               Mean episode length: 123.52
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13647872
                    Iteration time: 12.01s
                        Total time: 10903.10s
                               ETA: 1298005.7s

################################################################################
                    [1m Learning iteration 833/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.755s, learning 0.161s)
               Value function loss: 25.6614
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 229.41
               Mean episode length: 124.61
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13664256
                    Iteration time: 11.92s
                        Total time: 10915.02s
                               ETA: 1297853.2s

################################################################################
                    [1m Learning iteration 834/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.770s, learning 0.162s)
               Value function loss: 21.7355
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 223.53
               Mean episode length: 121.81
                  Mean reward/step: 1.84
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13680640
                    Iteration time: 11.93s
                        Total time: 10926.95s
                               ETA: 1297702.8s

################################################################################
                    [1m Learning iteration 835/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.635s, learning 0.332s)
               Value function loss: 15.7544
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 221.94
               Mean episode length: 122.42
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13697024
                    Iteration time: 11.97s
                        Total time: 10938.92s
                               ETA: 1297557.0s

################################################################################
                    [1m Learning iteration 836/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.836s, learning 0.161s)
               Value function loss: 19.6380
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 221.52
               Mean episode length: 122.45
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13713408
                    Iteration time: 12.00s
                        Total time: 10950.91s
                               ETA: 1297415.0s

################################################################################
                    [1m Learning iteration 837/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.710s, learning 0.160s)
               Value function loss: 15.1809
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 216.90
               Mean episode length: 119.82
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13729792
                    Iteration time: 11.87s
                        Total time: 10962.78s
                               ETA: 1297258.3s

################################################################################
                    [1m Learning iteration 838/100000 [0m                     

                       Computation: 1337 steps/s (collection: 12.024s, learning 0.226s)
               Value function loss: 17.2831
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 220.91
               Mean episode length: 122.49
                  Mean reward/step: 1.82
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13746176
                    Iteration time: 12.25s
                        Total time: 10975.03s
                               ETA: 1297146.8s

################################################################################
                    [1m Learning iteration 839/100000 [0m                     

                       Computation: 1359 steps/s (collection: 11.875s, learning 0.180s)
               Value function loss: 14.1540
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 221.72
               Mean episode length: 123.01
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13762560
                    Iteration time: 12.06s
                        Total time: 10987.09s
                               ETA: 1297012.7s

################################################################################
                    [1m Learning iteration 840/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.850s, learning 0.163s)
               Value function loss: 15.5495
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 212.90
               Mean episode length: 118.77
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13778944
                    Iteration time: 12.01s
                        Total time: 10999.10s
                               ETA: 1296873.8s

################################################################################
                    [1m Learning iteration 841/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.584s, learning 0.227s)
               Value function loss: 14.4578
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 216.74
               Mean episode length: 121.16
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13795328
                    Iteration time: 11.81s
                        Total time: 11010.91s
                               ETA: 1296711.3s

################################################################################
                    [1m Learning iteration 842/100000 [0m                     

                       Computation: 1384 steps/s (collection: 11.645s, learning 0.189s)
               Value function loss: 12.5234
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 211.12
               Mean episode length: 119.93
                  Mean reward/step: 1.80
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13811712
                    Iteration time: 11.83s
                        Total time: 11022.74s
                               ETA: 1296551.9s

################################################################################
                    [1m Learning iteration 843/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.896s, learning 0.179s)
               Value function loss: 41.0620
                    Surrogate loss: 0.0197
             Mean action noise std: 0.78
                       Mean reward: 228.67
               Mean episode length: 123.62
                  Mean reward/step: 1.79
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13828096
                    Iteration time: 12.07s
                        Total time: 11034.82s
                               ETA: 1296421.3s

################################################################################
                    [1m Learning iteration 844/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.941s, learning 0.167s)
               Value function loss: 11.5478
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 223.78
               Mean episode length: 122.13
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13844480
                    Iteration time: 12.11s
                        Total time: 11046.93s
                               ETA: 1296294.8s

################################################################################
                    [1m Learning iteration 845/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.618s, learning 0.158s)
               Value function loss: 39.2134
                    Surrogate loss: 0.0037
             Mean action noise std: 0.78
                       Mean reward: 227.78
               Mean episode length: 124.78
                  Mean reward/step: 1.81
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13860864
                    Iteration time: 11.78s
                        Total time: 11058.70s
                               ETA: 1296129.7s

################################################################################
                    [1m Learning iteration 846/100000 [0m                     

                       Computation: 1344 steps/s (collection: 11.945s, learning 0.242s)
               Value function loss: 40.9036
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 224.14
               Mean episode length: 123.80
                  Mean reward/step: 1.79
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13877248
                    Iteration time: 12.19s
                        Total time: 11070.89s
                               ETA: 1296013.1s

################################################################################
                    [1m Learning iteration 847/100000 [0m                     

                       Computation: 1373 steps/s (collection: 11.707s, learning 0.221s)
               Value function loss: 21.1831
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 225.11
               Mean episode length: 123.99
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13893632
                    Iteration time: 11.93s
                        Total time: 11082.82s
                               ETA: 1295866.3s

################################################################################
                    [1m Learning iteration 848/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.833s, learning 0.166s)
               Value function loss: 20.7769
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 228.37
               Mean episode length: 123.87
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13910016
                    Iteration time: 12.00s
                        Total time: 11094.82s
                               ETA: 1295728.2s

################################################################################
                    [1m Learning iteration 849/100000 [0m                     

                       Computation: 1432 steps/s (collection: 11.279s, learning 0.163s)
               Value function loss: 16.3729
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 223.28
               Mean episode length: 122.80
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13926400
                    Iteration time: 11.44s
                        Total time: 11106.26s
                               ETA: 1295525.4s

################################################################################
                    [1m Learning iteration 850/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.865s, learning 0.206s)
               Value function loss: 19.9741
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 227.09
               Mean episode length: 123.08
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13942784
                    Iteration time: 12.07s
                        Total time: 11118.33s
                               ETA: 1295396.4s

################################################################################
                    [1m Learning iteration 851/100000 [0m                     

                       Computation: 1362 steps/s (collection: 11.819s, learning 0.202s)
               Value function loss: 18.7097
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 226.17
               Mean episode length: 123.30
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13959168
                    Iteration time: 12.02s
                        Total time: 11130.35s
                               ETA: 1295261.8s

################################################################################
                    [1m Learning iteration 852/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.697s, learning 0.164s)
               Value function loss: 19.1840
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 222.81
               Mean episode length: 122.13
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13975552
                    Iteration time: 11.86s
                        Total time: 11142.21s
                               ETA: 1295109.0s

################################################################################
                    [1m Learning iteration 853/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.898s, learning 0.169s)
               Value function loss: 13.8847
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 224.51
               Mean episode length: 121.51
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 13991936
                    Iteration time: 12.07s
                        Total time: 11154.28s
                               ETA: 1294980.3s

################################################################################
                    [1m Learning iteration 854/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.415s, learning 0.162s)
               Value function loss: 19.3148
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 225.29
               Mean episode length: 122.99
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14008320
                    Iteration time: 11.58s
                        Total time: 11165.86s
                               ETA: 1294795.2s

################################################################################
                    [1m Learning iteration 855/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.696s, learning 0.169s)
               Value function loss: 15.2051
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 225.20
               Mean episode length: 122.45
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14024704
                    Iteration time: 11.87s
                        Total time: 11177.72s
                               ETA: 1294643.8s

################################################################################
                    [1m Learning iteration 856/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.845s, learning 0.212s)
               Value function loss: 18.6224
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 224.84
               Mean episode length: 123.20
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14041088
                    Iteration time: 12.06s
                        Total time: 11189.78s
                               ETA: 1294515.0s

################################################################################
                    [1m Learning iteration 857/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.736s, learning 0.220s)
               Value function loss: 14.5807
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 223.72
               Mean episode length: 121.61
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14057472
                    Iteration time: 11.96s
                        Total time: 11201.73s
                               ETA: 1294374.7s

################################################################################
                    [1m Learning iteration 858/100000 [0m                     

                       Computation: 1381 steps/s (collection: 11.655s, learning 0.208s)
               Value function loss: 14.1688
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 220.11
               Mean episode length: 119.51
                  Mean reward/step: 1.87
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14073856
                    Iteration time: 11.86s
                        Total time: 11213.60s
                               ETA: 1294224.0s

################################################################################
                    [1m Learning iteration 859/100000 [0m                     

                       Computation: 1354 steps/s (collection: 11.934s, learning 0.159s)
               Value function loss: 31.8494
                    Surrogate loss: 0.0075
             Mean action noise std: 0.78
                       Mean reward: 229.64
               Mean episode length: 122.98
                  Mean reward/step: 1.85
       Mean episode length/episode: 7.20
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14090240
                    Iteration time: 12.09s
                        Total time: 11225.69s
                               ETA: 1294100.1s

################################################################################
                    [1m Learning iteration 860/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.430s, learning 0.195s)
               Value function loss: 11.5230
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 225.13
               Mean episode length: 122.02
                  Mean reward/step: 1.89
       Mean episode length/episode: 7.82
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14106624
                    Iteration time: 11.62s
                        Total time: 11237.31s
                               ETA: 1293922.5s

################################################################################
                    [1m Learning iteration 861/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.704s, learning 0.163s)
               Value function loss: 76.5868
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 237.54
               Mean episode length: 124.24
                  Mean reward/step: 1.86
       Mean episode length/episode: 6.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14123008
                    Iteration time: 11.87s
                        Total time: 11249.18s
                               ETA: 1293773.3s

################################################################################
                    [1m Learning iteration 862/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.601s, learning 0.171s)
               Value function loss: 20.9245
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 232.61
               Mean episode length: 124.77
                  Mean reward/step: 1.88
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14139392
                    Iteration time: 11.77s
                        Total time: 11260.95s
                               ETA: 1293613.5s

################################################################################
                    [1m Learning iteration 863/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.879s, learning 0.157s)
               Value function loss: 24.4026
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 235.51
               Mean episode length: 124.56
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14155776
                    Iteration time: 12.04s
                        Total time: 11272.99s
                               ETA: 1293484.3s

################################################################################
                    [1m Learning iteration 864/100000 [0m                     

                       Computation: 1390 steps/s (collection: 11.524s, learning 0.262s)
               Value function loss: 22.1424
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 233.22
               Mean episode length: 122.84
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14172160
                    Iteration time: 11.79s
                        Total time: 11284.78s
                               ETA: 1293326.8s

################################################################################
                    [1m Learning iteration 865/100000 [0m                     

                       Computation: 1413 steps/s (collection: 11.392s, learning 0.197s)
               Value function loss: 19.3100
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 235.87
               Mean episode length: 124.20
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14188544
                    Iteration time: 11.59s
                        Total time: 11296.37s
                               ETA: 1293146.9s

################################################################################
                    [1m Learning iteration 866/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.825s, learning 0.163s)
               Value function loss: 18.9323
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 231.26
               Mean episode length: 122.64
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14204928
                    Iteration time: 11.99s
                        Total time: 11308.35s
                               ETA: 1293013.0s

################################################################################
                    [1m Learning iteration 867/100000 [0m                     

                       Computation: 1318 steps/s (collection: 12.194s, learning 0.236s)
               Value function loss: 20.9881
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 236.41
               Mean episode length: 123.40
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14221312
                    Iteration time: 12.43s
                        Total time: 11320.78s
                               ETA: 1292929.9s

################################################################################
                    [1m Learning iteration 868/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.003s, learning 0.257s)
               Value function loss: 17.7097
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 236.71
               Mean episode length: 124.38
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14237696
                    Iteration time: 12.26s
                        Total time: 11333.04s
                               ETA: 1292827.6s

################################################################################
                    [1m Learning iteration 869/100000 [0m                     

                       Computation: 1400 steps/s (collection: 11.485s, learning 0.212s)
               Value function loss: 19.2587
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 233.70
               Mean episode length: 121.82
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14254080
                    Iteration time: 11.70s
                        Total time: 11344.74s
                               ETA: 1292661.4s

################################################################################
                    [1m Learning iteration 870/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.705s, learning 0.187s)
               Value function loss: 17.9192
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 227.38
               Mean episode length: 120.59
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14270464
                    Iteration time: 11.89s
                        Total time: 11356.63s
                               ETA: 1292517.7s

################################################################################
                    [1m Learning iteration 871/100000 [0m                     

                       Computation: 1396 steps/s (collection: 11.571s, learning 0.162s)
               Value function loss: 14.2107
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 230.82
               Mean episode length: 122.10
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14286848
                    Iteration time: 11.73s
                        Total time: 11368.37s
                               ETA: 1292356.3s

################################################################################
                    [1m Learning iteration 872/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.740s, learning 0.200s)
               Value function loss: 20.7331
                    Surrogate loss: 0.0046
             Mean action noise std: 0.78
                       Mean reward: 238.57
               Mean episode length: 123.98
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14303232
                    Iteration time: 11.94s
                        Total time: 11380.31s
                               ETA: 1292218.8s

################################################################################
                    [1m Learning iteration 873/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.844s, learning 0.271s)
               Value function loss: 13.4443
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 236.10
               Mean episode length: 124.14
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14319616
                    Iteration time: 12.11s
                        Total time: 11392.42s
                               ETA: 1292101.3s

################################################################################
                    [1m Learning iteration 874/100000 [0m                     

                       Computation: 1377 steps/s (collection: 11.734s, learning 0.163s)
               Value function loss: 51.0451
                    Surrogate loss: -0.0020
             Mean action noise std: 0.78
                       Mean reward: 241.11
               Mean episode length: 125.00
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.18
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14336000
                    Iteration time: 11.90s
                        Total time: 11404.32s
                               ETA: 1291959.3s

################################################################################
                    [1m Learning iteration 875/100000 [0m                     

                       Computation: 1360 steps/s (collection: 11.843s, learning 0.202s)
               Value function loss: 14.4209
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 239.96
               Mean episode length: 123.82
                  Mean reward/step: 1.90
       Mean episode length/episode: 7.78
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14352384
                    Iteration time: 12.04s
                        Total time: 11416.36s
                               ETA: 1291834.3s

################################################################################
                    [1m Learning iteration 876/100000 [0m                     

                       Computation: 1339 steps/s (collection: 11.951s, learning 0.279s)
               Value function loss: 18.7792
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 235.79
               Mean episode length: 123.50
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14368768
                    Iteration time: 12.23s
                        Total time: 11428.59s
                               ETA: 1291730.6s

################################################################################
                    [1m Learning iteration 877/100000 [0m                     

                       Computation: 1393 steps/s (collection: 11.591s, learning 0.166s)
               Value function loss: 60.9500
                    Surrogate loss: 0.0113
             Mean action noise std: 0.78
                       Mean reward: 236.27
               Mean episode length: 124.01
                  Mean reward/step: 1.88
       Mean episode length/episode: 6.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14385152
                    Iteration time: 11.76s
                        Total time: 11440.35s
                               ETA: 1291573.7s

################################################################################
                    [1m Learning iteration 878/100000 [0m                     

                       Computation: 1329 steps/s (collection: 12.114s, learning 0.209s)
               Value function loss: 17.4237
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 236.10
               Mean episode length: 122.49
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14401536
                    Iteration time: 12.32s
                        Total time: 11452.67s
                               ETA: 1291481.0s

################################################################################
                    [1m Learning iteration 879/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.814s, learning 0.163s)
               Value function loss: 25.7364
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 237.96
               Mean episode length: 123.28
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14417920
                    Iteration time: 11.98s
                        Total time: 11464.65s
                               ETA: 1291349.5s

################################################################################
                    [1m Learning iteration 880/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.975s, learning 0.169s)
               Value function loss: 22.0223
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 237.68
               Mean episode length: 123.05
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14434304
                    Iteration time: 12.14s
                        Total time: 11476.79s
                               ETA: 1291237.0s

################################################################################
                    [1m Learning iteration 881/100000 [0m                     

                       Computation: 1394 steps/s (collection: 11.595s, learning 0.157s)
               Value function loss: 23.0640
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 238.84
               Mean episode length: 122.75
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14450688
                    Iteration time: 11.75s
                        Total time: 11488.55s
                               ETA: 1291080.7s

################################################################################
                    [1m Learning iteration 882/100000 [0m                     

                       Computation: 1361 steps/s (collection: 11.768s, learning 0.262s)
               Value function loss: 18.6860
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 239.30
               Mean episode length: 123.83
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14467072
                    Iteration time: 12.03s
                        Total time: 11500.58s
                               ETA: 1290955.9s

################################################################################
                    [1m Learning iteration 883/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.973s, learning 0.165s)
               Value function loss: 22.5063
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 239.39
               Mean episode length: 123.99
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14483456
                    Iteration time: 12.14s
                        Total time: 11512.71s
                               ETA: 1290843.4s

################################################################################
                    [1m Learning iteration 884/100000 [0m                     

                       Computation: 1333 steps/s (collection: 11.992s, learning 0.299s)
               Value function loss: 15.1759
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 235.45
               Mean episode length: 121.69
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14499840
                    Iteration time: 12.29s
                        Total time: 11525.00s
                               ETA: 1290748.3s

################################################################################
                    [1m Learning iteration 885/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.843s, learning 0.161s)
               Value function loss: 21.0079
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 237.54
               Mean episode length: 122.94
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14516224
                    Iteration time: 12.00s
                        Total time: 11537.01s
                               ETA: 1290621.3s

################################################################################
                    [1m Learning iteration 886/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.934s, learning 0.207s)
               Value function loss: 17.1884
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 240.05
               Mean episode length: 122.49
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14532608
                    Iteration time: 12.14s
                        Total time: 11549.15s
                               ETA: 1290509.9s

################################################################################
                    [1m Learning iteration 887/100000 [0m                     

                       Computation: 1406 steps/s (collection: 11.488s, learning 0.162s)
               Value function loss: 18.4184
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 239.86
               Mean episode length: 123.58
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14548992
                    Iteration time: 11.65s
                        Total time: 11560.80s
                               ETA: 1290344.0s

################################################################################
                    [1m Learning iteration 888/100000 [0m                     

                       Computation: 1314 steps/s (collection: 12.291s, learning 0.176s)
               Value function loss: 15.4723
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 235.47
               Mean episode length: 123.39
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14565376
                    Iteration time: 12.47s
                        Total time: 11573.27s
                               ETA: 1290269.5s

################################################################################
                    [1m Learning iteration 889/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.991s, learning 0.163s)
               Value function loss: 13.4642
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 232.66
               Mean episode length: 122.03
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14581760
                    Iteration time: 12.15s
                        Total time: 11585.42s
                               ETA: 1290160.2s

################################################################################
                    [1m Learning iteration 890/100000 [0m                     

                       Computation: 1424 steps/s (collection: 11.342s, learning 0.156s)
               Value function loss: 50.7410
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 243.16
               Mean episode length: 124.85
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.22
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14598144
                    Iteration time: 11.50s
                        Total time: 11596.92s
                               ETA: 1289978.2s

################################################################################
                    [1m Learning iteration 891/100000 [0m                     

                       Computation: 1336 steps/s (collection: 12.103s, learning 0.158s)
               Value function loss: 13.1971
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 236.96
               Mean episode length: 121.91
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.79
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14614528
                    Iteration time: 12.26s
                        Total time: 11609.18s
                               ETA: 1289881.3s

################################################################################
                    [1m Learning iteration 892/100000 [0m                     

                       Computation: 1372 steps/s (collection: 11.736s, learning 0.199s)
               Value function loss: 59.4609
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 242.95
               Mean episode length: 124.96
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14630912
                    Iteration time: 11.93s
                        Total time: 11621.11s
                               ETA: 1289748.4s

################################################################################
                    [1m Learning iteration 893/100000 [0m                     

                       Computation: 1345 steps/s (collection: 12.019s, learning 0.160s)
               Value function loss: 31.1595
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 246.15
               Mean episode length: 124.43
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.26
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14647296
                    Iteration time: 12.18s
                        Total time: 11633.29s
                               ETA: 1289643.0s

################################################################################
                    [1m Learning iteration 894/100000 [0m                     

                       Computation: 1399 steps/s (collection: 11.545s, learning 0.160s)
               Value function loss: 19.9020
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 243.83
               Mean episode length: 124.35
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14663680
                    Iteration time: 11.70s
                        Total time: 11645.00s
                               ETA: 1289485.1s

################################################################################
                    [1m Learning iteration 895/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.947s, learning 0.166s)
               Value function loss: 21.4509
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 238.35
               Mean episode length: 122.70
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14680064
                    Iteration time: 12.11s
                        Total time: 11657.11s
                               ETA: 1289372.7s

################################################################################
                    [1m Learning iteration 896/100000 [0m                     

                       Computation: 1387 steps/s (collection: 11.652s, learning 0.157s)
               Value function loss: 15.5204
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 243.71
               Mean episode length: 124.20
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14696448
                    Iteration time: 11.81s
                        Total time: 11668.92s
                               ETA: 1289227.0s

################################################################################
                    [1m Learning iteration 897/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.756s, learning 0.201s)
               Value function loss: 19.7512
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 245.05
               Mean episode length: 124.02
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14712832
                    Iteration time: 11.96s
                        Total time: 11680.88s
                               ETA: 1289097.9s

################################################################################
                    [1m Learning iteration 898/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.064s, learning 0.163s)
               Value function loss: 19.2925
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 239.43
               Mean episode length: 122.46
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14729216
                    Iteration time: 12.23s
                        Total time: 11693.10s
                               ETA: 1288998.8s

################################################################################
                    [1m Learning iteration 899/100000 [0m                     

                       Computation: 1367 steps/s (collection: 11.817s, learning 0.160s)
               Value function loss: 19.8723
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 246.40
               Mean episode length: 124.04
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14745600
                    Iteration time: 11.98s
                        Total time: 11705.08s
                               ETA: 1288872.5s

################################################################################
                    [1m Learning iteration 900/100000 [0m                     

                       Computation: 1417 steps/s (collection: 11.399s, learning 0.163s)
               Value function loss: 18.1301
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 243.02
               Mean episode length: 122.73
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14761984
                    Iteration time: 11.56s
                        Total time: 11716.64s
                               ETA: 1288700.7s

################################################################################
                    [1m Learning iteration 901/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.916s, learning 0.165s)
               Value function loss: 20.3283
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 243.09
               Mean episode length: 122.76
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14778368
                    Iteration time: 12.08s
                        Total time: 11728.72s
                               ETA: 1288586.2s

################################################################################
                    [1m Learning iteration 902/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.915s, learning 0.165s)
               Value function loss: 16.2428
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 241.01
               Mean episode length: 121.99
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14794752
                    Iteration time: 12.08s
                        Total time: 11740.80s
                               ETA: 1288471.8s

################################################################################
                    [1m Learning iteration 903/100000 [0m                     

                       Computation: 1408 steps/s (collection: 11.462s, learning 0.167s)
               Value function loss: 21.7682
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 241.43
               Mean episode length: 122.76
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14811136
                    Iteration time: 11.63s
                        Total time: 11752.43s
                               ETA: 1288308.3s

################################################################################
                    [1m Learning iteration 904/100000 [0m                     

                       Computation: 1404 steps/s (collection: 11.498s, learning 0.169s)
               Value function loss: 17.4638
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 242.04
               Mean episode length: 121.94
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14827520
                    Iteration time: 11.67s
                        Total time: 11764.10s
                               ETA: 1288149.3s

################################################################################
                    [1m Learning iteration 905/100000 [0m                     

                       Computation: 1409 steps/s (collection: 11.453s, learning 0.168s)
               Value function loss: 17.8524
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 242.74
               Mean episode length: 122.20
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14843904
                    Iteration time: 11.62s
                        Total time: 11775.72s
                               ETA: 1287985.6s

################################################################################
                    [1m Learning iteration 906/100000 [0m                     

                       Computation: 1349 steps/s (collection: 11.970s, learning 0.168s)
               Value function loss: 27.6748
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 244.59
               Mean episode length: 122.49
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14860288
                    Iteration time: 12.14s
                        Total time: 11787.86s
                               ETA: 1287878.7s

################################################################################
                    [1m Learning iteration 907/100000 [0m                     

                       Computation: 1407 steps/s (collection: 11.453s, learning 0.186s)
               Value function loss: 20.0405
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 245.67
               Mean episode length: 123.31
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14876672
                    Iteration time: 11.64s
                        Total time: 11799.50s
                               ETA: 1287717.6s

################################################################################
                    [1m Learning iteration 908/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.676s, learning 0.162s)
               Value function loss: 89.9682
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 245.74
               Mean episode length: 122.94
                  Mean reward/step: 1.97
       Mean episode length/episode: 6.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14893056
                    Iteration time: 11.84s
                        Total time: 11811.34s
                               ETA: 1287578.5s

################################################################################
                    [1m Learning iteration 909/100000 [0m                     

                       Computation: 1346 steps/s (collection: 12.006s, learning 0.162s)
               Value function loss: 23.9314
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: 243.17
               Mean episode length: 122.23
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14909440
                    Iteration time: 12.17s
                        Total time: 11823.50s
                               ETA: 1287475.5s

################################################################################
                    [1m Learning iteration 910/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.922s, learning 0.196s)
               Value function loss: 24.8007
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 250.22
               Mean episode length: 124.80
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14925824
                    Iteration time: 12.12s
                        Total time: 11835.62s
                               ETA: 1287367.4s

################################################################################
                    [1m Learning iteration 911/100000 [0m                     

                       Computation: 1386 steps/s (collection: 11.647s, learning 0.169s)
               Value function loss: 25.9226
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 243.72
               Mean episode length: 122.58
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14942208
                    Iteration time: 11.82s
                        Total time: 11847.44s
                               ETA: 1287226.7s

################################################################################
                    [1m Learning iteration 912/100000 [0m                     

                       Computation: 1421 steps/s (collection: 11.373s, learning 0.156s)
               Value function loss: 21.7709
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 249.95
               Mean episode length: 124.21
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14958592
                    Iteration time: 11.53s
                        Total time: 11858.97s
                               ETA: 1287055.0s

################################################################################
                    [1m Learning iteration 913/100000 [0m                     

                       Computation: 1347 steps/s (collection: 11.944s, learning 0.218s)
               Value function loss: 19.7573
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 244.44
               Mean episode length: 121.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14974976
                    Iteration time: 12.16s
                        Total time: 11871.13s
                               ETA: 1286952.4s

################################################################################
                    [1m Learning iteration 914/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.910s, learning 0.202s)
               Value function loss: 27.9469
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 250.20
               Mean episode length: 123.36
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 14991360
                    Iteration time: 12.11s
                        Total time: 11883.24s
                               ETA: 1286844.5s

################################################################################
                    [1m Learning iteration 915/100000 [0m                     

                       Computation: 1328 steps/s (collection: 12.130s, learning 0.198s)
               Value function loss: 23.7387
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 247.09
               Mean episode length: 123.02
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15007744
                    Iteration time: 12.33s
                        Total time: 11895.57s
                               ETA: 1286760.3s

################################################################################
                    [1m Learning iteration 916/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.450s, learning 0.157s)
               Value function loss: 25.9712
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 249.10
               Mean episode length: 123.48
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15024128
                    Iteration time: 11.61s
                        Total time: 11907.18s
                               ETA: 1286598.3s

################################################################################
                    [1m Learning iteration 917/100000 [0m                     

                       Computation: 1374 steps/s (collection: 11.751s, learning 0.165s)
               Value function loss: 24.2897
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 247.31
               Mean episode length: 122.45
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15040512
                    Iteration time: 11.92s
                        Total time: 11919.09s
                               ETA: 1286470.0s

################################################################################
                    [1m Learning iteration 918/100000 [0m                     

                       Computation: 1375 steps/s (collection: 11.740s, learning 0.168s)
               Value function loss: 22.9743
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 246.67
               Mean episode length: 122.21
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15056896
                    Iteration time: 11.91s
                        Total time: 11931.00s
                               ETA: 1286340.9s

################################################################################
                    [1m Learning iteration 919/100000 [0m                     

                       Computation: 1334 steps/s (collection: 12.113s, learning 0.162s)
               Value function loss: 23.9033
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 241.52
               Mean episode length: 120.33
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15073280
                    Iteration time: 12.28s
                        Total time: 11943.27s
                               ETA: 1286251.7s

################################################################################
                    [1m Learning iteration 920/100000 [0m                     

                       Computation: 1383 steps/s (collection: 11.683s, learning 0.156s)
               Value function loss: 19.1942
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 247.57
               Mean episode length: 122.69
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15089664
                    Iteration time: 11.84s
                        Total time: 11955.11s
                               ETA: 1286115.9s

################################################################################
                    [1m Learning iteration 921/100000 [0m                     

                       Computation: 1403 steps/s (collection: 11.487s, learning 0.191s)
               Value function loss: 61.8645
                    Surrogate loss: 0.0049
             Mean action noise std: 0.78
                       Mean reward: 252.31
               Mean episode length: 124.54
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15106048
                    Iteration time: 11.68s
                        Total time: 11966.79s
                               ETA: 1285962.8s

################################################################################
                    [1m Learning iteration 922/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.767s, learning 0.203s)
               Value function loss: 18.3141
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 246.95
               Mean episode length: 123.21
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15122432
                    Iteration time: 11.97s
                        Total time: 11978.76s
                               ETA: 1285841.5s

################################################################################
                    [1m Learning iteration 923/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.943s, learning 0.161s)
               Value function loss: 40.8786
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: 254.06
               Mean episode length: 124.18
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15138816
                    Iteration time: 12.10s
                        Total time: 11990.87s
                               ETA: 1285734.8s

################################################################################
                    [1m Learning iteration 924/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.834s, learning 0.283s)
               Value function loss: 70.3229
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 254.92
               Mean episode length: 124.77
                  Mean reward/step: 1.97
       Mean episode length/episode: 6.83
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15155200
                    Iteration time: 12.12s
                        Total time: 12002.98s
                               ETA: 1285629.7s

################################################################################
                    [1m Learning iteration 925/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.796s, learning 0.169s)
               Value function loss: 29.7126
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 254.14
               Mean episode length: 124.94
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15171584
                    Iteration time: 11.96s
                        Total time: 12014.95s
                               ETA: 1285508.5s

################################################################################
                    [1m Learning iteration 926/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.799s, learning 0.170s)
               Value function loss: 34.2924
                    Surrogate loss: -0.0024
             Mean action noise std: 0.78
                       Mean reward: 250.52
               Mean episode length: 123.95
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15187968
                    Iteration time: 11.97s
                        Total time: 12026.92s
                               ETA: 1285388.1s

################################################################################
                    [1m Learning iteration 927/100000 [0m                     

                       Computation: 1419 steps/s (collection: 11.378s, learning 0.160s)
               Value function loss: 29.0940
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 252.72
               Mean episode length: 123.65
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15204352
                    Iteration time: 11.54s
                        Total time: 12038.46s
                               ETA: 1285221.8s

################################################################################
                    [1m Learning iteration 928/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.889s, learning 0.187s)
               Value function loss: 33.1732
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 251.08
               Mean episode length: 123.62
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15220736
                    Iteration time: 12.08s
                        Total time: 12050.53s
                               ETA: 1285113.3s

################################################################################
                    [1m Learning iteration 929/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.790s, learning 0.183s)
               Value function loss: 27.1942
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 249.75
               Mean episode length: 123.27
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15237120
                    Iteration time: 11.97s
                        Total time: 12062.50s
                               ETA: 1284993.9s

################################################################################
                    [1m Learning iteration 930/100000 [0m                     

                       Computation: 1352 steps/s (collection: 11.905s, learning 0.208s)
               Value function loss: 32.0158
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 254.18
               Mean episode length: 124.90
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15253504
                    Iteration time: 12.11s
                        Total time: 12074.62s
                               ETA: 1284889.7s

################################################################################
                    [1m Learning iteration 931/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.765s, learning 0.227s)
               Value function loss: 24.4595
                    Surrogate loss: -0.0024
             Mean action noise std: 0.78
                       Mean reward: 253.36
               Mean episode length: 124.48
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15269888
                    Iteration time: 11.99s
                        Total time: 12086.61s
                               ETA: 1284772.9s

################################################################################
                    [1m Learning iteration 932/100000 [0m                     

                       Computation: 1363 steps/s (collection: 11.804s, learning 0.209s)
               Value function loss: 31.8330
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 251.43
               Mean episode length: 123.36
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15286272
                    Iteration time: 12.01s
                        Total time: 12098.62s
                               ETA: 1284658.5s

################################################################################
                    [1m Learning iteration 933/100000 [0m                     

                       Computation: 1411 steps/s (collection: 11.437s, learning 0.169s)
               Value function loss: 26.2366
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 246.51
               Mean episode length: 122.61
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15302656
                    Iteration time: 11.61s
                        Total time: 12110.23s
                               ETA: 1284501.1s

################################################################################
                    [1m Learning iteration 934/100000 [0m                     

                       Computation: 1348 steps/s (collection: 11.976s, learning 0.173s)
               Value function loss: 32.8575
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 252.43
               Mean episode length: 124.76
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15319040
                    Iteration time: 12.15s
                        Total time: 12122.38s
                               ETA: 1284401.6s

################################################################################
                    [1m Learning iteration 935/100000 [0m                     

                       Computation: 906 steps/s (collection: 17.833s, learning 0.240s)
               Value function loss: 26.1504
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 249.81
               Mean episode length: 123.99
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15335424
                    Iteration time: 18.07s
                        Total time: 12140.45s
                               ETA: 1284929.3s

################################################################################
                    [1m Learning iteration 936/100000 [0m                     

                       Computation: 701 steps/s (collection: 23.151s, learning 0.194s)
               Value function loss: 26.3601
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 248.99
               Mean episode length: 123.79
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15351808
                    Iteration time: 23.34s
                        Total time: 12163.80s
                               ETA: 1286013.1s

################################################################################
                    [1m Learning iteration 937/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.359s, learning 0.231s)
               Value function loss: 58.1848
                    Surrogate loss: 0.0037
             Mean action noise std: 0.78
                       Mean reward: 249.60
               Mean episode length: 123.81
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15368192
                    Iteration time: 23.59s
                        Total time: 12187.39s
                               ETA: 1287120.6s

################################################################################
                    [1m Learning iteration 938/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.259s, learning 0.246s)
               Value function loss: 22.6337
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 245.95
               Mean episode length: 123.19
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.77
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15384576
                    Iteration time: 23.50s
                        Total time: 12210.89s
                               ETA: 1288216.5s

################################################################################
                    [1m Learning iteration 939/100000 [0m                     

                       Computation: 691 steps/s (collection: 23.494s, learning 0.217s)
               Value function loss: 133.1587
                    Surrogate loss: 0.0202
             Mean action noise std: 0.78
                       Mean reward: 254.69
               Mean episode length: 124.81
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15400960
                    Iteration time: 23.71s
                        Total time: 12234.60s
                               ETA: 1289331.8s

################################################################################
                    [1m Learning iteration 940/100000 [0m                     

                       Computation: 700 steps/s (collection: 23.151s, learning 0.233s)
               Value function loss: 36.1369
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 252.77
               Mean episode length: 124.93
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15417344
                    Iteration time: 23.38s
                        Total time: 12257.99s
                               ETA: 1290410.3s

################################################################################
                    [1m Learning iteration 941/100000 [0m                     

                       Computation: 688 steps/s (collection: 23.617s, learning 0.190s)
               Value function loss: 36.2185
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 247.20
               Mean episode length: 123.25
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15433728
                    Iteration time: 23.81s
                        Total time: 12281.79s
                               ETA: 1291531.0s

################################################################################
                    [1m Learning iteration 942/100000 [0m                     

                       Computation: 705 steps/s (collection: 23.026s, learning 0.207s)
               Value function loss: 35.6303
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 249.63
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15450112
                    Iteration time: 23.23s
                        Total time: 12305.03s
                               ETA: 1292588.9s

################################################################################
                    [1m Learning iteration 943/100000 [0m                     

                       Computation: 710 steps/s (collection: 22.870s, learning 0.186s)
               Value function loss: 28.5431
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 248.49
               Mean episode length: 123.17
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15466496
                    Iteration time: 23.06s
                        Total time: 12328.08s
                               ETA: 1293625.9s

################################################################################
                    [1m Learning iteration 944/100000 [0m                     

                       Computation: 703 steps/s (collection: 23.118s, learning 0.161s)
               Value function loss: 34.6871
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 241.50
               Mean episode length: 122.24
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15482880
                    Iteration time: 23.28s
                        Total time: 12351.36s
                               ETA: 1294684.0s

################################################################################
                    [1m Learning iteration 945/100000 [0m                     

                       Computation: 709 steps/s (collection: 22.841s, learning 0.238s)
               Value function loss: 36.5010
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 250.39
               Mean episode length: 124.78
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15499264
                    Iteration time: 23.08s
                        Total time: 12374.44s
                               ETA: 1295719.0s

################################################################################
                    [1m Learning iteration 946/100000 [0m                     

                       Computation: 699 steps/s (collection: 23.250s, learning 0.186s)
               Value function loss: 28.4663
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 249.18
               Mean episode length: 124.13
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15515648
                    Iteration time: 23.44s
                        Total time: 12397.88s
                               ETA: 1296789.0s

################################################################################
                    [1m Learning iteration 947/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.499s, learning 0.220s)
               Value function loss: 28.9316
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 246.91
               Mean episode length: 123.63
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15532032
                    Iteration time: 23.72s
                        Total time: 12421.60s
                               ETA: 1297886.3s

################################################################################
                    [1m Learning iteration 948/100000 [0m                     

                       Computation: 689 steps/s (collection: 23.499s, learning 0.248s)
               Value function loss: 36.6663
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 249.10
               Mean episode length: 124.98
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15548416
                    Iteration time: 23.75s
                        Total time: 12445.34s
                               ETA: 1298984.2s

################################################################################
                    [1m Learning iteration 949/100000 [0m                     

                       Computation: 705 steps/s (collection: 23.013s, learning 0.215s)
               Value function loss: 26.5553
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 249.21
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15564800
                    Iteration time: 23.23s
                        Total time: 12468.57s
                               ETA: 1300025.5s

################################################################################
                    [1m Learning iteration 950/100000 [0m                     

                       Computation: 688 steps/s (collection: 23.560s, learning 0.238s)
               Value function loss: 36.3190
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 248.31
               Mean episode length: 123.89
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15581184
                    Iteration time: 23.80s
                        Total time: 12492.37s
                               ETA: 1301124.0s

################################################################################
                    [1m Learning iteration 951/100000 [0m                     

                       Computation: 705 steps/s (collection: 23.031s, learning 0.202s)
               Value function loss: 26.2462
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 249.17
               Mean episode length: 124.63
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15597568
                    Iteration time: 23.23s
                        Total time: 12515.60s
                               ETA: 1302161.5s

################################################################################
                    [1m Learning iteration 952/100000 [0m                     

                       Computation: 706 steps/s (collection: 23.039s, learning 0.163s)
               Value function loss: 86.5787
                    Surrogate loss: 0.0248
             Mean action noise std: 0.78
                       Mean reward: 249.15
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15613952
                    Iteration time: 23.20s
                        Total time: 12538.80s
                               ETA: 1303193.4s

################################################################################
                    [1m Learning iteration 953/100000 [0m                     

                       Computation: 694 steps/s (collection: 23.420s, learning 0.179s)
               Value function loss: 22.8566
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 246.14
               Mean episode length: 123.34
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15630336
                    Iteration time: 23.60s
                        Total time: 12562.40s
                               ETA: 1304264.3s

################################################################################
                    [1m Learning iteration 954/100000 [0m                     

                       Computation: 695 steps/s (collection: 23.347s, learning 0.218s)
               Value function loss: 29.4002
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 251.04
               Mean episode length: 123.96
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15646720
                    Iteration time: 23.57s
                        Total time: 12585.97s
                               ETA: 1305329.4s

################################################################################
                    [1m Learning iteration 955/100000 [0m                     

                       Computation: 707 steps/s (collection: 22.979s, learning 0.175s)
               Value function loss: 108.2732
                    Surrogate loss: 0.0189
             Mean action noise std: 0.78
                       Mean reward: 251.62
               Mean episode length: 124.82
                  Mean reward/step: 1.93
       Mean episode length/episode: 6.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15663104
                    Iteration time: 23.15s
                        Total time: 12609.12s
                               ETA: 1306349.7s

################################################################################
                    [1m Learning iteration 956/100000 [0m                     

                       Computation: 701 steps/s (collection: 23.118s, learning 0.231s)
               Value function loss: 27.6225
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 247.42
               Mean episode length: 124.12
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15679488
                    Iteration time: 23.35s
                        Total time: 12632.47s
                               ETA: 1307388.1s

################################################################################
                    [1m Learning iteration 957/100000 [0m                     

                       Computation: 700 steps/s (collection: 23.203s, learning 0.194s)
               Value function loss: 36.0946
                    Surrogate loss: -0.0042
             Mean action noise std: 0.78
                       Mean reward: 248.71
               Mean episode length: 124.59
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15695872
                    Iteration time: 23.40s
                        Total time: 12655.87s
                               ETA: 1308429.0s

################################################################################
                    [1m Learning iteration 958/100000 [0m                     

                       Computation: 699 steps/s (collection: 23.276s, learning 0.160s)
               Value function loss: 34.9736
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 246.75
               Mean episode length: 124.81
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15712256
                    Iteration time: 23.44s
                        Total time: 12679.30s
                               ETA: 1309471.9s

################################################################################
                    [1m Learning iteration 959/100000 [0m                     

                       Computation: 696 steps/s (collection: 23.351s, learning 0.165s)
               Value function loss: 35.7053
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 243.73
               Mean episode length: 122.14
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15728640
                    Iteration time: 23.52s
                        Total time: 12702.82s
                               ETA: 1310520.7s

################################################################################
                    [1m Learning iteration 960/100000 [0m                     

                       Computation: 701 steps/s (collection: 23.177s, learning 0.177s)
               Value function loss: 27.7812
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 242.17
               Mean episode length: 123.03
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15745024
                    Iteration time: 23.35s
                        Total time: 12726.17s
                               ETA: 1311550.6s

################################################################################
                    [1m Learning iteration 961/100000 [0m                     

                       Computation: 690 steps/s (collection: 23.543s, learning 0.201s)
               Value function loss: 36.7956
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 245.92
               Mean episode length: 123.77
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15761408
                    Iteration time: 23.74s
                        Total time: 12749.92s
                               ETA: 1312618.4s

################################################################################
                    [1m Learning iteration 962/100000 [0m                     

                       Computation: 693 steps/s (collection: 23.411s, learning 0.204s)
               Value function loss: 26.3046
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 241.40
               Mean episode length: 123.71
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15777792
                    Iteration time: 23.61s
                        Total time: 12773.53s
                               ETA: 1313670.7s

################################################################################
                    [1m Learning iteration 963/100000 [0m                     

                       Computation: 711 steps/s (collection: 22.847s, learning 0.192s)
               Value function loss: 30.5561
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 241.88
               Mean episode length: 123.11
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15794176
                    Iteration time: 23.04s
                        Total time: 12796.57s
                               ETA: 1314661.6s

################################################################################
                    [1m Learning iteration 964/100000 [0m                     

                       Computation: 689 steps/s (collection: 23.434s, learning 0.329s)
               Value function loss: 27.0079
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 243.31
               Mean episode length: 124.15
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15810560
                    Iteration time: 23.76s
                        Total time: 12820.33s
                               ETA: 1315724.8s

################################################################################
                    [1m Learning iteration 965/100000 [0m                     

                       Computation: 683 steps/s (collection: 23.711s, learning 0.272s)
               Value function loss: 28.6398
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 246.63
               Mean episode length: 124.65
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15826944
                    Iteration time: 23.98s
                        Total time: 12844.32s
                               ETA: 1316808.2s

################################################################################
                    [1m Learning iteration 966/100000 [0m                     

                       Computation: 693 steps/s (collection: 23.293s, learning 0.333s)
               Value function loss: 27.1212
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 243.36
               Mean episode length: 123.48
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15843328
                    Iteration time: 23.63s
                        Total time: 12867.94s
                               ETA: 1317852.8s

################################################################################
                    [1m Learning iteration 967/100000 [0m                     

                       Computation: 686 steps/s (collection: 23.598s, learning 0.266s)
               Value function loss: 23.8454
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 243.25
               Mean episode length: 123.86
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15859712
                    Iteration time: 23.86s
                        Total time: 12891.80s
                               ETA: 1318919.5s

################################################################################
                    [1m Learning iteration 968/100000 [0m                     

                       Computation: 680 steps/s (collection: 23.911s, learning 0.169s)
               Value function loss: 64.2780
                    Surrogate loss: 0.0036
             Mean action noise std: 0.78
                       Mean reward: 245.18
               Mean episode length: 124.63
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.28
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15876096
                    Iteration time: 24.08s
                        Total time: 12915.88s
                               ETA: 1320006.1s

################################################################################
                    [1m Learning iteration 969/100000 [0m                     

                       Computation: 718 steps/s (collection: 22.640s, learning 0.167s)
               Value function loss: 20.8810
                    Surrogate loss: -0.0059
             Mean action noise std: 0.78
                       Mean reward: 238.96
               Mean episode length: 122.84
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15892480
                    Iteration time: 22.81s
                        Total time: 12938.69s
                               ETA: 1320960.4s

################################################################################
                    [1m Learning iteration 970/100000 [0m                     

                       Computation: 703 steps/s (collection: 23.094s, learning 0.183s)
               Value function loss: 62.7838
                    Surrogate loss: 0.0027
             Mean action noise std: 0.78
                       Mean reward: 244.63
               Mean episode length: 124.40
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15908864
                    Iteration time: 23.28s
                        Total time: 12961.97s
                               ETA: 1321960.6s

################################################################################
                    [1m Learning iteration 971/100000 [0m                     

                       Computation: 704 steps/s (collection: 23.038s, learning 0.216s)
               Value function loss: 53.9723
                    Surrogate loss: 0.0041
             Mean action noise std: 0.78
                       Mean reward: 243.08
               Mean episode length: 124.30
                  Mean reward/step: 1.91
       Mean episode length/episode: 7.03
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15925248
                    Iteration time: 23.25s
                        Total time: 12985.22s
                               ETA: 1322956.4s

################################################################################
                    [1m Learning iteration 972/100000 [0m                     

                       Computation: 697 steps/s (collection: 23.337s, learning 0.161s)
               Value function loss: 28.3097
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 243.84
               Mean episode length: 123.86
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15941632
                    Iteration time: 23.50s
                        Total time: 13008.72s
                               ETA: 1323975.0s

################################################################################
                    [1m Learning iteration 973/100000 [0m                     

                       Computation: 1250 steps/s (collection: 12.940s, learning 0.160s)
               Value function loss: 32.7428
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 242.30
               Mean episode length: 124.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15958016
                    Iteration time: 13.10s
                        Total time: 13021.82s
                               ETA: 1323934.2s

################################################################################
                    [1m Learning iteration 974/100000 [0m                     

                       Computation: 1378 steps/s (collection: 11.721s, learning 0.164s)
               Value function loss: 25.6579
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 238.54
               Mean episode length: 122.24
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15974400
                    Iteration time: 11.88s
                        Total time: 13033.71s
                               ETA: 1323770.1s

################################################################################
                    [1m Learning iteration 975/100000 [0m                     

                       Computation: 1338 steps/s (collection: 12.002s, learning 0.241s)
               Value function loss: 32.4407
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 245.01
               Mean episode length: 124.53
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 15990784
                    Iteration time: 12.24s
                        Total time: 13045.95s
                               ETA: 1323642.5s

################################################################################
                    [1m Learning iteration 976/100000 [0m                     

                       Computation: 1364 steps/s (collection: 11.833s, learning 0.176s)
               Value function loss: 29.2016
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 247.30
               Mean episode length: 124.90
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16007168
                    Iteration time: 12.01s
                        Total time: 13057.96s
                               ETA: 1323491.6s

################################################################################
                    [1m Learning iteration 977/100000 [0m                     

                       Computation: 1346 steps/s (collection: 12.001s, learning 0.163s)
               Value function loss: 26.9822
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 241.05
               Mean episode length: 123.15
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16023552
                    Iteration time: 12.16s
                        Total time: 13070.12s
                               ETA: 1323356.5s

################################################################################
                    [1m Learning iteration 978/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.536s, learning 0.179s)
               Value function loss: 24.2453
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 239.97
               Mean episode length: 123.02
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16039936
                    Iteration time: 11.72s
                        Total time: 13081.84s
                               ETA: 1323176.4s

################################################################################
                    [1m Learning iteration 979/100000 [0m                     

                       Computation: 1366 steps/s (collection: 11.821s, learning 0.167s)
               Value function loss: 27.0430
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 240.70
               Mean episode length: 123.74
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16056320
                    Iteration time: 11.99s
                        Total time: 13093.82s
                               ETA: 1323024.1s

################################################################################
                    [1m Learning iteration 980/100000 [0m                     

                       Computation: 1339 steps/s (collection: 12.071s, learning 0.162s)
               Value function loss: 23.7000
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 238.61
               Mean episode length: 122.06
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16072704
                    Iteration time: 12.23s
                        Total time: 13106.06s
                               ETA: 1322896.9s

################################################################################
                    [1m Learning iteration 981/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.787s, learning 0.164s)
               Value function loss: 27.4550
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 246.63
               Mean episode length: 124.80
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16089088
                    Iteration time: 11.95s
                        Total time: 13118.01s
                               ETA: 1322741.6s

################################################################################
                    [1m Learning iteration 982/100000 [0m                     

                       Computation: 1379 steps/s (collection: 11.690s, learning 0.188s)
               Value function loss: 21.2522
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 240.44
               Mean episode length: 123.34
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16105472
                    Iteration time: 11.88s
                        Total time: 13129.89s
                               ETA: 1322579.0s

################################################################################
                    [1m Learning iteration 983/100000 [0m                     

                       Computation: 1382 steps/s (collection: 11.655s, learning 0.193s)
               Value function loss: 22.5640
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 245.39
               Mean episode length: 124.69
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16121856
                    Iteration time: 11.85s
                        Total time: 13141.73s
                               ETA: 1322413.8s

################################################################################
                    [1m Learning iteration 984/100000 [0m                     

                       Computation: 1353 steps/s (collection: 11.875s, learning 0.233s)
               Value function loss: 39.7976
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 242.04
               Mean episode length: 123.54
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.31
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16138240
                    Iteration time: 12.11s
                        Total time: 13153.84s
                               ETA: 1322275.0s

################################################################################
                    [1m Learning iteration 985/100000 [0m                     

                       Computation: 1369 steps/s (collection: 11.740s, learning 0.226s)
               Value function loss: 21.7973
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 240.91
               Mean episode length: 123.32
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.76
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16154624
                    Iteration time: 11.97s
                        Total time: 13165.81s
                               ETA: 1322122.2s

################################################################################
                    [1m Learning iteration 986/100000 [0m                     

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.160s)
               Value function loss: 100.9857
                    Surrogate loss: 0.0208
             Mean action noise std: 0.78
                       Mean reward: 244.88
               Mean episode length: 125.00
                  Mean reward/step: 1.93
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16171008
                    Iteration time: 11.57s
                        Total time: 13177.38s
                               ETA: 1321930.1s

################################################################################
                    [1m Learning iteration 987/100000 [0m                     

                       Computation: 1370 steps/s (collection: 11.734s, learning 0.225s)
               Value function loss: 26.3013
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 243.57
               Mean episode length: 123.83
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16187392
                    Iteration time: 11.96s
                        Total time: 13189.34s
                               ETA: 1321777.2s

################################################################################
                    [1m Learning iteration 988/100000 [0m                     

                       Computation: 1357 steps/s (collection: 11.877s, learning 0.191s)
               Value function loss: 32.0206
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 246.30
               Mean episode length: 124.50
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16203776
                    Iteration time: 12.07s
                        Total time: 13201.41s
                               ETA: 1321635.7s

################################################################################
                    [1m Learning iteration 989/100000 [0m                     

                       Computation: 1389 steps/s (collection: 11.566s, learning 0.224s)
               Value function loss: 33.4399
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 239.97
               Mean episode length: 122.62
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16220160
                    Iteration time: 11.79s
                        Total time: 13213.20s
                               ETA: 1321466.5s

################################################################################
                    [1m Learning iteration 990/100000 [0m                     

                       Computation: 1371 steps/s (collection: 11.763s, learning 0.181s)
               Value function loss: 27.3688
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 243.94
               Mean episode length: 124.04
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16236544
                    Iteration time: 11.94s
                        Total time: 13225.14s
                               ETA: 1321313.0s

################################################################################
                    [1m Learning iteration 991/100000 [0m                     

                       Computation: 1398 steps/s (collection: 11.450s, learning 0.262s)
               Value function loss: 28.6166
                    Surrogate loss: -0.0001
             Mean action noise std: 0.78
                       Mean reward: 244.88
               Mean episode length: 124.17
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16252928
                    Iteration time: 11.71s
                        Total time: 13236.85s
                               ETA: 1321136.7s

################################################################################
                    [1m Learning iteration 992/100000 [0m                     

                       Computation: 1310 steps/s (collection: 12.332s, learning 0.173s)
               Value function loss: 29.7498
                    Surrogate loss: 0.0019
             Mean action noise std: 0.78
                       Mean reward: 246.81
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16269312
                    Iteration time: 12.51s
                        Total time: 13249.36s
                               ETA: 1321039.8s

################################################################################
                    [1m Learning iteration 993/100000 [0m                     

                       Computation: 1368 steps/s (collection: 11.818s, learning 0.158s)
               Value function loss: 27.3781
                    Surrogate loss: -0.0026
             Mean action noise std: 0.78
                       Mean reward: 249.01
               Mean episode length: 124.53
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16285696
                    Iteration time: 11.98s
                        Total time: 13261.33s
                               ETA: 1320890.2s

################################################################################
                    [1m Learning iteration 994/100000 [0m                     

                       Computation: 1358 steps/s (collection: 11.889s, learning 0.174s)
               Value function loss: 29.7105
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 246.86
               Mean episode length: 124.27
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16302080
                    Iteration time: 12.06s
                        Total time: 13273.40s
                               ETA: 1320749.7s

################################################################################
                    [1m Learning iteration 995/100000 [0m                     

                       Computation: 1365 steps/s (collection: 11.734s, learning 0.262s)
               Value function loss: 30.3049
                    Surrogate loss: -0.0022
             Mean action noise std: 0.78
                       Mean reward: 246.92
               Mean episode length: 123.94
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16318464
                    Iteration time: 12.00s
                        Total time: 13285.39s
                               ETA: 1320602.7s

################################################################################
                    [1m Learning iteration 996/100000 [0m                     

                       Computation: 1335 steps/s (collection: 12.059s, learning 0.209s)
               Value function loss: 27.0654
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 244.56
               Mean episode length: 123.76
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16334848
                    Iteration time: 12.27s
                        Total time: 13297.66s
                               ETA: 1320483.0s

################################################################################
                    [1m Learning iteration 997/100000 [0m                     

                       Computation: 1356 steps/s (collection: 11.923s, learning 0.160s)
               Value function loss: 31.6747
                    Surrogate loss: 0.0040
             Mean action noise std: 0.78
                       Mean reward: 246.96
               Mean episode length: 124.62
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16351232
                    Iteration time: 12.08s
                        Total time: 13309.74s
                               ETA: 1320345.2s

################################################################################
                    [1m Learning iteration 998/100000 [0m                     

                       Computation: 1391 steps/s (collection: 11.613s, learning 0.161s)
               Value function loss: 24.4641
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 244.98
               Mean episode length: 123.58
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16367616
                    Iteration time: 11.77s
                        Total time: 13321.52s
                               ETA: 1320177.0s

################################################################################
                    [1m Learning iteration 999/100000 [0m                     

                       Computation: 1380 steps/s (collection: 11.627s, learning 0.245s)
               Value function loss: 66.0611
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 247.34
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16384000
                    Iteration time: 11.87s
                        Total time: 13333.39s
                               ETA: 1320018.9s

################################################################################
                    [1m Learning iteration 1000/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.330s, learning 0.160s)
               Value function loss: 29.1533
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 249.33
               Mean episode length: 124.83
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16400384
                    Iteration time: 11.49s
                        Total time: 13344.88s
                               ETA: 1319823.3s

################################################################################
                    [1m Learning iteration 1001/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.535s, learning 0.201s)
               Value function loss: 36.0334
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 243.59
               Mean episode length: 122.76
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16416768
                    Iteration time: 11.74s
                        Total time: 13356.62s
                               ETA: 1319652.3s

################################################################################
                    [1m Learning iteration 1002/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.591s, learning 0.215s)
               Value function loss: 104.6289
                    Surrogate loss: 0.0171
             Mean action noise std: 0.78
                       Mean reward: 250.07
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 6.81
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16433152
                    Iteration time: 11.81s
                        Total time: 13368.42s
                               ETA: 1319488.6s

################################################################################
                    [1m Learning iteration 1003/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.834s, learning 0.205s)
               Value function loss: 29.2135
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 247.97
               Mean episode length: 123.99
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16449536
                    Iteration time: 12.04s
                        Total time: 13380.46s
                               ETA: 1319348.1s

################################################################################
                    [1m Learning iteration 1004/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.773s, learning 0.171s)
               Value function loss: 37.7537
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 246.36
               Mean episode length: 124.32
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16465920
                    Iteration time: 11.94s
                        Total time: 13392.40s
                               ETA: 1319198.4s

################################################################################
                    [1m Learning iteration 1005/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.254s, learning 0.163s)
               Value function loss: 39.5444
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 247.89
               Mean episode length: 124.19
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16482304
                    Iteration time: 11.42s
                        Total time: 13403.82s
                               ETA: 1318997.3s

################################################################################
                    [1m Learning iteration 1006/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.538s, learning 0.204s)
               Value function loss: 34.4154
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 243.14
               Mean episode length: 121.71
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16498688
                    Iteration time: 11.74s
                        Total time: 13415.56s
                               ETA: 1318828.4s

################################################################################
                    [1m Learning iteration 1007/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.719s, learning 0.205s)
               Value function loss: 28.6791
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 244.07
               Mean episode length: 122.36
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16515072
                    Iteration time: 11.92s
                        Total time: 13427.49s
                               ETA: 1318677.7s

################################################################################
                    [1m Learning iteration 1008/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.637s, learning 0.164s)
               Value function loss: 33.4237
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 245.76
               Mean episode length: 122.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16531456
                    Iteration time: 11.80s
                        Total time: 13439.29s
                               ETA: 1318515.3s

################################################################################
                    [1m Learning iteration 1009/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.703s, learning 0.207s)
               Value function loss: 25.1561
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 246.90
               Mean episode length: 123.21
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16547840
                    Iteration time: 11.91s
                        Total time: 13451.20s
                               ETA: 1318363.9s

################################################################################
                    [1m Learning iteration 1010/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.072s, learning 0.165s)
               Value function loss: 33.3185
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 243.42
               Mean episode length: 122.70
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16564224
                    Iteration time: 12.24s
                        Total time: 13463.43s
                               ETA: 1318244.7s

################################################################################
                    [1m Learning iteration 1011/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.587s, learning 0.161s)
               Value function loss: 27.4492
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 245.22
               Mean episode length: 122.66
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16580608
                    Iteration time: 11.75s
                        Total time: 13475.18s
                               ETA: 1318078.0s

################################################################################
                    [1m Learning iteration 1012/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.447s, learning 0.290s)
               Value function loss: 27.4019
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 246.26
               Mean episode length: 123.02
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16596992
                    Iteration time: 11.74s
                        Total time: 13486.92s
                               ETA: 1317910.5s

################################################################################
                    [1m Learning iteration 1013/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.990s, learning 0.174s)
               Value function loss: 25.4574
                    Surrogate loss: -0.0043
             Mean action noise std: 0.78
                       Mean reward: 242.91
               Mean episode length: 122.85
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16613376
                    Iteration time: 12.16s
                        Total time: 13499.09s
                               ETA: 1317785.0s

################################################################################
                    [1m Learning iteration 1014/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.614s, learning 0.160s)
               Value function loss: 22.2507
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 248.07
               Mean episode length: 124.32
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16629760
                    Iteration time: 11.77s
                        Total time: 13510.86s
                               ETA: 1317621.6s

################################################################################
                    [1m Learning iteration 1015/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.510s, learning 0.160s)
               Value function loss: 53.6707
                    Surrogate loss: 0.0030
             Mean action noise std: 0.78
                       Mean reward: 246.03
               Mean episode length: 123.01
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.30
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16646144
                    Iteration time: 11.67s
                        Total time: 13522.53s
                               ETA: 1317448.4s

################################################################################
                    [1m Learning iteration 1016/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.773s, learning 0.166s)
               Value function loss: 20.3594
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 243.62
               Mean episode length: 121.66
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.75
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16662528
                    Iteration time: 11.94s
                        Total time: 13534.47s
                               ETA: 1317301.8s

################################################################################
                    [1m Learning iteration 1017/100000 [0m                    

                       Computation: 1280 steps/s (collection: 12.568s, learning 0.224s)
               Value function loss: 82.0929
                    Surrogate loss: 0.0033
             Mean action noise std: 0.78
                       Mean reward: 249.15
               Mean episode length: 124.81
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16678912
                    Iteration time: 12.79s
                        Total time: 13547.26s
                               ETA: 1317238.2s

################################################################################
                    [1m Learning iteration 1018/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.038s, learning 0.167s)
               Value function loss: 41.5394
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 251.27
               Mean episode length: 124.56
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16695296
                    Iteration time: 12.21s
                        Total time: 13559.47s
                               ETA: 1317117.8s

################################################################################
                    [1m Learning iteration 1019/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.990s, learning 0.165s)
               Value function loss: 31.2759
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 247.27
               Mean episode length: 123.97
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16711680
                    Iteration time: 12.16s
                        Total time: 13571.62s
                               ETA: 1316992.8s

################################################################################
                    [1m Learning iteration 1020/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.883s, learning 0.239s)
               Value function loss: 33.5031
                    Surrogate loss: 0.0232
             Mean action noise std: 0.78
                       Mean reward: 244.41
               Mean episode length: 123.37
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16728064
                    Iteration time: 12.12s
                        Total time: 13583.74s
                               ETA: 1316864.8s

################################################################################
                    [1m Learning iteration 1021/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.609s, learning 0.196s)
               Value function loss: 23.0459
                    Surrogate loss: -0.0021
             Mean action noise std: 0.78
                       Mean reward: 249.99
               Mean episode length: 124.04
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16744448
                    Iteration time: 11.81s
                        Total time: 13595.55s
                               ETA: 1316706.2s

################################################################################
                    [1m Learning iteration 1022/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.747s, learning 0.215s)
               Value function loss: 30.8817
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 248.90
               Mean episode length: 123.88
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16760832
                    Iteration time: 11.96s
                        Total time: 13607.51s
                               ETA: 1316563.2s

################################################################################
                    [1m Learning iteration 1023/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.567s, learning 0.163s)
               Value function loss: 27.2612
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 245.83
               Mean episode length: 121.88
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16777216
                    Iteration time: 11.73s
                        Total time: 13619.24s
                               ETA: 1316397.9s

################################################################################
                    [1m Learning iteration 1024/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.919s, learning 0.159s)
               Value function loss: 27.3676
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 250.67
               Mean episode length: 123.77
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16793600
                    Iteration time: 12.08s
                        Total time: 13631.32s
                               ETA: 1316266.6s

################################################################################
                    [1m Learning iteration 1025/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.657s, learning 0.176s)
               Value function loss: 24.2960
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 249.74
               Mean episode length: 123.27
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16809984
                    Iteration time: 11.83s
                        Total time: 13643.15s
                               ETA: 1316111.9s

################################################################################
                    [1m Learning iteration 1026/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.824s, learning 0.173s)
               Value function loss: 27.5093
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 249.28
               Mean episode length: 123.34
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16826368
                    Iteration time: 12.00s
                        Total time: 13655.15s
                               ETA: 1315973.3s

################################################################################
                    [1m Learning iteration 1027/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.738s, learning 0.170s)
               Value function loss: 25.0476
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 251.80
               Mean episode length: 123.88
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16842752
                    Iteration time: 11.91s
                        Total time: 13667.06s
                               ETA: 1315826.4s

################################################################################
                    [1m Learning iteration 1028/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.887s, learning 0.158s)
               Value function loss: 31.3819
                    Surrogate loss: -0.0009
             Mean action noise std: 0.78
                       Mean reward: 248.86
               Mean episode length: 123.16
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16859136
                    Iteration time: 12.04s
                        Total time: 13679.10s
                               ETA: 1315692.8s

################################################################################
                    [1m Learning iteration 1029/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.320s, learning 0.164s)
               Value function loss: 23.5288
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 251.30
               Mean episode length: 124.19
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16875520
                    Iteration time: 11.48s
                        Total time: 13690.59s
                               ETA: 1315505.7s

################################################################################
                    [1m Learning iteration 1030/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.889s, learning 0.212s)
               Value function loss: 23.6563
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 248.95
               Mean episode length: 122.94
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16891904
                    Iteration time: 12.10s
                        Total time: 13702.69s
                               ETA: 1315378.1s

################################################################################
                    [1m Learning iteration 1031/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.706s, learning 0.253s)
               Value function loss: 30.3806
                    Surrogate loss: -0.0020
             Mean action noise std: 0.78
                       Mean reward: 253.73
               Mean episode length: 124.84
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16908288
                    Iteration time: 11.96s
                        Total time: 13714.64s
                               ETA: 1315237.1s

################################################################################
                    [1m Learning iteration 1032/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.539s, learning 0.165s)
               Value function loss: 27.1432
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 250.74
               Mean episode length: 123.53
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16924672
                    Iteration time: 11.70s
                        Total time: 13726.35s
                               ETA: 1315071.9s

################################################################################
                    [1m Learning iteration 1033/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.548s, learning 0.163s)
               Value function loss: 92.9145
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 250.96
               Mean episode length: 124.73
                  Mean reward/step: 1.98
       Mean episode length/episode: 6.88
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16941056
                    Iteration time: 11.71s
                        Total time: 13738.06s
                               ETA: 1314907.7s

################################################################################
                    [1m Learning iteration 1034/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.562s, learning 0.163s)
               Value function loss: 33.5242
                    Surrogate loss: -0.0011
             Mean action noise std: 0.78
                       Mean reward: 250.22
               Mean episode length: 124.05
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16957440
                    Iteration time: 11.73s
                        Total time: 13749.79s
                               ETA: 1314745.2s

################################################################################
                    [1m Learning iteration 1035/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.851s, learning 0.160s)
               Value function loss: 32.7355
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 250.72
               Mean episode length: 124.73
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16973824
                    Iteration time: 12.01s
                        Total time: 13761.80s
                               ETA: 1314610.2s

################################################################################
                    [1m Learning iteration 1036/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.682s, learning 0.160s)
               Value function loss: 37.3341
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 251.82
               Mean episode length: 124.61
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 16990208
                    Iteration time: 11.84s
                        Total time: 13773.64s
                               ETA: 1314459.4s

################################################################################
                    [1m Learning iteration 1037/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.652s, learning 0.164s)
               Value function loss: 30.0506
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 248.23
               Mean episode length: 124.38
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17006592
                    Iteration time: 11.82s
                        Total time: 13785.45s
                               ETA: 1314306.3s

################################################################################
                    [1m Learning iteration 1038/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.771s, learning 0.207s)
               Value function loss: 30.3409
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 244.48
               Mean episode length: 122.59
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17022976
                    Iteration time: 11.98s
                        Total time: 13797.43s
                               ETA: 1314168.9s

################################################################################
                    [1m Learning iteration 1039/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.859s, learning 0.190s)
               Value function loss: 31.9002
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 247.61
               Mean episode length: 123.40
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17039360
                    Iteration time: 12.05s
                        Total time: 13809.48s
                               ETA: 1314038.5s

################################################################################
                    [1m Learning iteration 1040/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.506s, learning 0.206s)
               Value function loss: 23.8457
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 249.13
               Mean episode length: 123.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17055744
                    Iteration time: 11.71s
                        Total time: 13821.19s
                               ETA: 1313876.3s

################################################################################
                    [1m Learning iteration 1041/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.552s, learning 0.204s)
               Value function loss: 24.8606
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 250.94
               Mean episode length: 124.12
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17072128
                    Iteration time: 11.76s
                        Total time: 13832.95s
                               ETA: 1313718.6s

################################################################################
                    [1m Learning iteration 1042/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.081s, learning 0.162s)
               Value function loss: 24.4197
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 248.66
               Mean episode length: 123.04
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17088512
                    Iteration time: 12.24s
                        Total time: 13845.19s
                               ETA: 1313607.3s

################################################################################
                    [1m Learning iteration 1043/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.935s, learning 0.172s)
               Value function loss: 23.7276
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 249.76
               Mean episode length: 123.39
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17104896
                    Iteration time: 12.11s
                        Total time: 13857.30s
                               ETA: 1313483.3s

################################################################################
                    [1m Learning iteration 1044/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.726s, learning 0.257s)
               Value function loss: 27.1588
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 252.52
               Mean episode length: 124.06
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17121280
                    Iteration time: 11.98s
                        Total time: 13869.28s
                               ETA: 1313347.9s

################################################################################
                    [1m Learning iteration 1045/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.433s, learning 0.194s)
               Value function loss: 20.0069
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 250.12
               Mean episode length: 123.54
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17137664
                    Iteration time: 11.63s
                        Total time: 13880.91s
                               ETA: 1313179.0s

################################################################################
                    [1m Learning iteration 1046/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.710s, learning 0.209s)
               Value function loss: 48.2754
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 252.36
               Mean episode length: 124.29
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17154048
                    Iteration time: 11.92s
                        Total time: 13892.83s
                               ETA: 1313038.0s

################################################################################
                    [1m Learning iteration 1047/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.692s, learning 0.158s)
               Value function loss: 19.5840
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 248.32
               Mean episode length: 123.59
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17170432
                    Iteration time: 11.85s
                        Total time: 13904.68s
                               ETA: 1312890.7s

################################################################################
                    [1m Learning iteration 1048/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.943s, learning 0.160s)
               Value function loss: 33.2277
                    Surrogate loss: -0.0021
             Mean action noise std: 0.78
                       Mean reward: 250.84
               Mean episode length: 124.37
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17186816
                    Iteration time: 12.10s
                        Total time: 13916.78s
                               ETA: 1312767.6s

################################################################################
                    [1m Learning iteration 1049/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.359s, learning 0.161s)
               Value function loss: 49.0647
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 255.62
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 6.98
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17203200
                    Iteration time: 11.52s
                        Total time: 13928.30s
                               ETA: 1312589.7s

################################################################################
                    [1m Learning iteration 1050/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.749s, learning 0.195s)
               Value function loss: 21.1299
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 252.51
               Mean episode length: 123.83
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17219584
                    Iteration time: 11.94s
                        Total time: 13940.24s
                               ETA: 1312452.0s

################################################################################
                    [1m Learning iteration 1051/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.978s, learning 0.183s)
               Value function loss: 26.5545
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 252.73
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17235968
                    Iteration time: 12.16s
                        Total time: 13952.40s
                               ETA: 1312335.1s

################################################################################
                    [1m Learning iteration 1052/100000 [0m                    

                       Computation: 1347 steps/s (collection: 12.002s, learning 0.160s)
               Value function loss: 23.3414
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 253.19
               Mean episode length: 124.92
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17252352
                    Iteration time: 12.16s
                        Total time: 13964.57s
                               ETA: 1312218.4s

################################################################################
                    [1m Learning iteration 1053/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.162s, learning 0.215s)
               Value function loss: 25.7501
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 250.62
               Mean episode length: 123.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17268736
                    Iteration time: 12.38s
                        Total time: 13976.94s
                               ETA: 1312122.1s

################################################################################
                    [1m Learning iteration 1054/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.886s, learning 0.165s)
               Value function loss: 22.3237
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 251.33
               Mean episode length: 124.56
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17285120
                    Iteration time: 12.05s
                        Total time: 13988.99s
                               ETA: 1311995.3s

################################################################################
                    [1m Learning iteration 1055/100000 [0m                    

                       Computation: 1456 steps/s (collection: 11.080s, learning 0.171s)
               Value function loss: 23.3931
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 250.70
               Mean episode length: 123.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17301504
                    Iteration time: 11.25s
                        Total time: 14000.25s
                               ETA: 1311793.8s

################################################################################
                    [1m Learning iteration 1056/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.555s, learning 0.200s)
               Value function loss: 20.5711
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 253.10
               Mean episode length: 124.57
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17317888
                    Iteration time: 11.76s
                        Total time: 14012.00s
                               ETA: 1311639.9s

################################################################################
                    [1m Learning iteration 1057/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.369s, learning 0.162s)
               Value function loss: 25.9420
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 254.20
               Mean episode length: 124.41
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17334272
                    Iteration time: 11.53s
                        Total time: 14023.53s
                               ETA: 1311465.4s

################################################################################
                    [1m Learning iteration 1058/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.408s, learning 0.154s)
               Value function loss: 20.1368
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 253.72
               Mean episode length: 124.59
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17350656
                    Iteration time: 11.56s
                        Total time: 14035.10s
                               ETA: 1311294.0s

################################################################################
                    [1m Learning iteration 1059/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.590s, learning 0.165s)
               Value function loss: 22.8561
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 254.88
               Mean episode length: 124.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17367040
                    Iteration time: 11.75s
                        Total time: 14046.85s
                               ETA: 1311140.9s

################################################################################
                    [1m Learning iteration 1060/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.953s, learning 0.198s)
               Value function loss: 21.3212
                    Surrogate loss: -0.0027
             Mean action noise std: 0.78
                       Mean reward: 253.69
               Mean episode length: 124.26
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17383424
                    Iteration time: 12.15s
                        Total time: 14059.00s
                               ETA: 1311025.0s

################################################################################
                    [1m Learning iteration 1061/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.742s, learning 0.162s)
               Value function loss: 21.0680
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 249.68
               Mean episode length: 123.42
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17399808
                    Iteration time: 11.90s
                        Total time: 14070.90s
                               ETA: 1310886.3s

################################################################################
                    [1m Learning iteration 1062/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.796s, learning 0.211s)
               Value function loss: 37.9123
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 253.75
               Mean episode length: 124.79
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17416192
                    Iteration time: 12.01s
                        Total time: 14082.91s
                               ETA: 1310757.4s

################################################################################
                    [1m Learning iteration 1063/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.698s, learning 0.177s)
               Value function loss: 19.2564
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 248.61
               Mean episode length: 123.67
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.74
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17432576
                    Iteration time: 11.87s
                        Total time: 14094.79s
                               ETA: 1310616.4s

################################################################################
                    [1m Learning iteration 1064/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.873s, learning 0.290s)
               Value function loss: 70.8889
                    Surrogate loss: -0.0007
             Mean action noise std: 0.78
                       Mean reward: 255.10
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17448960
                    Iteration time: 12.16s
                        Total time: 14106.95s
                               ETA: 1310502.4s

################################################################################
                    [1m Learning iteration 1065/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.647s, learning 0.162s)
               Value function loss: 29.2822
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 252.15
               Mean episode length: 124.04
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17465344
                    Iteration time: 11.81s
                        Total time: 14118.76s
                               ETA: 1310355.9s

################################################################################
                    [1m Learning iteration 1066/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.564s, learning 0.167s)
               Value function loss: 28.8662
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 252.77
               Mean episode length: 124.11
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17481728
                    Iteration time: 11.73s
                        Total time: 14130.49s
                               ETA: 1310202.3s

################################################################################
                    [1m Learning iteration 1067/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.835s, learning 0.226s)
               Value function loss: 24.6416
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 255.90
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17498112
                    Iteration time: 12.06s
                        Total time: 14142.55s
                               ETA: 1310079.5s

################################################################################
                    [1m Learning iteration 1068/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.433s, learning 0.171s)
               Value function loss: 19.0486
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 255.46
               Mean episode length: 124.82
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17514496
                    Iteration time: 11.60s
                        Total time: 14154.15s
                               ETA: 1309914.6s

################################################################################
                    [1m Learning iteration 1069/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.842s, learning 0.218s)
               Value function loss: 25.0853
                    Surrogate loss: -0.0019
             Mean action noise std: 0.78
                       Mean reward: 252.15
               Mean episode length: 124.74
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17530880
                    Iteration time: 12.06s
                        Total time: 14166.21s
                               ETA: 1309792.3s

################################################################################
                    [1m Learning iteration 1070/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.989s, learning 0.164s)
               Value function loss: 21.6696
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 254.20
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17547264
                    Iteration time: 12.15s
                        Total time: 14178.37s
                               ETA: 1309678.6s

################################################################################
                    [1m Learning iteration 1071/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.916s, learning 0.168s)
               Value function loss: 21.3943
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 254.35
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17563648
                    Iteration time: 12.08s
                        Total time: 14190.45s
                               ETA: 1309558.9s

################################################################################
                    [1m Learning iteration 1072/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.510s, learning 0.164s)
               Value function loss: 20.3828
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 256.53
               Mean episode length: 124.91
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17580032
                    Iteration time: 11.67s
                        Total time: 14202.13s
                               ETA: 1309401.5s

################################################################################
                    [1m Learning iteration 1073/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.692s, learning 0.168s)
               Value function loss: 22.8967
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 257.10
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17596416
                    Iteration time: 11.86s
                        Total time: 14213.99s
                               ETA: 1309261.6s

################################################################################
                    [1m Learning iteration 1074/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.611s, learning 0.204s)
               Value function loss: 21.4201
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 251.43
               Mean episode length: 124.97
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17612800
                    Iteration time: 11.81s
                        Total time: 14225.80s
                               ETA: 1309117.6s

################################################################################
                    [1m Learning iteration 1075/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.948s, learning 0.271s)
               Value function loss: 23.9209
                    Surrogate loss: 0.0144
             Mean action noise std: 0.78
                       Mean reward: 253.92
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17629184
                    Iteration time: 12.22s
                        Total time: 14238.02s
                               ETA: 1309011.1s

################################################################################
                    [1m Learning iteration 1076/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.114s, learning 0.174s)
               Value function loss: 21.2151
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 253.85
               Mean episode length: 124.97
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17645568
                    Iteration time: 12.29s
                        Total time: 14250.31s
                               ETA: 1308911.2s

################################################################################
                    [1m Learning iteration 1077/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.759s, learning 0.190s)
               Value function loss: 43.7850
                    Surrogate loss: 0.0173
             Mean action noise std: 0.78
                       Mean reward: 256.00
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17661952
                    Iteration time: 11.95s
                        Total time: 14262.26s
                               ETA: 1308780.3s

################################################################################
                    [1m Learning iteration 1078/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.045s, learning 0.172s)
               Value function loss: 20.8619
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 253.61
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17678336
                    Iteration time: 12.22s
                        Total time: 14274.47s
                               ETA: 1308674.3s

################################################################################
                    [1m Learning iteration 1079/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.715s, learning 0.194s)
               Value function loss: 22.4464
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 255.06
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17694720
                    Iteration time: 11.91s
                        Total time: 14286.38s
                               ETA: 1308540.1s

################################################################################
                    [1m Learning iteration 1080/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.508s, learning 0.163s)
               Value function loss: 65.5963
                    Surrogate loss: 0.0228
             Mean action noise std: 0.78
                       Mean reward: 254.57
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 6.86
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17711104
                    Iteration time: 11.67s
                        Total time: 14298.05s
                               ETA: 1308384.3s

################################################################################
                    [1m Learning iteration 1081/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.726s, learning 0.171s)
               Value function loss: 19.9205
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 252.85
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17727488
                    Iteration time: 11.90s
                        Total time: 14309.95s
                               ETA: 1308249.6s

################################################################################
                    [1m Learning iteration 1082/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.917s, learning 0.164s)
               Value function loss: 25.5838
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 256.20
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17743872
                    Iteration time: 12.08s
                        Total time: 14322.03s
                               ETA: 1308131.9s

################################################################################
                    [1m Learning iteration 1083/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.592s, learning 0.345s)
               Value function loss: 30.4557
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 253.64
               Mean episode length: 124.32
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17760256
                    Iteration time: 11.94s
                        Total time: 14333.97s
                               ETA: 1308001.1s

################################################################################
                    [1m Learning iteration 1084/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.597s, learning 0.174s)
               Value function loss: 29.6193
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 248.57
               Mean episode length: 122.85
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17776640
                    Iteration time: 11.77s
                        Total time: 14345.74s
                               ETA: 1307855.6s

################################################################################
                    [1m Learning iteration 1085/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.502s, learning 0.180s)
               Value function loss: 26.8261
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 254.25
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17793024
                    Iteration time: 11.68s
                        Total time: 14357.42s
                               ETA: 1307702.1s

################################################################################
                    [1m Learning iteration 1086/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.926s, learning 0.231s)
               Value function loss: 29.8025
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 250.64
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17809408
                    Iteration time: 12.16s
                        Total time: 14369.58s
                               ETA: 1307592.1s

################################################################################
                    [1m Learning iteration 1087/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.793s, learning 0.226s)
               Value function loss: 25.0440
                    Surrogate loss: -0.0016
             Mean action noise std: 0.78
                       Mean reward: 255.74
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17825792
                    Iteration time: 12.02s
                        Total time: 14381.60s
                               ETA: 1307469.7s

################################################################################
                    [1m Learning iteration 1088/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.892s, learning 0.160s)
               Value function loss: 28.1643
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 249.96
               Mean episode length: 124.25
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17842176
                    Iteration time: 12.05s
                        Total time: 14393.65s
                               ETA: 1307350.5s

################################################################################
                    [1m Learning iteration 1089/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.078s, learning 0.248s)
               Value function loss: 24.4699
                    Surrogate loss: 0.0033
             Mean action noise std: 0.78
                       Mean reward: 251.61
               Mean episode length: 124.49
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17858560
                    Iteration time: 12.33s
                        Total time: 14405.98s
                               ETA: 1307256.4s

################################################################################
                    [1m Learning iteration 1090/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.979s, learning 0.193s)
               Value function loss: 25.8022
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 251.13
               Mean episode length: 124.57
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17874944
                    Iteration time: 12.17s
                        Total time: 14418.15s
                               ETA: 1307148.6s

################################################################################
                    [1m Learning iteration 1091/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.999s, learning 0.164s)
               Value function loss: 24.7376
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 253.08
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17891328
                    Iteration time: 12.16s
                        Total time: 14430.31s
                               ETA: 1307040.0s

################################################################################
                    [1m Learning iteration 1092/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.985s, learning 0.207s)
               Value function loss: 22.3910
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 253.82
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17907712
                    Iteration time: 12.19s
                        Total time: 14442.50s
                               ETA: 1306934.2s

################################################################################
                    [1m Learning iteration 1093/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.752s, learning 0.306s)
               Value function loss: 56.7569
                    Surrogate loss: 0.0038
             Mean action noise std: 0.78
                       Mean reward: 250.80
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17924096
                    Iteration time: 12.06s
                        Total time: 14454.56s
                               ETA: 1306816.5s

################################################################################
                    [1m Learning iteration 1094/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.755s, learning 0.167s)
               Value function loss: 20.9226
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 248.70
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17940480
                    Iteration time: 11.92s
                        Total time: 14466.48s
                               ETA: 1306686.7s

################################################################################
                    [1m Learning iteration 1095/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.239s, learning 0.213s)
               Value function loss: 53.7010
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 254.07
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17956864
                    Iteration time: 12.45s
                        Total time: 14478.94s
                               ETA: 1306605.0s

################################################################################
                    [1m Learning iteration 1096/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.876s, learning 0.190s)
               Value function loss: 52.3920
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 254.29
               Mean episode length: 125.00
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.13
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17973248
                    Iteration time: 12.07s
                        Total time: 14491.00s
                               ETA: 1306488.6s

################################################################################
                    [1m Learning iteration 1097/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.704s, learning 0.157s)
               Value function loss: 22.6760
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 250.29
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 17989632
                    Iteration time: 11.86s
                        Total time: 14502.86s
                               ETA: 1306353.9s

################################################################################
                    [1m Learning iteration 1098/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.047s, learning 0.232s)
               Value function loss: 29.5896
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 253.23
               Mean episode length: 124.66
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18006016
                    Iteration time: 12.28s
                        Total time: 14515.14s
                               ETA: 1306257.1s

################################################################################
                    [1m Learning iteration 1099/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.720s, learning 0.156s)
               Value function loss: 21.4180
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 253.98
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18022400
                    Iteration time: 11.88s
                        Total time: 14527.02s
                               ETA: 1306124.2s

################################################################################
                    [1m Learning iteration 1100/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.732s, learning 0.170s)
               Value function loss: 28.4584
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 253.81
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18038784
                    Iteration time: 11.90s
                        Total time: 14538.92s
                               ETA: 1305993.8s

################################################################################
                    [1m Learning iteration 1101/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.153s, learning 0.208s)
               Value function loss: 24.8303
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 249.20
               Mean episode length: 123.97
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18055168
                    Iteration time: 12.36s
                        Total time: 14551.28s
                               ETA: 1305904.8s

################################################################################
                    [1m Learning iteration 1102/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.049s, learning 0.302s)
               Value function loss: 22.7144
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 252.39
               Mean episode length: 124.48
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18071552
                    Iteration time: 12.35s
                        Total time: 14563.63s
                               ETA: 1305815.0s

################################################################################
                    [1m Learning iteration 1103/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.411s, learning 0.166s)
               Value function loss: 20.6996
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 251.12
               Mean episode length: 124.48
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18087936
                    Iteration time: 11.58s
                        Total time: 14575.21s
                               ETA: 1305656.1s

################################################################################
                    [1m Learning iteration 1104/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.571s, learning 0.165s)
               Value function loss: 28.7092
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 253.07
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18104320
                    Iteration time: 11.74s
                        Total time: 14586.94s
                               ETA: 1305511.6s

################################################################################
                    [1m Learning iteration 1105/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.800s, learning 0.216s)
               Value function loss: 24.2437
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 255.67
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18120704
                    Iteration time: 12.02s
                        Total time: 14598.96s
                               ETA: 1305392.4s

################################################################################
                    [1m Learning iteration 1106/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.799s, learning 0.246s)
               Value function loss: 26.4467
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 252.95
               Mean episode length: 124.36
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18137088
                    Iteration time: 12.04s
                        Total time: 14611.00s
                               ETA: 1305276.0s

################################################################################
                    [1m Learning iteration 1107/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.895s, learning 0.168s)
               Value function loss: 22.6128
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 254.12
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18153472
                    Iteration time: 12.06s
                        Total time: 14623.07s
                               ETA: 1305161.5s

################################################################################
                    [1m Learning iteration 1108/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.595s, learning 0.164s)
               Value function loss: 21.5919
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 251.47
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18169856
                    Iteration time: 11.76s
                        Total time: 14634.83s
                               ETA: 1305020.0s

################################################################################
                    [1m Learning iteration 1109/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.639s, learning 0.198s)
               Value function loss: 38.9443
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 254.90
               Mean episode length: 124.08
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18186240
                    Iteration time: 11.84s
                        Total time: 14646.66s
                               ETA: 1304885.7s

################################################################################
                    [1m Learning iteration 1110/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.764s, learning 0.270s)
               Value function loss: 23.3311
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 250.44
               Mean episode length: 124.67
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.73
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18202624
                    Iteration time: 12.03s
                        Total time: 14658.70s
                               ETA: 1304769.1s

################################################################################
                    [1m Learning iteration 1111/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.989s, learning 0.160s)
               Value function loss: 84.1674
                    Surrogate loss: 0.0056
             Mean action noise std: 0.78
                       Mean reward: 252.29
               Mean episode length: 124.22
                  Mean reward/step: 2.01
       Mean episode length/episode: 6.94
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18219008
                    Iteration time: 12.15s
                        Total time: 14670.85s
                               ETA: 1304663.1s

################################################################################
                    [1m Learning iteration 1112/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.852s, learning 0.159s)
               Value function loss: 26.7298
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 254.22
               Mean episode length: 124.23
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18235392
                    Iteration time: 12.01s
                        Total time: 14682.86s
                               ETA: 1304544.8s

################################################################################
                    [1m Learning iteration 1113/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.751s, learning 0.170s)
               Value function loss: 29.7833
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 256.82
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18251776
                    Iteration time: 11.92s
                        Total time: 14694.78s
                               ETA: 1304418.8s

################################################################################
                    [1m Learning iteration 1114/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.802s, learning 0.163s)
               Value function loss: 31.6965
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 254.29
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18268160
                    Iteration time: 11.96s
                        Total time: 14706.74s
                               ETA: 1304296.9s

################################################################################
                    [1m Learning iteration 1115/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.723s, learning 0.207s)
               Value function loss: 27.1651
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 254.54
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18284544
                    Iteration time: 11.93s
                        Total time: 14718.67s
                               ETA: 1304172.1s

################################################################################
                    [1m Learning iteration 1116/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.906s, learning 0.156s)
               Value function loss: 29.4161
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 253.10
               Mean episode length: 124.70
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18300928
                    Iteration time: 12.06s
                        Total time: 14730.74s
                               ETA: 1304059.1s

################################################################################
                    [1m Learning iteration 1117/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.042s, learning 0.299s)
               Value function loss: 29.0271
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 256.21
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18317312
                    Iteration time: 12.34s
                        Total time: 14743.08s
                               ETA: 1303971.0s

################################################################################
                    [1m Learning iteration 1118/100000 [0m                    

                       Computation: 1312 steps/s (collection: 12.275s, learning 0.206s)
               Value function loss: 25.0643
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 248.77
               Mean episode length: 123.09
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18333696
                    Iteration time: 12.48s
                        Total time: 14755.56s
                               ETA: 1303895.4s

################################################################################
                    [1m Learning iteration 1119/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.556s, learning 0.166s)
               Value function loss: 24.6363
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 251.90
               Mean episode length: 124.18
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18350080
                    Iteration time: 11.72s
                        Total time: 14767.28s
                               ETA: 1303752.9s

################################################################################
                    [1m Learning iteration 1120/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.895s, learning 0.161s)
               Value function loss: 26.6635
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 252.46
               Mean episode length: 123.97
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18366464
                    Iteration time: 12.06s
                        Total time: 14779.34s
                               ETA: 1303640.2s

################################################################################
                    [1m Learning iteration 1121/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.779s, learning 0.172s)
               Value function loss: 24.2022
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 254.44
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18382848
                    Iteration time: 11.95s
                        Total time: 14791.29s
                               ETA: 1303518.3s

################################################################################
                    [1m Learning iteration 1122/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.687s, learning 0.158s)
               Value function loss: 26.6184
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 250.55
               Mean episode length: 124.45
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18399232
                    Iteration time: 11.84s
                        Total time: 14803.13s
                               ETA: 1303387.3s

################################################################################
                    [1m Learning iteration 1123/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.676s, learning 0.170s)
               Value function loss: 22.5187
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 255.25
               Mean episode length: 124.71
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18415616
                    Iteration time: 11.85s
                        Total time: 14814.98s
                               ETA: 1303256.7s

################################################################################
                    [1m Learning iteration 1124/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.418s, learning 0.169s)
               Value function loss: 51.3024
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 255.47
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18432000
                    Iteration time: 11.59s
                        Total time: 14826.56s
                               ETA: 1303103.4s

################################################################################
                    [1m Learning iteration 1125/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.195s, learning 0.240s)
               Value function loss: 22.3629
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 255.68
               Mean episode length: 124.89
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18448384
                    Iteration time: 12.43s
                        Total time: 14839.00s
                               ETA: 1303024.9s

################################################################################
                    [1m Learning iteration 1126/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.541s, learning 0.161s)
               Value function loss: 27.1978
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 250.84
               Mean episode length: 123.53
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18464768
                    Iteration time: 11.70s
                        Total time: 14850.70s
                               ETA: 1302882.2s

################################################################################
                    [1m Learning iteration 1127/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.755s, learning 0.173s)
               Value function loss: 61.5372
                    Surrogate loss: 0.0064
             Mean action noise std: 0.78
                       Mean reward: 253.42
               Mean episode length: 124.53
                  Mean reward/step: 2.00
       Mean episode length/episode: 6.91
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18481152
                    Iteration time: 11.93s
                        Total time: 14862.63s
                               ETA: 1302759.5s

################################################################################
                    [1m Learning iteration 1128/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.760s, learning 0.167s)
               Value function loss: 22.9460
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 253.71
               Mean episode length: 124.32
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18497536
                    Iteration time: 11.93s
                        Total time: 14874.56s
                               ETA: 1302637.0s

################################################################################
                    [1m Learning iteration 1129/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.627s, learning 0.216s)
               Value function loss: 29.0674
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 256.91
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18513920
                    Iteration time: 11.84s
                        Total time: 14886.40s
                               ETA: 1302507.3s

################################################################################
                    [1m Learning iteration 1130/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.643s, learning 0.155s)
               Value function loss: 27.8456
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 255.19
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18530304
                    Iteration time: 11.80s
                        Total time: 14898.20s
                               ETA: 1302373.9s

################################################################################
                    [1m Learning iteration 1131/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.452s, learning 0.168s)
               Value function loss: 33.5278
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 256.77
               Mean episode length: 124.76
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18546688
                    Iteration time: 11.62s
                        Total time: 14909.82s
                               ETA: 1302225.1s

################################################################################
                    [1m Learning iteration 1132/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.890s, learning 0.202s)
               Value function loss: 29.4881
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 254.91
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18563072
                    Iteration time: 12.09s
                        Total time: 14921.91s
                               ETA: 1302117.7s

################################################################################
                    [1m Learning iteration 1133/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.381s, learning 0.205s)
               Value function loss: 26.4564
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 253.15
               Mean episode length: 124.93
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18579456
                    Iteration time: 11.59s
                        Total time: 14933.50s
                               ETA: 1301966.5s

################################################################################
                    [1m Learning iteration 1134/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.580s, learning 0.214s)
               Value function loss: 21.9075
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 255.11
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18595840
                    Iteration time: 11.79s
                        Total time: 14945.29s
                               ETA: 1301833.6s

################################################################################
                    [1m Learning iteration 1135/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.332s, learning 0.169s)
               Value function loss: 31.8013
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 253.76
               Mean episode length: 124.07
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18612224
                    Iteration time: 11.50s
                        Total time: 14956.79s
                               ETA: 1301675.4s

################################################################################
                    [1m Learning iteration 1136/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.691s, learning 0.184s)
               Value function loss: 25.5156
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 253.88
               Mean episode length: 124.74
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18628608
                    Iteration time: 11.88s
                        Total time: 14968.67s
                               ETA: 1301550.0s

################################################################################
                    [1m Learning iteration 1137/100000 [0m                    

                       Computation: 1337 steps/s (collection: 11.949s, learning 0.304s)
               Value function loss: 25.5215
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: 253.51
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18644992
                    Iteration time: 12.25s
                        Total time: 14980.92s
                               ETA: 1301457.6s

################################################################################
                    [1m Learning iteration 1138/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.336s, learning 0.159s)
               Value function loss: 23.6247
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 256.42
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18661376
                    Iteration time: 12.49s
                        Total time: 14993.42s
                               ETA: 1301386.3s

################################################################################
                    [1m Learning iteration 1139/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.884s, learning 0.236s)
               Value function loss: 21.7237
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 254.24
               Mean episode length: 124.08
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18677760
                    Iteration time: 12.12s
                        Total time: 15005.53s
                               ETA: 1301282.6s

################################################################################
                    [1m Learning iteration 1140/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.582s, learning 0.157s)
               Value function loss: 52.0379
                    Surrogate loss: -0.0019
             Mean action noise std: 0.78
                       Mean reward: 254.49
               Mean episode length: 124.81
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18694144
                    Iteration time: 11.74s
                        Total time: 15017.27s
                               ETA: 1301146.1s

################################################################################
                    [1m Learning iteration 1141/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.702s, learning 0.243s)
               Value function loss: 22.4677
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 253.83
               Mean episode length: 124.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18710528
                    Iteration time: 11.94s
                        Total time: 15029.22s
                               ETA: 1301027.6s

################################################################################
                    [1m Learning iteration 1142/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.755s, learning 0.165s)
               Value function loss: 77.1176
                    Surrogate loss: 0.0027
             Mean action noise std: 0.78
                       Mean reward: 255.12
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18726912
                    Iteration time: 11.92s
                        Total time: 15041.14s
                               ETA: 1300907.2s

################################################################################
                    [1m Learning iteration 1143/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.963s, learning 0.162s)
               Value function loss: 38.0714
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 255.24
               Mean episode length: 124.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.38
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18743296
                    Iteration time: 12.12s
                        Total time: 15053.26s
                               ETA: 1300804.6s

################################################################################
                    [1m Learning iteration 1144/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.858s, learning 0.204s)
               Value function loss: 28.0413
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 253.50
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18759680
                    Iteration time: 12.06s
                        Total time: 15065.33s
                               ETA: 1300696.7s

################################################################################
                    [1m Learning iteration 1145/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.744s, learning 0.156s)
               Value function loss: 33.4657
                    Surrogate loss: 0.0019
             Mean action noise std: 0.78
                       Mean reward: 255.69
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18776064
                    Iteration time: 11.90s
                        Total time: 15077.22s
                               ETA: 1300575.1s

################################################################################
                    [1m Learning iteration 1146/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.853s, learning 0.171s)
               Value function loss: 25.4235
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 255.52
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18792448
                    Iteration time: 12.02s
                        Total time: 15089.25s
                               ETA: 1300464.3s

################################################################################
                    [1m Learning iteration 1147/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.633s, learning 0.314s)
               Value function loss: 36.3582
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 250.84
               Mean episode length: 123.35
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18808832
                    Iteration time: 11.95s
                        Total time: 15101.20s
                               ETA: 1300347.1s

################################################################################
                    [1m Learning iteration 1148/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.587s, learning 0.185s)
               Value function loss: 34.5155
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 256.95
               Mean episode length: 124.82
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18825216
                    Iteration time: 11.77s
                        Total time: 15112.97s
                               ETA: 1300215.0s

################################################################################
                    [1m Learning iteration 1149/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.618s, learning 0.173s)
               Value function loss: 31.9971
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 251.91
               Mean episode length: 124.47
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18841600
                    Iteration time: 11.79s
                        Total time: 15124.76s
                               ETA: 1300084.8s

################################################################################
                    [1m Learning iteration 1150/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.979s, learning 0.223s)
               Value function loss: 30.4563
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 253.26
               Mean episode length: 124.03
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18857984
                    Iteration time: 12.20s
                        Total time: 15136.96s
                               ETA: 1299990.1s

################################################################################
                    [1m Learning iteration 1151/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.301s, learning 0.154s)
               Value function loss: 28.3107
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 254.07
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18874368
                    Iteration time: 11.45s
                        Total time: 15148.42s
                               ETA: 1299831.4s

################################################################################
                    [1m Learning iteration 1152/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.535s, learning 0.170s)
               Value function loss: 27.7105
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 251.52
               Mean episode length: 124.02
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18890752
                    Iteration time: 11.70s
                        Total time: 15160.12s
                               ETA: 1299694.3s

################################################################################
                    [1m Learning iteration 1153/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.838s, learning 0.231s)
               Value function loss: 30.1417
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 254.44
               Mean episode length: 124.02
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18907136
                    Iteration time: 12.07s
                        Total time: 15172.19s
                               ETA: 1299588.7s

################################################################################
                    [1m Learning iteration 1154/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.086s, learning 0.161s)
               Value function loss: 25.8951
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 252.24
               Mean episode length: 123.79
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18923520
                    Iteration time: 12.25s
                        Total time: 15184.44s
                               ETA: 1299498.5s

################################################################################
                    [1m Learning iteration 1155/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.745s, learning 0.202s)
               Value function loss: 24.8014
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 252.22
               Mean episode length: 124.71
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18939904
                    Iteration time: 11.95s
                        Total time: 15196.38s
                               ETA: 1299382.7s

################################################################################
                    [1m Learning iteration 1156/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.842s, learning 0.228s)
               Value function loss: 32.3737
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 252.84
               Mean episode length: 124.67
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.36
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18956288
                    Iteration time: 12.07s
                        Total time: 15208.45s
                               ETA: 1299277.7s

################################################################################
                    [1m Learning iteration 1157/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.314s, learning 0.170s)
               Value function loss: 30.2157
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 255.61
               Mean episode length: 124.38
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18972672
                    Iteration time: 11.48s
                        Total time: 15219.94s
                               ETA: 1299122.8s

################################################################################
                    [1m Learning iteration 1158/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.862s, learning 0.204s)
               Value function loss: 88.2085
                    Surrogate loss: 0.0041
             Mean action noise std: 0.78
                       Mean reward: 255.16
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 6.95
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 18989056
                    Iteration time: 12.07s
                        Total time: 15232.00s
                               ETA: 1299017.8s

################################################################################
                    [1m Learning iteration 1159/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.856s, learning 0.187s)
               Value function loss: 22.2218
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 255.60
               Mean episode length: 124.95
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19005440
                    Iteration time: 12.04s
                        Total time: 15244.05s
                               ETA: 1298911.0s

################################################################################
                    [1m Learning iteration 1160/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.950s, learning 0.160s)
               Value function loss: 25.7354
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 254.14
               Mean episode length: 124.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19021824
                    Iteration time: 12.11s
                        Total time: 15256.16s
                               ETA: 1298810.0s

################################################################################
                    [1m Learning iteration 1161/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.535s, learning 0.195s)
               Value function loss: 32.9852
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 254.91
               Mean episode length: 124.13
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19038208
                    Iteration time: 11.73s
                        Total time: 15267.89s
                               ETA: 1298676.9s

################################################################################
                    [1m Learning iteration 1162/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.554s, learning 0.171s)
               Value function loss: 26.6121
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 254.79
               Mean episode length: 124.67
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19054592
                    Iteration time: 11.72s
                        Total time: 15279.61s
                               ETA: 1298543.6s

################################################################################
                    [1m Learning iteration 1163/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.791s, learning 0.164s)
               Value function loss: 27.0278
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 254.63
               Mean episode length: 124.33
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19070976
                    Iteration time: 11.95s
                        Total time: 15291.57s
                               ETA: 1298429.9s

################################################################################
                    [1m Learning iteration 1164/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.569s, learning 0.181s)
               Value function loss: 29.0617
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 253.66
               Mean episode length: 124.75
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19087360
                    Iteration time: 11.75s
                        Total time: 15303.32s
                               ETA: 1298299.2s

################################################################################
                    [1m Learning iteration 1165/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.953s, learning 0.176s)
               Value function loss: 22.9106
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 255.24
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19103744
                    Iteration time: 12.13s
                        Total time: 15315.45s
                               ETA: 1298200.7s

################################################################################
                    [1m Learning iteration 1166/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.250s, learning 0.157s)
               Value function loss: 27.4349
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 253.15
               Mean episode length: 123.80
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19120128
                    Iteration time: 11.41s
                        Total time: 15326.85s
                               ETA: 1298041.2s

################################################################################
                    [1m Learning iteration 1167/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.691s, learning 0.178s)
               Value function loss: 26.2550
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 253.63
               Mean episode length: 123.98
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19136512
                    Iteration time: 11.87s
                        Total time: 15338.72s
                               ETA: 1297921.1s

################################################################################
                    [1m Learning iteration 1168/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.253s, learning 0.198s)
               Value function loss: 27.3229
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 255.45
               Mean episode length: 124.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19152896
                    Iteration time: 12.45s
                        Total time: 15351.17s
                               ETA: 1297850.3s

################################################################################
                    [1m Learning iteration 1169/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.551s, learning 0.230s)
               Value function loss: 28.7522
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 257.53
               Mean episode length: 124.53
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19169280
                    Iteration time: 11.78s
                        Total time: 15362.95s
                               ETA: 1297723.1s

################################################################################
                    [1m Learning iteration 1170/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.697s, learning 0.204s)
               Value function loss: 23.6494
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 253.61
               Mean episode length: 123.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19185664
                    Iteration time: 11.90s
                        Total time: 15374.85s
                               ETA: 1297606.2s

################################################################################
                    [1m Learning iteration 1171/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.609s, learning 0.248s)
               Value function loss: 54.4072
                    Surrogate loss: 0.0047
             Mean action noise std: 0.78
                       Mean reward: 254.29
               Mean episode length: 123.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19202048
                    Iteration time: 11.86s
                        Total time: 15386.71s
                               ETA: 1297485.8s

################################################################################
                    [1m Learning iteration 1172/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.799s, learning 0.161s)
               Value function loss: 21.5118
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 251.81
               Mean episode length: 122.59
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19218432
                    Iteration time: 11.96s
                        Total time: 15398.67s
                               ETA: 1297374.2s

################################################################################
                    [1m Learning iteration 1173/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.733s, learning 0.191s)
               Value function loss: 39.8385
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 252.65
               Mean episode length: 123.48
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19234816
                    Iteration time: 11.92s
                        Total time: 15410.60s
                               ETA: 1297259.8s

################################################################################
                    [1m Learning iteration 1174/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.652s, learning 0.156s)
               Value function loss: 53.3091
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 250.40
               Mean episode length: 123.37
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.04
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19251200
                    Iteration time: 11.81s
                        Total time: 15422.40s
                               ETA: 1297135.8s

################################################################################
                    [1m Learning iteration 1175/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.346s, learning 0.159s)
               Value function loss: 24.7301
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 257.25
               Mean episode length: 124.60
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19267584
                    Iteration time: 11.51s
                        Total time: 15433.91s
                               ETA: 1296986.5s

################################################################################
                    [1m Learning iteration 1176/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.702s, learning 0.165s)
               Value function loss: 33.1133
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 255.80
               Mean episode length: 124.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19283968
                    Iteration time: 11.87s
                        Total time: 15445.78s
                               ETA: 1296867.8s

################################################################################
                    [1m Learning iteration 1177/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.591s, learning 0.168s)
               Value function loss: 29.9230
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 251.59
               Mean episode length: 122.96
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19300352
                    Iteration time: 11.76s
                        Total time: 15457.53s
                               ETA: 1296740.2s

################################################################################
                    [1m Learning iteration 1178/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.916s, learning 0.167s)
               Value function loss: 33.5170
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 254.78
               Mean episode length: 123.88
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19316736
                    Iteration time: 12.08s
                        Total time: 15469.62s
                               ETA: 1296640.0s

################################################################################
                    [1m Learning iteration 1179/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.457s, learning 0.275s)
               Value function loss: 28.6246
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 250.31
               Mean episode length: 123.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19333120
                    Iteration time: 11.73s
                        Total time: 15481.35s
                               ETA: 1296510.5s

################################################################################
                    [1m Learning iteration 1180/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.695s, learning 0.199s)
               Value function loss: 27.7211
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 254.64
               Mean episode length: 123.94
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19349504
                    Iteration time: 11.89s
                        Total time: 15493.24s
                               ETA: 1296394.8s

################################################################################
                    [1m Learning iteration 1181/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.461s, learning 0.163s)
               Value function loss: 20.9006
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 250.44
               Mean episode length: 122.35
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19365888
                    Iteration time: 11.62s
                        Total time: 15504.87s
                               ETA: 1296256.8s

################################################################################
                    [1m Learning iteration 1182/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.908s, learning 0.176s)
               Value function loss: 30.0048
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 255.80
               Mean episode length: 124.30
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19382272
                    Iteration time: 12.08s
                        Total time: 15516.95s
                               ETA: 1296157.3s

################################################################################
                    [1m Learning iteration 1183/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.671s, learning 0.264s)
               Value function loss: 26.8424
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 249.76
               Mean episode length: 122.07
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19398656
                    Iteration time: 11.93s
                        Total time: 15528.89s
                               ETA: 1296045.6s

################################################################################
                    [1m Learning iteration 1184/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.823s, learning 0.158s)
               Value function loss: 29.3914
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 248.83
               Mean episode length: 121.84
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19415040
                    Iteration time: 11.98s
                        Total time: 15540.87s
                               ETA: 1295937.9s

################################################################################
                    [1m Learning iteration 1185/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.603s, learning 0.156s)
               Value function loss: 26.6860
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 252.33
               Mean episode length: 122.86
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19431424
                    Iteration time: 11.76s
                        Total time: 15552.63s
                               ETA: 1295811.8s

################################################################################
                    [1m Learning iteration 1186/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.756s, learning 0.160s)
               Value function loss: 25.0006
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 251.00
               Mean episode length: 123.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.72
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19447808
                    Iteration time: 11.92s
                        Total time: 15564.54s
                               ETA: 1295699.0s

################################################################################
                    [1m Learning iteration 1187/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.792s, learning 0.169s)
               Value function loss: 45.7025
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 250.38
               Mean episode length: 122.94
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19464192
                    Iteration time: 11.96s
                        Total time: 15576.50s
                               ETA: 1295590.2s

################################################################################
                    [1m Learning iteration 1188/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.328s, learning 0.202s)
               Value function loss: 25.2871
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: 249.64
               Mean episode length: 123.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.71
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19480576
                    Iteration time: 12.53s
                        Total time: 15589.03s
                               ETA: 1295528.7s

################################################################################
                    [1m Learning iteration 1189/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.781s, learning 0.188s)
               Value function loss: 86.1762
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 256.19
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19496960
                    Iteration time: 11.97s
                        Total time: 15601.00s
                               ETA: 1295420.8s

################################################################################
                    [1m Learning iteration 1190/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.158s)
               Value function loss: 33.5548
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 247.47
               Mean episode length: 121.50
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19513344
                    Iteration time: 11.67s
                        Total time: 15612.68s
                               ETA: 1295288.4s

################################################################################
                    [1m Learning iteration 1191/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.905s, learning 0.222s)
               Value function loss: 30.4172
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 254.89
               Mean episode length: 124.05
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19529728
                    Iteration time: 12.13s
                        Total time: 15624.80s
                               ETA: 1295193.9s

################################################################################
                    [1m Learning iteration 1192/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.142s, learning 0.229s)
               Value function loss: 32.7109
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 257.09
               Mean episode length: 124.78
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19546112
                    Iteration time: 12.37s
                        Total time: 15637.17s
                               ETA: 1295119.8s

################################################################################
                    [1m Learning iteration 1193/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.452s, learning 0.186s)
               Value function loss: 25.8059
                    Surrogate loss: 0.0094
             Mean action noise std: 0.78
                       Mean reward: 252.30
               Mean episode length: 122.92
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19562496
                    Iteration time: 11.64s
                        Total time: 15648.81s
                               ETA: 1294985.1s

################################################################################
                    [1m Learning iteration 1194/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.703s, learning 0.182s)
               Value function loss: 32.3015
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 251.03
               Mean episode length: 123.48
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19578880
                    Iteration time: 11.89s
                        Total time: 15660.70s
                               ETA: 1294871.1s

################################################################################
                    [1m Learning iteration 1195/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.555s, learning 0.188s)
               Value function loss: 29.3429
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 250.29
               Mean episode length: 122.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19595264
                    Iteration time: 11.74s
                        Total time: 15672.44s
                               ETA: 1294745.4s

################################################################################
                    [1m Learning iteration 1196/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.688s, learning 0.204s)
               Value function loss: 27.0643
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 250.70
               Mean episode length: 122.34
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19611648
                    Iteration time: 11.89s
                        Total time: 15684.33s
                               ETA: 1294632.2s

################################################################################
                    [1m Learning iteration 1197/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.798s, learning 0.172s)
               Value function loss: 29.9487
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 250.58
               Mean episode length: 122.25
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19628032
                    Iteration time: 11.97s
                        Total time: 15696.30s
                               ETA: 1294525.7s

################################################################################
                    [1m Learning iteration 1198/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.052s, learning 0.224s)
               Value function loss: 31.3026
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 249.91
               Mean episode length: 121.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19644416
                    Iteration time: 12.28s
                        Total time: 15708.58s
                               ETA: 1294444.5s

################################################################################
                    [1m Learning iteration 1199/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.974s, learning 0.170s)
               Value function loss: 28.0057
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 251.88
               Mean episode length: 123.29
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19660800
                    Iteration time: 12.14s
                        Total time: 15720.72s
                               ETA: 1294352.6s

################################################################################
                    [1m Learning iteration 1200/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.102s, learning 0.210s)
               Value function loss: 34.6573
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 246.44
               Mean episode length: 121.35
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19677184
                    Iteration time: 12.31s
                        Total time: 15733.03s
                               ETA: 1294274.6s

################################################################################
                    [1m Learning iteration 1201/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.785s, learning 0.256s)
               Value function loss: 28.9530
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 248.26
               Mean episode length: 122.03
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19693568
                    Iteration time: 12.04s
                        Total time: 15745.07s
                               ETA: 1294174.4s

################################################################################
                    [1m Learning iteration 1202/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.790s, learning 0.165s)
               Value function loss: 57.1636
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 256.44
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.70
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19709952
                    Iteration time: 11.95s
                        Total time: 15757.03s
                               ETA: 1294067.3s

################################################################################
                    [1m Learning iteration 1203/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.702s, learning 0.160s)
               Value function loss: 30.3884
                    Surrogate loss: 0.0068
             Mean action noise std: 0.78
                       Mean reward: 248.96
               Mean episode length: 122.16
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19726336
                    Iteration time: 11.86s
                        Total time: 15768.89s
                               ETA: 1293952.8s

################################################################################
                    [1m Learning iteration 1204/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.658s, learning 0.163s)
               Value function loss: 33.7905
                    Surrogate loss: 0.0002
             Mean action noise std: 0.78
                       Mean reward: 248.01
               Mean episode length: 122.82
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19742720
                    Iteration time: 11.82s
                        Total time: 15780.71s
                               ETA: 1293835.1s

################################################################################
                    [1m Learning iteration 1205/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.164s)
               Value function loss: 77.3498
                    Surrogate loss: 0.0126
             Mean action noise std: 0.78
                       Mean reward: 253.98
               Mean episode length: 124.07
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.02
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19759104
                    Iteration time: 11.73s
                        Total time: 15792.44s
                               ETA: 1293710.0s

################################################################################
                    [1m Learning iteration 1206/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.572s, learning 0.258s)
               Value function loss: 30.8289
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 248.21
               Mean episode length: 122.69
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19775488
                    Iteration time: 11.83s
                        Total time: 15804.27s
                               ETA: 1293593.4s

################################################################################
                    [1m Learning iteration 1207/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.716s, learning 0.297s)
               Value function loss: 36.0975
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 253.00
               Mean episode length: 123.32
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19791872
                    Iteration time: 12.01s
                        Total time: 15816.28s
                               ETA: 1293491.9s

################################################################################
                    [1m Learning iteration 1208/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.778s, learning 0.160s)
               Value function loss: 34.9010
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 249.18
               Mean episode length: 123.10
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19808256
                    Iteration time: 11.94s
                        Total time: 15828.22s
                               ETA: 1293384.5s

################################################################################
                    [1m Learning iteration 1209/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.574s, learning 0.156s)
               Value function loss: 37.9684
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 251.99
               Mean episode length: 123.66
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19824640
                    Iteration time: 11.73s
                        Total time: 15839.95s
                               ETA: 1293260.2s

################################################################################
                    [1m Learning iteration 1210/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.412s, learning 0.162s)
               Value function loss: 33.5200
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 249.45
               Mean episode length: 122.55
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19841024
                    Iteration time: 11.57s
                        Total time: 15851.53s
                               ETA: 1293123.3s

################################################################################
                    [1m Learning iteration 1211/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.923s, learning 0.165s)
               Value function loss: 34.8668
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 248.93
               Mean episode length: 123.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19857408
                    Iteration time: 12.09s
                        Total time: 15863.62s
                               ETA: 1293028.6s

################################################################################
                    [1m Learning iteration 1212/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.669s, learning 0.310s)
               Value function loss: 25.5667
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 247.63
               Mean episode length: 123.17
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19873792
                    Iteration time: 11.98s
                        Total time: 15875.59s
                               ETA: 1292925.2s

################################################################################
                    [1m Learning iteration 1213/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.089s, learning 0.158s)
               Value function loss: 27.4597
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 241.80
               Mean episode length: 121.84
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19890176
                    Iteration time: 12.25s
                        Total time: 15887.84s
                               ETA: 1292843.6s

################################################################################
                    [1m Learning iteration 1214/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.044s, learning 0.191s)
               Value function loss: 24.5231
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 242.96
               Mean episode length: 121.56
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19906560
                    Iteration time: 12.24s
                        Total time: 15900.08s
                               ETA: 1292761.3s

################################################################################
                    [1m Learning iteration 1215/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.019s, learning 0.157s)
               Value function loss: 28.4846
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 248.52
               Mean episode length: 122.46
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19922944
                    Iteration time: 12.18s
                        Total time: 15912.25s
                               ETA: 1292674.3s

################################################################################
                    [1m Learning iteration 1216/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.952s, learning 0.239s)
               Value function loss: 27.7164
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 251.10
               Mean episode length: 124.24
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19939328
                    Iteration time: 12.19s
                        Total time: 15924.44s
                               ETA: 1292588.6s

################################################################################
                    [1m Learning iteration 1217/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.029s, learning 0.197s)
               Value function loss: 26.0478
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 245.95
               Mean episode length: 121.80
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19955712
                    Iteration time: 12.23s
                        Total time: 15936.67s
                               ETA: 1292505.8s

################################################################################
                    [1m Learning iteration 1218/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.951s, learning 0.161s)
               Value function loss: 46.1443
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 249.58
               Mean episode length: 124.06
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19972096
                    Iteration time: 12.11s
                        Total time: 15948.78s
                               ETA: 1292413.9s

################################################################################
                    [1m Learning iteration 1219/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.520s, learning 0.221s)
               Value function loss: 25.0391
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 248.42
               Mean episode length: 122.66
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 19988480
                    Iteration time: 11.74s
                        Total time: 15960.52s
                               ETA: 1292292.1s

################################################################################
                    [1m Learning iteration 1220/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.866s, learning 0.166s)
               Value function loss: 46.1906
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 250.04
               Mean episode length: 124.17
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20004864
                    Iteration time: 12.03s
                        Total time: 15972.55s
                               ETA: 1292194.0s

################################################################################
                    [1m Learning iteration 1221/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.323s, learning 0.191s)
               Value function loss: 40.0082
                    Surrogate loss: -0.0032
             Mean action noise std: 0.78
                       Mean reward: 250.04
               Mean episode length: 123.63
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.27
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20021248
                    Iteration time: 11.51s
                        Total time: 15984.07s
                               ETA: 1292054.3s

################################################################################
                    [1m Learning iteration 1222/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.516s, learning 0.191s)
               Value function loss: 25.2033
                    Surrogate loss: -0.0177
             Mean action noise std: 0.78
                       Mean reward: 242.70
               Mean episode length: 120.65
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20037632
                    Iteration time: 11.71s
                        Total time: 15995.78s
                               ETA: 1291930.3s

################################################################################
                    [1m Learning iteration 1223/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.743s, learning 0.166s)
               Value function loss: 32.1475
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 242.96
               Mean episode length: 120.78
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20054016
                    Iteration time: 11.91s
                        Total time: 16007.69s
                               ETA: 1291822.8s

################################################################################
                    [1m Learning iteration 1224/100000 [0m                    

                       Computation: 1437 steps/s (collection: 11.234s, learning 0.165s)
               Value function loss: 22.7486
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 247.39
               Mean episode length: 122.79
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20070400
                    Iteration time: 11.40s
                        Total time: 16019.08s
                               ETA: 1291674.3s

################################################################################
                    [1m Learning iteration 1225/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.578s, learning 0.164s)
               Value function loss: 32.6479
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 251.29
               Mean episode length: 124.04
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20086784
                    Iteration time: 11.74s
                        Total time: 16030.83s
                               ETA: 1291553.7s

################################################################################
                    [1m Learning iteration 1226/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.346s, learning 0.169s)
               Value function loss: 28.8373
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 251.09
               Mean episode length: 124.42
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20103168
                    Iteration time: 11.51s
                        Total time: 16042.34s
                               ETA: 1291414.9s

################################################################################
                    [1m Learning iteration 1227/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.702s, learning 0.159s)
               Value function loss: 23.8481
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 249.48
               Mean episode length: 123.46
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20119552
                    Iteration time: 11.86s
                        Total time: 16054.20s
                               ETA: 1291304.3s

################################################################################
                    [1m Learning iteration 1228/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.862s, learning 0.234s)
               Value function loss: 23.7363
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 240.00
               Mean episode length: 120.70
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20135936
                    Iteration time: 12.10s
                        Total time: 16066.30s
                               ETA: 1291212.6s

################################################################################
                    [1m Learning iteration 1229/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.773s, learning 0.257s)
               Value function loss: 27.8151
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 246.60
               Mean episode length: 122.91
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20152320
                    Iteration time: 12.03s
                        Total time: 16078.33s
                               ETA: 1291115.9s

################################################################################
                    [1m Learning iteration 1230/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.929s, learning 0.164s)
               Value function loss: 27.5064
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 245.51
               Mean episode length: 121.92
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20168704
                    Iteration time: 12.09s
                        Total time: 16090.42s
                               ETA: 1291024.2s

################################################################################
                    [1m Learning iteration 1231/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.791s, learning 0.186s)
               Value function loss: 29.6645
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 241.28
               Mean episode length: 120.54
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20185088
                    Iteration time: 11.98s
                        Total time: 16102.40s
                               ETA: 1290923.5s

################################################################################
                    [1m Learning iteration 1232/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.637s, learning 0.159s)
               Value function loss: 26.0645
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 248.33
               Mean episode length: 123.45
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20201472
                    Iteration time: 11.80s
                        Total time: 16114.19s
                               ETA: 1290808.3s

################################################################################
                    [1m Learning iteration 1233/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.788s, learning 0.216s)
               Value function loss: 25.0394
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 246.32
               Mean episode length: 122.58
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20217856
                    Iteration time: 12.00s
                        Total time: 16126.20s
                               ETA: 1290710.0s

################################################################################
                    [1m Learning iteration 1234/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.886s, learning 0.168s)
               Value function loss: 35.5984
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 248.29
               Mean episode length: 124.29
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20234240
                    Iteration time: 12.05s
                        Total time: 16138.25s
                               ETA: 1290615.7s

################################################################################
                    [1m Learning iteration 1235/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.619s, learning 0.155s)
               Value function loss: 23.1953
                    Surrogate loss: -0.0171
             Mean action noise std: 0.78
                       Mean reward: 244.33
               Mean episode length: 121.30
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20250624
                    Iteration time: 11.77s
                        Total time: 16150.03s
                               ETA: 1290499.4s

################################################################################
                    [1m Learning iteration 1236/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.517s, learning 0.201s)
               Value function loss: 63.8989
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 247.68
               Mean episode length: 124.02
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.14
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20267008
                    Iteration time: 11.72s
                        Total time: 16161.74s
                               ETA: 1290378.7s

################################################################################
                    [1m Learning iteration 1237/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.731s, learning 0.364s)
               Value function loss: 23.1665
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 246.79
               Mean episode length: 122.89
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20283392
                    Iteration time: 12.10s
                        Total time: 16173.84s
                               ETA: 1290288.3s

################################################################################
                    [1m Learning iteration 1238/100000 [0m                    

                       Computation: 1291 steps/s (collection: 12.322s, learning 0.364s)
               Value function loss: 26.6194
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 248.80
               Mean episode length: 123.14
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20299776
                    Iteration time: 12.69s
                        Total time: 16186.53s
                               ETA: 1290245.0s

################################################################################
                    [1m Learning iteration 1239/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.758s, learning 0.238s)
               Value function loss: 27.8485
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 249.92
               Mean episode length: 123.18
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20316160
                    Iteration time: 12.00s
                        Total time: 16198.52s
                               ETA: 1290146.9s

################################################################################
                    [1m Learning iteration 1240/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.549s, learning 0.164s)
               Value function loss: 23.4077
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 242.31
               Mean episode length: 121.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20332544
                    Iteration time: 11.71s
                        Total time: 16210.23s
                               ETA: 1290026.3s

################################################################################
                    [1m Learning iteration 1241/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.048s, learning 0.186s)
               Value function loss: 25.3726
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 243.44
               Mean episode length: 122.17
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20348928
                    Iteration time: 12.23s
                        Total time: 16222.47s
                               ETA: 1289947.4s

################################################################################
                    [1m Learning iteration 1242/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.513s, learning 0.188s)
               Value function loss: 23.5207
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 243.99
               Mean episode length: 121.71
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20365312
                    Iteration time: 11.70s
                        Total time: 16234.17s
                               ETA: 1289826.2s

################################################################################
                    [1m Learning iteration 1243/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.849s, learning 0.203s)
               Value function loss: 20.6708
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 239.52
               Mean episode length: 120.74
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20381696
                    Iteration time: 12.05s
                        Total time: 16246.22s
                               ETA: 1289733.1s

################################################################################
                    [1m Learning iteration 1244/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.043s, learning 0.165s)
               Value function loss: 22.9194
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 247.98
               Mean episode length: 123.11
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20398080
                    Iteration time: 12.21s
                        Total time: 16258.43s
                               ETA: 1289652.5s

################################################################################
                    [1m Learning iteration 1245/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.327s, learning 0.167s)
               Value function loss: 22.5614
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 243.49
               Mean episode length: 121.46
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20414464
                    Iteration time: 12.49s
                        Total time: 16270.92s
                               ETA: 1289594.7s

################################################################################
                    [1m Learning iteration 1246/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.778s, learning 0.167s)
               Value function loss: 24.1382
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 244.72
               Mean episode length: 121.90
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20430848
                    Iteration time: 11.95s
                        Total time: 16282.87s
                               ETA: 1289493.5s

################################################################################
                    [1m Learning iteration 1247/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.886s, learning 0.170s)
               Value function loss: 23.6981
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 248.02
               Mean episode length: 123.38
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20447232
                    Iteration time: 12.06s
                        Total time: 16294.92s
                               ETA: 1289401.1s

################################################################################
                    [1m Learning iteration 1248/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.970s, learning 0.187s)
               Value function loss: 20.6264
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 252.07
               Mean episode length: 123.62
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20463616
                    Iteration time: 12.16s
                        Total time: 16307.08s
                               ETA: 1289316.9s

################################################################################
                    [1m Learning iteration 1249/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.745s, learning 0.196s)
               Value function loss: 41.2844
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 252.85
               Mean episode length: 124.42
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.39
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20480000
                    Iteration time: 11.94s
                        Total time: 16319.02s
                               ETA: 1289215.8s

################################################################################
                    [1m Learning iteration 1250/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.024s, learning 0.186s)
               Value function loss: 24.2002
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 250.88
               Mean episode length: 122.87
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20496384
                    Iteration time: 12.21s
                        Total time: 16331.23s
                               ETA: 1289136.0s

################################################################################
                    [1m Learning iteration 1251/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.479s, learning 0.166s)
               Value function loss: 26.8032
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 249.09
               Mean episode length: 122.63
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20512768
                    Iteration time: 11.64s
                        Total time: 16342.88s
                               ETA: 1289011.7s

################################################################################
                    [1m Learning iteration 1252/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.898s, learning 0.175s)
               Value function loss: 48.7965
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 246.86
               Mean episode length: 122.88
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.10
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20529152
                    Iteration time: 12.07s
                        Total time: 16354.95s
                               ETA: 1288921.5s

################################################################################
                    [1m Learning iteration 1253/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.770s, learning 0.272s)
               Value function loss: 25.1478
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 255.37
               Mean episode length: 123.58
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20545536
                    Iteration time: 12.04s
                        Total time: 16366.99s
                               ETA: 1288828.8s

################################################################################
                    [1m Learning iteration 1254/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.847s, learning 0.273s)
               Value function loss: 29.7130
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 242.46
               Mean episode length: 120.20
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20561920
                    Iteration time: 12.12s
                        Total time: 16379.11s
                               ETA: 1288742.5s

################################################################################
                    [1m Learning iteration 1255/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.800s, learning 0.205s)
               Value function loss: 29.6856
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 245.44
               Mean episode length: 120.89
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20578304
                    Iteration time: 12.01s
                        Total time: 16391.12s
                               ETA: 1288647.2s

################################################################################
                    [1m Learning iteration 1256/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.859s, learning 0.237s)
               Value function loss: 27.6878
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 250.88
               Mean episode length: 123.28
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20594688
                    Iteration time: 12.10s
                        Total time: 16403.21s
                               ETA: 1288559.2s

################################################################################
                    [1m Learning iteration 1257/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.591s, learning 0.174s)
               Value function loss: 27.4961
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 247.66
               Mean episode length: 121.86
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20611072
                    Iteration time: 11.76s
                        Total time: 16414.98s
                               ETA: 1288445.3s

################################################################################
                    [1m Learning iteration 1258/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.596s, learning 0.225s)
               Value function loss: 24.7768
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 250.14
               Mean episode length: 123.33
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20627456
                    Iteration time: 11.82s
                        Total time: 16426.80s
                               ETA: 1288336.0s

################################################################################
                    [1m Learning iteration 1259/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.728s, learning 0.266s)
               Value function loss: 24.8921
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 243.23
               Mean episode length: 119.74
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20643840
                    Iteration time: 11.99s
                        Total time: 16438.79s
                               ETA: 1288240.4s

################################################################################
                    [1m Learning iteration 1260/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.464s, learning 0.170s)
               Value function loss: 24.2068
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 248.52
               Mean episode length: 122.26
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20660224
                    Iteration time: 11.63s
                        Total time: 16450.43s
                               ETA: 1288116.6s

################################################################################
                    [1m Learning iteration 1261/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.412s, learning 0.182s)
               Value function loss: 24.3309
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 244.04
               Mean episode length: 119.80
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20676608
                    Iteration time: 11.59s
                        Total time: 16462.02s
                               ETA: 1287990.0s

################################################################################
                    [1m Learning iteration 1262/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.788s, learning 0.273s)
               Value function loss: 27.3105
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 247.12
               Mean episode length: 121.67
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20692992
                    Iteration time: 12.06s
                        Total time: 16474.08s
                               ETA: 1287900.1s

################################################################################
                    [1m Learning iteration 1263/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.531s, learning 0.164s)
               Value function loss: 24.9703
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 245.84
               Mean episode length: 120.87
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20709376
                    Iteration time: 11.70s
                        Total time: 16485.78s
                               ETA: 1287781.8s

################################################################################
                    [1m Learning iteration 1264/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.850s, learning 0.187s)
               Value function loss: 22.5443
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 233.08
               Mean episode length: 116.01
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20725760
                    Iteration time: 12.04s
                        Total time: 16497.81s
                               ETA: 1287690.2s

################################################################################
                    [1m Learning iteration 1265/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.108s, learning 0.241s)
               Value function loss: 38.2537
                    Surrogate loss: 0.0089
             Mean action noise std: 0.78
                       Mean reward: 249.66
               Mean episode length: 122.43
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20742144
                    Iteration time: 12.35s
                        Total time: 16510.16s
                               ETA: 1287623.1s

################################################################################
                    [1m Learning iteration 1266/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.469s, learning 0.167s)
               Value function loss: 23.4694
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 245.72
               Mean episode length: 121.25
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20758528
                    Iteration time: 11.64s
                        Total time: 16521.80s
                               ETA: 1287500.6s

################################################################################
                    [1m Learning iteration 1267/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.802s, learning 0.170s)
               Value function loss: 55.8577
                    Surrogate loss: -0.0062
             Mean action noise std: 0.78
                       Mean reward: 250.28
               Mean episode length: 123.12
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.34
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20774912
                    Iteration time: 11.97s
                        Total time: 16533.77s
                               ETA: 1287404.3s

################################################################################
                    [1m Learning iteration 1268/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.131s, learning 0.256s)
               Value function loss: 31.1839
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 248.42
               Mean episode length: 123.19
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20791296
                    Iteration time: 12.39s
                        Total time: 16546.16s
                               ETA: 1287340.5s

################################################################################
                    [1m Learning iteration 1269/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.879s, learning 0.171s)
               Value function loss: 24.3798
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 249.23
               Mean episode length: 122.88
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20807680
                    Iteration time: 12.05s
                        Total time: 16558.21s
                               ETA: 1287250.6s

################################################################################
                    [1m Learning iteration 1270/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.542s, learning 0.156s)
               Value function loss: 30.4279
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 250.50
               Mean episode length: 123.18
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20824064
                    Iteration time: 11.70s
                        Total time: 16569.90s
                               ETA: 1287133.5s

################################################################################
                    [1m Learning iteration 1271/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.660s, learning 0.197s)
               Value function loss: 21.0680
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 251.22
               Mean episode length: 123.40
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20840448
                    Iteration time: 11.86s
                        Total time: 16581.76s
                               ETA: 1287028.8s

################################################################################
                    [1m Learning iteration 1272/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.534s, learning 0.172s)
               Value function loss: 28.1895
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 244.14
               Mean episode length: 120.27
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20856832
                    Iteration time: 11.71s
                        Total time: 16593.47s
                               ETA: 1286912.7s

################################################################################
                    [1m Learning iteration 1273/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.935s, learning 0.252s)
               Value function loss: 27.7872
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 251.63
               Mean episode length: 122.32
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20873216
                    Iteration time: 12.19s
                        Total time: 16605.65s
                               ETA: 1286834.0s

################################################################################
                    [1m Learning iteration 1274/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.913s, learning 0.208s)
               Value function loss: 23.1180
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 245.60
               Mean episode length: 121.61
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20889600
                    Iteration time: 12.12s
                        Total time: 16617.78s
                               ETA: 1286750.2s

################################################################################
                    [1m Learning iteration 1275/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.850s, learning 0.161s)
               Value function loss: 24.5705
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 245.07
               Mean episode length: 122.31
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20905984
                    Iteration time: 12.01s
                        Total time: 16629.79s
                               ETA: 1286658.1s

################################################################################
                    [1m Learning iteration 1276/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.569s, learning 0.159s)
               Value function loss: 27.3330
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 241.10
               Mean episode length: 119.36
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20922368
                    Iteration time: 11.73s
                        Total time: 16641.52s
                               ETA: 1286544.2s

################################################################################
                    [1m Learning iteration 1277/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.274s, learning 0.187s)
               Value function loss: 23.4532
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 243.82
               Mean episode length: 121.50
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20938752
                    Iteration time: 12.46s
                        Total time: 16653.98s
                               ETA: 1286487.1s

################################################################################
                    [1m Learning iteration 1278/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.425s, learning 0.184s)
               Value function loss: 24.5683
                    Surrogate loss: -0.0153
             Mean action noise std: 0.78
                       Mean reward: 243.89
               Mean episode length: 120.03
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20955136
                    Iteration time: 11.61s
                        Total time: 16665.58s
                               ETA: 1286364.2s

################################################################################
                    [1m Learning iteration 1279/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.937s, learning 0.166s)
               Value function loss: 22.8023
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 248.58
               Mean episode length: 122.20
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20971520
                    Iteration time: 12.10s
                        Total time: 16677.69s
                               ETA: 1286279.7s

################################################################################
                    [1m Learning iteration 1280/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.728s, learning 0.171s)
               Value function loss: 23.5213
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 239.77
               Mean episode length: 119.24
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 20987904
                    Iteration time: 11.90s
                        Total time: 16689.59s
                               ETA: 1286179.6s

################################################################################
                    [1m Learning iteration 1281/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.025s, learning 0.158s)
               Value function loss: 25.8551
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 248.74
               Mean episode length: 122.58
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21004288
                    Iteration time: 12.18s
                        Total time: 16701.77s
                               ETA: 1286101.4s

################################################################################
                    [1m Learning iteration 1282/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.668s, learning 0.250s)
               Value function loss: 24.5429
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 243.52
               Mean episode length: 120.53
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21020672
                    Iteration time: 11.92s
                        Total time: 16713.69s
                               ETA: 1286003.0s

################################################################################
                    [1m Learning iteration 1283/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.563s, learning 0.198s)
               Value function loss: 55.8843
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 248.31
               Mean episode length: 122.09
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.12
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21037056
                    Iteration time: 11.76s
                        Total time: 16725.45s
                               ETA: 1285892.6s

################################################################################
                    [1m Learning iteration 1284/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.850s, learning 0.290s)
               Value function loss: 29.2850
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 243.30
               Mean episode length: 121.15
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21053440
                    Iteration time: 12.14s
                        Total time: 16737.59s
                               ETA: 1285811.5s

################################################################################
                    [1m Learning iteration 1285/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.578s, learning 0.165s)
               Value function loss: 26.3292
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 247.24
               Mean episode length: 122.36
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21069824
                    Iteration time: 11.74s
                        Total time: 16749.33s
                               ETA: 1285700.1s

################################################################################
                    [1m Learning iteration 1286/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.618s, learning 0.163s)
               Value function loss: 35.4089
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 242.74
               Mean episode length: 121.31
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21086208
                    Iteration time: 11.78s
                        Total time: 16761.11s
                               ETA: 1285591.7s

################################################################################
                    [1m Learning iteration 1287/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.412s, learning 0.164s)
               Value function loss: 20.9945
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 242.84
               Mean episode length: 120.54
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21102592
                    Iteration time: 11.58s
                        Total time: 16772.69s
                               ETA: 1285467.8s

################################################################################
                    [1m Learning iteration 1288/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.920s, learning 0.256s)
               Value function loss: 25.6766
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 243.50
               Mean episode length: 120.77
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21118976
                    Iteration time: 12.18s
                        Total time: 16784.87s
                               ETA: 1285390.0s

################################################################################
                    [1m Learning iteration 1289/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.790s, learning 0.158s)
               Value function loss: 25.1356
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 243.82
               Mean episode length: 120.55
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21135360
                    Iteration time: 11.95s
                        Total time: 16796.81s
                               ETA: 1285294.8s

################################################################################
                    [1m Learning iteration 1290/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.841s, learning 0.269s)
               Value function loss: 25.5193
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 244.35
               Mean episode length: 121.13
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21151744
                    Iteration time: 12.11s
                        Total time: 16808.92s
                               ETA: 1285212.2s

################################################################################
                    [1m Learning iteration 1291/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.658s, learning 0.246s)
               Value function loss: 22.3741
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 238.39
               Mean episode length: 119.67
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21168128
                    Iteration time: 11.90s
                        Total time: 16820.83s
                               ETA: 1285113.9s

################################################################################
                    [1m Learning iteration 1292/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.879s, learning 0.164s)
               Value function loss: 21.8852
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 246.36
               Mean episode length: 121.27
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21184512
                    Iteration time: 12.04s
                        Total time: 16832.87s
                               ETA: 1285026.4s

################################################################################
                    [1m Learning iteration 1293/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.595s, learning 0.157s)
               Value function loss: 26.4103
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 245.94
               Mean episode length: 121.29
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21200896
                    Iteration time: 11.75s
                        Total time: 16844.62s
                               ETA: 1284916.7s

################################################################################
                    [1m Learning iteration 1294/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.928s, learning 0.214s)
               Value function loss: 21.3182
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 243.84
               Mean episode length: 120.91
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21217280
                    Iteration time: 12.14s
                        Total time: 16856.77s
                               ETA: 1284837.0s

################################################################################
                    [1m Learning iteration 1295/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.519s, learning 0.163s)
               Value function loss: 23.1465
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 244.75
               Mean episode length: 120.95
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21233664
                    Iteration time: 11.68s
                        Total time: 16868.45s
                               ETA: 1284722.3s

################################################################################
                    [1m Learning iteration 1296/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.477s, learning 0.162s)
               Value function loss: 38.8347
                    Surrogate loss: 0.0010
             Mean action noise std: 0.78
                       Mean reward: 249.13
               Mean episode length: 123.60
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21250048
                    Iteration time: 11.64s
                        Total time: 16880.09s
                               ETA: 1284604.6s

################################################################################
                    [1m Learning iteration 1297/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.220s, learning 0.162s)
               Value function loss: 27.2724
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 244.14
               Mean episode length: 121.27
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21266432
                    Iteration time: 12.38s
                        Total time: 16892.47s
                               ETA: 1284543.5s

################################################################################
                    [1m Learning iteration 1298/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.541s, learning 0.163s)
               Value function loss: 31.2666
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 246.52
               Mean episode length: 122.16
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21282816
                    Iteration time: 11.70s
                        Total time: 16904.17s
                               ETA: 1284430.9s

################################################################################
                    [1m Learning iteration 1299/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.710s, learning 0.215s)
               Value function loss: 36.4731
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 247.77
               Mean episode length: 122.78
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21299200
                    Iteration time: 11.92s
                        Total time: 16916.10s
                               ETA: 1284335.2s

################################################################################
                    [1m Learning iteration 1300/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.710s, learning 0.181s)
               Value function loss: 22.1147
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 247.58
               Mean episode length: 122.37
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21315584
                    Iteration time: 11.89s
                        Total time: 16927.99s
                               ETA: 1284237.2s

################################################################################
                    [1m Learning iteration 1301/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.604s, learning 0.258s)
               Value function loss: 25.2859
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 245.73
               Mean episode length: 121.88
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21331968
                    Iteration time: 11.86s
                        Total time: 16939.85s
                               ETA: 1284137.0s

################################################################################
                    [1m Learning iteration 1302/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.451s, learning 0.194s)
               Value function loss: 23.9879
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 249.89
               Mean episode length: 122.93
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21348352
                    Iteration time: 11.64s
                        Total time: 16951.50s
                               ETA: 1284020.5s

################################################################################
                    [1m Learning iteration 1303/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.679s, learning 0.173s)
               Value function loss: 24.2313
                    Surrogate loss: -0.0045
             Mean action noise std: 0.78
                       Mean reward: 248.60
               Mean episode length: 122.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21364736
                    Iteration time: 11.85s
                        Total time: 16963.35s
                               ETA: 1283919.9s

################################################################################
                    [1m Learning iteration 1304/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.960s, learning 0.193s)
               Value function loss: 25.9603
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 246.64
               Mean episode length: 122.14
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21381120
                    Iteration time: 12.15s
                        Total time: 16975.50s
                               ETA: 1283842.2s

################################################################################
                    [1m Learning iteration 1305/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.971s, learning 0.173s)
               Value function loss: 25.6816
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 243.90
               Mean episode length: 120.87
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21397504
                    Iteration time: 12.14s
                        Total time: 16987.65s
                               ETA: 1283763.9s

################################################################################
                    [1m Learning iteration 1306/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.724s, learning 0.165s)
               Value function loss: 27.8026
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 242.55
               Mean episode length: 120.25
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21413888
                    Iteration time: 11.89s
                        Total time: 16999.54s
                               ETA: 1283666.5s

################################################################################
                    [1m Learning iteration 1307/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.008s, learning 0.216s)
               Value function loss: 26.2678
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 239.61
               Mean episode length: 118.58
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21430272
                    Iteration time: 12.22s
                        Total time: 17011.76s
                               ETA: 1283594.4s

################################################################################
                    [1m Learning iteration 1308/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.054s, learning 0.159s)
               Value function loss: 26.2929
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 250.15
               Mean episode length: 122.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21446656
                    Iteration time: 12.21s
                        Total time: 17023.97s
                               ETA: 1283521.6s

################################################################################
                    [1m Learning iteration 1309/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.040s, learning 0.288s)
               Value function loss: 27.9485
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 244.20
               Mean episode length: 120.39
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21463040
                    Iteration time: 12.33s
                        Total time: 17036.30s
                               ETA: 1283457.7s

################################################################################
                    [1m Learning iteration 1310/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.783s, learning 0.206s)
               Value function loss: 26.8538
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 240.19
               Mean episode length: 119.06
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21479424
                    Iteration time: 11.99s
                        Total time: 17048.29s
                               ETA: 1283368.2s

################################################################################
                    [1m Learning iteration 1311/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.830s, learning 0.214s)
               Value function loss: 27.2698
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 240.04
               Mean episode length: 119.55
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21495808
                    Iteration time: 12.04s
                        Total time: 17060.33s
                               ETA: 1283283.0s

################################################################################
                    [1m Learning iteration 1312/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.228s, learning 0.165s)
               Value function loss: 34.1172
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 239.68
               Mean episode length: 120.34
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.42
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21512192
                    Iteration time: 12.39s
                        Total time: 17072.73s
                               ETA: 1283224.2s

################################################################################
                    [1m Learning iteration 1313/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.582s, learning 0.163s)
               Value function loss: 24.7438
                    Surrogate loss: -0.0163
             Mean action noise std: 0.78
                       Mean reward: 248.24
               Mean episode length: 122.49
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21528576
                    Iteration time: 11.75s
                        Total time: 17084.47s
                               ETA: 1283116.7s

################################################################################
                    [1m Learning iteration 1314/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.776s, learning 0.170s)
               Value function loss: 48.2334
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 247.42
               Mean episode length: 124.41
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21544960
                    Iteration time: 11.95s
                        Total time: 17096.42s
                               ETA: 1283024.4s

################################################################################
                    [1m Learning iteration 1315/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.791s, learning 0.200s)
               Value function loss: 25.0887
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 247.35
               Mean episode length: 122.01
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21561344
                    Iteration time: 11.99s
                        Total time: 17108.41s
                               ETA: 1282935.7s

################################################################################
                    [1m Learning iteration 1316/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.164s)
               Value function loss: 24.7482
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 240.40
               Mean episode length: 120.82
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21577728
                    Iteration time: 11.64s
                        Total time: 17120.05s
                               ETA: 1282820.5s

################################################################################
                    [1m Learning iteration 1317/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.774s, learning 0.160s)
               Value function loss: 20.7734
                    Surrogate loss: 0.0075
             Mean action noise std: 0.78
                       Mean reward: 249.77
               Mean episode length: 124.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21594112
                    Iteration time: 11.93s
                        Total time: 17131.98s
                               ETA: 1282727.8s

################################################################################
                    [1m Learning iteration 1318/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.792s, learning 0.185s)
               Value function loss: 17.3200
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 243.10
               Mean episode length: 122.51
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.69
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21610496
                    Iteration time: 11.98s
                        Total time: 17143.96s
                               ETA: 1282638.3s

################################################################################
                    [1m Learning iteration 1319/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.571s, learning 0.234s)
               Value function loss: 19.7427
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 241.76
               Mean episode length: 121.69
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21626880
                    Iteration time: 11.81s
                        Total time: 17155.76s
                               ETA: 1282536.2s

################################################################################
                    [1m Learning iteration 1320/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.114s, learning 0.169s)
               Value function loss: 21.6249
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 238.60
               Mean episode length: 119.60
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21643264
                    Iteration time: 12.28s
                        Total time: 17168.05s
                               ETA: 1282469.9s

################################################################################
                    [1m Learning iteration 1321/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.385s, learning 0.159s)
               Value function loss: 20.3815
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 249.41
               Mean episode length: 123.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21659648
                    Iteration time: 11.54s
                        Total time: 17179.59s
                               ETA: 1282348.5s

################################################################################
                    [1m Learning iteration 1322/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.440s, learning 0.165s)
               Value function loss: 20.4763
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 248.25
               Mean episode length: 123.25
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21676032
                    Iteration time: 11.61s
                        Total time: 17191.20s
                               ETA: 1282231.9s

################################################################################
                    [1m Learning iteration 1323/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.744s, learning 0.165s)
               Value function loss: 22.2859
                    Surrogate loss: -0.0031
             Mean action noise std: 0.78
                       Mean reward: 245.36
               Mean episode length: 122.54
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21692416
                    Iteration time: 11.91s
                        Total time: 17203.10s
                               ETA: 1282138.0s

################################################################################
                    [1m Learning iteration 1324/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.440s, learning 0.165s)
               Value function loss: 24.8422
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 238.52
               Mean episode length: 119.57
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21708800
                    Iteration time: 11.60s
                        Total time: 17214.71s
                               ETA: 1282021.6s

################################################################################
                    [1m Learning iteration 1325/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.451s, learning 0.162s)
               Value function loss: 24.6635
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 243.70
               Mean episode length: 122.57
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21725184
                    Iteration time: 11.61s
                        Total time: 17226.32s
                               ETA: 1281906.0s

################################################################################
                    [1m Learning iteration 1326/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.705s, learning 0.166s)
               Value function loss: 23.6499
                    Surrogate loss: -0.0165
             Mean action noise std: 0.78
                       Mean reward: 244.47
               Mean episode length: 121.47
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21741568
                    Iteration time: 11.87s
                        Total time: 17238.19s
                               ETA: 1281809.7s

################################################################################
                    [1m Learning iteration 1327/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.228s, learning 0.229s)
               Value function loss: 33.2684
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 249.13
               Mean episode length: 124.18
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21757952
                    Iteration time: 12.46s
                        Total time: 17250.65s
                               ETA: 1281757.1s

################################################################################
                    [1m Learning iteration 1328/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.885s, learning 0.169s)
               Value function loss: 25.5342
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 237.18
               Mean episode length: 120.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21774336
                    Iteration time: 12.05s
                        Total time: 17262.70s
                               ETA: 1281674.5s

################################################################################
                    [1m Learning iteration 1329/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.803s, learning 0.163s)
               Value function loss: 26.8231
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 247.73
               Mean episode length: 122.75
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21790720
                    Iteration time: 11.97s
                        Total time: 17274.67s
                               ETA: 1281585.6s

################################################################################
                    [1m Learning iteration 1330/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.728s, learning 0.202s)
               Value function loss: 37.1974
                    Surrogate loss: -0.0036
             Mean action noise std: 0.78
                       Mean reward: 245.53
               Mean episode length: 123.39
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.24
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21807104
                    Iteration time: 11.93s
                        Total time: 17286.60s
                               ETA: 1281494.1s

################################################################################
                    [1m Learning iteration 1331/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.296s, learning 0.164s)
               Value function loss: 22.9294
                    Surrogate loss: -0.0075
             Mean action noise std: 0.78
                       Mean reward: 239.55
               Mean episode length: 119.94
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21823488
                    Iteration time: 12.46s
                        Total time: 17299.06s
                               ETA: 1281442.1s

################################################################################
                    [1m Learning iteration 1332/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.603s, learning 0.166s)
               Value function loss: 21.1474
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 238.32
               Mean episode length: 119.63
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21839872
                    Iteration time: 11.77s
                        Total time: 17310.83s
                               ETA: 1281338.9s

################################################################################
                    [1m Learning iteration 1333/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.346s, learning 0.174s)
               Value function loss: 22.6851
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 241.13
               Mean episode length: 121.61
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21856256
                    Iteration time: 11.52s
                        Total time: 17322.35s
                               ETA: 1281217.5s

################################################################################
                    [1m Learning iteration 1334/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.769s, learning 0.160s)
               Value function loss: 21.8296
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 240.16
               Mean episode length: 120.14
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21872640
                    Iteration time: 11.93s
                        Total time: 17334.28s
                               ETA: 1281126.4s

################################################################################
                    [1m Learning iteration 1335/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.073s, learning 0.210s)
               Value function loss: 22.9928
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 237.73
               Mean episode length: 119.56
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21889024
                    Iteration time: 12.28s
                        Total time: 17346.56s
                               ETA: 1281061.7s

################################################################################
                    [1m Learning iteration 1336/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.938s, learning 0.161s)
               Value function loss: 23.4642
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 235.42
               Mean episode length: 119.19
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21905408
                    Iteration time: 12.10s
                        Total time: 17358.66s
                               ETA: 1280983.4s

################################################################################
                    [1m Learning iteration 1337/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.995s, learning 0.159s)
               Value function loss: 27.0233
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 248.26
               Mean episode length: 123.03
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21921792
                    Iteration time: 12.15s
                        Total time: 17370.81s
                               ETA: 1280909.3s

################################################################################
                    [1m Learning iteration 1338/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.691s, learning 0.233s)
               Value function loss: 23.3954
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 232.49
               Mean episode length: 120.51
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21938176
                    Iteration time: 11.92s
                        Total time: 17382.74s
                               ETA: 1280818.4s

################################################################################
                    [1m Learning iteration 1339/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.380s, learning 0.160s)
               Value function loss: 23.4315
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 236.38
               Mean episode length: 119.99
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21954560
                    Iteration time: 12.54s
                        Total time: 17395.28s
                               ETA: 1280772.8s

################################################################################
                    [1m Learning iteration 1340/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.679s, learning 0.169s)
               Value function loss: 27.4442
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 238.03
               Mean episode length: 120.59
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21970944
                    Iteration time: 11.85s
                        Total time: 17407.13s
                               ETA: 1280676.5s

################################################################################
                    [1m Learning iteration 1341/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.639s, learning 0.208s)
               Value function loss: 22.6572
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 232.70
               Mean episode length: 118.78
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 21987328
                    Iteration time: 11.85s
                        Total time: 17418.97s
                               ETA: 1280580.2s

################################################################################
                    [1m Learning iteration 1342/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.092s, learning 0.344s)
               Value function loss: 25.4153
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 229.91
               Mean episode length: 118.40
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22003712
                    Iteration time: 12.44s
                        Total time: 17431.41s
                               ETA: 1280527.2s

################################################################################
                    [1m Learning iteration 1343/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.228s, learning 0.243s)
               Value function loss: 34.4698
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 243.91
               Mean episode length: 124.33
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22020096
                    Iteration time: 12.47s
                        Total time: 17443.88s
                               ETA: 1280477.0s

################################################################################
                    [1m Learning iteration 1344/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.538s, learning 0.154s)
               Value function loss: 26.5962
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 237.49
               Mean episode length: 120.86
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22036480
                    Iteration time: 11.69s
                        Total time: 17455.57s
                               ETA: 1280369.6s

################################################################################
                    [1m Learning iteration 1345/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.985s, learning 0.165s)
               Value function loss: 38.6946
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 244.73
               Mean episode length: 123.45
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22052864
                    Iteration time: 12.15s
                        Total time: 17467.72s
                               ETA: 1280296.0s

################################################################################
                    [1m Learning iteration 1346/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.431s, learning 0.326s)
               Value function loss: 34.0960
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 246.85
               Mean episode length: 123.45
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.44
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22069248
                    Iteration time: 12.76s
                        Total time: 17480.48s
                               ETA: 1280266.9s

################################################################################
                    [1m Learning iteration 1347/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.927s, learning 0.229s)
               Value function loss: 28.2476
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 235.90
               Mean episode length: 119.22
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22085632
                    Iteration time: 12.16s
                        Total time: 17492.64s
                               ETA: 1280193.8s

################################################################################
                    [1m Learning iteration 1348/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.708s, learning 0.217s)
               Value function loss: 25.9999
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 239.42
               Mean episode length: 122.15
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22102016
                    Iteration time: 11.93s
                        Total time: 17504.56s
                               ETA: 1280103.9s

################################################################################
                    [1m Learning iteration 1349/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.070s, learning 0.166s)
               Value function loss: 22.5629
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 234.16
               Mean episode length: 119.85
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22118400
                    Iteration time: 12.24s
                        Total time: 17516.80s
                               ETA: 1280036.8s

################################################################################
                    [1m Learning iteration 1350/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.541s, learning 0.160s)
               Value function loss: 28.0164
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 243.98
               Mean episode length: 123.62
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22134784
                    Iteration time: 11.70s
                        Total time: 17528.50s
                               ETA: 1279930.8s

################################################################################
                    [1m Learning iteration 1351/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.077s, learning 0.237s)
               Value function loss: 33.7574
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 239.74
               Mean episode length: 122.01
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22151168
                    Iteration time: 12.31s
                        Total time: 17540.81s
                               ETA: 1279869.7s

################################################################################
                    [1m Learning iteration 1352/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.791s, learning 0.164s)
               Value function loss: 28.6094
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 241.90
               Mean episode length: 122.53
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22167552
                    Iteration time: 11.95s
                        Total time: 17552.77s
                               ETA: 1279782.4s

################################################################################
                    [1m Learning iteration 1353/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.901s, learning 0.217s)
               Value function loss: 27.1062
                    Surrogate loss: -0.0074
             Mean action noise std: 0.78
                       Mean reward: 236.50
               Mean episode length: 121.33
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22183936
                    Iteration time: 12.12s
                        Total time: 17564.89s
                               ETA: 1279707.1s

################################################################################
                    [1m Learning iteration 1354/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.686s, learning 0.158s)
               Value function loss: 30.4202
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 237.37
               Mean episode length: 121.01
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22200320
                    Iteration time: 11.84s
                        Total time: 17576.73s
                               ETA: 1279611.9s

################################################################################
                    [1m Learning iteration 1355/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.178s)
               Value function loss: 24.2573
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 241.52
               Mean episode length: 122.65
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22216704
                    Iteration time: 11.50s
                        Total time: 17588.24s
                               ETA: 1279492.2s

################################################################################
                    [1m Learning iteration 1356/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.683s, learning 0.153s)
               Value function loss: 26.2295
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 240.89
               Mean episode length: 121.38
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22233088
                    Iteration time: 11.84s
                        Total time: 17600.07s
                               ETA: 1279396.8s

################################################################################
                    [1m Learning iteration 1357/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.920s, learning 0.210s)
               Value function loss: 24.5349
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 242.92
               Mean episode length: 121.58
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22249472
                    Iteration time: 12.13s
                        Total time: 17612.20s
                               ETA: 1279322.8s

################################################################################
                    [1m Learning iteration 1358/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.687s, learning 0.221s)
               Value function loss: 25.5526
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 244.83
               Mean episode length: 123.63
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22265856
                    Iteration time: 11.91s
                        Total time: 17624.11s
                               ETA: 1279232.9s

################################################################################
                    [1m Learning iteration 1359/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.489s, learning 0.195s)
               Value function loss: 24.8437
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 239.74
               Mean episode length: 123.27
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22282240
                    Iteration time: 11.68s
                        Total time: 17635.79s
                               ETA: 1279126.7s

################################################################################
                    [1m Learning iteration 1360/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.913s, learning 0.160s)
               Value function loss: 25.3614
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 236.65
               Mean episode length: 121.90
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22298624
                    Iteration time: 12.07s
                        Total time: 17647.87s
                               ETA: 1279048.9s

################################################################################
                    [1m Learning iteration 1361/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.643s, learning 0.166s)
               Value function loss: 42.2748
                    Surrogate loss: -0.0005
             Mean action noise std: 0.78
                       Mean reward: 240.10
               Mean episode length: 122.50
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.29
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22315008
                    Iteration time: 11.81s
                        Total time: 17659.68s
                               ETA: 1278952.1s

################################################################################
                    [1m Learning iteration 1362/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.724s, learning 0.163s)
               Value function loss: 23.1562
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 227.86
               Mean episode length: 118.57
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22331392
                    Iteration time: 11.89s
                        Total time: 17671.56s
                               ETA: 1278861.0s

################################################################################
                    [1m Learning iteration 1363/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.835s, learning 0.165s)
               Value function loss: 24.6641
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 237.92
               Mean episode length: 119.82
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22347776
                    Iteration time: 12.00s
                        Total time: 17683.56s
                               ETA: 1278778.3s

################################################################################
                    [1m Learning iteration 1364/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.741s, learning 0.170s)
               Value function loss: 22.5530
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 236.08
               Mean episode length: 120.95
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22364160
                    Iteration time: 11.91s
                        Total time: 17695.47s
                               ETA: 1278689.2s

################################################################################
                    [1m Learning iteration 1365/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.695s, learning 0.157s)
               Value function loss: 19.7101
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 235.11
               Mean episode length: 119.64
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22380544
                    Iteration time: 11.85s
                        Total time: 17707.33s
                               ETA: 1278596.0s

################################################################################
                    [1m Learning iteration 1366/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.908s, learning 0.197s)
               Value function loss: 20.7471
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 234.97
               Mean episode length: 121.46
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22396928
                    Iteration time: 12.11s
                        Total time: 17719.43s
                               ETA: 1278521.1s

################################################################################
                    [1m Learning iteration 1367/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.166s, learning 0.200s)
               Value function loss: 24.0348
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 239.41
               Mean episode length: 122.29
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22413312
                    Iteration time: 12.37s
                        Total time: 17731.80s
                               ETA: 1278465.2s

################################################################################
                    [1m Learning iteration 1368/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.682s, learning 0.172s)
               Value function loss: 25.2951
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 241.12
               Mean episode length: 122.83
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22429696
                    Iteration time: 11.85s
                        Total time: 17743.65s
                               ETA: 1278372.4s

################################################################################
                    [1m Learning iteration 1369/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.294s, learning 0.234s)
               Value function loss: 22.7763
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 232.81
               Mean episode length: 120.23
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22446080
                    Iteration time: 12.53s
                        Total time: 17756.18s
                               ETA: 1278328.2s

################################################################################
                    [1m Learning iteration 1370/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.038s, learning 0.164s)
               Value function loss: 22.3000
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 233.56
               Mean episode length: 122.65
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22462464
                    Iteration time: 12.20s
                        Total time: 17768.38s
                               ETA: 1278260.6s

################################################################################
                    [1m Learning iteration 1371/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.125s, learning 0.160s)
               Value function loss: 25.9820
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 238.03
               Mean episode length: 123.17
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22478848
                    Iteration time: 12.28s
                        Total time: 17780.66s
                               ETA: 1278199.1s

################################################################################
                    [1m Learning iteration 1372/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.509s, learning 0.194s)
               Value function loss: 24.3685
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 236.97
               Mean episode length: 122.20
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22495232
                    Iteration time: 11.70s
                        Total time: 17792.37s
                               ETA: 1278095.9s

################################################################################
                    [1m Learning iteration 1373/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.623s, learning 0.179s)
               Value function loss: 22.9792
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 236.62
               Mean episode length: 122.37
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22511616
                    Iteration time: 11.80s
                        Total time: 17804.17s
                               ETA: 1277999.9s

################################################################################
                    [1m Learning iteration 1374/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.775s, learning 0.268s)
               Value function loss: 33.9807
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 237.88
               Mean episode length: 122.65
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22528000
                    Iteration time: 12.04s
                        Total time: 17816.21s
                               ETA: 1277921.4s

################################################################################
                    [1m Learning iteration 1375/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.853s, learning 0.163s)
               Value function loss: 25.4056
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 231.32
               Mean episode length: 120.80
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22544384
                    Iteration time: 12.02s
                        Total time: 17828.23s
                               ETA: 1277841.0s

################################################################################
                    [1m Learning iteration 1376/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.732s, learning 0.171s)
               Value function loss: 30.6490
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 236.04
               Mean episode length: 121.14
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22560768
                    Iteration time: 11.90s
                        Total time: 17840.13s
                               ETA: 1277752.5s

################################################################################
                    [1m Learning iteration 1377/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.498s, learning 0.191s)
               Value function loss: 37.3625
                    Surrogate loss: -0.0021
             Mean action noise std: 0.78
                       Mean reward: 238.33
               Mean episode length: 122.62
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.33
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22577152
                    Iteration time: 11.69s
                        Total time: 17851.82s
                               ETA: 1277648.9s

################################################################################
                    [1m Learning iteration 1378/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.061s, learning 0.217s)
               Value function loss: 27.1961
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 238.33
               Mean episode length: 121.92
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22593536
                    Iteration time: 12.28s
                        Total time: 17864.10s
                               ETA: 1277587.5s

################################################################################
                    [1m Learning iteration 1379/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.452s, learning 0.199s)
               Value function loss: 24.1393
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 239.37
               Mean episode length: 122.45
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22609920
                    Iteration time: 11.65s
                        Total time: 17875.75s
                               ETA: 1277481.4s

################################################################################
                    [1m Learning iteration 1380/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.804s, learning 0.165s)
               Value function loss: 23.3773
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 244.32
               Mean episode length: 123.76
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22626304
                    Iteration time: 11.97s
                        Total time: 17887.72s
                               ETA: 1277398.2s

################################################################################
                    [1m Learning iteration 1381/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.549s, learning 0.194s)
               Value function loss: 23.7009
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 239.80
               Mean episode length: 122.13
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22642688
                    Iteration time: 11.74s
                        Total time: 17899.46s
                               ETA: 1277298.9s

################################################################################
                    [1m Learning iteration 1382/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.092s, learning 0.158s)
               Value function loss: 23.5858
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 242.95
               Mean episode length: 123.64
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22659072
                    Iteration time: 12.25s
                        Total time: 17911.71s
                               ETA: 1277235.9s

################################################################################
                    [1m Learning iteration 1383/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.684s, learning 0.166s)
               Value function loss: 25.5711
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 244.91
               Mean episode length: 124.58
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22675456
                    Iteration time: 11.85s
                        Total time: 17923.56s
                               ETA: 1277144.4s

################################################################################
                    [1m Learning iteration 1384/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.851s, learning 0.243s)
               Value function loss: 26.6944
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 240.17
               Mean episode length: 123.84
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22691840
                    Iteration time: 12.09s
                        Total time: 17935.66s
                               ETA: 1277070.5s

################################################################################
                    [1m Learning iteration 1385/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.539s, learning 0.167s)
               Value function loss: 25.4379
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 240.06
               Mean episode length: 123.44
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22708224
                    Iteration time: 11.71s
                        Total time: 17947.36s
                               ETA: 1276969.0s

################################################################################
                    [1m Learning iteration 1386/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.283s, learning 0.176s)
               Value function loss: 24.1984
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 237.75
               Mean episode length: 121.85
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22724608
                    Iteration time: 11.46s
                        Total time: 17958.82s
                               ETA: 1276850.1s

################################################################################
                    [1m Learning iteration 1387/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.517s, learning 0.263s)
               Value function loss: 30.1102
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 238.61
               Mean episode length: 121.27
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22740992
                    Iteration time: 11.78s
                        Total time: 17970.60s
                               ETA: 1276754.2s

################################################################################
                    [1m Learning iteration 1388/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.921s, learning 0.275s)
               Value function loss: 23.8448
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 245.64
               Mean episode length: 124.83
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22757376
                    Iteration time: 12.20s
                        Total time: 17982.80s
                               ETA: 1276687.9s

################################################################################
                    [1m Learning iteration 1389/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.620s, learning 0.164s)
               Value function loss: 21.8063
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 241.11
               Mean episode length: 123.65
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22773760
                    Iteration time: 11.78s
                        Total time: 17994.58s
                               ETA: 1276592.5s

################################################################################
                    [1m Learning iteration 1390/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.685s, learning 0.166s)
               Value function loss: 33.8502
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 241.65
               Mean episode length: 124.03
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22790144
                    Iteration time: 11.85s
                        Total time: 18006.43s
                               ETA: 1276501.9s

################################################################################
                    [1m Learning iteration 1391/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.155s, learning 0.157s)
               Value function loss: 27.5782
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 240.83
               Mean episode length: 122.59
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22806528
                    Iteration time: 11.31s
                        Total time: 18017.74s
                               ETA: 1276373.3s

################################################################################
                    [1m Learning iteration 1392/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.953s, learning 0.266s)
               Value function loss: 46.5463
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 242.53
               Mean episode length: 124.13
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22822912
                    Iteration time: 12.22s
                        Total time: 18029.96s
                               ETA: 1276309.0s

################################################################################
                    [1m Learning iteration 1393/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.723s, learning 0.161s)
               Value function loss: 29.5229
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 253.52
               Mean episode length: 124.96
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22839296
                    Iteration time: 11.88s
                        Total time: 18041.85s
                               ETA: 1276221.1s

################################################################################
                    [1m Learning iteration 1394/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.506s, learning 0.160s)
               Value function loss: 26.9861
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 244.28
               Mean episode length: 122.51
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22855680
                    Iteration time: 11.67s
                        Total time: 18053.51s
                               ETA: 1276117.9s

################################################################################
                    [1m Learning iteration 1395/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.964s, learning 0.185s)
               Value function loss: 27.4566
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 236.84
               Mean episode length: 121.23
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22872064
                    Iteration time: 12.15s
                        Total time: 18065.66s
                               ETA: 1276049.0s

################################################################################
                    [1m Learning iteration 1396/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.342s, learning 0.172s)
               Value function loss: 18.6935
                    Surrogate loss: -0.0172
             Mean action noise std: 0.78
                       Mean reward: 240.48
               Mean episode length: 123.18
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22888448
                    Iteration time: 11.51s
                        Total time: 18077.17s
                               ETA: 1275935.4s

################################################################################
                    [1m Learning iteration 1397/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.891s, learning 0.265s)
               Value function loss: 22.4865
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 241.11
               Mean episode length: 123.32
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22904832
                    Iteration time: 12.16s
                        Total time: 18089.33s
                               ETA: 1275867.2s

################################################################################
                    [1m Learning iteration 1398/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.361s, learning 0.258s)
               Value function loss: 22.3640
                    Surrogate loss: -0.0170
             Mean action noise std: 0.78
                       Mean reward: 239.44
               Mean episode length: 123.00
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22921216
                    Iteration time: 11.62s
                        Total time: 18100.95s
                               ETA: 1275761.2s

################################################################################
                    [1m Learning iteration 1399/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.033s, learning 0.202s)
               Value function loss: 22.9737
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 244.47
               Mean episode length: 123.15
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22937600
                    Iteration time: 12.23s
                        Total time: 18113.18s
                               ETA: 1275698.7s

################################################################################
                    [1m Learning iteration 1400/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.758s, learning 0.187s)
               Value function loss: 23.9069
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 242.96
               Mean episode length: 123.21
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22953984
                    Iteration time: 11.94s
                        Total time: 18125.13s
                               ETA: 1275615.8s

################################################################################
                    [1m Learning iteration 1401/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.705s, learning 0.242s)
               Value function loss: 24.7081
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 248.64
               Mean episode length: 124.42
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22970368
                    Iteration time: 11.95s
                        Total time: 18137.08s
                               ETA: 1275533.3s

################################################################################
                    [1m Learning iteration 1402/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.714s, learning 0.207s)
               Value function loss: 22.7058
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 244.74
               Mean episode length: 123.63
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 22986752
                    Iteration time: 11.92s
                        Total time: 18149.00s
                               ETA: 1275449.0s

################################################################################
                    [1m Learning iteration 1403/100000 [0m                    

                       Computation: 752 steps/s (collection: 21.578s, learning 0.194s)
               Value function loss: 25.1204
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 239.94
               Mean episode length: 121.74
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23003136
                    Iteration time: 21.77s
                        Total time: 18170.77s
                               ETA: 1276056.6s

################################################################################
                    [1m Learning iteration 1404/100000 [0m                    

                       Computation: 706 steps/s (collection: 22.986s, learning 0.199s)
               Value function loss: 22.4537
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 243.75
               Mean episode length: 123.04
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23019520
                    Iteration time: 23.19s
                        Total time: 18193.96s
                               ETA: 1276762.5s

################################################################################
                    [1m Learning iteration 1405/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.510s, learning 0.170s)
               Value function loss: 22.6593
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 235.50
               Mean episode length: 120.87
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23035904
                    Iteration time: 23.68s
                        Total time: 18217.64s
                               ETA: 1277502.1s

################################################################################
                    [1m Learning iteration 1406/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.042s, learning 0.240s)
               Value function loss: 26.0148
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 239.28
               Mean episode length: 121.77
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23052288
                    Iteration time: 23.28s
                        Total time: 18240.92s
                               ETA: 1278212.6s

################################################################################
                    [1m Learning iteration 1407/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.084s, learning 0.271s)
               Value function loss: 27.4180
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 242.77
               Mean episode length: 123.51
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23068672
                    Iteration time: 23.36s
                        Total time: 18264.27s
                               ETA: 1278927.3s

################################################################################
                    [1m Learning iteration 1408/100000 [0m                    

                       Computation: 729 steps/s (collection: 22.296s, learning 0.171s)
               Value function loss: 38.4240
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 248.24
               Mean episode length: 124.56
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.32
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23085056
                    Iteration time: 22.47s
                        Total time: 18286.74s
                               ETA: 1279578.8s

################################################################################
                    [1m Learning iteration 1409/100000 [0m                    

                       Computation: 706 steps/s (collection: 22.924s, learning 0.282s)
               Value function loss: 25.7774
                    Surrogate loss: -0.0179
             Mean action noise std: 0.78
                       Mean reward: 245.29
               Mean episode length: 124.23
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23101440
                    Iteration time: 23.21s
                        Total time: 18309.95s
                               ETA: 1280280.9s

################################################################################
                    [1m Learning iteration 1410/100000 [0m                    

                       Computation: 706 steps/s (collection: 23.031s, learning 0.172s)
               Value function loss: 25.8661
                    Surrogate loss: -0.0024
             Mean action noise std: 0.78
                       Mean reward: 245.56
               Mean episode length: 123.08
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23117824
                    Iteration time: 23.20s
                        Total time: 18333.15s
                               ETA: 1280981.8s

################################################################################
                    [1m Learning iteration 1411/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.074s, learning 0.173s)
               Value function loss: 22.8531
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 242.84
               Mean episode length: 123.95
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23134208
                    Iteration time: 23.25s
                        Total time: 18356.40s
                               ETA: 1281684.8s

################################################################################
                    [1m Learning iteration 1412/100000 [0m                    

                       Computation: 709 steps/s (collection: 22.936s, learning 0.167s)
               Value function loss: 21.8871
                    Surrogate loss: -0.0014
             Mean action noise std: 0.78
                       Mean reward: 242.16
               Mean episode length: 120.71
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23150592
                    Iteration time: 23.10s
                        Total time: 18379.50s
                               ETA: 1282376.7s

################################################################################
                    [1m Learning iteration 1413/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.802s, learning 0.180s)
               Value function loss: 24.1351
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 242.00
               Mean episode length: 123.24
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23166976
                    Iteration time: 22.98s
                        Total time: 18402.48s
                               ETA: 1283059.1s

################################################################################
                    [1m Learning iteration 1414/100000 [0m                    

                       Computation: 718 steps/s (collection: 22.504s, learning 0.311s)
               Value function loss: 31.6018
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 243.92
               Mean episode length: 122.92
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23183360
                    Iteration time: 22.81s
                        Total time: 18425.30s
                               ETA: 1283728.9s

################################################################################
                    [1m Learning iteration 1415/100000 [0m                    

                       Computation: 719 steps/s (collection: 22.542s, learning 0.237s)
               Value function loss: 28.1347
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 242.36
               Mean episode length: 122.32
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23199744
                    Iteration time: 22.78s
                        Total time: 18448.08s
                               ETA: 1284395.2s

################################################################################
                    [1m Learning iteration 1416/100000 [0m                    

                       Computation: 714 steps/s (collection: 22.709s, learning 0.209s)
               Value function loss: 24.6162
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 248.73
               Mean episode length: 123.84
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23216128
                    Iteration time: 22.92s
                        Total time: 18470.99s
                               ETA: 1285070.2s

################################################################################
                    [1m Learning iteration 1417/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.355s, learning 0.174s)
               Value function loss: 25.6561
                    Surrogate loss: -0.0039
             Mean action noise std: 0.78
                       Mean reward: 241.57
               Mean episode length: 122.61
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23232512
                    Iteration time: 23.53s
                        Total time: 18494.52s
                               ETA: 1285786.7s

################################################################################
                    [1m Learning iteration 1418/100000 [0m                    

                       Computation: 716 steps/s (collection: 22.707s, learning 0.160s)
               Value function loss: 29.6504
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 246.37
               Mean episode length: 124.03
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23248896
                    Iteration time: 22.87s
                        Total time: 18517.39s
                               ETA: 1286456.2s

################################################################################
                    [1m Learning iteration 1419/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.115s, learning 0.244s)
               Value function loss: 24.2669
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 240.81
               Mean episode length: 120.69
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23265280
                    Iteration time: 23.36s
                        Total time: 18540.75s
                               ETA: 1287158.9s

################################################################################
                    [1m Learning iteration 1420/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.216s, learning 0.170s)
               Value function loss: 26.8719
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 233.29
               Mean episode length: 118.95
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23281664
                    Iteration time: 23.39s
                        Total time: 18564.13s
                               ETA: 1287862.3s

################################################################################
                    [1m Learning iteration 1421/100000 [0m                    

                       Computation: 705 steps/s (collection: 23.072s, learning 0.161s)
               Value function loss: 33.4401
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 247.59
               Mean episode length: 123.84
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23298048
                    Iteration time: 23.23s
                        Total time: 18587.37s
                               ETA: 1288554.2s

################################################################################
                    [1m Learning iteration 1422/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.076s, learning 0.186s)
               Value function loss: 27.1695
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 245.79
               Mean episode length: 122.19
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23314432
                    Iteration time: 23.26s
                        Total time: 18610.63s
                               ETA: 1289247.1s

################################################################################
                    [1m Learning iteration 1423/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.018s, learning 0.276s)
               Value function loss: 25.8576
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 251.23
               Mean episode length: 125.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23330816
                    Iteration time: 23.29s
                        Total time: 18633.92s
                               ETA: 1289941.2s

################################################################################
                    [1m Learning iteration 1424/100000 [0m                    

                       Computation: 682 steps/s (collection: 23.765s, learning 0.232s)
               Value function loss: 27.2298
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 243.55
               Mean episode length: 121.92
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.43
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23347200
                    Iteration time: 24.00s
                        Total time: 18657.92s
                               ETA: 1290682.9s

################################################################################
                    [1m Learning iteration 1425/100000 [0m                    

                       Computation: 707 steps/s (collection: 22.827s, learning 0.325s)
               Value function loss: 23.4756
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 248.41
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23363584
                    Iteration time: 23.15s
                        Total time: 18681.07s
                               ETA: 1291365.2s

################################################################################
                    [1m Learning iteration 1426/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.832s, learning 0.161s)
               Value function loss: 24.7837
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 242.33
               Mean episode length: 122.36
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23379968
                    Iteration time: 22.99s
                        Total time: 18704.07s
                               ETA: 1292035.5s

################################################################################
                    [1m Learning iteration 1427/100000 [0m                    

                       Computation: 707 steps/s (collection: 22.985s, learning 0.166s)
               Value function loss: 20.9180
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 241.09
               Mean episode length: 121.86
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23396352
                    Iteration time: 23.15s
                        Total time: 18727.22s
                               ETA: 1292715.7s

################################################################################
                    [1m Learning iteration 1428/100000 [0m                    

                       Computation: 707 steps/s (collection: 22.869s, learning 0.280s)
               Value function loss: 24.5256
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 247.61
               Mean episode length: 123.25
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23412736
                    Iteration time: 23.15s
                        Total time: 18750.37s
                               ETA: 1293394.7s

################################################################################
                    [1m Learning iteration 1429/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.484s, learning 0.169s)
               Value function loss: 24.3524
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 248.00
               Mean episode length: 123.73
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23429120
                    Iteration time: 23.65s
                        Total time: 18774.02s
                               ETA: 1294107.5s

################################################################################
                    [1m Learning iteration 1430/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.076s, learning 0.170s)
               Value function loss: 24.6840
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 243.23
               Mean episode length: 120.83
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23445504
                    Iteration time: 23.25s
                        Total time: 18797.26s
                               ETA: 1294791.3s

################################################################################
                    [1m Learning iteration 1431/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.584s, learning 0.290s)
               Value function loss: 24.3650
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 244.64
               Mean episode length: 122.56
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23461888
                    Iteration time: 23.87s
                        Total time: 18821.14s
                               ETA: 1295517.4s

################################################################################
                    [1m Learning iteration 1432/100000 [0m                    

                       Computation: 705 steps/s (collection: 23.001s, learning 0.216s)
               Value function loss: 28.6449
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 247.92
               Mean episode length: 121.52
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23478272
                    Iteration time: 23.22s
                        Total time: 18844.36s
                               ETA: 1296197.2s

################################################################################
                    [1m Learning iteration 1433/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.176s, learning 0.275s)
               Value function loss: 23.8947
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 238.42
               Mean episode length: 120.01
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23494656
                    Iteration time: 23.45s
                        Total time: 18867.81s
                               ETA: 1296892.0s

################################################################################
                    [1m Learning iteration 1434/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.030s, learning 0.243s)
               Value function loss: 28.3661
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 249.01
               Mean episode length: 122.74
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23511040
                    Iteration time: 23.27s
                        Total time: 18891.08s
                               ETA: 1297573.7s

################################################################################
                    [1m Learning iteration 1435/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.758s, learning 0.230s)
               Value function loss: 23.8415
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 249.43
               Mean episode length: 123.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23527424
                    Iteration time: 22.99s
                        Total time: 18914.07s
                               ETA: 1298234.8s

################################################################################
                    [1m Learning iteration 1436/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.799s, learning 0.202s)
               Value function loss: 26.8827
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 246.85
               Mean episode length: 123.74
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23543808
                    Iteration time: 23.00s
                        Total time: 18937.07s
                               ETA: 1298895.9s

################################################################################
                    [1m Learning iteration 1437/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.023s, learning 0.256s)
               Value function loss: 30.3074
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 247.97
               Mean episode length: 123.70
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23560192
                    Iteration time: 23.28s
                        Total time: 18960.35s
                               ETA: 1299575.0s

################################################################################
                    [1m Learning iteration 1438/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.296s, learning 0.229s)
               Value function loss: 27.6069
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 244.89
               Mean episode length: 122.42
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23576576
                    Iteration time: 23.52s
                        Total time: 18983.87s
                               ETA: 1300269.9s

################################################################################
                    [1m Learning iteration 1439/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.803s, learning 0.202s)
               Value function loss: 38.7422
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 249.83
               Mean episode length: 124.40
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23592960
                    Iteration time: 23.00s
                        Total time: 19006.88s
                               ETA: 1300928.4s

################################################################################
                    [1m Learning iteration 1440/100000 [0m                    

                       Computation: 806 steps/s (collection: 20.127s, learning 0.178s)
               Value function loss: 24.6693
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 246.66
               Mean episode length: 123.24
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23609344
                    Iteration time: 20.30s
                        Total time: 19027.18s
                               ETA: 1301401.1s

################################################################################
                    [1m Learning iteration 1441/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.592s, learning 0.163s)
               Value function loss: 23.6979
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 244.19
               Mean episode length: 121.55
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23625728
                    Iteration time: 11.75s
                        Total time: 19038.94s
                               ETA: 1301288.9s

################################################################################
                    [1m Learning iteration 1442/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.142s, learning 0.182s)
               Value function loss: 24.5035
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 237.65
               Mean episode length: 118.91
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23642112
                    Iteration time: 12.32s
                        Total time: 19051.26s
                               ETA: 1301215.6s

################################################################################
                    [1m Learning iteration 1443/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.997s, learning 0.170s)
               Value function loss: 21.7124
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 248.14
               Mean episode length: 123.26
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23658496
                    Iteration time: 12.17s
                        Total time: 19063.43s
                               ETA: 1301131.8s

################################################################################
                    [1m Learning iteration 1444/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.649s, learning 0.168s)
               Value function loss: 23.9644
                    Surrogate loss: -0.0164
             Mean action noise std: 0.78
                       Mean reward: 249.10
               Mean episode length: 123.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23674880
                    Iteration time: 11.82s
                        Total time: 19075.25s
                               ETA: 1301024.1s

################################################################################
                    [1m Learning iteration 1445/100000 [0m                    

                       Computation: 1444 steps/s (collection: 11.176s, learning 0.170s)
               Value function loss: 25.7350
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 248.58
               Mean episode length: 123.02
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23691264
                    Iteration time: 11.35s
                        Total time: 19086.59s
                               ETA: 1300884.5s

################################################################################
                    [1m Learning iteration 1446/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.401s, learning 0.175s)
               Value function loss: 21.5602
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 245.55
               Mean episode length: 124.19
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23707648
                    Iteration time: 11.58s
                        Total time: 19098.17s
                               ETA: 1300760.7s

################################################################################
                    [1m Learning iteration 1447/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.980s, learning 0.166s)
               Value function loss: 24.6424
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 244.55
               Mean episode length: 121.58
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23724032
                    Iteration time: 12.15s
                        Total time: 19110.31s
                               ETA: 1300675.9s

################################################################################
                    [1m Learning iteration 1448/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.206s, learning 0.201s)
               Value function loss: 26.0976
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 243.02
               Mean episode length: 120.16
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23740416
                    Iteration time: 12.41s
                        Total time: 19122.72s
                               ETA: 1300608.9s

################################################################################
                    [1m Learning iteration 1449/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.050s, learning 0.260s)
               Value function loss: 23.8760
                    Surrogate loss: -0.0157
             Mean action noise std: 0.78
                       Mean reward: 243.70
               Mean episode length: 123.08
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23756800
                    Iteration time: 12.31s
                        Total time: 19135.03s
                               ETA: 1300535.4s

################################################################################
                    [1m Learning iteration 1450/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.476s, learning 0.170s)
               Value function loss: 24.3607
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 250.92
               Mean episode length: 123.84
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23773184
                    Iteration time: 11.65s
                        Total time: 19146.68s
                               ETA: 1300416.9s

################################################################################
                    [1m Learning iteration 1451/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.982s, learning 0.163s)
               Value function loss: 28.5414
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 248.34
               Mean episode length: 123.60
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23789568
                    Iteration time: 12.14s
                        Total time: 19158.82s
                               ETA: 1300332.3s

################################################################################
                    [1m Learning iteration 1452/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.783s, learning 0.180s)
               Value function loss: 33.7821
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 251.53
               Mean episode length: 124.21
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23805952
                    Iteration time: 11.96s
                        Total time: 19170.78s
                               ETA: 1300235.6s

################################################################################
                    [1m Learning iteration 1453/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.546s, learning 0.165s)
               Value function loss: 25.8086
                    Surrogate loss: -0.0194
             Mean action noise std: 0.78
                       Mean reward: 244.14
               Mean episode length: 121.65
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23822336
                    Iteration time: 11.71s
                        Total time: 19182.49s
                               ETA: 1300121.9s

################################################################################
                    [1m Learning iteration 1454/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.063s, learning 0.224s)
               Value function loss: 27.9857
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 245.81
               Mean episode length: 123.66
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23838720
                    Iteration time: 12.29s
                        Total time: 19194.78s
                               ETA: 1300047.3s

################################################################################
                    [1m Learning iteration 1455/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.859s, learning 0.233s)
               Value function loss: 35.9186
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 247.39
               Mean episode length: 122.93
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.35
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23855104
                    Iteration time: 12.09s
                        Total time: 19206.87s
                               ETA: 1299959.6s

################################################################################
                    [1m Learning iteration 1456/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.597s, learning 0.216s)
               Value function loss: 23.0476
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 246.64
               Mean episode length: 123.42
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23871488
                    Iteration time: 11.81s
                        Total time: 19218.68s
                               ETA: 1299853.2s

################################################################################
                    [1m Learning iteration 1457/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.632s, learning 0.160s)
               Value function loss: 23.4111
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 241.60
               Mean episode length: 121.01
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23887872
                    Iteration time: 11.79s
                        Total time: 19230.48s
                               ETA: 1299745.5s

################################################################################
                    [1m Learning iteration 1458/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.911s, learning 0.273s)
               Value function loss: 23.6713
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 245.99
               Mean episode length: 124.15
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23904256
                    Iteration time: 12.18s
                        Total time: 19242.66s
                               ETA: 1299664.4s

################################################################################
                    [1m Learning iteration 1459/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.960s, learning 0.231s)
               Value function loss: 26.2974
                    Surrogate loss: -0.0183
             Mean action noise std: 0.78
                       Mean reward: 244.66
               Mean episode length: 122.89
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23920640
                    Iteration time: 12.19s
                        Total time: 19254.85s
                               ETA: 1299583.9s

################################################################################
                    [1m Learning iteration 1460/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.614s, learning 0.159s)
               Value function loss: 24.9152
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 242.58
               Mean episode length: 122.06
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23937024
                    Iteration time: 11.77s
                        Total time: 19266.63s
                               ETA: 1299475.2s

################################################################################
                    [1m Learning iteration 1461/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.695s, learning 0.262s)
               Value function loss: 28.2883
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 245.30
               Mean episode length: 122.75
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23953408
                    Iteration time: 11.96s
                        Total time: 19278.58s
                               ETA: 1299379.1s

################################################################################
                    [1m Learning iteration 1462/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.978s, learning 0.173s)
               Value function loss: 27.9238
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 242.04
               Mean episode length: 120.47
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23969792
                    Iteration time: 12.15s
                        Total time: 19290.73s
                               ETA: 1299296.2s

################################################################################
                    [1m Learning iteration 1463/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.716s, learning 0.157s)
               Value function loss: 24.0776
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 242.63
               Mean episode length: 122.47
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 23986176
                    Iteration time: 11.87s
                        Total time: 19302.61s
                               ETA: 1299194.7s

################################################################################
                    [1m Learning iteration 1464/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.862s, learning 0.205s)
               Value function loss: 25.0689
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 248.13
               Mean episode length: 122.36
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24002560
                    Iteration time: 12.07s
                        Total time: 19314.67s
                               ETA: 1299106.2s

################################################################################
                    [1m Learning iteration 1465/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.989s, learning 0.179s)
               Value function loss: 31.6729
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 244.89
               Mean episode length: 122.75
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24018944
                    Iteration time: 12.17s
                        Total time: 19326.84s
                               ETA: 1299024.7s

################################################################################
                    [1m Learning iteration 1466/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.838s, learning 0.242s)
               Value function loss: 26.1088
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 244.08
               Mean episode length: 121.89
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24035328
                    Iteration time: 12.08s
                        Total time: 19338.92s
                               ETA: 1298937.4s

################################################################################
                    [1m Learning iteration 1467/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.615s, learning 0.201s)
               Value function loss: 27.2314
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 244.54
               Mean episode length: 123.38
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24051712
                    Iteration time: 11.82s
                        Total time: 19350.74s
                               ETA: 1298832.5s

################################################################################
                    [1m Learning iteration 1468/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.858s, learning 0.159s)
               Value function loss: 34.9790
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 248.50
               Mean episode length: 124.03
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24068096
                    Iteration time: 12.02s
                        Total time: 19362.75s
                               ETA: 1298741.2s

################################################################################
                    [1m Learning iteration 1469/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.522s, learning 0.186s)
               Value function loss: 34.4500
                    Surrogate loss: -0.0189
             Mean action noise std: 0.78
                       Mean reward: 243.67
               Mean episode length: 120.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24084480
                    Iteration time: 11.71s
                        Total time: 19374.46s
                               ETA: 1298629.3s

################################################################################
                    [1m Learning iteration 1470/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.137s, learning 0.205s)
               Value function loss: 40.9500
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 249.63
               Mean episode length: 123.33
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24100864
                    Iteration time: 12.34s
                        Total time: 19386.80s
                               ETA: 1298560.0s

################################################################################
                    [1m Learning iteration 1471/100000 [0m                    

                       Computation: 1424 steps/s (collection: 11.326s, learning 0.173s)
               Value function loss: 31.5499
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 244.35
               Mean episode length: 121.24
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24117248
                    Iteration time: 11.50s
                        Total time: 19398.30s
                               ETA: 1298434.3s

################################################################################
                    [1m Learning iteration 1472/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.085s, learning 0.254s)
               Value function loss: 27.6041
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 243.46
               Mean episode length: 121.69
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24133632
                    Iteration time: 12.34s
                        Total time: 19410.64s
                               ETA: 1298365.0s

################################################################################
                    [1m Learning iteration 1473/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.961s, learning 0.243s)
               Value function loss: 28.5904
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 248.75
               Mean episode length: 123.66
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24150016
                    Iteration time: 12.20s
                        Total time: 19422.85s
                               ETA: 1298286.8s

################################################################################
                    [1m Learning iteration 1474/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.914s, learning 0.216s)
               Value function loss: 23.5890
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 242.84
               Mean episode length: 121.93
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24166400
                    Iteration time: 12.13s
                        Total time: 19434.98s
                               ETA: 1298203.6s

################################################################################
                    [1m Learning iteration 1475/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.034s, learning 0.164s)
               Value function loss: 29.7583
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 252.58
               Mean episode length: 123.87
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24182784
                    Iteration time: 12.20s
                        Total time: 19447.17s
                               ETA: 1298125.1s

################################################################################
                    [1m Learning iteration 1476/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.884s, learning 0.159s)
               Value function loss: 28.2681
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 246.56
               Mean episode length: 122.33
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24199168
                    Iteration time: 12.04s
                        Total time: 19459.22s
                               ETA: 1298036.4s

################################################################################
                    [1m Learning iteration 1477/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.493s, learning 0.206s)
               Value function loss: 24.3320
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 247.09
               Mean episode length: 122.23
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24215552
                    Iteration time: 11.70s
                        Total time: 19470.92s
                               ETA: 1297924.9s

################################################################################
                    [1m Learning iteration 1478/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.966s, learning 0.238s)
               Value function loss: 25.2593
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 242.89
               Mean episode length: 120.95
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24231936
                    Iteration time: 12.20s
                        Total time: 19483.12s
                               ETA: 1297847.1s

################################################################################
                    [1m Learning iteration 1479/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.653s, learning 0.238s)
               Value function loss: 29.3321
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 243.67
               Mean episode length: 120.53
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24248320
                    Iteration time: 11.89s
                        Total time: 19495.01s
                               ETA: 1297748.6s

################################################################################
                    [1m Learning iteration 1480/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.766s, learning 0.158s)
               Value function loss: 24.5288
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 245.84
               Mean episode length: 121.57
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24264704
                    Iteration time: 11.92s
                        Total time: 19506.93s
                               ETA: 1297652.4s

################################################################################
                    [1m Learning iteration 1481/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.755s, learning 0.193s)
               Value function loss: 25.2438
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 247.70
               Mean episode length: 123.42
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24281088
                    Iteration time: 11.95s
                        Total time: 19518.88s
                               ETA: 1297557.8s

################################################################################
                    [1m Learning iteration 1482/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.658s, learning 0.163s)
               Value function loss: 26.9162
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 244.83
               Mean episode length: 122.33
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24297472
                    Iteration time: 11.82s
                        Total time: 19530.70s
                               ETA: 1297455.0s

################################################################################
                    [1m Learning iteration 1483/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.691s, learning 0.260s)
               Value function loss: 26.4341
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 248.42
               Mean episode length: 122.96
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24313856
                    Iteration time: 11.95s
                        Total time: 19542.65s
                               ETA: 1297361.0s

################################################################################
                    [1m Learning iteration 1484/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.749s, learning 0.167s)
               Value function loss: 29.0440
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 252.96
               Mean episode length: 123.11
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24330240
                    Iteration time: 11.92s
                        Total time: 19554.57s
                               ETA: 1297264.7s

################################################################################
                    [1m Learning iteration 1485/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.352s, learning 0.189s)
               Value function loss: 26.5809
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 247.65
               Mean episode length: 121.73
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24346624
                    Iteration time: 12.54s
                        Total time: 19567.11s
                               ETA: 1297210.0s

################################################################################
                    [1m Learning iteration 1486/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.097s, learning 0.219s)
               Value function loss: 34.1850
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 249.60
               Mean episode length: 123.61
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.40
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24363008
                    Iteration time: 12.32s
                        Total time: 19579.43s
                               ETA: 1297140.4s

################################################################################
                    [1m Learning iteration 1487/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.893s, learning 0.162s)
               Value function loss: 26.1135
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 252.24
               Mean episode length: 122.94
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24379392
                    Iteration time: 12.06s
                        Total time: 19591.48s
                               ETA: 1297053.6s

################################################################################
                    [1m Learning iteration 1488/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.864s, learning 0.168s)
               Value function loss: 24.8763
                    Surrogate loss: -0.0103
             Mean action noise std: 0.78
                       Mean reward: 251.00
               Mean episode length: 122.98
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24395776
                    Iteration time: 12.03s
                        Total time: 19603.51s
                               ETA: 1296965.4s

################################################################################
                    [1m Learning iteration 1489/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.730s, learning 0.181s)
               Value function loss: 24.3272
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 243.73
               Mean episode length: 120.10
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24412160
                    Iteration time: 11.91s
                        Total time: 19615.43s
                               ETA: 1296869.3s

################################################################################
                    [1m Learning iteration 1490/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.850s, learning 0.182s)
               Value function loss: 21.8822
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 245.15
               Mean episode length: 122.66
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24428544
                    Iteration time: 12.03s
                        Total time: 19627.46s
                               ETA: 1296781.2s

################################################################################
                    [1m Learning iteration 1491/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.806s, learning 0.187s)
               Value function loss: 26.6797
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 252.19
               Mean episode length: 123.80
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24444928
                    Iteration time: 11.99s
                        Total time: 19639.45s
                               ETA: 1296690.7s

################################################################################
                    [1m Learning iteration 1492/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.678s, learning 0.183s)
               Value function loss: 30.6686
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 248.98
               Mean episode length: 122.82
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24461312
                    Iteration time: 11.86s
                        Total time: 19651.31s
                               ETA: 1296591.6s

################################################################################
                    [1m Learning iteration 1493/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.964s, learning 0.208s)
               Value function loss: 26.5147
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 249.38
               Mean episode length: 122.86
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24477696
                    Iteration time: 12.17s
                        Total time: 19663.48s
                               ETA: 1296513.2s

################################################################################
                    [1m Learning iteration 1494/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.131s, learning 0.172s)
               Value function loss: 25.4225
                    Surrogate loss: -0.0159
             Mean action noise std: 0.78
                       Mean reward: 246.88
               Mean episode length: 121.95
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24494080
                    Iteration time: 12.30s
                        Total time: 19675.79s
                               ETA: 1296443.4s

################################################################################
                    [1m Learning iteration 1495/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.912s, learning 0.270s)
               Value function loss: 27.8661
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 250.51
               Mean episode length: 122.96
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24510464
                    Iteration time: 12.18s
                        Total time: 19687.97s
                               ETA: 1296365.8s

################################################################################
                    [1m Learning iteration 1496/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.030s, learning 0.171s)
               Value function loss: 28.6096
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 248.26
               Mean episode length: 123.14
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24526848
                    Iteration time: 12.20s
                        Total time: 19700.17s
                               ETA: 1296289.5s

################################################################################
                    [1m Learning iteration 1497/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.914s, learning 0.187s)
               Value function loss: 24.4974
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 250.47
               Mean episode length: 122.57
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24543232
                    Iteration time: 12.10s
                        Total time: 19712.27s
                               ETA: 1296206.7s

################################################################################
                    [1m Learning iteration 1498/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.018s, learning 0.301s)
               Value function loss: 25.6480
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 251.75
               Mean episode length: 123.77
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24559616
                    Iteration time: 12.32s
                        Total time: 19724.59s
                               ETA: 1296138.3s

################################################################################
                    [1m Learning iteration 1499/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.778s, learning 0.270s)
               Value function loss: 28.7690
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 244.62
               Mean episode length: 120.66
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24576000
                    Iteration time: 12.05s
                        Total time: 19736.64s
                               ETA: 1296052.2s

################################################################################
                    [1m Learning iteration 1500/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.878s, learning 0.197s)
               Value function loss: 25.1083
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 246.93
               Mean episode length: 122.15
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24592384
                    Iteration time: 12.08s
                        Total time: 19748.71s
                               ETA: 1295968.0s

################################################################################
                    [1m Learning iteration 1501/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.464s, learning 0.186s)
               Value function loss: 28.5056
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 247.11
               Mean episode length: 122.34
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24608768
                    Iteration time: 11.65s
                        Total time: 19760.36s
                               ETA: 1295856.0s

################################################################################
                    [1m Learning iteration 1502/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.658s, learning 0.158s)
               Value function loss: 31.9893
                    Surrogate loss: -0.0083
             Mean action noise std: 0.78
                       Mean reward: 252.86
               Mean episode length: 123.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.41
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24625152
                    Iteration time: 11.82s
                        Total time: 19772.18s
                               ETA: 1295755.1s

################################################################################
                    [1m Learning iteration 1503/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.848s, learning 0.161s)
               Value function loss: 25.3985
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 245.11
               Mean episode length: 121.76
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24641536
                    Iteration time: 12.01s
                        Total time: 19784.19s
                               ETA: 1295666.9s

################################################################################
                    [1m Learning iteration 1504/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.959s, learning 0.253s)
               Value function loss: 27.0880
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 248.28
               Mean episode length: 122.52
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24657920
                    Iteration time: 12.21s
                        Total time: 19796.40s
                               ETA: 1295592.1s

################################################################################
                    [1m Learning iteration 1505/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.850s, learning 0.174s)
               Value function loss: 22.9500
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 244.56
               Mean episode length: 121.88
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24674304
                    Iteration time: 12.02s
                        Total time: 19808.42s
                               ETA: 1295505.0s

################################################################################
                    [1m Learning iteration 1506/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.443s, learning 0.192s)
               Value function loss: 24.1051
                    Surrogate loss: -0.0155
             Mean action noise std: 0.78
                       Mean reward: 249.51
               Mean episode length: 124.03
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24690688
                    Iteration time: 11.63s
                        Total time: 19820.06s
                               ETA: 1295392.6s

################################################################################
                    [1m Learning iteration 1507/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.782s, learning 0.210s)
               Value function loss: 27.2266
                    Surrogate loss: -0.0167
             Mean action noise std: 0.78
                       Mean reward: 242.68
               Mean episode length: 119.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24707072
                    Iteration time: 11.99s
                        Total time: 19832.05s
                               ETA: 1295303.7s

################################################################################
                    [1m Learning iteration 1508/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.791s, learning 0.163s)
               Value function loss: 25.8381
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 238.61
               Mean episode length: 119.91
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24723456
                    Iteration time: 11.95s
                        Total time: 19844.00s
                               ETA: 1295212.4s

################################################################################
                    [1m Learning iteration 1509/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.760s, learning 0.196s)
               Value function loss: 29.7757
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 246.42
               Mean episode length: 122.72
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24739840
                    Iteration time: 11.96s
                        Total time: 19855.96s
                               ETA: 1295121.3s

################################################################################
                    [1m Learning iteration 1510/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.765s, learning 0.283s)
               Value function loss: 28.7090
                    Surrogate loss: -0.0070
             Mean action noise std: 0.78
                       Mean reward: 246.17
               Mean episode length: 123.12
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24756224
                    Iteration time: 12.05s
                        Total time: 19868.01s
                               ETA: 1295036.4s

################################################################################
                    [1m Learning iteration 1511/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.617s, learning 0.203s)
               Value function loss: 26.3914
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 245.77
               Mean episode length: 121.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24772608
                    Iteration time: 11.82s
                        Total time: 19879.83s
                               ETA: 1294936.7s

################################################################################
                    [1m Learning iteration 1512/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.872s, learning 0.202s)
               Value function loss: 27.6058
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 242.27
               Mean episode length: 121.26
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24788992
                    Iteration time: 12.07s
                        Total time: 19891.90s
                               ETA: 1294853.6s

################################################################################
                    [1m Learning iteration 1513/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.659s, learning 0.172s)
               Value function loss: 23.1145
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 243.93
               Mean episode length: 122.37
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24805376
                    Iteration time: 11.83s
                        Total time: 19903.73s
                               ETA: 1294754.8s

################################################################################
                    [1m Learning iteration 1514/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.480s, learning 0.171s)
               Value function loss: 23.0108
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 246.90
               Mean episode length: 122.38
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24821760
                    Iteration time: 11.65s
                        Total time: 19915.38s
                               ETA: 1294644.5s

################################################################################
                    [1m Learning iteration 1515/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.625s, learning 0.163s)
               Value function loss: 29.5664
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 246.90
               Mean episode length: 122.79
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24838144
                    Iteration time: 11.79s
                        Total time: 19927.17s
                               ETA: 1294543.1s

################################################################################
                    [1m Learning iteration 1516/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.577s, learning 0.205s)
               Value function loss: 27.6988
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 240.14
               Mean episode length: 120.06
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24854528
                    Iteration time: 11.78s
                        Total time: 19938.95s
                               ETA: 1294441.5s

################################################################################
                    [1m Learning iteration 1517/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.992s, learning 0.161s)
               Value function loss: 36.7484
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 246.58
               Mean episode length: 121.74
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24870912
                    Iteration time: 12.15s
                        Total time: 19951.10s
                               ETA: 1294364.0s

################################################################################
                    [1m Learning iteration 1518/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.778s, learning 0.220s)
               Value function loss: 28.9214
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 248.88
               Mean episode length: 123.18
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24887296
                    Iteration time: 12.00s
                        Total time: 19963.10s
                               ETA: 1294276.7s

################################################################################
                    [1m Learning iteration 1519/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.679s, learning 0.210s)
               Value function loss: 32.1085
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 246.07
               Mean episode length: 120.87
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24903680
                    Iteration time: 11.89s
                        Total time: 19974.99s
                               ETA: 1294182.3s

################################################################################
                    [1m Learning iteration 1520/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.888s, learning 0.203s)
               Value function loss: 29.6638
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 242.61
               Mean episode length: 121.29
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24920064
                    Iteration time: 12.09s
                        Total time: 19987.08s
                               ETA: 1294101.2s

################################################################################
                    [1m Learning iteration 1521/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.852s, learning 0.199s)
               Value function loss: 23.8905
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 243.39
               Mean episode length: 121.43
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24936448
                    Iteration time: 12.05s
                        Total time: 19999.13s
                               ETA: 1294017.6s

################################################################################
                    [1m Learning iteration 1522/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.862s, learning 0.172s)
               Value function loss: 22.6511
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 244.03
               Mean episode length: 119.45
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24952832
                    Iteration time: 12.03s
                        Total time: 20011.17s
                               ETA: 1293932.9s

################################################################################
                    [1m Learning iteration 1523/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.893s, learning 0.193s)
               Value function loss: 28.6728
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 241.07
               Mean episode length: 120.22
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24969216
                    Iteration time: 12.09s
                        Total time: 20023.25s
                               ETA: 1293851.7s

################################################################################
                    [1m Learning iteration 1524/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.939s, learning 0.216s)
               Value function loss: 23.0511
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 245.87
               Mean episode length: 121.36
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 24985600
                    Iteration time: 12.15s
                        Total time: 20035.41s
                               ETA: 1293775.0s

################################################################################
                    [1m Learning iteration 1525/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.768s, learning 0.304s)
               Value function loss: 24.8524
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 247.88
               Mean episode length: 121.34
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25001984
                    Iteration time: 12.07s
                        Total time: 20047.48s
                               ETA: 1293693.1s

################################################################################
                    [1m Learning iteration 1526/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.170s)
               Value function loss: 27.6215
                    Surrogate loss: -0.0124
             Mean action noise std: 0.78
                       Mean reward: 250.64
               Mean episode length: 122.57
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25018368
                    Iteration time: 11.73s
                        Total time: 20059.22s
                               ETA: 1293589.5s

################################################################################
                    [1m Learning iteration 1527/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.703s, learning 0.164s)
               Value function loss: 23.2565
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 244.68
               Mean episode length: 120.89
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25034752
                    Iteration time: 11.87s
                        Total time: 20071.08s
                               ETA: 1293494.5s

################################################################################
                    [1m Learning iteration 1528/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.698s, learning 0.216s)
               Value function loss: 21.6525
                    Surrogate loss: -0.0143
             Mean action noise std: 0.78
                       Mean reward: 247.70
               Mean episode length: 123.33
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25051136
                    Iteration time: 11.91s
                        Total time: 20083.00s
                               ETA: 1293402.7s

################################################################################
                    [1m Learning iteration 1529/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.911s, learning 0.174s)
               Value function loss: 23.8012
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 244.11
               Mean episode length: 119.70
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25067520
                    Iteration time: 12.08s
                        Total time: 20095.08s
                               ETA: 1293322.0s

################################################################################
                    [1m Learning iteration 1530/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.605s, learning 0.163s)
               Value function loss: 23.6665
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 248.59
               Mean episode length: 121.93
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25083904
                    Iteration time: 11.77s
                        Total time: 20106.85s
                               ETA: 1293221.0s

################################################################################
                    [1m Learning iteration 1531/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.560s, learning 0.174s)
               Value function loss: 27.8376
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 243.14
               Mean episode length: 119.31
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25100288
                    Iteration time: 11.73s
                        Total time: 20118.58s
                               ETA: 1293118.0s

################################################################################
                    [1m Learning iteration 1532/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.761s, learning 0.180s)
               Value function loss: 24.9754
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 251.40
               Mean episode length: 122.28
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25116672
                    Iteration time: 11.94s
                        Total time: 20130.52s
                               ETA: 1293028.4s

################################################################################
                    [1m Learning iteration 1533/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.869s, learning 0.224s)
               Value function loss: 35.3771
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 254.59
               Mean episode length: 123.41
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.37
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25133056
                    Iteration time: 12.09s
                        Total time: 20142.62s
                               ETA: 1292948.6s

################################################################################
                    [1m Learning iteration 1534/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.150s, learning 0.157s)
               Value function loss: 23.9790
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 245.44
               Mean episode length: 120.07
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25149440
                    Iteration time: 12.31s
                        Total time: 20154.92s
                               ETA: 1292882.6s

################################################################################
                    [1m Learning iteration 1535/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.678s, learning 0.182s)
               Value function loss: 27.0103
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 246.69
               Mean episode length: 120.84
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25165824
                    Iteration time: 11.86s
                        Total time: 20166.78s
                               ETA: 1292788.0s

################################################################################
                    [1m Learning iteration 1536/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.475s, learning 0.180s)
               Value function loss: 23.1309
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 247.02
               Mean episode length: 121.20
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25182208
                    Iteration time: 11.65s
                        Total time: 20178.44s
                               ETA: 1292680.4s

################################################################################
                    [1m Learning iteration 1537/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.517s, learning 0.200s)
               Value function loss: 21.3569
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 252.87
               Mean episode length: 123.32
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25198592
                    Iteration time: 11.72s
                        Total time: 20190.16s
                               ETA: 1292576.9s

################################################################################
                    [1m Learning iteration 1538/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.766s, learning 0.166s)
               Value function loss: 24.4266
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 245.52
               Mean episode length: 121.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25214976
                    Iteration time: 11.93s
                        Total time: 20202.09s
                               ETA: 1292487.3s

################################################################################
                    [1m Learning iteration 1539/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.646s, learning 0.304s)
               Value function loss: 28.8586
                    Surrogate loss: -0.0158
             Mean action noise std: 0.78
                       Mean reward: 245.24
               Mean episode length: 121.09
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25231360
                    Iteration time: 11.95s
                        Total time: 20214.04s
                               ETA: 1292399.0s

################################################################################
                    [1m Learning iteration 1540/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.644s, learning 0.157s)
               Value function loss: 25.8085
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 248.55
               Mean episode length: 121.03
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25247744
                    Iteration time: 11.80s
                        Total time: 20225.84s
                               ETA: 1292301.2s

################################################################################
                    [1m Learning iteration 1541/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.723s, learning 0.225s)
               Value function loss: 26.1332
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 248.47
               Mean episode length: 122.34
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25264128
                    Iteration time: 11.95s
                        Total time: 20237.79s
                               ETA: 1292212.9s

################################################################################
                    [1m Learning iteration 1542/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.622s, learning 0.168s)
               Value function loss: 29.3111
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 251.19
               Mean episode length: 121.72
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25280512
                    Iteration time: 11.79s
                        Total time: 20249.58s
                               ETA: 1292114.6s

################################################################################
                    [1m Learning iteration 1543/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.679s, learning 0.158s)
               Value function loss: 27.1395
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 241.90
               Mean episode length: 118.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25296896
                    Iteration time: 11.84s
                        Total time: 20261.41s
                               ETA: 1292019.4s

################################################################################
                    [1m Learning iteration 1544/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.785s, learning 0.162s)
               Value function loss: 23.1559
                    Surrogate loss: -0.0190
             Mean action noise std: 0.78
                       Mean reward: 250.15
               Mean episode length: 121.96
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25313280
                    Iteration time: 11.95s
                        Total time: 20273.36s
                               ETA: 1291931.4s

################################################################################
                    [1m Learning iteration 1545/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.561s, learning 0.196s)
               Value function loss: 23.8776
                    Surrogate loss: -0.0168
             Mean action noise std: 0.78
                       Mean reward: 241.22
               Mean episode length: 119.61
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25329664
                    Iteration time: 11.76s
                        Total time: 20285.12s
                               ETA: 1291831.3s

################################################################################
                    [1m Learning iteration 1546/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.019s, learning 0.202s)
               Value function loss: 26.4673
                    Surrogate loss: 0.0016
             Mean action noise std: 0.78
                       Mean reward: 248.92
               Mean episode length: 121.66
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25346048
                    Iteration time: 12.22s
                        Total time: 20297.34s
                               ETA: 1291760.9s

################################################################################
                    [1m Learning iteration 1547/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.684s, learning 0.180s)
               Value function loss: 27.4254
                    Surrogate loss: -0.0162
             Mean action noise std: 0.78
                       Mean reward: 243.45
               Mean episode length: 120.22
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25362432
                    Iteration time: 11.86s
                        Total time: 20309.20s
                               ETA: 1291667.9s

################################################################################
                    [1m Learning iteration 1548/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.668s, learning 0.172s)
               Value function loss: 27.8083
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 246.18
               Mean episode length: 120.19
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25378816
                    Iteration time: 11.84s
                        Total time: 20321.04s
                               ETA: 1291573.5s

################################################################################
                    [1m Learning iteration 1549/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.540s, learning 0.210s)
               Value function loss: 25.9569
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 248.97
               Mean episode length: 121.67
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25395200
                    Iteration time: 11.75s
                        Total time: 20332.79s
                               ETA: 1291473.5s

################################################################################
                    [1m Learning iteration 1550/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.663s, learning 0.166s)
               Value function loss: 24.9947
                    Surrogate loss: -0.0111
             Mean action noise std: 0.78
                       Mean reward: 246.10
               Mean episode length: 120.18
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25411584
                    Iteration time: 11.83s
                        Total time: 20344.62s
                               ETA: 1291378.5s

################################################################################
                    [1m Learning iteration 1551/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.761s, learning 0.179s)
               Value function loss: 25.6099
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 245.01
               Mean episode length: 120.11
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25427968
                    Iteration time: 11.94s
                        Total time: 20356.56s
                               ETA: 1291290.7s

################################################################################
                    [1m Learning iteration 1552/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.541s, learning 0.165s)
               Value function loss: 23.6441
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 241.12
               Mean episode length: 118.33
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25444352
                    Iteration time: 11.71s
                        Total time: 20368.27s
                               ETA: 1291188.1s

################################################################################
                    [1m Learning iteration 1553/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.316s, learning 0.171s)
               Value function loss: 22.5327
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 243.63
               Mean episode length: 119.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25460736
                    Iteration time: 11.49s
                        Total time: 20379.75s
                               ETA: 1291071.9s

################################################################################
                    [1m Learning iteration 1554/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.514s, learning 0.170s)
               Value function loss: 29.2670
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 252.16
               Mean episode length: 121.65
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25477120
                    Iteration time: 11.68s
                        Total time: 20391.44s
                               ETA: 1290968.2s

################################################################################
                    [1m Learning iteration 1555/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.729s, learning 0.167s)
               Value function loss: 28.7749
                    Surrogate loss: 0.0027
             Mean action noise std: 0.78
                       Mean reward: 255.98
               Mean episode length: 123.25
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25493504
                    Iteration time: 11.90s
                        Total time: 20403.33s
                               ETA: 1290878.1s

################################################################################
                    [1m Learning iteration 1556/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.738s, learning 0.159s)
               Value function loss: 26.4611
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 247.73
               Mean episode length: 121.09
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25509888
                    Iteration time: 11.90s
                        Total time: 20415.23s
                               ETA: 1290788.1s

################################################################################
                    [1m Learning iteration 1557/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.996s, learning 0.175s)
               Value function loss: 30.2243
                    Surrogate loss: -0.0030
             Mean action noise std: 0.78
                       Mean reward: 249.75
               Mean episode length: 121.42
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25526272
                    Iteration time: 12.17s
                        Total time: 20427.40s
                               ETA: 1290715.6s

################################################################################
                    [1m Learning iteration 1558/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.887s, learning 0.168s)
               Value function loss: 26.4772
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 248.76
               Mean episode length: 120.82
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25542656
                    Iteration time: 12.06s
                        Total time: 20439.46s
                               ETA: 1290635.8s

################################################################################
                    [1m Learning iteration 1559/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.527s, learning 0.164s)
               Value function loss: 27.5546
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 248.76
               Mean episode length: 121.13
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25559040
                    Iteration time: 11.69s
                        Total time: 20451.15s
                               ETA: 1290533.1s

################################################################################
                    [1m Learning iteration 1560/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.003s, learning 0.161s)
               Value function loss: 25.2019
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 247.96
               Mean episode length: 119.72
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25575424
                    Iteration time: 12.16s
                        Total time: 20463.31s
                               ETA: 1290460.3s

################################################################################
                    [1m Learning iteration 1561/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.660s, learning 0.398s)
               Value function loss: 26.0658
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 251.88
               Mean episode length: 120.96
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25591808
                    Iteration time: 12.06s
                        Total time: 20475.37s
                               ETA: 1290381.0s

################################################################################
                    [1m Learning iteration 1562/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.821s, learning 0.171s)
               Value function loss: 28.3619
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 243.39
               Mean episode length: 118.06
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25608192
                    Iteration time: 11.99s
                        Total time: 20487.36s
                               ETA: 1290297.6s

################################################################################
                    [1m Learning iteration 1563/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.966s, learning 0.195s)
               Value function loss: 31.0113
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 242.28
               Mean episode length: 118.10
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25624576
                    Iteration time: 12.16s
                        Total time: 20499.52s
                               ETA: 1290224.9s

################################################################################
                    [1m Learning iteration 1564/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.025s, learning 0.160s)
               Value function loss: 34.9749
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 249.69
               Mean episode length: 120.87
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25640960
                    Iteration time: 12.18s
                        Total time: 20511.71s
                               ETA: 1290153.7s

################################################################################
                    [1m Learning iteration 1565/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.535s, learning 0.167s)
               Value function loss: 21.9717
                    Surrogate loss: -0.0176
             Mean action noise std: 0.78
                       Mean reward: 249.53
               Mean episode length: 119.51
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25657344
                    Iteration time: 11.70s
                        Total time: 20523.41s
                               ETA: 1290052.4s

################################################################################
                    [1m Learning iteration 1566/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.042s, learning 0.275s)
               Value function loss: 34.0036
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 247.48
               Mean episode length: 119.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25673728
                    Iteration time: 12.32s
                        Total time: 20535.73s
                               ETA: 1289989.8s

################################################################################
                    [1m Learning iteration 1567/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.011s, learning 0.175s)
               Value function loss: 27.1040
                    Surrogate loss: -0.0160
             Mean action noise std: 0.78
                       Mean reward: 248.33
               Mean episode length: 120.57
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25690112
                    Iteration time: 12.19s
                        Total time: 20547.92s
                               ETA: 1289919.0s

################################################################################
                    [1m Learning iteration 1568/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.741s, learning 0.195s)
               Value function loss: 27.0441
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 248.51
               Mean episode length: 119.59
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25706496
                    Iteration time: 11.94s
                        Total time: 20559.85s
                               ETA: 1289832.6s

################################################################################
                    [1m Learning iteration 1569/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.690s, learning 0.250s)
               Value function loss: 23.7207
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 243.80
               Mean episode length: 118.03
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25722880
                    Iteration time: 11.94s
                        Total time: 20571.79s
                               ETA: 1289746.5s

################################################################################
                    [1m Learning iteration 1570/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.236s, learning 0.177s)
               Value function loss: 28.9376
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 247.09
               Mean episode length: 120.01
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25739264
                    Iteration time: 12.41s
                        Total time: 20584.20s
                               ETA: 1289690.2s

################################################################################
                    [1m Learning iteration 1571/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.848s, learning 0.174s)
               Value function loss: 27.5008
                    Surrogate loss: -0.0166
             Mean action noise std: 0.78
                       Mean reward: 244.66
               Mean episode length: 117.44
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25755648
                    Iteration time: 12.02s
                        Total time: 20596.23s
                               ETA: 1289609.4s

################################################################################
                    [1m Learning iteration 1572/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.233s, learning 0.169s)
               Value function loss: 31.1007
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 251.45
               Mean episode length: 121.11
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25772032
                    Iteration time: 12.40s
                        Total time: 20608.63s
                               ETA: 1289552.5s

################################################################################
                    [1m Learning iteration 1573/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.713s, learning 0.165s)
               Value function loss: 30.2195
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 243.23
               Mean episode length: 117.04
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25788416
                    Iteration time: 11.88s
                        Total time: 20620.51s
                               ETA: 1289462.9s

################################################################################
                    [1m Learning iteration 1574/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.823s, learning 0.165s)
               Value function loss: 31.7606
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 242.51
               Mean episode length: 116.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25804800
                    Iteration time: 11.99s
                        Total time: 20632.49s
                               ETA: 1289380.3s

################################################################################
                    [1m Learning iteration 1575/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.988s, learning 0.195s)
               Value function loss: 23.6099
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 243.86
               Mean episode length: 117.67
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25821184
                    Iteration time: 12.18s
                        Total time: 20644.68s
                               ETA: 1289309.9s

################################################################################
                    [1m Learning iteration 1576/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.090s, learning 0.168s)
               Value function loss: 28.7431
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 249.75
               Mean episode length: 120.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25837568
                    Iteration time: 12.26s
                        Total time: 20656.94s
                               ETA: 1289244.3s

################################################################################
                    [1m Learning iteration 1577/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.783s, learning 0.269s)
               Value function loss: 28.4506
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 255.21
               Mean episode length: 123.09
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25853952
                    Iteration time: 12.05s
                        Total time: 20668.99s
                               ETA: 1289165.9s

################################################################################
                    [1m Learning iteration 1578/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.971s, learning 0.255s)
               Value function loss: 28.7935
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 248.32
               Mean episode length: 120.07
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25870336
                    Iteration time: 12.23s
                        Total time: 20681.21s
                               ETA: 1289098.5s

################################################################################
                    [1m Learning iteration 1579/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.834s, learning 0.160s)
               Value function loss: 25.6175
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 251.21
               Mean episode length: 120.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25886720
                    Iteration time: 11.99s
                        Total time: 20693.21s
                               ETA: 1289016.6s

################################################################################
                    [1m Learning iteration 1580/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.799s, learning 0.198s)
               Value function loss: 30.2765
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 257.31
               Mean episode length: 123.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25903104
                    Iteration time: 12.00s
                        Total time: 20705.20s
                               ETA: 1288935.0s

################################################################################
                    [1m Learning iteration 1581/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.146s, learning 0.169s)
               Value function loss: 22.4560
                    Surrogate loss: -0.0175
             Mean action noise std: 0.78
                       Mean reward: 250.12
               Mean episode length: 121.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25919488
                    Iteration time: 12.31s
                        Total time: 20717.52s
                               ETA: 1288873.3s

################################################################################
                    [1m Learning iteration 1582/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.946s, learning 0.163s)
               Value function loss: 27.3769
                    Surrogate loss: -0.0112
             Mean action noise std: 0.78
                       Mean reward: 252.58
               Mean episode length: 122.30
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25935872
                    Iteration time: 12.11s
                        Total time: 20729.63s
                               ETA: 1288798.8s

################################################################################
                    [1m Learning iteration 1583/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.881s, learning 0.160s)
               Value function loss: 21.2893
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 255.81
               Mean episode length: 122.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25952256
                    Iteration time: 12.04s
                        Total time: 20741.67s
                               ETA: 1288720.2s

################################################################################
                    [1m Learning iteration 1584/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.123s, learning 0.164s)
               Value function loss: 23.1019
                    Surrogate loss: -0.0191
             Mean action noise std: 0.78
                       Mean reward: 245.17
               Mean episode length: 118.88
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25968640
                    Iteration time: 12.29s
                        Total time: 20753.96s
                               ETA: 1288657.0s

################################################################################
                    [1m Learning iteration 1585/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.919s, learning 0.171s)
               Value function loss: 25.7994
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 249.90
               Mean episode length: 120.22
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 25985024
                    Iteration time: 12.09s
                        Total time: 20766.05s
                               ETA: 1288581.6s

################################################################################
                    [1m Learning iteration 1586/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.925s, learning 0.277s)
               Value function loss: 27.9933
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 252.06
               Mean episode length: 121.85
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26001408
                    Iteration time: 12.20s
                        Total time: 20778.25s
                               ETA: 1288513.2s

################################################################################
                    [1m Learning iteration 1587/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.787s, learning 0.256s)
               Value function loss: 24.5834
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 254.62
               Mean episode length: 122.84
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26017792
                    Iteration time: 12.04s
                        Total time: 20790.29s
                               ETA: 1288435.1s

################################################################################
                    [1m Learning iteration 1588/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.515s, learning 0.209s)
               Value function loss: 28.0345
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 252.68
               Mean episode length: 120.93
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26034176
                    Iteration time: 11.72s
                        Total time: 20802.02s
                               ETA: 1288337.3s

################################################################################
                    [1m Learning iteration 1589/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.387s, learning 0.165s)
               Value function loss: 23.6727
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 250.45
               Mean episode length: 120.86
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26050560
                    Iteration time: 11.55s
                        Total time: 20813.57s
                               ETA: 1288228.9s

################################################################################
                    [1m Learning iteration 1590/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.723s, learning 0.158s)
               Value function loss: 26.4393
                    Surrogate loss: -0.0185
             Mean action noise std: 0.78
                       Mean reward: 255.45
               Mean episode length: 122.53
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26066944
                    Iteration time: 11.88s
                        Total time: 20825.45s
                               ETA: 1288141.0s

################################################################################
                    [1m Learning iteration 1591/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.803s, learning 0.197s)
               Value function loss: 23.0276
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 255.59
               Mean episode length: 123.34
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26083328
                    Iteration time: 12.00s
                        Total time: 20837.45s
                               ETA: 1288060.6s

################################################################################
                    [1m Learning iteration 1592/100000 [0m                    

                       Computation: 1292 steps/s (collection: 12.426s, learning 0.252s)
               Value function loss: 25.2625
                    Surrogate loss: -0.0152
             Mean action noise std: 0.78
                       Mean reward: 257.11
               Mean episode length: 123.88
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26099712
                    Iteration time: 12.68s
                        Total time: 20850.13s
                               ETA: 1288022.1s

################################################################################
                    [1m Learning iteration 1593/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.651s, learning 0.212s)
               Value function loss: 26.0866
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 251.71
               Mean episode length: 121.20
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26116096
                    Iteration time: 11.86s
                        Total time: 20861.99s
                               ETA: 1287933.3s

################################################################################
                    [1m Learning iteration 1594/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.623s, learning 0.165s)
               Value function loss: 25.1025
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 255.69
               Mean episode length: 122.01
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26132480
                    Iteration time: 11.79s
                        Total time: 20873.78s
                               ETA: 1287840.1s

################################################################################
                    [1m Learning iteration 1595/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.588s, learning 0.157s)
               Value function loss: 29.2637
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 253.95
               Mean episode length: 121.63
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26148864
                    Iteration time: 11.75s
                        Total time: 20885.52s
                               ETA: 1287744.3s

################################################################################
                    [1m Learning iteration 1596/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.071s, learning 0.164s)
               Value function loss: 23.1725
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 253.58
               Mean episode length: 121.19
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26165248
                    Iteration time: 12.23s
                        Total time: 20897.76s
                               ETA: 1287678.7s

################################################################################
                    [1m Learning iteration 1597/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.817s, learning 0.157s)
               Value function loss: 27.3880
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 251.90
               Mean episode length: 120.70
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26181632
                    Iteration time: 11.97s
                        Total time: 20909.73s
                               ETA: 1287597.2s

################################################################################
                    [1m Learning iteration 1598/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.701s, learning 0.203s)
               Value function loss: 26.9873
                    Surrogate loss: -0.0131
             Mean action noise std: 0.78
                       Mean reward: 254.32
               Mean episode length: 122.45
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26198016
                    Iteration time: 11.90s
                        Total time: 20921.64s
                               ETA: 1287511.4s

################################################################################
                    [1m Learning iteration 1599/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.882s, learning 0.158s)
               Value function loss: 25.9881
                    Surrogate loss: -0.0121
             Mean action noise std: 0.78
                       Mean reward: 258.80
               Mean episode length: 123.62
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26214400
                    Iteration time: 12.04s
                        Total time: 20933.67s
                               ETA: 1287434.1s

################################################################################
                    [1m Learning iteration 1600/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.832s, learning 0.244s)
               Value function loss: 24.5897
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 253.95
               Mean episode length: 122.09
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26230784
                    Iteration time: 12.08s
                        Total time: 20945.75s
                               ETA: 1287359.1s

################################################################################
                    [1m Learning iteration 1601/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.134s, learning 0.191s)
               Value function loss: 31.5174
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 250.58
               Mean episode length: 120.28
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26247168
                    Iteration time: 12.33s
                        Total time: 20958.08s
                               ETA: 1287299.5s

################################################################################
                    [1m Learning iteration 1602/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.856s, learning 0.271s)
               Value function loss: 30.1604
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 250.05
               Mean episode length: 120.67
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26263552
                    Iteration time: 12.13s
                        Total time: 20970.20s
                               ETA: 1287227.8s

################################################################################
                    [1m Learning iteration 1603/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.886s, learning 0.164s)
               Value function loss: 29.5979
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 255.07
               Mean episode length: 122.26
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26279936
                    Iteration time: 12.05s
                        Total time: 20982.25s
                               ETA: 1287151.4s

################################################################################
                    [1m Learning iteration 1604/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.835s, learning 0.158s)
               Value function loss: 33.3870
                    Surrogate loss: 0.0063
             Mean action noise std: 0.78
                       Mean reward: 257.56
               Mean episode length: 123.33
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26296320
                    Iteration time: 11.99s
                        Total time: 20994.25s
                               ETA: 1287071.6s

################################################################################
                    [1m Learning iteration 1605/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.868s, learning 0.196s)
               Value function loss: 26.7154
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 254.01
               Mean episode length: 121.40
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26312704
                    Iteration time: 12.06s
                        Total time: 21006.31s
                               ETA: 1286996.3s

################################################################################
                    [1m Learning iteration 1606/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.882s, learning 0.161s)
               Value function loss: 25.7784
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 254.19
               Mean episode length: 121.75
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26329088
                    Iteration time: 12.04s
                        Total time: 21018.35s
                               ETA: 1286919.7s

################################################################################
                    [1m Learning iteration 1607/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.718s, learning 0.167s)
               Value function loss: 27.4503
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 250.06
               Mean episode length: 120.47
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26345472
                    Iteration time: 11.88s
                        Total time: 21030.24s
                               ETA: 1286833.5s

################################################################################
                    [1m Learning iteration 1608/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.914s, learning 0.264s)
               Value function loss: 27.4292
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 254.17
               Mean episode length: 122.54
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26361856
                    Iteration time: 12.18s
                        Total time: 21042.42s
                               ETA: 1286765.4s

################################################################################
                    [1m Learning iteration 1609/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.650s, learning 0.200s)
               Value function loss: 29.3690
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 245.84
               Mean episode length: 119.68
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26378240
                    Iteration time: 11.85s
                        Total time: 21054.27s
                               ETA: 1286677.3s

################################################################################
                    [1m Learning iteration 1610/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.970s, learning 0.251s)
               Value function loss: 27.3012
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 249.49
               Mean episode length: 120.57
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26394624
                    Iteration time: 12.22s
                        Total time: 21066.49s
                               ETA: 1286611.9s

################################################################################
                    [1m Learning iteration 1611/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.683s, learning 0.191s)
               Value function loss: 28.9017
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 258.33
               Mean episode length: 124.06
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26411008
                    Iteration time: 11.87s
                        Total time: 21078.36s
                               ETA: 1286525.4s

################################################################################
                    [1m Learning iteration 1612/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.018s, learning 0.203s)
               Value function loss: 22.0462
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 251.30
               Mean episode length: 120.13
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26427392
                    Iteration time: 12.22s
                        Total time: 21090.58s
                               ETA: 1286460.2s

################################################################################
                    [1m Learning iteration 1613/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.690s, learning 0.239s)
               Value function loss: 31.4294
                    Surrogate loss: -0.0146
             Mean action noise std: 0.78
                       Mean reward: 255.71
               Mean episode length: 121.62
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26443776
                    Iteration time: 11.93s
                        Total time: 21102.51s
                               ETA: 1286377.2s

################################################################################
                    [1m Learning iteration 1614/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.908s, learning 0.205s)
               Value function loss: 25.1710
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 253.24
               Mean episode length: 121.65
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26460160
                    Iteration time: 12.11s
                        Total time: 21114.62s
                               ETA: 1286305.6s

################################################################################
                    [1m Learning iteration 1615/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.036s, learning 0.162s)
               Value function loss: 26.5749
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 255.64
               Mean episode length: 122.41
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26476544
                    Iteration time: 12.20s
                        Total time: 21126.82s
                               ETA: 1286239.1s

################################################################################
                    [1m Learning iteration 1616/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.649s, learning 0.159s)
               Value function loss: 30.7777
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 251.76
               Mean episode length: 120.84
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26492928
                    Iteration time: 11.81s
                        Total time: 21138.63s
                               ETA: 1286149.1s

################################################################################
                    [1m Learning iteration 1617/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.655s, learning 0.163s)
               Value function loss: 37.6555
                    Surrogate loss: -0.0128
             Mean action noise std: 0.78
                       Mean reward: 250.90
               Mean episode length: 119.98
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26509312
                    Iteration time: 11.82s
                        Total time: 21150.45s
                               ETA: 1286059.7s

################################################################################
                    [1m Learning iteration 1618/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.661s, learning 0.168s)
               Value function loss: 30.8142
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 254.18
               Mean episode length: 121.60
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26525696
                    Iteration time: 11.83s
                        Total time: 21162.28s
                               ETA: 1285971.1s

################################################################################
                    [1m Learning iteration 1619/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.970s, learning 0.168s)
               Value function loss: 31.3520
                    Surrogate loss: -0.0134
             Mean action noise std: 0.78
                       Mean reward: 258.28
               Mean episode length: 123.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26542080
                    Iteration time: 12.14s
                        Total time: 21174.42s
                               ETA: 1285901.4s

################################################################################
                    [1m Learning iteration 1620/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.608s, learning 0.166s)
               Value function loss: 31.4705
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 251.16
               Mean episode length: 120.49
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26558464
                    Iteration time: 11.77s
                        Total time: 21186.19s
                               ETA: 1285809.6s

################################################################################
                    [1m Learning iteration 1621/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.567s, learning 0.212s)
               Value function loss: 31.9448
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 252.26
               Mean episode length: 120.73
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26574848
                    Iteration time: 11.78s
                        Total time: 21197.97s
                               ETA: 1285718.2s

################################################################################
                    [1m Learning iteration 1622/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.913s, learning 0.161s)
               Value function loss: 26.1836
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 254.00
               Mean episode length: 121.61
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26591232
                    Iteration time: 12.07s
                        Total time: 21210.04s
                               ETA: 1285644.9s

################################################################################
                    [1m Learning iteration 1623/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.839s, learning 0.164s)
               Value function loss: 33.4948
                    Surrogate loss: -0.0091
             Mean action noise std: 0.78
                       Mean reward: 255.34
               Mean episode length: 121.44
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26607616
                    Iteration time: 12.00s
                        Total time: 21222.05s
                               ETA: 1285567.3s

################################################################################
                    [1m Learning iteration 1624/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.913s, learning 0.309s)
               Value function loss: 33.3848
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: 251.18
               Mean episode length: 120.26
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26624000
                    Iteration time: 12.22s
                        Total time: 21234.27s
                               ETA: 1285503.0s

################################################################################
                    [1m Learning iteration 1625/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.652s, learning 0.163s)
               Value function loss: 28.4116
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 259.64
               Mean episode length: 124.44
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26640384
                    Iteration time: 11.81s
                        Total time: 21246.08s
                               ETA: 1285414.2s

################################################################################
                    [1m Learning iteration 1626/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.918s, learning 0.219s)
               Value function loss: 31.7180
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 253.98
               Mean episode length: 122.23
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26656768
                    Iteration time: 12.14s
                        Total time: 21258.22s
                               ETA: 1285344.9s

################################################################################
                    [1m Learning iteration 1627/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.112s, learning 0.209s)
               Value function loss: 30.7461
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 256.72
               Mean episode length: 123.01
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26673152
                    Iteration time: 12.32s
                        Total time: 21270.54s
                               ETA: 1285286.9s

################################################################################
                    [1m Learning iteration 1628/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.171s, learning 0.173s)
               Value function loss: 30.2726
                    Surrogate loss: -0.0089
             Mean action noise std: 0.78
                       Mean reward: 251.05
               Mean episode length: 121.32
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26689536
                    Iteration time: 12.34s
                        Total time: 21282.89s
                               ETA: 1285230.2s

################################################################################
                    [1m Learning iteration 1629/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.668s, learning 0.212s)
               Value function loss: 31.1547
                    Surrogate loss: -0.0117
             Mean action noise std: 0.78
                       Mean reward: 257.18
               Mean episode length: 123.32
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26705920
                    Iteration time: 11.88s
                        Total time: 21294.77s
                               ETA: 1285145.7s

################################################################################
                    [1m Learning iteration 1630/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.634s, learning 0.163s)
               Value function loss: 30.8315
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 255.76
               Mean episode length: 122.79
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26722304
                    Iteration time: 11.80s
                        Total time: 21306.56s
                               ETA: 1285056.2s

################################################################################
                    [1m Learning iteration 1631/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.783s, learning 0.166s)
               Value function loss: 30.2577
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 253.83
               Mean episode length: 122.25
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26738688
                    Iteration time: 11.95s
                        Total time: 21318.51s
                               ETA: 1284976.0s

################################################################################
                    [1m Learning iteration 1632/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.913s, learning 0.157s)
               Value function loss: 33.7379
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 259.90
               Mean episode length: 124.79
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26755072
                    Iteration time: 12.07s
                        Total time: 21330.58s
                               ETA: 1284903.1s

################################################################################
                    [1m Learning iteration 1633/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.145s, learning 0.196s)
               Value function loss: 32.3591
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 256.16
               Mean episode length: 122.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26771456
                    Iteration time: 12.34s
                        Total time: 21342.92s
                               ETA: 1284846.6s

################################################################################
                    [1m Learning iteration 1634/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.740s, learning 0.167s)
               Value function loss: 32.4445
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 253.21
               Mean episode length: 122.56
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26787840
                    Iteration time: 11.91s
                        Total time: 21354.83s
                               ETA: 1284764.1s

################################################################################
                    [1m Learning iteration 1635/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.963s, learning 0.168s)
               Value function loss: 35.6002
                    Surrogate loss: -0.0071
             Mean action noise std: 0.78
                       Mean reward: 254.65
               Mean episode length: 122.66
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26804224
                    Iteration time: 12.13s
                        Total time: 21366.96s
                               ETA: 1284695.1s

################################################################################
                    [1m Learning iteration 1636/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.603s, learning 0.174s)
               Value function loss: 33.9093
                    Surrogate loss: -0.0038
             Mean action noise std: 0.78
                       Mean reward: 255.45
               Mean episode length: 123.48
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26820608
                    Iteration time: 11.78s
                        Total time: 21378.74s
                               ETA: 1284605.0s

################################################################################
                    [1m Learning iteration 1637/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.810s, learning 0.302s)
               Value function loss: 34.1005
                    Surrogate loss: -0.0058
             Mean action noise std: 0.78
                       Mean reward: 256.98
               Mean episode length: 124.25
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26836992
                    Iteration time: 12.11s
                        Total time: 21390.85s
                               ETA: 1284535.0s

################################################################################
                    [1m Learning iteration 1638/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.801s, learning 0.172s)
               Value function loss: 29.2645
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 257.54
               Mean episode length: 124.01
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26853376
                    Iteration time: 11.97s
                        Total time: 21402.82s
                               ETA: 1284456.7s

################################################################################
                    [1m Learning iteration 1639/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.894s, learning 0.272s)
               Value function loss: 31.0179
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 253.85
               Mean episode length: 122.55
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26869760
                    Iteration time: 12.17s
                        Total time: 21414.99s
                               ETA: 1284390.1s

################################################################################
                    [1m Learning iteration 1640/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.899s, learning 0.237s)
               Value function loss: 37.8016
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 252.73
               Mean episode length: 122.26
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26886144
                    Iteration time: 12.14s
                        Total time: 21427.13s
                               ETA: 1284321.8s

################################################################################
                    [1m Learning iteration 1641/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.967s, learning 0.164s)
               Value function loss: 33.2199
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 257.75
               Mean episode length: 123.78
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26902528
                    Iteration time: 12.13s
                        Total time: 21439.26s
                               ETA: 1284253.2s

################################################################################
                    [1m Learning iteration 1642/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.575s, learning 0.164s)
               Value function loss: 45.2429
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 257.12
               Mean episode length: 123.39
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26918912
                    Iteration time: 11.74s
                        Total time: 21450.99s
                               ETA: 1284161.3s

################################################################################
                    [1m Learning iteration 1643/100000 [0m                    

                       Computation: 1419 steps/s (collection: 11.368s, learning 0.170s)
               Value function loss: 26.2722
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 256.29
               Mean episode length: 123.29
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26935296
                    Iteration time: 11.54s
                        Total time: 21462.53s
                               ETA: 1284057.4s

################################################################################
                    [1m Learning iteration 1644/100000 [0m                    

                       Computation: 1423 steps/s (collection: 11.307s, learning 0.199s)
               Value function loss: 33.5125
                    Surrogate loss: -0.0048
             Mean action noise std: 0.78
                       Mean reward: 259.43
               Mean episode length: 123.54
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26951680
                    Iteration time: 11.51s
                        Total time: 21474.04s
                               ETA: 1283951.8s

################################################################################
                    [1m Learning iteration 1645/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.818s, learning 0.158s)
               Value function loss: 35.8319
                    Surrogate loss: -0.0032
             Mean action noise std: 0.78
                       Mean reward: 255.96
               Mean episode length: 123.41
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26968064
                    Iteration time: 11.98s
                        Total time: 21486.02s
                               ETA: 1283874.3s

################################################################################
                    [1m Learning iteration 1646/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.926s, learning 0.168s)
               Value function loss: 28.6240
                    Surrogate loss: -0.0056
             Mean action noise std: 0.78
                       Mean reward: 256.81
               Mean episode length: 123.42
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 26984448
                    Iteration time: 12.09s
                        Total time: 21498.11s
                               ETA: 1283803.9s

################################################################################
                    [1m Learning iteration 1647/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.876s, learning 0.270s)
               Value function loss: 29.4603
                    Surrogate loss: -0.0078
             Mean action noise std: 0.78
                       Mean reward: 257.03
               Mean episode length: 123.64
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27000832
                    Iteration time: 12.15s
                        Total time: 21510.26s
                               ETA: 1283736.7s

################################################################################
                    [1m Learning iteration 1648/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.857s, learning 0.162s)
               Value function loss: 37.8054
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 259.95
               Mean episode length: 124.67
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27017216
                    Iteration time: 12.02s
                        Total time: 21522.28s
                               ETA: 1283662.1s

################################################################################
                    [1m Learning iteration 1649/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.773s, learning 0.201s)
               Value function loss: 34.6925
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 253.12
               Mean episode length: 121.62
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27033600
                    Iteration time: 11.97s
                        Total time: 21534.25s
                               ETA: 1283584.8s

################################################################################
                    [1m Learning iteration 1650/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.558s, learning 0.168s)
               Value function loss: 30.8291
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 254.18
               Mean episode length: 122.56
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27049984
                    Iteration time: 11.73s
                        Total time: 21545.98s
                               ETA: 1283492.8s

################################################################################
                    [1m Learning iteration 1651/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.919s, learning 0.233s)
               Value function loss: 41.1876
                    Surrogate loss: -0.0113
             Mean action noise std: 0.78
                       Mean reward: 254.04
               Mean episode length: 122.28
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27066368
                    Iteration time: 12.15s
                        Total time: 21558.13s
                               ETA: 1283426.3s

################################################################################
                    [1m Learning iteration 1652/100000 [0m                    

                       Computation: 1347 steps/s (collection: 12.003s, learning 0.158s)
               Value function loss: 38.9365
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 256.59
               Mean episode length: 123.75
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27082752
                    Iteration time: 12.16s
                        Total time: 21570.29s
                               ETA: 1283360.4s

################################################################################
                    [1m Learning iteration 1653/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.578s, learning 0.164s)
               Value function loss: 33.5310
                    Surrogate loss: 0.0032
             Mean action noise std: 0.78
                       Mean reward: 250.31
               Mean episode length: 121.51
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27099136
                    Iteration time: 11.74s
                        Total time: 21582.03s
                               ETA: 1283269.6s

################################################################################
                    [1m Learning iteration 1654/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.571s, learning 0.196s)
               Value function loss: 33.9763
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 253.77
               Mean episode length: 123.93
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27115520
                    Iteration time: 11.77s
                        Total time: 21593.80s
                               ETA: 1283180.4s

################################################################################
                    [1m Learning iteration 1655/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.571s, learning 0.170s)
               Value function loss: 33.0291
                    Surrogate loss: 0.0018
             Mean action noise std: 0.78
                       Mean reward: 251.79
               Mean episode length: 122.18
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27131904
                    Iteration time: 11.74s
                        Total time: 21605.54s
                               ETA: 1283089.8s

################################################################################
                    [1m Learning iteration 1656/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.095s, learning 0.269s)
               Value function loss: 31.9570
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 255.72
               Mean episode length: 124.84
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27148288
                    Iteration time: 12.36s
                        Total time: 21617.90s
                               ETA: 1283036.2s

################################################################################
                    [1m Learning iteration 1657/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.918s, learning 0.274s)
               Value function loss: 34.6036
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 259.83
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27164672
                    Iteration time: 12.19s
                        Total time: 21630.10s
                               ETA: 1282972.5s

################################################################################
                    [1m Learning iteration 1658/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.695s, learning 0.166s)
               Value function loss: 40.3121
                    Surrogate loss: 0.0029
             Mean action noise std: 0.78
                       Mean reward: 255.13
               Mean episode length: 123.20
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27181056
                    Iteration time: 11.86s
                        Total time: 21641.96s
                               ETA: 1282889.3s

################################################################################
                    [1m Learning iteration 1659/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.886s, learning 0.166s)
               Value function loss: 30.3884
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 253.79
               Mean episode length: 122.48
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27197440
                    Iteration time: 12.05s
                        Total time: 21654.01s
                               ETA: 1282817.4s

################################################################################
                    [1m Learning iteration 1660/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.528s, learning 0.167s)
               Value function loss: 31.3905
                    Surrogate loss: -0.0076
             Mean action noise std: 0.78
                       Mean reward: 255.74
               Mean episode length: 124.31
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27213824
                    Iteration time: 11.69s
                        Total time: 21665.70s
                               ETA: 1282724.5s

################################################################################
                    [1m Learning iteration 1661/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.106s, learning 0.228s)
               Value function loss: 29.6467
                    Surrogate loss: -0.0065
             Mean action noise std: 0.78
                       Mean reward: 253.85
               Mean episode length: 122.98
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27230208
                    Iteration time: 12.33s
                        Total time: 21678.04s
                               ETA: 1282669.4s

################################################################################
                    [1m Learning iteration 1662/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.638s, learning 0.171s)
               Value function loss: 31.8766
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 250.65
               Mean episode length: 122.19
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27246592
                    Iteration time: 11.81s
                        Total time: 21689.85s
                               ETA: 1282583.4s

################################################################################
                    [1m Learning iteration 1663/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.514s, learning 0.164s)
               Value function loss: 29.2945
                    Surrogate loss: 0.0175
             Mean action noise std: 0.78
                       Mean reward: 254.67
               Mean episode length: 123.52
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27262976
                    Iteration time: 11.68s
                        Total time: 21701.53s
                               ETA: 1282489.7s

################################################################################
                    [1m Learning iteration 1664/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.664s, learning 0.229s)
               Value function loss: 34.7549
                    Surrogate loss: 0.0054
             Mean action noise std: 0.78
                       Mean reward: 259.67
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27279360
                    Iteration time: 11.89s
                        Total time: 21713.42s
                               ETA: 1282408.9s

################################################################################
                    [1m Learning iteration 1665/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.210s, learning 0.167s)
               Value function loss: 26.6325
                    Surrogate loss: -0.0030
             Mean action noise std: 0.78
                       Mean reward: 254.27
               Mean episode length: 123.39
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27295744
                    Iteration time: 12.38s
                        Total time: 21725.80s
                               ETA: 1282356.7s

################################################################################
                    [1m Learning iteration 1666/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.795s, learning 0.199s)
               Value function loss: 31.1246
                    Surrogate loss: 0.0013
             Mean action noise std: 0.78
                       Mean reward: 253.42
               Mean episode length: 124.57
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27312128
                    Iteration time: 11.99s
                        Total time: 21737.79s
                               ETA: 1282281.9s

################################################################################
                    [1m Learning iteration 1667/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.108s, learning 0.179s)
               Value function loss: 36.8733
                    Surrogate loss: -0.0049
             Mean action noise std: 0.78
                       Mean reward: 257.05
               Mean episode length: 124.57
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27328512
                    Iteration time: 12.29s
                        Total time: 21750.08s
                               ETA: 1282224.5s

################################################################################
                    [1m Learning iteration 1668/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.841s, learning 0.168s)
               Value function loss: 37.8824
                    Surrogate loss: 0.0161
             Mean action noise std: 0.78
                       Mean reward: 255.16
               Mean episode length: 123.78
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27344896
                    Iteration time: 12.01s
                        Total time: 21762.09s
                               ETA: 1282150.7s

################################################################################
                    [1m Learning iteration 1669/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.235s, learning 0.271s)
               Value function loss: 26.5978
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 251.95
               Mean episode length: 122.14
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27361280
                    Iteration time: 12.51s
                        Total time: 21774.59s
                               ETA: 1282106.3s

################################################################################
                    [1m Learning iteration 1670/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.698s, learning 0.204s)
               Value function loss: 32.6856
                    Surrogate loss: 0.0046
             Mean action noise std: 0.78
                       Mean reward: 258.24
               Mean episode length: 123.92
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27377664
                    Iteration time: 11.90s
                        Total time: 21786.49s
                               ETA: 1282026.3s

################################################################################
                    [1m Learning iteration 1671/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.635s, learning 0.177s)
               Value function loss: 32.1552
                    Surrogate loss: -0.0011
             Mean action noise std: 0.78
                       Mean reward: 249.36
               Mean episode length: 121.31
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27394048
                    Iteration time: 11.81s
                        Total time: 21798.31s
                               ETA: 1281941.2s

################################################################################
                    [1m Learning iteration 1672/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.490s, learning 0.180s)
               Value function loss: 30.7321
                    Surrogate loss: 0.0075
             Mean action noise std: 0.78
                       Mean reward: 257.35
               Mean episode length: 124.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27410432
                    Iteration time: 11.67s
                        Total time: 21809.98s
                               ETA: 1281847.8s

################################################################################
                    [1m Learning iteration 1673/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.807s, learning 0.160s)
               Value function loss: 33.2088
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: 253.65
               Mean episode length: 124.37
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27426816
                    Iteration time: 11.97s
                        Total time: 21821.94s
                               ETA: 1281772.0s

################################################################################
                    [1m Learning iteration 1674/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.405s, learning 0.186s)
               Value function loss: 25.5505
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 253.94
               Mean episode length: 123.39
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27443200
                    Iteration time: 11.59s
                        Total time: 21833.53s
                               ETA: 1281674.1s

################################################################################
                    [1m Learning iteration 1675/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.581s, learning 0.206s)
               Value function loss: 29.3251
                    Surrogate loss: 0.0045
             Mean action noise std: 0.78
                       Mean reward: 254.51
               Mean episode length: 124.58
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27459584
                    Iteration time: 11.79s
                        Total time: 21845.32s
                               ETA: 1281587.8s

################################################################################
                    [1m Learning iteration 1676/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.501s, learning 0.185s)
               Value function loss: 31.4448
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 256.13
               Mean episode length: 124.22
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27475968
                    Iteration time: 11.69s
                        Total time: 21857.01s
                               ETA: 1281495.7s

################################################################################
                    [1m Learning iteration 1677/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.416s, learning 0.174s)
               Value function loss: 27.7696
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 248.14
               Mean episode length: 121.57
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27492352
                    Iteration time: 11.59s
                        Total time: 21868.60s
                               ETA: 1281398.2s

################################################################################
                    [1m Learning iteration 1678/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.844s, learning 0.156s)
               Value function loss: 22.7484
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 258.00
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27508736
                    Iteration time: 12.00s
                        Total time: 21880.60s
                               ETA: 1281324.7s

################################################################################
                    [1m Learning iteration 1679/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.858s, learning 0.321s)
               Value function loss: 29.9962
                    Surrogate loss: 0.0007
             Mean action noise std: 0.78
                       Mean reward: 248.70
               Mean episode length: 122.15
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27525120
                    Iteration time: 12.18s
                        Total time: 21892.78s
                               ETA: 1281261.7s

################################################################################
                    [1m Learning iteration 1680/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.567s, learning 0.341s)
               Value function loss: 28.7267
                    Surrogate loss: -0.0052
             Mean action noise std: 0.78
                       Mean reward: 254.11
               Mean episode length: 124.87
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27541504
                    Iteration time: 11.91s
                        Total time: 21904.68s
                               ETA: 1281182.9s

################################################################################
                    [1m Learning iteration 1681/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.751s, learning 0.159s)
               Value function loss: 22.4907
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 255.36
               Mean episode length: 124.28
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27557888
                    Iteration time: 11.91s
                        Total time: 21916.59s
                               ETA: 1281104.4s

################################################################################
                    [1m Learning iteration 1682/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.765s, learning 0.204s)
               Value function loss: 28.2270
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 250.36
               Mean episode length: 123.74
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27574272
                    Iteration time: 11.97s
                        Total time: 21928.56s
                               ETA: 1281029.4s

################################################################################
                    [1m Learning iteration 1683/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.829s, learning 0.215s)
               Value function loss: 27.6309
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 248.57
               Mean episode length: 123.32
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27590656
                    Iteration time: 12.04s
                        Total time: 21940.61s
                               ETA: 1280958.9s

################################################################################
                    [1m Learning iteration 1684/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.648s, learning 0.168s)
               Value function loss: 31.8763
                    Surrogate loss: 0.0205
             Mean action noise std: 0.78
                       Mean reward: 255.87
               Mean episode length: 124.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27607040
                    Iteration time: 11.82s
                        Total time: 21952.42s
                               ETA: 1280875.0s

################################################################################
                    [1m Learning iteration 1685/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.366s, learning 0.228s)
               Value function loss: 26.4551
                    Surrogate loss: -0.0002
             Mean action noise std: 0.78
                       Mean reward: 251.59
               Mean episode length: 124.70
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27623424
                    Iteration time: 11.59s
                        Total time: 21964.02s
                               ETA: 1280778.4s

################################################################################
                    [1m Learning iteration 1686/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.787s, learning 0.161s)
               Value function loss: 28.9594
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 253.85
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27639808
                    Iteration time: 11.95s
                        Total time: 21975.97s
                               ETA: 1280702.5s

################################################################################
                    [1m Learning iteration 1687/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.889s, learning 0.171s)
               Value function loss: 32.6364
                    Surrogate loss: 0.0032
             Mean action noise std: 0.78
                       Mean reward: 254.12
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27656192
                    Iteration time: 12.06s
                        Total time: 21988.03s
                               ETA: 1280633.2s

################################################################################
                    [1m Learning iteration 1688/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.608s, learning 0.186s)
               Value function loss: 30.3713
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 247.54
               Mean episode length: 122.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27672576
                    Iteration time: 11.79s
                        Total time: 21999.82s
                               ETA: 1280548.5s

################################################################################
                    [1m Learning iteration 1689/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.033s, learning 0.161s)
               Value function loss: 32.1718
                    Surrogate loss: 0.0105
             Mean action noise std: 0.78
                       Mean reward: 254.32
               Mean episode length: 125.00
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27688960
                    Iteration time: 12.19s
                        Total time: 22012.01s
                               ETA: 1280487.1s

################################################################################
                    [1m Learning iteration 1690/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.714s, learning 0.165s)
               Value function loss: 19.9660
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 248.98
               Mean episode length: 123.69
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27705344
                    Iteration time: 11.88s
                        Total time: 22023.89s
                               ETA: 1280407.4s

################################################################################
                    [1m Learning iteration 1691/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.732s, learning 0.321s)
               Value function loss: 27.6330
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 250.14
               Mean episode length: 123.44
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27721728
                    Iteration time: 12.05s
                        Total time: 22035.95s
                               ETA: 1280338.0s

################################################################################
                    [1m Learning iteration 1692/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.751s, learning 0.199s)
               Value function loss: 29.7675
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 253.31
               Mean episode length: 124.92
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27738112
                    Iteration time: 11.95s
                        Total time: 22047.90s
                               ETA: 1280262.6s

################################################################################
                    [1m Learning iteration 1693/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.976s, learning 0.164s)
               Value function loss: 28.0491
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 249.12
               Mean episode length: 123.84
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27754496
                    Iteration time: 12.14s
                        Total time: 22060.04s
                               ETA: 1280198.3s

################################################################################
                    [1m Learning iteration 1694/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.842s, learning 0.183s)
               Value function loss: 27.6821
                    Surrogate loss: -0.0030
             Mean action noise std: 0.78
                       Mean reward: 247.09
               Mean episode length: 123.03
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27770880
                    Iteration time: 12.02s
                        Total time: 22072.06s
                               ETA: 1280127.5s

################################################################################
                    [1m Learning iteration 1695/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.059s, learning 0.200s)
               Value function loss: 31.3115
                    Surrogate loss: 0.0056
             Mean action noise std: 0.78
                       Mean reward: 247.29
               Mean episode length: 124.03
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27787264
                    Iteration time: 12.26s
                        Total time: 22084.32s
                               ETA: 1280070.2s

################################################################################
                    [1m Learning iteration 1696/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.471s, learning 0.169s)
               Value function loss: 27.1547
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 252.68
               Mean episode length: 125.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27803648
                    Iteration time: 11.64s
                        Total time: 22095.96s
                               ETA: 1279977.2s

################################################################################
                    [1m Learning iteration 1697/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.430s, learning 0.154s)
               Value function loss: 28.8393
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 242.68
               Mean episode length: 123.20
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27820032
                    Iteration time: 11.58s
                        Total time: 22107.54s
                               ETA: 1279881.0s

################################################################################
                    [1m Learning iteration 1698/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.977s, learning 0.166s)
               Value function loss: 33.7424
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 247.55
               Mean episode length: 124.73
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27836416
                    Iteration time: 12.14s
                        Total time: 22119.69s
                               ETA: 1279817.2s

################################################################################
                    [1m Learning iteration 1699/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.852s, learning 0.164s)
               Value function loss: 33.2066
                    Surrogate loss: -0.0023
             Mean action noise std: 0.78
                       Mean reward: 247.19
               Mean episode length: 123.93
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27852800
                    Iteration time: 12.02s
                        Total time: 22131.70s
                               ETA: 1279746.2s

################################################################################
                    [1m Learning iteration 1700/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.245s, learning 0.162s)
               Value function loss: 25.4487
                    Surrogate loss: -0.0082
             Mean action noise std: 0.78
                       Mean reward: 242.35
               Mean episode length: 123.40
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27869184
                    Iteration time: 11.41s
                        Total time: 22143.11s
                               ETA: 1279640.0s

################################################################################
                    [1m Learning iteration 1701/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.175s, learning 0.239s)
               Value function loss: 34.5649
                    Surrogate loss: -0.0038
             Mean action noise std: 0.78
                       Mean reward: 246.39
               Mean episode length: 123.93
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27885568
                    Iteration time: 12.41s
                        Total time: 22155.52s
                               ETA: 1279592.1s

################################################################################
                    [1m Learning iteration 1702/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.000s, learning 0.166s)
               Value function loss: 34.3733
                    Surrogate loss: -0.0050
             Mean action noise std: 0.78
                       Mean reward: 241.75
               Mean episode length: 124.03
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27901952
                    Iteration time: 12.17s
                        Total time: 22167.69s
                               ETA: 1279530.0s

################################################################################
                    [1m Learning iteration 1703/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.619s, learning 0.169s)
               Value function loss: 35.4477
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 243.87
               Mean episode length: 124.06
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27918336
                    Iteration time: 11.79s
                        Total time: 22179.48s
                               ETA: 1279446.0s

################################################################################
                    [1m Learning iteration 1704/100000 [0m                    

                       Computation: 1332 steps/s (collection: 11.994s, learning 0.302s)
               Value function loss: 35.4200
                    Surrogate loss: -0.0030
             Mean action noise std: 0.78
                       Mean reward: 243.53
               Mean episode length: 125.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27934720
                    Iteration time: 12.30s
                        Total time: 22191.77s
                               ETA: 1279391.5s

################################################################################
                    [1m Learning iteration 1705/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.850s, learning 0.164s)
               Value function loss: 31.7808
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 244.75
               Mean episode length: 124.77
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27951104
                    Iteration time: 12.01s
                        Total time: 22203.79s
                               ETA: 1279320.7s

################################################################################
                    [1m Learning iteration 1706/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.545s, learning 0.196s)
               Value function loss: 30.5344
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 237.42
               Mean episode length: 121.53
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27967488
                    Iteration time: 11.74s
                        Total time: 22215.53s
                               ETA: 1279234.4s

################################################################################
                    [1m Learning iteration 1707/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.720s, learning 0.163s)
               Value function loss: 34.8517
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 237.42
               Mean episode length: 122.69
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 27983872
                    Iteration time: 11.88s
                        Total time: 22227.41s
                               ETA: 1279156.2s

################################################################################
                    [1m Learning iteration 1708/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.872s, learning 0.166s)
               Value function loss: 24.4028
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 244.39
               Mean episode length: 124.60
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28000256
                    Iteration time: 12.04s
                        Total time: 22239.45s
                               ETA: 1279087.0s

################################################################################
                    [1m Learning iteration 1709/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.642s, learning 0.162s)
               Value function loss: 31.7318
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 244.11
               Mean episode length: 123.13
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28016640
                    Iteration time: 11.80s
                        Total time: 22251.25s
                               ETA: 1279004.5s

################################################################################
                    [1m Learning iteration 1710/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.563s, learning 0.163s)
               Value function loss: 34.5256
                    Surrogate loss: 0.0262
             Mean action noise std: 0.78
                       Mean reward: 246.67
               Mean episode length: 124.95
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28033024
                    Iteration time: 11.73s
                        Total time: 22262.98s
                               ETA: 1278917.6s

################################################################################
                    [1m Learning iteration 1711/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.501s, learning 0.192s)
               Value function loss: 30.5075
                    Surrogate loss: 0.0020
             Mean action noise std: 0.78
                       Mean reward: 240.74
               Mean episode length: 122.80
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28049408
                    Iteration time: 11.69s
                        Total time: 22274.67s
                               ETA: 1278828.9s

################################################################################
                    [1m Learning iteration 1712/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.626s, learning 0.163s)
               Value function loss: 27.3369
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 245.03
               Mean episode length: 124.02
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28065792
                    Iteration time: 11.79s
                        Total time: 22286.46s
                               ETA: 1278745.7s

################################################################################
                    [1m Learning iteration 1713/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.560s, learning 0.166s)
               Value function loss: 28.3737
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 244.81
               Mean episode length: 124.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28082176
                    Iteration time: 11.73s
                        Total time: 22298.18s
                               ETA: 1278659.0s

################################################################################
                    [1m Learning iteration 1714/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.418s, learning 0.170s)
               Value function loss: 27.4884
                    Surrogate loss: -0.0010
             Mean action noise std: 0.78
                       Mean reward: 248.10
               Mean episode length: 125.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28098560
                    Iteration time: 11.59s
                        Total time: 22309.77s
                               ETA: 1278564.6s

################################################################################
                    [1m Learning iteration 1715/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.761s, learning 0.208s)
               Value function loss: 31.3804
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 236.23
               Mean episode length: 120.29
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28114944
                    Iteration time: 11.97s
                        Total time: 22321.74s
                               ETA: 1278492.0s

################################################################################
                    [1m Learning iteration 1716/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.275s, learning 0.272s)
               Value function loss: 29.0688
                    Surrogate loss: -0.0030
             Mean action noise std: 0.78
                       Mean reward: 235.33
               Mean episode length: 119.27
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28131328
                    Iteration time: 12.55s
                        Total time: 22334.29s
                               ETA: 1278452.6s

################################################################################
                    [1m Learning iteration 1717/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.781s, learning 0.243s)
               Value function loss: 30.9225
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 242.52
               Mean episode length: 124.06
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28147712
                    Iteration time: 12.02s
                        Total time: 22346.31s
                               ETA: 1278383.3s

################################################################################
                    [1m Learning iteration 1718/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.992s, learning 0.213s)
               Value function loss: 31.5048
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 244.93
               Mean episode length: 124.35
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28164096
                    Iteration time: 12.20s
                        Total time: 22358.52s
                               ETA: 1278324.4s

################################################################################
                    [1m Learning iteration 1719/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.102s, learning 0.161s)
               Value function loss: 30.2087
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 244.00
               Mean episode length: 124.66
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28180480
                    Iteration time: 12.26s
                        Total time: 22370.78s
                               ETA: 1278268.9s

################################################################################
                    [1m Learning iteration 1720/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.665s, learning 0.173s)
               Value function loss: 39.2820
                    Surrogate loss: -0.0051
             Mean action noise std: 0.78
                       Mean reward: 238.97
               Mean episode length: 125.00
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28196864
                    Iteration time: 11.84s
                        Total time: 22382.62s
                               ETA: 1278189.2s

################################################################################
                    [1m Learning iteration 1721/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.576s, learning 0.267s)
               Value function loss: 24.4719
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 244.70
               Mean episode length: 123.99
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28213248
                    Iteration time: 11.84s
                        Total time: 22394.46s
                               ETA: 1278109.8s

################################################################################
                    [1m Learning iteration 1722/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.818s, learning 0.199s)
               Value function loss: 33.8823
                    Surrogate loss: -0.0022
             Mean action noise std: 0.78
                       Mean reward: 240.92
               Mean episode length: 122.84
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28229632
                    Iteration time: 12.02s
                        Total time: 22406.48s
                               ETA: 1278040.4s

################################################################################
                    [1m Learning iteration 1723/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.996s, learning 0.166s)
               Value function loss: 33.1474
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 243.97
               Mean episode length: 122.86
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28246016
                    Iteration time: 12.16s
                        Total time: 22418.64s
                               ETA: 1277979.4s

################################################################################
                    [1m Learning iteration 1724/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.892s, learning 0.171s)
               Value function loss: 24.6222
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 239.41
               Mean episode length: 122.53
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28262400
                    Iteration time: 12.06s
                        Total time: 22430.70s
                               ETA: 1277912.8s

################################################################################
                    [1m Learning iteration 1725/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.696s, learning 0.211s)
               Value function loss: 21.5780
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 243.49
               Mean episode length: 124.02
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28278784
                    Iteration time: 11.91s
                        Total time: 22442.61s
                               ETA: 1277837.4s

################################################################################
                    [1m Learning iteration 1726/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.292s, learning 0.158s)
               Value function loss: 28.9581
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 239.33
               Mean episode length: 123.03
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28295168
                    Iteration time: 11.45s
                        Total time: 22454.06s
                               ETA: 1277736.1s

################################################################################
                    [1m Learning iteration 1727/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.805s, learning 0.161s)
               Value function loss: 24.7390
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 239.90
               Mean episode length: 123.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28311552
                    Iteration time: 11.97s
                        Total time: 22466.03s
                               ETA: 1277664.2s

################################################################################
                    [1m Learning iteration 1728/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.790s, learning 0.194s)
               Value function loss: 21.2993
                    Surrogate loss: -0.0104
             Mean action noise std: 0.78
                       Mean reward: 241.32
               Mean episode length: 123.85
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28327936
                    Iteration time: 11.98s
                        Total time: 22478.01s
                               ETA: 1277593.4s

################################################################################
                    [1m Learning iteration 1729/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.048s, learning 0.186s)
               Value function loss: 31.1179
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 243.57
               Mean episode length: 124.28
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28344320
                    Iteration time: 12.23s
                        Total time: 22490.24s
                               ETA: 1277536.9s

################################################################################
                    [1m Learning iteration 1730/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.294s, learning 0.157s)
               Value function loss: 24.8031
                    Surrogate loss: -0.0085
             Mean action noise std: 0.78
                       Mean reward: 238.70
               Mean episode length: 122.18
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28360704
                    Iteration time: 11.45s
                        Total time: 22501.70s
                               ETA: 1277435.9s

################################################################################
                    [1m Learning iteration 1731/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.852s, learning 0.162s)
               Value function loss: 25.4329
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 242.67
               Mean episode length: 123.50
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28377088
                    Iteration time: 12.01s
                        Total time: 22513.71s
                               ETA: 1277367.0s

################################################################################
                    [1m Learning iteration 1732/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.001s, learning 0.301s)
               Value function loss: 26.7182
                    Surrogate loss: -0.0097
             Mean action noise std: 0.78
                       Mean reward: 241.03
               Mean episode length: 123.02
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28393472
                    Iteration time: 12.30s
                        Total time: 22526.01s
                               ETA: 1277314.5s

################################################################################
                    [1m Learning iteration 1733/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.760s, learning 0.169s)
               Value function loss: 26.1459
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 245.96
               Mean episode length: 123.27
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28409856
                    Iteration time: 11.93s
                        Total time: 22537.94s
                               ETA: 1277240.9s

################################################################################
                    [1m Learning iteration 1734/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.592s, learning 0.161s)
               Value function loss: 28.7376
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 242.13
               Mean episode length: 122.76
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28426240
                    Iteration time: 11.75s
                        Total time: 22549.69s
                               ETA: 1277157.4s

################################################################################
                    [1m Learning iteration 1735/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.817s, learning 0.244s)
               Value function loss: 30.5008
                    Surrogate loss: -0.0067
             Mean action noise std: 0.78
                       Mean reward: 241.97
               Mean episode length: 124.19
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28442624
                    Iteration time: 12.06s
                        Total time: 22561.75s
                               ETA: 1277091.4s

################################################################################
                    [1m Learning iteration 1736/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.766s, learning 0.223s)
               Value function loss: 32.2207
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 237.56
               Mean episode length: 121.95
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28459008
                    Iteration time: 11.99s
                        Total time: 22573.74s
                               ETA: 1277021.5s

################################################################################
                    [1m Learning iteration 1737/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.455s, learning 0.169s)
               Value function loss: 25.3821
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 241.80
               Mean episode length: 123.56
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28475392
                    Iteration time: 11.62s
                        Total time: 22585.37s
                               ETA: 1276930.9s

################################################################################
                    [1m Learning iteration 1738/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.557s, learning 0.155s)
               Value function loss: 30.2659
                    Surrogate loss: 0.0140
             Mean action noise std: 0.78
                       Mean reward: 244.63
               Mean episode length: 124.86
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28491776
                    Iteration time: 11.71s
                        Total time: 22597.08s
                               ETA: 1276845.4s

################################################################################
                    [1m Learning iteration 1739/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.833s, learning 0.161s)
               Value function loss: 24.9228
                    Surrogate loss: 0.0015
             Mean action noise std: 0.78
                       Mean reward: 237.49
               Mean episode length: 122.67
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28508160
                    Iteration time: 11.99s
                        Total time: 22609.07s
                               ETA: 1276775.9s

################################################################################
                    [1m Learning iteration 1740/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.681s, learning 0.164s)
               Value function loss: 22.8544
                    Surrogate loss: -0.0055
             Mean action noise std: 0.78
                       Mean reward: 236.41
               Mean episode length: 120.78
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28524544
                    Iteration time: 11.85s
                        Total time: 22620.92s
                               ETA: 1276698.1s

################################################################################
                    [1m Learning iteration 1741/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.882s, learning 0.196s)
               Value function loss: 24.3410
                    Surrogate loss: 0.0032
             Mean action noise std: 0.78
                       Mean reward: 241.48
               Mean episode length: 124.01
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28540928
                    Iteration time: 12.08s
                        Total time: 22633.00s
                               ETA: 1276633.5s

################################################################################
                    [1m Learning iteration 1742/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.567s, learning 0.162s)
               Value function loss: 26.7646
                    Surrogate loss: -0.0014
             Mean action noise std: 0.78
                       Mean reward: 237.97
               Mean episode length: 122.49
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28557312
                    Iteration time: 11.73s
                        Total time: 22644.73s
                               ETA: 1276549.3s

################################################################################
                    [1m Learning iteration 1743/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.636s, learning 0.170s)
               Value function loss: 23.0235
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 238.12
               Mean episode length: 123.21
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28573696
                    Iteration time: 11.81s
                        Total time: 22656.53s
                               ETA: 1276469.5s

################################################################################
                    [1m Learning iteration 1744/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.912s, learning 0.164s)
               Value function loss: 23.5130
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 242.45
               Mean episode length: 123.07
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28590080
                    Iteration time: 12.08s
                        Total time: 22668.61s
                               ETA: 1276405.0s

################################################################################
                    [1m Learning iteration 1745/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.070s, learning 0.175s)
               Value function loss: 23.9321
                    Surrogate loss: -0.0064
             Mean action noise std: 0.78
                       Mean reward: 237.94
               Mean episode length: 121.89
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28606464
                    Iteration time: 12.25s
                        Total time: 22680.85s
                               ETA: 1276350.0s

################################################################################
                    [1m Learning iteration 1746/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.870s, learning 0.159s)
               Value function loss: 25.0164
                    Surrogate loss: -0.0015
             Mean action noise std: 0.78
                       Mean reward: 240.54
               Mean episode length: 123.86
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28622848
                    Iteration time: 12.03s
                        Total time: 22692.88s
                               ETA: 1276283.0s

################################################################################
                    [1m Learning iteration 1747/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.584s, learning 0.159s)
               Value function loss: 22.5298
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 241.00
               Mean episode length: 123.74
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28639232
                    Iteration time: 11.74s
                        Total time: 22704.63s
                               ETA: 1276200.0s

################################################################################
                    [1m Learning iteration 1748/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.892s, learning 0.176s)
               Value function loss: 23.8857
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 239.13
               Mean episode length: 123.07
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28655616
                    Iteration time: 12.07s
                        Total time: 22716.69s
                               ETA: 1276135.3s

################################################################################
                    [1m Learning iteration 1749/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.636s, learning 0.274s)
               Value function loss: 23.7582
                    Surrogate loss: 0.0014
             Mean action noise std: 0.78
                       Mean reward: 243.20
               Mean episode length: 124.10
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28672000
                    Iteration time: 11.91s
                        Total time: 22728.60s
                               ETA: 1276061.7s

################################################################################
                    [1m Learning iteration 1750/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.533s, learning 0.201s)
               Value function loss: 23.1299
                    Surrogate loss: 0.0012
             Mean action noise std: 0.78
                       Mean reward: 240.88
               Mean episode length: 122.23
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28688384
                    Iteration time: 11.73s
                        Total time: 22740.34s
                               ETA: 1275978.4s

################################################################################
                    [1m Learning iteration 1751/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.131s, learning 0.204s)
               Value function loss: 22.9938
                    Surrogate loss: 0.0000
             Mean action noise std: 0.78
                       Mean reward: 229.26
               Mean episode length: 119.70
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28704768
                    Iteration time: 12.33s
                        Total time: 22752.67s
                               ETA: 1275928.8s

################################################################################
                    [1m Learning iteration 1752/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.261s, learning 0.171s)
               Value function loss: 23.3927
                    Surrogate loss: -0.0042
             Mean action noise std: 0.78
                       Mean reward: 242.42
               Mean episode length: 125.00
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28721152
                    Iteration time: 11.43s
                        Total time: 22764.10s
                               ETA: 1275828.7s

################################################################################
                    [1m Learning iteration 1753/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.242s, learning 0.207s)
               Value function loss: 22.6488
                    Surrogate loss: 0.0007
             Mean action noise std: 0.78
                       Mean reward: 234.91
               Mean episode length: 119.36
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28737536
                    Iteration time: 11.45s
                        Total time: 22775.55s
                               ETA: 1275729.6s

################################################################################
                    [1m Learning iteration 1754/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.431s, learning 0.173s)
               Value function loss: 26.1513
                    Surrogate loss: -0.0024
             Mean action noise std: 0.78
                       Mean reward: 239.54
               Mean episode length: 121.68
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28753920
                    Iteration time: 11.60s
                        Total time: 22787.16s
                               ETA: 1275639.3s

################################################################################
                    [1m Learning iteration 1755/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.098s, learning 0.173s)
               Value function loss: 22.4192
                    Surrogate loss: 0.0017
             Mean action noise std: 0.78
                       Mean reward: 241.11
               Mean episode length: 124.01
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28770304
                    Iteration time: 12.27s
                        Total time: 22799.43s
                               ETA: 1275586.4s

################################################################################
                    [1m Learning iteration 1756/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.890s, learning 0.173s)
               Value function loss: 21.2666
                    Surrogate loss: -0.0033
             Mean action noise std: 0.78
                       Mean reward: 239.69
               Mean episode length: 121.43
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28786688
                    Iteration time: 12.06s
                        Total time: 22811.49s
                               ETA: 1275522.0s

################################################################################
                    [1m Learning iteration 1757/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.804s, learning 0.238s)
               Value function loss: 26.3931
                    Surrogate loss: 0.0076
             Mean action noise std: 0.78
                       Mean reward: 243.93
               Mean episode length: 123.98
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28803072
                    Iteration time: 12.04s
                        Total time: 22823.53s
                               ETA: 1275456.4s

################################################################################
                    [1m Learning iteration 1758/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.862s, learning 0.159s)
               Value function loss: 25.2154
                    Surrogate loss: -0.0024
             Mean action noise std: 0.78
                       Mean reward: 243.49
               Mean episode length: 123.39
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28819456
                    Iteration time: 12.02s
                        Total time: 22835.55s
                               ETA: 1275389.7s

################################################################################
                    [1m Learning iteration 1759/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.286s, learning 0.170s)
               Value function loss: 24.0570
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 238.04
               Mean episode length: 122.41
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28835840
                    Iteration time: 11.46s
                        Total time: 22847.01s
                               ETA: 1275291.5s

################################################################################
                    [1m Learning iteration 1760/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.723s, learning 0.161s)
               Value function loss: 29.8788
                    Surrogate loss: 0.0039
             Mean action noise std: 0.78
                       Mean reward: 237.96
               Mean episode length: 121.74
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28852224
                    Iteration time: 11.88s
                        Total time: 22858.89s
                               ETA: 1275217.4s

################################################################################
                    [1m Learning iteration 1761/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.688s, learning 0.216s)
               Value function loss: 26.2618
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 239.47
               Mean episode length: 122.01
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28868608
                    Iteration time: 11.90s
                        Total time: 22870.80s
                               ETA: 1275144.4s

################################################################################
                    [1m Learning iteration 1762/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.841s, learning 0.177s)
               Value function loss: 35.8148
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 246.07
               Mean episode length: 125.00
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28884992
                    Iteration time: 12.02s
                        Total time: 22882.82s
                               ETA: 1275077.8s

################################################################################
                    [1m Learning iteration 1763/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.566s, learning 0.198s)
               Value function loss: 32.9324
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: 234.39
               Mean episode length: 120.36
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28901376
                    Iteration time: 11.76s
                        Total time: 22894.58s
                               ETA: 1274997.1s

################################################################################
                    [1m Learning iteration 1764/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.775s, learning 0.167s)
               Value function loss: 33.0602
                    Surrogate loss: -0.0035
             Mean action noise std: 0.78
                       Mean reward: 241.05
               Mean episode length: 123.90
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28917760
                    Iteration time: 11.94s
                        Total time: 22906.52s
                               ETA: 1274926.4s

################################################################################
                    [1m Learning iteration 1765/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.624s, learning 0.205s)
               Value function loss: 33.3391
                    Surrogate loss: -0.0108
             Mean action noise std: 0.78
                       Mean reward: 241.06
               Mean episode length: 123.12
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28934144
                    Iteration time: 11.83s
                        Total time: 22918.35s
                               ETA: 1274849.6s

################################################################################
                    [1m Learning iteration 1766/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.552s, learning 0.188s)
               Value function loss: 30.1429
                    Surrogate loss: 0.0372
             Mean action noise std: 0.78
                       Mean reward: 235.90
               Mean episode length: 120.28
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28950528
                    Iteration time: 11.74s
                        Total time: 22930.09s
                               ETA: 1274767.8s

################################################################################
                    [1m Learning iteration 1767/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.603s, learning 0.173s)
               Value function loss: 30.1718
                    Surrogate loss: -0.0057
             Mean action noise std: 0.78
                       Mean reward: 233.20
               Mean episode length: 121.13
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28966912
                    Iteration time: 11.78s
                        Total time: 22941.87s
                               ETA: 1274688.1s

################################################################################
                    [1m Learning iteration 1768/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.033s, learning 0.160s)
               Value function loss: 21.1185
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 241.47
               Mean episode length: 124.14
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.68
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28983296
                    Iteration time: 12.19s
                        Total time: 22954.06s
                               ETA: 1274631.6s

################################################################################
                    [1m Learning iteration 1769/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.692s, learning 0.198s)
               Value function loss: 24.1067
                    Surrogate loss: -0.0038
             Mean action noise std: 0.78
                       Mean reward: 237.55
               Mean episode length: 121.78
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 28999680
                    Iteration time: 11.89s
                        Total time: 22965.95s
                               ETA: 1274558.4s

################################################################################
                    [1m Learning iteration 1770/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.186s)
               Value function loss: 27.8458
                    Surrogate loss: -0.0028
             Mean action noise std: 0.78
                       Mean reward: 238.12
               Mean episode length: 123.99
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29016064
                    Iteration time: 12.11s
                        Total time: 22978.06s
                               ETA: 1274497.4s

################################################################################
                    [1m Learning iteration 1771/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.706s, learning 0.163s)
               Value function loss: 23.3738
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 230.94
               Mean episode length: 120.09
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29032448
                    Iteration time: 11.87s
                        Total time: 22989.93s
                               ETA: 1274423.1s

################################################################################
                    [1m Learning iteration 1772/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.430s, learning 0.168s)
               Value function loss: 19.0354
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 236.85
               Mean episode length: 123.42
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29048832
                    Iteration time: 11.60s
                        Total time: 23001.53s
                               ETA: 1274333.9s

################################################################################
                    [1m Learning iteration 1773/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.746s, learning 0.164s)
               Value function loss: 25.0661
                    Surrogate loss: -0.0081
             Mean action noise std: 0.78
                       Mean reward: 233.24
               Mean episode length: 122.32
                  Mean reward/step: 1.92
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29065216
                    Iteration time: 11.91s
                        Total time: 23013.44s
                               ETA: 1274262.0s

################################################################################
                    [1m Learning iteration 1774/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.868s, learning 0.200s)
               Value function loss: 22.5483
                    Surrogate loss: -0.0011
             Mean action noise std: 0.78
                       Mean reward: 233.69
               Mean episode length: 121.77
                  Mean reward/step: 1.93
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29081600
                    Iteration time: 12.07s
                        Total time: 23025.50s
                               ETA: 1274199.0s

################################################################################
                    [1m Learning iteration 1775/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.584s, learning 0.179s)
               Value function loss: 22.1720
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 228.97
               Mean episode length: 120.39
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29097984
                    Iteration time: 11.76s
                        Total time: 23037.27s
                               ETA: 1274119.2s

################################################################################
                    [1m Learning iteration 1776/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.315s, learning 0.199s)
               Value function loss: 27.1675
                    Surrogate loss: -0.0087
             Mean action noise std: 0.78
                       Mean reward: 241.97
               Mean episode length: 124.36
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29114368
                    Iteration time: 11.51s
                        Total time: 23048.78s
                               ETA: 1274025.7s

################################################################################
                    [1m Learning iteration 1777/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.456s, learning 0.165s)
               Value function loss: 26.6431
                    Surrogate loss: 0.0010
             Mean action noise std: 0.78
                       Mean reward: 235.28
               Mean episode length: 121.74
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29130752
                    Iteration time: 11.62s
                        Total time: 23060.40s
                               ETA: 1273938.1s

################################################################################
                    [1m Learning iteration 1778/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.786s, learning 0.162s)
               Value function loss: 23.3286
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 239.71
               Mean episode length: 123.07
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29147136
                    Iteration time: 11.95s
                        Total time: 23072.35s
                               ETA: 1273868.7s

################################################################################
                    [1m Learning iteration 1779/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.938s, learning 0.170s)
               Value function loss: 26.6303
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 227.40
               Mean episode length: 120.24
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29163520
                    Iteration time: 12.11s
                        Total time: 23084.46s
                               ETA: 1273808.2s

################################################################################
                    [1m Learning iteration 1780/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.171s)
               Value function loss: 20.6228
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 241.26
               Mean episode length: 123.13
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29179904
                    Iteration time: 12.21s
                        Total time: 23096.67s
                               ETA: 1273753.5s

################################################################################
                    [1m Learning iteration 1781/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.949s, learning 0.218s)
               Value function loss: 22.4134
                    Surrogate loss: -0.0129
             Mean action noise std: 0.78
                       Mean reward: 236.45
               Mean episode length: 122.06
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29196288
                    Iteration time: 12.17s
                        Total time: 23108.84s
                               ETA: 1273696.4s

################################################################################
                    [1m Learning iteration 1782/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.666s, learning 0.187s)
               Value function loss: 25.1530
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 242.87
               Mean episode length: 123.91
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29212672
                    Iteration time: 11.85s
                        Total time: 23120.69s
                               ETA: 1273622.0s

################################################################################
                    [1m Learning iteration 1783/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.611s, learning 0.167s)
               Value function loss: 25.1906
                    Surrogate loss: -0.0073
             Mean action noise std: 0.78
                       Mean reward: 243.59
               Mean episode length: 124.02
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29229056
                    Iteration time: 11.78s
                        Total time: 23132.47s
                               ETA: 1273543.5s

################################################################################
                    [1m Learning iteration 1784/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.698s, learning 0.198s)
               Value function loss: 21.4656
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 245.22
               Mean episode length: 123.18
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29245440
                    Iteration time: 11.90s
                        Total time: 23144.36s
                               ETA: 1273471.6s

################################################################################
                    [1m Learning iteration 1785/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.870s, learning 0.238s)
               Value function loss: 26.6481
                    Surrogate loss: -0.0013
             Mean action noise std: 0.78
                       Mean reward: 240.16
               Mean episode length: 122.35
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29261824
                    Iteration time: 12.11s
                        Total time: 23156.47s
                               ETA: 1273411.5s

################################################################################
                    [1m Learning iteration 1786/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.397s, learning 0.221s)
               Value function loss: 23.0090
                    Surrogate loss: -0.0061
             Mean action noise std: 0.78
                       Mean reward: 240.89
               Mean episode length: 123.01
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29278208
                    Iteration time: 11.62s
                        Total time: 23168.09s
                               ETA: 1273324.5s

################################################################################
                    [1m Learning iteration 1787/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.635s, learning 0.162s)
               Value function loss: 22.8719
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 241.22
               Mean episode length: 124.93
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29294592
                    Iteration time: 11.80s
                        Total time: 23179.89s
                               ETA: 1273247.4s

################################################################################
                    [1m Learning iteration 1788/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.501s, learning 0.179s)
               Value function loss: 24.2299
                    Surrogate loss: -0.0017
             Mean action noise std: 0.78
                       Mean reward: 232.44
               Mean episode length: 121.12
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29310976
                    Iteration time: 11.68s
                        Total time: 23191.57s
                               ETA: 1273163.9s

################################################################################
                    [1m Learning iteration 1789/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.766s, learning 0.208s)
               Value function loss: 28.8140
                    Surrogate loss: 0.0008
             Mean action noise std: 0.78
                       Mean reward: 236.57
               Mean episode length: 123.48
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29327360
                    Iteration time: 11.97s
                        Total time: 23203.54s
                               ETA: 1273096.6s

################################################################################
                    [1m Learning iteration 1790/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.418s, learning 0.171s)
               Value function loss: 23.1056
                    Surrogate loss: -0.0110
             Mean action noise std: 0.78
                       Mean reward: 241.93
               Mean episode length: 123.30
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29343744
                    Iteration time: 11.59s
                        Total time: 23215.13s
                               ETA: 1273008.3s

################################################################################
                    [1m Learning iteration 1791/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.137s, learning 0.324s)
               Value function loss: 27.1953
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 240.72
               Mean episode length: 123.98
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29360128
                    Iteration time: 12.46s
                        Total time: 23227.59s
                               ETA: 1272967.8s

################################################################################
                    [1m Learning iteration 1792/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.741s, learning 0.228s)
               Value function loss: 28.8684
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 242.34
               Mean episode length: 123.05
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29376512
                    Iteration time: 11.97s
                        Total time: 23239.56s
                               ETA: 1272900.5s

################################################################################
                    [1m Learning iteration 1793/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.609s, learning 0.202s)
               Value function loss: 33.2087
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 238.64
               Mean episode length: 123.07
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29392896
                    Iteration time: 11.81s
                        Total time: 23251.37s
                               ETA: 1272824.6s

################################################################################
                    [1m Learning iteration 1794/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.347s, learning 0.169s)
               Value function loss: 28.2996
                    Surrogate loss: -0.0060
             Mean action noise std: 0.78
                       Mean reward: 238.75
               Mean episode length: 122.04
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29409280
                    Iteration time: 12.52s
                        Total time: 23263.89s
                               ETA: 1272787.4s

################################################################################
                    [1m Learning iteration 1795/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.936s, learning 0.202s)
               Value function loss: 29.6308
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 237.51
               Mean episode length: 121.90
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29425664
                    Iteration time: 12.14s
                        Total time: 23276.03s
                               ETA: 1272729.4s

################################################################################
                    [1m Learning iteration 1796/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.946s, learning 0.213s)
               Value function loss: 25.0887
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 241.49
               Mean episode length: 124.04
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29442048
                    Iteration time: 12.16s
                        Total time: 23288.18s
                               ETA: 1272672.7s

################################################################################
                    [1m Learning iteration 1797/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.686s, learning 0.163s)
               Value function loss: 24.4884
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 226.19
               Mean episode length: 116.91
                  Mean reward/step: 1.94
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29458432
                    Iteration time: 11.85s
                        Total time: 23300.03s
                               ETA: 1272599.1s

################################################################################
                    [1m Learning iteration 1798/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.852s, learning 0.165s)
               Value function loss: 28.2386
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 235.33
               Mean episode length: 121.20
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29474816
                    Iteration time: 12.02s
                        Total time: 23312.05s
                               ETA: 1272534.7s

################################################################################
                    [1m Learning iteration 1799/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.858s, learning 0.198s)
               Value function loss: 24.0519
                    Surrogate loss: 0.0004
             Mean action noise std: 0.78
                       Mean reward: 242.20
               Mean episode length: 123.62
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29491200
                    Iteration time: 12.06s
                        Total time: 23324.11s
                               ETA: 1272472.5s

################################################################################
                    [1m Learning iteration 1800/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.020s, learning 0.195s)
               Value function loss: 23.8551
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 239.74
               Mean episode length: 123.25
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29507584
                    Iteration time: 12.22s
                        Total time: 23336.32s
                               ETA: 1272419.0s

################################################################################
                    [1m Learning iteration 1801/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.041s, learning 0.169s)
               Value function loss: 28.0875
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 238.25
               Mean episode length: 123.11
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29523968
                    Iteration time: 12.21s
                        Total time: 23348.53s
                               ETA: 1272365.3s

################################################################################
                    [1m Learning iteration 1802/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.428s, learning 0.163s)
               Value function loss: 23.2461
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 235.49
               Mean episode length: 121.16
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29540352
                    Iteration time: 11.59s
                        Total time: 23360.12s
                               ETA: 1272278.0s

################################################################################
                    [1m Learning iteration 1803/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.274s, learning 0.166s)
               Value function loss: 21.0827
                    Surrogate loss: -0.0154
             Mean action noise std: 0.78
                       Mean reward: 230.42
               Mean episode length: 120.23
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29556736
                    Iteration time: 11.44s
                        Total time: 23371.56s
                               ETA: 1272182.5s

################################################################################
                    [1m Learning iteration 1804/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.688s, learning 0.165s)
               Value function loss: 27.6463
                    Surrogate loss: -0.0079
             Mean action noise std: 0.78
                       Mean reward: 243.72
               Mean episode length: 124.66
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29573120
                    Iteration time: 11.85s
                        Total time: 23383.41s
                               ETA: 1272109.6s

################################################################################
                    [1m Learning iteration 1805/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.380s, learning 0.173s)
               Value function loss: 26.5241
                    Surrogate loss: -0.0095
             Mean action noise std: 0.78
                       Mean reward: 243.16
               Mean episode length: 123.78
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29589504
                    Iteration time: 11.55s
                        Total time: 23394.97s
                               ETA: 1272020.4s

################################################################################
                    [1m Learning iteration 1806/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.561s, learning 0.169s)
               Value function loss: 24.0718
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 243.48
               Mean episode length: 124.02
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29605888
                    Iteration time: 11.73s
                        Total time: 23406.70s
                               ETA: 1271941.0s

################################################################################
                    [1m Learning iteration 1807/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.036s, learning 0.258s)
               Value function loss: 29.9907
                    Surrogate loss: -0.0046
             Mean action noise std: 0.78
                       Mean reward: 238.06
               Mean episode length: 120.30
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29622272
                    Iteration time: 12.29s
                        Total time: 23418.99s
                               ETA: 1271892.2s

################################################################################
                    [1m Learning iteration 1808/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.496s, learning 0.165s)
               Value function loss: 26.0072
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 233.91
               Mean episode length: 120.62
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29638656
                    Iteration time: 11.66s
                        Total time: 23430.65s
                               ETA: 1271809.1s

################################################################################
                    [1m Learning iteration 1809/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.766s, learning 0.161s)
               Value function loss: 28.1576
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 234.57
               Mean episode length: 121.38
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29655040
                    Iteration time: 11.93s
                        Total time: 23442.58s
                               ETA: 1271740.6s

################################################################################
                    [1m Learning iteration 1810/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.472s, learning 0.159s)
               Value function loss: 29.0598
                    Surrogate loss: -0.0018
             Mean action noise std: 0.78
                       Mean reward: 234.64
               Mean episode length: 121.55
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29671424
                    Iteration time: 11.63s
                        Total time: 23454.21s
                               ETA: 1271656.0s

################################################################################
                    [1m Learning iteration 1811/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.834s, learning 0.252s)
               Value function loss: 28.2404
                    Surrogate loss: -0.0084
             Mean action noise std: 0.78
                       Mean reward: 239.59
               Mean episode length: 123.34
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29687808
                    Iteration time: 12.09s
                        Total time: 23466.30s
                               ETA: 1271596.2s

################################################################################
                    [1m Learning iteration 1812/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.499s, learning 0.215s)
               Value function loss: 30.7484
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 242.80
               Mean episode length: 121.97
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29704192
                    Iteration time: 11.71s
                        Total time: 23478.01s
                               ETA: 1271516.3s

################################################################################
                    [1m Learning iteration 1813/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.062s, learning 0.164s)
               Value function loss: 28.5662
                    Surrogate loss: -0.0137
             Mean action noise std: 0.78
                       Mean reward: 242.06
               Mean episode length: 121.69
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29720576
                    Iteration time: 12.23s
                        Total time: 23490.24s
                               ETA: 1271464.1s

################################################################################
                    [1m Learning iteration 1814/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.747s, learning 0.238s)
               Value function loss: 31.3607
                    Surrogate loss: -0.0116
             Mean action noise std: 0.78
                       Mean reward: 239.00
               Mean episode length: 122.06
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29736960
                    Iteration time: 11.98s
                        Total time: 23502.22s
                               ETA: 1271399.0s

################################################################################
                    [1m Learning iteration 1815/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.973s, learning 0.208s)
               Value function loss: 23.0184
                    Surrogate loss: -0.0102
             Mean action noise std: 0.78
                       Mean reward: 233.33
               Mean episode length: 119.59
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.67
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29753344
                    Iteration time: 12.18s
                        Total time: 23514.40s
                               ETA: 1271344.5s

################################################################################
                    [1m Learning iteration 1816/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.500s, learning 0.177s)
               Value function loss: 28.6500
                    Surrogate loss: -0.0040
             Mean action noise std: 0.78
                       Mean reward: 237.06
               Mean episode length: 118.62
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29769728
                    Iteration time: 11.68s
                        Total time: 23526.08s
                               ETA: 1271262.8s

################################################################################
                    [1m Learning iteration 1817/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.492s, learning 0.162s)
               Value function loss: 27.7138
                    Surrogate loss: -0.0150
             Mean action noise std: 0.78
                       Mean reward: 234.96
               Mean episode length: 119.45
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29786112
                    Iteration time: 11.65s
                        Total time: 23537.73s
                               ETA: 1271180.0s

################################################################################
                    [1m Learning iteration 1818/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.812s, learning 0.210s)
               Value function loss: 28.0507
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 236.29
               Mean episode length: 121.46
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29802496
                    Iteration time: 12.02s
                        Total time: 23549.76s
                               ETA: 1271117.1s

################################################################################
                    [1m Learning iteration 1819/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.837s, learning 0.292s)
               Value function loss: 25.8033
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 230.47
               Mean episode length: 117.86
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29818880
                    Iteration time: 12.13s
                        Total time: 23561.88s
                               ETA: 1271060.1s

################################################################################
                    [1m Learning iteration 1820/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.692s, learning 0.177s)
               Value function loss: 31.2243
                    Surrogate loss: -0.0107
             Mean action noise std: 0.78
                       Mean reward: 231.32
               Mean episode length: 118.09
                  Mean reward/step: 1.95
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29835264
                    Iteration time: 11.87s
                        Total time: 23573.75s
                               ETA: 1270989.1s

################################################################################
                    [1m Learning iteration 1821/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.744s, learning 0.191s)
               Value function loss: 26.6366
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 242.02
               Mean episode length: 121.55
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29851648
                    Iteration time: 11.93s
                        Total time: 23585.69s
                               ETA: 1270921.7s

################################################################################
                    [1m Learning iteration 1822/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.424s, learning 0.192s)
               Value function loss: 29.2141
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 237.73
               Mean episode length: 120.80
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29868032
                    Iteration time: 11.62s
                        Total time: 23597.31s
                               ETA: 1270837.2s

################################################################################
                    [1m Learning iteration 1823/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.893s, learning 0.164s)
               Value function loss: 32.5346
                    Surrogate loss: -0.0096
             Mean action noise std: 0.78
                       Mean reward: 232.33
               Mean episode length: 118.65
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29884416
                    Iteration time: 12.06s
                        Total time: 23609.36s
                               ETA: 1270776.5s

################################################################################
                    [1m Learning iteration 1824/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.610s, learning 0.168s)
               Value function loss: 31.1792
                    Surrogate loss: -0.0106
             Mean action noise std: 0.78
                       Mean reward: 244.85
               Mean episode length: 124.05
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29900800
                    Iteration time: 11.78s
                        Total time: 23621.14s
                               ETA: 1270700.9s

################################################################################
                    [1m Learning iteration 1825/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.537s, learning 0.162s)
               Value function loss: 27.1312
                    Surrogate loss: -0.0182
             Mean action noise std: 0.78
                       Mean reward: 244.21
               Mean episode length: 123.12
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29917184
                    Iteration time: 11.70s
                        Total time: 23632.84s
                               ETA: 1270621.1s

################################################################################
                    [1m Learning iteration 1826/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.696s, learning 0.182s)
               Value function loss: 35.8357
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 243.53
               Mean episode length: 122.97
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29933568
                    Iteration time: 11.88s
                        Total time: 23644.72s
                               ETA: 1270551.0s

################################################################################
                    [1m Learning iteration 1827/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.774s, learning 0.264s)
               Value function loss: 29.7952
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 234.70
               Mean episode length: 120.14
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29949952
                    Iteration time: 12.04s
                        Total time: 23656.76s
                               ETA: 1270489.5s

################################################################################
                    [1m Learning iteration 1828/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.017s, learning 0.196s)
               Value function loss: 30.0914
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 232.07
               Mean episode length: 118.80
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29966336
                    Iteration time: 12.21s
                        Total time: 23668.97s
                               ETA: 1270437.4s

################################################################################
                    [1m Learning iteration 1829/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.794s, learning 0.251s)
               Value function loss: 31.4740
                    Surrogate loss: -0.0098
             Mean action noise std: 0.78
                       Mean reward: 236.16
               Mean episode length: 119.77
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29982720
                    Iteration time: 12.05s
                        Total time: 23681.01s
                               ETA: 1270376.5s

################################################################################
                    [1m Learning iteration 1830/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.791s, learning 0.231s)
               Value function loss: 29.1330
                    Surrogate loss: -0.0086
             Mean action noise std: 0.78
                       Mean reward: 237.48
               Mean episode length: 121.88
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 29999104
                    Iteration time: 12.02s
                        Total time: 23693.04s
                               ETA: 1270314.3s

################################################################################
                    [1m Learning iteration 1831/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.650s, learning 0.164s)
               Value function loss: 29.9363
                    Surrogate loss: -0.0109
             Mean action noise std: 0.78
                       Mean reward: 241.83
               Mean episode length: 120.77
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30015488
                    Iteration time: 11.81s
                        Total time: 23704.85s
                               ETA: 1270241.0s

################################################################################
                    [1m Learning iteration 1832/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.589s, learning 0.223s)
               Value function loss: 32.5182
                    Surrogate loss: -0.0101
             Mean action noise std: 0.78
                       Mean reward: 234.22
               Mean episode length: 119.58
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30031872
                    Iteration time: 11.81s
                        Total time: 23716.66s
                               ETA: 1270167.7s

################################################################################
                    [1m Learning iteration 1833/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.508s, learning 0.306s)
               Value function loss: 23.3838
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 232.22
               Mean episode length: 118.49
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30048256
                    Iteration time: 11.81s
                        Total time: 23728.48s
                               ETA: 1270094.6s

################################################################################
                    [1m Learning iteration 1834/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.880s, learning 0.256s)
               Value function loss: 23.7482
                    Surrogate loss: -0.0047
             Mean action noise std: 0.78
                       Mean reward: 245.55
               Mean episode length: 123.21
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30064640
                    Iteration time: 12.14s
                        Total time: 23740.61s
                               ETA: 1270038.8s

################################################################################
                    [1m Learning iteration 1835/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.815s, learning 0.158s)
               Value function loss: 28.5612
                    Surrogate loss: -0.0120
             Mean action noise std: 0.78
                       Mean reward: 233.15
               Mean episode length: 118.90
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30081024
                    Iteration time: 11.97s
                        Total time: 23752.59s
                               ETA: 1269974.3s

################################################################################
                    [1m Learning iteration 1836/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.663s, learning 0.279s)
               Value function loss: 28.9625
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 239.68
               Mean episode length: 121.73
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30097408
                    Iteration time: 11.94s
                        Total time: 23764.53s
                               ETA: 1269908.1s

################################################################################
                    [1m Learning iteration 1837/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.720s, learning 0.162s)
               Value function loss: 28.3865
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 232.81
               Mean episode length: 118.65
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30113792
                    Iteration time: 11.88s
                        Total time: 23776.41s
                               ETA: 1269838.9s

################################################################################
                    [1m Learning iteration 1838/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.536s, learning 0.268s)
               Value function loss: 35.9348
                    Surrogate loss: -0.0034
             Mean action noise std: 0.78
                       Mean reward: 246.03
               Mean episode length: 123.61
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30130176
                    Iteration time: 11.80s
                        Total time: 23788.22s
                               ETA: 1269765.5s

################################################################################
                    [1m Learning iteration 1839/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.729s, learning 0.165s)
               Value function loss: 29.2139
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 241.62
               Mean episode length: 122.53
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30146560
                    Iteration time: 11.89s
                        Total time: 23800.11s
                               ETA: 1269697.1s

################################################################################
                    [1m Learning iteration 1840/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.934s, learning 0.170s)
               Value function loss: 32.2004
                    Surrogate loss: -0.0105
             Mean action noise std: 0.78
                       Mean reward: 233.04
               Mean episode length: 117.96
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30162944
                    Iteration time: 12.10s
                        Total time: 23812.21s
                               ETA: 1269639.8s

################################################################################
                    [1m Learning iteration 1841/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.786s, learning 0.233s)
               Value function loss: 28.3755
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 230.56
               Mean episode length: 118.70
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30179328
                    Iteration time: 12.02s
                        Total time: 23824.23s
                               ETA: 1269578.1s

################################################################################
                    [1m Learning iteration 1842/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.824s, learning 0.158s)
               Value function loss: 29.0122
                    Surrogate loss: -0.0139
             Mean action noise std: 0.78
                       Mean reward: 238.84
               Mean episode length: 120.18
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30195712
                    Iteration time: 11.98s
                        Total time: 23836.22s
                               ETA: 1269514.5s

################################################################################
                    [1m Learning iteration 1843/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.391s, learning 0.177s)
               Value function loss: 28.4840
                    Surrogate loss: -0.0136
             Mean action noise std: 0.78
                       Mean reward: 233.03
               Mean episode length: 119.75
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30212096
                    Iteration time: 11.57s
                        Total time: 23847.78s
                               ETA: 1269428.9s

################################################################################
                    [1m Learning iteration 1844/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.950s, learning 0.170s)
               Value function loss: 29.2302
                    Surrogate loss: -0.0003
             Mean action noise std: 0.78
                       Mean reward: 226.12
               Mean episode length: 115.57
                  Mean reward/step: 1.96
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30228480
                    Iteration time: 12.12s
                        Total time: 23859.90s
                               ETA: 1269372.7s

################################################################################
                    [1m Learning iteration 1845/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.634s, learning 0.172s)
               Value function loss: 32.5294
                    Surrogate loss: -0.0025
             Mean action noise std: 0.78
                       Mean reward: 231.36
               Mean episode length: 119.26
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30244864
                    Iteration time: 11.81s
                        Total time: 23871.71s
                               ETA: 1269299.9s

################################################################################
                    [1m Learning iteration 1846/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.880s, learning 0.163s)
               Value function loss: 24.3314
                    Surrogate loss: -0.0123
             Mean action noise std: 0.78
                       Mean reward: 225.38
               Mean episode length: 115.57
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30261248
                    Iteration time: 12.04s
                        Total time: 23883.75s
                               ETA: 1269239.8s

################################################################################
                    [1m Learning iteration 1847/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.966s, learning 0.267s)
               Value function loss: 28.5326
                    Surrogate loss: -0.0094
             Mean action noise std: 0.78
                       Mean reward: 239.39
               Mean episode length: 121.87
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30277632
                    Iteration time: 12.23s
                        Total time: 23895.98s
                               ETA: 1269189.7s

################################################################################
                    [1m Learning iteration 1848/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.813s, learning 0.220s)
               Value function loss: 35.2743
                    Surrogate loss: -0.0063
             Mean action noise std: 0.78
                       Mean reward: 231.15
               Mean episode length: 118.16
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30294016
                    Iteration time: 12.03s
                        Total time: 23908.02s
                               ETA: 1269129.1s

################################################################################
                    [1m Learning iteration 1849/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.932s, learning 0.178s)
               Value function loss: 28.5389
                    Surrogate loss: -0.0037
             Mean action noise std: 0.78
                       Mean reward: 226.04
               Mean episode length: 115.29
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30310400
                    Iteration time: 12.11s
                        Total time: 23920.13s
                               ETA: 1269072.6s

################################################################################
                    [1m Learning iteration 1850/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.831s, learning 0.247s)
               Value function loss: 24.2937
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 237.11
               Mean episode length: 119.58
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30326784
                    Iteration time: 12.08s
                        Total time: 23932.21s
                               ETA: 1269014.5s

################################################################################
                    [1m Learning iteration 1851/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.022s, learning 0.169s)
               Value function loss: 33.8451
                    Surrogate loss: -0.0130
             Mean action noise std: 0.78
                       Mean reward: 235.07
               Mean episode length: 118.84
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30343168
                    Iteration time: 12.19s
                        Total time: 23944.40s
                               ETA: 1268962.5s

################################################################################
                    [1m Learning iteration 1852/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.873s, learning 0.164s)
               Value function loss: 27.8781
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 237.98
               Mean episode length: 118.91
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30359552
                    Iteration time: 12.04s
                        Total time: 23956.43s
                               ETA: 1268902.3s

################################################################################
                    [1m Learning iteration 1853/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.609s, learning 0.161s)
               Value function loss: 29.6316
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 238.97
               Mean episode length: 121.56
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30375936
                    Iteration time: 11.77s
                        Total time: 23968.20s
                               ETA: 1268828.1s

################################################################################
                    [1m Learning iteration 1854/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.350s, learning 0.167s)
               Value function loss: 35.8038
                    Surrogate loss: -0.0054
             Mean action noise std: 0.78
                       Mean reward: 245.28
               Mean episode length: 123.93
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30392320
                    Iteration time: 11.52s
                        Total time: 23979.72s
                               ETA: 1268740.5s

################################################################################
                    [1m Learning iteration 1855/100000 [0m                    

                       Computation: 1426 steps/s (collection: 11.316s, learning 0.171s)
               Value function loss: 30.6448
                    Surrogate loss: -0.0041
             Mean action noise std: 0.78
                       Mean reward: 233.23
               Mean episode length: 118.73
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30408704
                    Iteration time: 11.49s
                        Total time: 23991.21s
                               ETA: 1268651.4s

################################################################################
                    [1m Learning iteration 1856/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.987s, learning 0.225s)
               Value function loss: 32.7839
                    Surrogate loss: -0.0006
             Mean action noise std: 0.78
                       Mean reward: 246.09
               Mean episode length: 122.20
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30425088
                    Iteration time: 12.21s
                        Total time: 24003.42s
                               ETA: 1268600.7s

################################################################################
                    [1m Learning iteration 1857/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.659s, learning 0.310s)
               Value function loss: 32.7399
                    Surrogate loss: -0.0029
             Mean action noise std: 0.78
                       Mean reward: 235.69
               Mean episode length: 118.53
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30441472
                    Iteration time: 11.97s
                        Total time: 24015.39s
                               ETA: 1268537.3s

################################################################################
                    [1m Learning iteration 1858/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.734s, learning 0.200s)
               Value function loss: 31.2108
                    Surrogate loss: -0.0068
             Mean action noise std: 0.78
                       Mean reward: 234.31
               Mean episode length: 118.28
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30457856
                    Iteration time: 11.93s
                        Total time: 24027.32s
                               ETA: 1268472.0s

################################################################################
                    [1m Learning iteration 1859/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.699s, learning 0.178s)
               Value function loss: 30.7977
                    Surrogate loss: -0.0119
             Mean action noise std: 0.78
                       Mean reward: 235.06
               Mean episode length: 118.79
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30474240
                    Iteration time: 11.88s
                        Total time: 24039.20s
                               ETA: 1268403.8s

################################################################################
                    [1m Learning iteration 1860/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.934s, learning 0.291s)
               Value function loss: 32.2727
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 237.74
               Mean episode length: 122.01
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30490624
                    Iteration time: 12.22s
                        Total time: 24051.42s
                               ETA: 1268353.9s

################################################################################
                    [1m Learning iteration 1861/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.113s, learning 0.317s)
               Value function loss: 32.4835
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 234.29
               Mean episode length: 117.89
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30507008
                    Iteration time: 12.43s
                        Total time: 24063.85s
                               ETA: 1268315.0s

################################################################################
                    [1m Learning iteration 1862/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.951s, learning 0.250s)
               Value function loss: 26.6740
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 228.05
               Mean episode length: 116.71
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30523392
                    Iteration time: 12.20s
                        Total time: 24076.06s
                               ETA: 1268264.0s

################################################################################
                    [1m Learning iteration 1863/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.051s, learning 0.189s)
               Value function loss: 39.1832
                    Surrogate loss: -0.0186
             Mean action noise std: 0.78
                       Mean reward: 240.59
               Mean episode length: 121.88
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30539776
                    Iteration time: 12.24s
                        Total time: 24088.30s
                               ETA: 1268215.2s

################################################################################
                    [1m Learning iteration 1864/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.150s, learning 0.191s)
               Value function loss: 35.5375
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 234.39
               Mean episode length: 117.79
                  Mean reward/step: 1.97
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30556160
                    Iteration time: 12.34s
                        Total time: 24100.64s
                               ETA: 1268171.6s

################################################################################
                    [1m Learning iteration 1865/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.564s, learning 0.174s)
               Value function loss: 29.7249
                    Surrogate loss: 0.0035
             Mean action noise std: 0.78
                       Mean reward: 237.31
               Mean episode length: 119.79
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30572544
                    Iteration time: 11.74s
                        Total time: 24112.37s
                               ETA: 1268096.4s

################################################################################
                    [1m Learning iteration 1866/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.671s, learning 0.159s)
               Value function loss: 35.8348
                    Surrogate loss: -0.0077
             Mean action noise std: 0.78
                       Mean reward: 233.68
               Mean episode length: 118.38
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30588928
                    Iteration time: 11.83s
                        Total time: 24124.20s
                               ETA: 1268026.1s

################################################################################
                    [1m Learning iteration 1867/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.771s, learning 0.161s)
               Value function loss: 32.7881
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 231.63
               Mean episode length: 119.73
                  Mean reward/step: 1.98
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30605312
                    Iteration time: 11.93s
                        Total time: 24136.14s
                               ETA: 1267961.2s

################################################################################
                    [1m Learning iteration 1868/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.251s, learning 0.165s)
               Value function loss: 29.5272
                    Surrogate loss: -0.0100
             Mean action noise std: 0.78
                       Mean reward: 231.62
               Mean episode length: 117.72
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30621696
                    Iteration time: 11.42s
                        Total time: 24147.55s
                               ETA: 1267869.3s

################################################################################
                    [1m Learning iteration 1869/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.858s, learning 0.166s)
               Value function loss: 34.1038
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 232.64
               Mean episode length: 118.30
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30638080
                    Iteration time: 12.02s
                        Total time: 24159.58s
                               ETA: 1267809.3s

################################################################################
                    [1m Learning iteration 1870/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.834s, learning 0.162s)
               Value function loss: 29.1807
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 240.90
               Mean episode length: 121.25
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30654464
                    Iteration time: 12.00s
                        Total time: 24171.57s
                               ETA: 1267748.0s

################################################################################
                    [1m Learning iteration 1871/100000 [0m                    

                       Computation: 775 steps/s (collection: 20.906s, learning 0.232s)
               Value function loss: 35.8409
                    Surrogate loss: -0.0133
             Mean action noise std: 0.78
                       Mean reward: 233.12
               Mean episode length: 117.04
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30670848
                    Iteration time: 21.14s
                        Total time: 24192.71s
                               ETA: 1268165.9s

################################################################################
                    [1m Learning iteration 1872/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.281s, learning 0.215s)
               Value function loss: 30.5009
                    Surrogate loss: -0.0126
             Mean action noise std: 0.78
                       Mean reward: 243.02
               Mean episode length: 120.82
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30687232
                    Iteration time: 23.50s
                        Total time: 24216.21s
                               ETA: 1268706.8s

################################################################################
                    [1m Learning iteration 1873/100000 [0m                    

                       Computation: 702 steps/s (collection: 23.020s, learning 0.298s)
               Value function loss: 31.4136
                    Surrogate loss: -0.0002
             Mean action noise std: 0.78
                       Mean reward: 239.79
               Mean episode length: 118.16
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30703616
                    Iteration time: 23.32s
                        Total time: 24239.52s
                               ETA: 1269237.9s

################################################################################
                    [1m Learning iteration 1874/100000 [0m                    

                       Computation: 714 steps/s (collection: 22.773s, learning 0.164s)
               Value function loss: 29.1766
                    Surrogate loss: 0.0015
             Mean action noise std: 0.78
                       Mean reward: 231.53
               Mean episode length: 117.91
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30720000
                    Iteration time: 22.94s
                        Total time: 24262.46s
                               ETA: 1269748.4s

################################################################################
                    [1m Learning iteration 1875/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.449s, learning 0.221s)
               Value function loss: 31.7494
                    Surrogate loss: 0.0023
             Mean action noise std: 0.78
                       Mean reward: 242.41
               Mean episode length: 120.19
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30736384
                    Iteration time: 23.67s
                        Total time: 24286.13s
                               ETA: 1270296.7s

################################################################################
                    [1m Learning iteration 1876/100000 [0m                    

                       Computation: 687 steps/s (collection: 23.674s, learning 0.168s)
               Value function loss: 30.7666
                    Surrogate loss: -0.0093
             Mean action noise std: 0.78
                       Mean reward: 233.20
               Mean episode length: 117.05
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30752768
                    Iteration time: 23.84s
                        Total time: 24309.97s
                               ETA: 1270853.4s

################################################################################
                    [1m Learning iteration 1877/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.418s, learning 0.242s)
               Value function loss: 26.7122
                    Surrogate loss: -0.0044
             Mean action noise std: 0.78
                       Mean reward: 247.43
               Mean episode length: 122.21
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30769152
                    Iteration time: 23.66s
                        Total time: 24333.63s
                               ETA: 1271399.9s

################################################################################
                    [1m Learning iteration 1878/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.201s, learning 0.168s)
               Value function loss: 29.7193
                    Surrogate loss: -0.0118
             Mean action noise std: 0.78
                       Mean reward: 233.34
               Mean episode length: 117.38
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30785536
                    Iteration time: 23.37s
                        Total time: 24357.00s
                               ETA: 1271930.6s

################################################################################
                    [1m Learning iteration 1879/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.360s, learning 0.167s)
               Value function loss: 38.9692
                    Surrogate loss: -0.0053
             Mean action noise std: 0.78
                       Mean reward: 241.38
               Mean episode length: 120.10
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30801920
                    Iteration time: 23.53s
                        Total time: 24380.53s
                               ETA: 1272469.0s

################################################################################
                    [1m Learning iteration 1880/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.675s, learning 0.229s)
               Value function loss: 29.6192
                    Surrogate loss: -0.0181
             Mean action noise std: 0.78
                       Mean reward: 229.51
               Mean episode length: 115.69
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30818304
                    Iteration time: 23.90s
                        Total time: 24404.43s
                               ETA: 1273026.6s

################################################################################
                    [1m Learning iteration 1881/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.947s, learning 0.167s)
               Value function loss: 30.2154
                    Surrogate loss: -0.0090
             Mean action noise std: 0.78
                       Mean reward: 241.85
               Mean episode length: 120.45
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30834688
                    Iteration time: 23.11s
                        Total time: 24427.55s
                               ETA: 1273542.3s

################################################################################
                    [1m Learning iteration 1882/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.354s, learning 0.167s)
               Value function loss: 38.1261
                    Surrogate loss: -0.0072
             Mean action noise std: 0.78
                       Mean reward: 246.15
               Mean episode length: 121.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30851072
                    Iteration time: 23.52s
                        Total time: 24451.07s
                               ETA: 1274078.6s

################################################################################
                    [1m Learning iteration 1883/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.408s, learning 0.231s)
               Value function loss: 28.3939
                    Surrogate loss: -0.0161
             Mean action noise std: 0.78
                       Mean reward: 241.25
               Mean episode length: 118.65
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30867456
                    Iteration time: 23.64s
                        Total time: 24474.71s
                               ETA: 1274620.4s

################################################################################
                    [1m Learning iteration 1884/100000 [0m                    

                       Computation: 678 steps/s (collection: 23.866s, learning 0.288s)
               Value function loss: 29.8727
                    Surrogate loss: -0.0099
             Mean action noise std: 0.78
                       Mean reward: 241.82
               Mean episode length: 121.49
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30883840
                    Iteration time: 24.15s
                        Total time: 24498.86s
                               ETA: 1275188.5s

################################################################################
                    [1m Learning iteration 1885/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.430s, learning 0.178s)
               Value function loss: 27.9183
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 244.75
               Mean episode length: 121.87
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30900224
                    Iteration time: 23.61s
                        Total time: 24522.47s
                               ETA: 1275727.5s

################################################################################
                    [1m Learning iteration 1886/100000 [0m                    

                       Computation: 689 steps/s (collection: 23.547s, learning 0.210s)
               Value function loss: 25.4514
                    Surrogate loss: -0.0169
             Mean action noise std: 0.78
                       Mean reward: 241.97
               Mean episode length: 122.27
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30916608
                    Iteration time: 23.76s
                        Total time: 24546.23s
                               ETA: 1276273.7s

################################################################################
                    [1m Learning iteration 1887/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.406s, learning 0.251s)
               Value function loss: 28.7101
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 227.30
               Mean episode length: 115.02
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30932992
                    Iteration time: 23.66s
                        Total time: 24569.88s
                               ETA: 1276814.1s

################################################################################
                    [1m Learning iteration 1888/100000 [0m                    

                       Computation: 714 steps/s (collection: 22.757s, learning 0.163s)
               Value function loss: 28.6192
                    Surrogate loss: -0.0174
             Mean action noise std: 0.78
                       Mean reward: 245.27
               Mean episode length: 121.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30949376
                    Iteration time: 22.92s
                        Total time: 24592.80s
                               ETA: 1277315.6s

################################################################################
                    [1m Learning iteration 1889/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.137s, learning 0.204s)
               Value function loss: 23.7739
                    Surrogate loss: -0.0069
             Mean action noise std: 0.78
                       Mean reward: 244.30
               Mean episode length: 122.46
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30965760
                    Iteration time: 23.34s
                        Total time: 24616.14s
                               ETA: 1277838.4s

################################################################################
                    [1m Learning iteration 1890/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.247s, learning 0.205s)
               Value function loss: 26.8134
                    Surrogate loss: -0.0138
             Mean action noise std: 0.78
                       Mean reward: 243.52
               Mean episode length: 119.97
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30982144
                    Iteration time: 23.45s
                        Total time: 24639.60s
                               ETA: 1278366.3s

################################################################################
                    [1m Learning iteration 1891/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.161s, learning 0.179s)
               Value function loss: 27.0956
                    Surrogate loss: -0.0088
             Mean action noise std: 0.78
                       Mean reward: 246.08
               Mean episode length: 121.82
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 30998528
                    Iteration time: 23.34s
                        Total time: 24662.94s
                               ETA: 1278887.9s

################################################################################
                    [1m Learning iteration 1892/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.378s, learning 0.202s)
               Value function loss: 29.7826
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 237.28
               Mean episode length: 117.83
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31014912
                    Iteration time: 23.58s
                        Total time: 24686.52s
                               ETA: 1279421.4s

################################################################################
                    [1m Learning iteration 1893/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.283s, learning 0.237s)
               Value function loss: 22.8438
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 234.28
               Mean episode length: 118.04
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31031296
                    Iteration time: 23.52s
                        Total time: 24710.04s
                               ETA: 1279951.2s

################################################################################
                    [1m Learning iteration 1894/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.423s, learning 0.273s)
               Value function loss: 25.4288
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 241.15
               Mean episode length: 120.55
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31047680
                    Iteration time: 23.70s
                        Total time: 24733.73s
                               ETA: 1280489.5s

################################################################################
                    [1m Learning iteration 1895/100000 [0m                    

                       Computation: 707 steps/s (collection: 22.985s, learning 0.161s)
               Value function loss: 32.0633
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 241.09
               Mean episode length: 120.34
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31064064
                    Iteration time: 23.15s
                        Total time: 24756.88s
                               ETA: 1280998.7s

################################################################################
                    [1m Learning iteration 1896/100000 [0m                    

                       Computation: 705 steps/s (collection: 22.945s, learning 0.276s)
               Value function loss: 27.5166
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 244.69
               Mean episode length: 122.64
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31080448
                    Iteration time: 23.22s
                        Total time: 24780.10s
                               ETA: 1281511.2s

################################################################################
                    [1m Learning iteration 1897/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.236s, learning 0.164s)
               Value function loss: 22.7944
                    Surrogate loss: -0.0149
             Mean action noise std: 0.78
                       Mean reward: 238.44
               Mean episode length: 119.04
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31096832
                    Iteration time: 23.40s
                        Total time: 24803.50s
                               ETA: 1282032.5s

################################################################################
                    [1m Learning iteration 1898/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.962s, learning 0.167s)
               Value function loss: 29.5044
                    Surrogate loss: -0.0092
             Mean action noise std: 0.78
                       Mean reward: 241.83
               Mean episode length: 121.04
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31113216
                    Iteration time: 23.13s
                        Total time: 24826.63s
                               ETA: 1282539.2s

################################################################################
                    [1m Learning iteration 1899/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.384s, learning 0.240s)
               Value function loss: 24.8982
                    Surrogate loss: -0.0141
             Mean action noise std: 0.78
                       Mean reward: 235.53
               Mean episode length: 117.34
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31129600
                    Iteration time: 23.62s
                        Total time: 24850.25s
                               ETA: 1283070.9s

################################################################################
                    [1m Learning iteration 1900/100000 [0m                    

                       Computation: 705 steps/s (collection: 23.026s, learning 0.194s)
               Value function loss: 26.0740
                    Surrogate loss: -0.0114
             Mean action noise std: 0.78
                       Mean reward: 239.24
               Mean episode length: 118.13
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31145984
                    Iteration time: 23.22s
                        Total time: 24873.47s
                               ETA: 1283581.1s

################################################################################
                    [1m Learning iteration 1901/100000 [0m                    

                       Computation: 722 steps/s (collection: 22.527s, learning 0.161s)
               Value function loss: 24.2176
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 238.28
               Mean episode length: 119.09
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31162368
                    Iteration time: 22.69s
                        Total time: 24896.16s
                               ETA: 1284063.3s

################################################################################
                    [1m Learning iteration 1902/100000 [0m                    

                       Computation: 713 steps/s (collection: 22.803s, learning 0.173s)
               Value function loss: 28.1927
                    Surrogate loss: -0.0127
             Mean action noise std: 0.78
                       Mean reward: 238.93
               Mean episode length: 121.05
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31178752
                    Iteration time: 22.98s
                        Total time: 24919.14s
                               ETA: 1284559.9s

################################################################################
                    [1m Learning iteration 1903/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.951s, learning 0.176s)
               Value function loss: 23.9454
                    Surrogate loss: -0.0184
             Mean action noise std: 0.78
                       Mean reward: 236.68
               Mean episode length: 119.43
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31195136
                    Iteration time: 23.13s
                        Total time: 24942.26s
                               ETA: 1285063.7s

################################################################################
                    [1m Learning iteration 1904/100000 [0m                    

                       Computation: 711 steps/s (collection: 22.840s, learning 0.175s)
               Value function loss: 25.2919
                    Surrogate loss: -0.0122
             Mean action noise std: 0.78
                       Mean reward: 243.31
               Mean episode length: 122.11
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31211520
                    Iteration time: 23.01s
                        Total time: 24965.28s
                               ETA: 1285561.2s

################################################################################
                    [1m Learning iteration 1905/100000 [0m                    

                       Computation: 710 steps/s (collection: 22.843s, learning 0.232s)
               Value function loss: 24.6802
                    Surrogate loss: -0.0080
             Mean action noise std: 0.78
                       Mean reward: 234.64
               Mean episode length: 118.48
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31227904
                    Iteration time: 23.08s
                        Total time: 24988.35s
                               ETA: 1286061.2s

################################################################################
                    [1m Learning iteration 1906/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.188s, learning 0.184s)
               Value function loss: 26.2597
                    Surrogate loss: -0.0142
             Mean action noise std: 0.78
                       Mean reward: 240.78
               Mean episode length: 120.46
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31244288
                    Iteration time: 23.37s
                        Total time: 25011.73s
                               ETA: 1286575.9s

################################################################################
                    [1m Learning iteration 1907/100000 [0m                    

                       Computation: 702 steps/s (collection: 23.118s, learning 0.200s)
               Value function loss: 26.6125
                    Surrogate loss: -0.0125
             Mean action noise std: 0.78
                       Mean reward: 233.30
               Mean episode length: 117.87
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31260672
                    Iteration time: 23.32s
                        Total time: 25035.04s
                               ETA: 1287087.3s

################################################################################
                    [1m Learning iteration 1908/100000 [0m                    

                       Computation: 743 steps/s (collection: 21.870s, learning 0.176s)
               Value function loss: 24.0234
                    Surrogate loss: -0.0148
             Mean action noise std: 0.78
                       Mean reward: 244.66
               Mean episode length: 122.54
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31277056
                    Iteration time: 22.05s
                        Total time: 25057.09s
                               ETA: 1287532.9s

################################################################################
                    [1m Learning iteration 1909/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.680s, learning 0.181s)
               Value function loss: 25.5114
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 237.58
               Mean episode length: 118.52
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31293440
                    Iteration time: 11.86s
                        Total time: 25068.95s
                               ETA: 1287454.8s

################################################################################
                    [1m Learning iteration 1910/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.584s, learning 0.165s)
               Value function loss: 27.9315
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 246.07
               Mean episode length: 121.49
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31309824
                    Iteration time: 11.75s
                        Total time: 25080.70s
                               ETA: 1287371.1s

################################################################################
                    [1m Learning iteration 1911/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.529s, learning 0.191s)
               Value function loss: 24.3691
                    Surrogate loss: -0.0147
             Mean action noise std: 0.78
                       Mean reward: 239.22
               Mean episode length: 120.26
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31326208
                    Iteration time: 11.72s
                        Total time: 25092.42s
                               ETA: 1287285.9s

################################################################################
                    [1m Learning iteration 1912/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.802s, learning 0.172s)
               Value function loss: 23.9054
                    Surrogate loss: -0.0144
             Mean action noise std: 0.78
                       Mean reward: 239.10
               Mean episode length: 119.13
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31342592
                    Iteration time: 11.97s
                        Total time: 25104.40s
                               ETA: 1287213.8s

################################################################################
                    [1m Learning iteration 1913/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.824s, learning 0.175s)
               Value function loss: 23.1581
                    Surrogate loss: -0.0151
             Mean action noise std: 0.78
                       Mean reward: 242.38
               Mean episode length: 121.83
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31358976
                    Iteration time: 12.00s
                        Total time: 25116.39s
                               ETA: 1287143.1s

################################################################################
                    [1m Learning iteration 1914/100000 [0m                    

                       Computation: 1412 steps/s (collection: 11.418s, learning 0.184s)
               Value function loss: 27.7371
                    Surrogate loss: -0.0156
             Mean action noise std: 0.78
                       Mean reward: 245.83
               Mean episode length: 122.66
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31375360
                    Iteration time: 11.60s
                        Total time: 25128.00s
                               ETA: 1287052.0s

################################################################################
                    [1m Learning iteration 1915/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.855s, learning 0.318s)
               Value function loss: 23.8375
                    Surrogate loss: -0.0140
             Mean action noise std: 0.78
                       Mean reward: 240.61
               Mean episode length: 121.04
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31391744
                    Iteration time: 12.17s
                        Total time: 25140.17s
                               ETA: 1286990.4s

################################################################################
                    [1m Learning iteration 1916/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.120s, learning 0.259s)
               Value function loss: 26.6084
                    Surrogate loss: -0.0145
             Mean action noise std: 0.78
                       Mean reward: 243.24
               Mean episode length: 121.48
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31408128
                    Iteration time: 12.38s
                        Total time: 25152.55s
                               ETA: 1286939.3s

################################################################################
                    [1m Learning iteration 1917/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.881s, learning 0.174s)
               Value function loss: 22.1405
                    Surrogate loss: -0.0115
             Mean action noise std: 0.78
                       Mean reward: 242.84
               Mean episode length: 119.21
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31424512
                    Iteration time: 12.06s
                        Total time: 25164.61s
                               ETA: 1286871.7s

################################################################################
                    [1m Learning iteration 1918/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.652s, learning 0.188s)
               Value function loss: 32.6317
                    Surrogate loss: -0.0066
             Mean action noise std: 0.78
                       Mean reward: 240.99
               Mean episode length: 120.48
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31440896
                    Iteration time: 11.84s
                        Total time: 25176.44s
                               ETA: 1286793.1s

################################################################################
                    [1m Learning iteration 1919/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.631s, learning 0.181s)
               Value function loss: 27.3609
                    Surrogate loss: -0.0132
             Mean action noise std: 0.78
                       Mean reward: 242.87
               Mean episode length: 120.91
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31457280
                    Iteration time: 11.81s
                        Total time: 25188.26s
                               ETA: 1286713.2s

################################################################################
                    [1m Learning iteration 1920/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.890s, learning 0.239s)
               Value function loss: 22.6557
                    Surrogate loss: -0.0173
             Mean action noise std: 0.78
                       Mean reward: 248.06
               Mean episode length: 123.77
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31473664
                    Iteration time: 12.13s
                        Total time: 25200.39s
                               ETA: 1286649.6s

################################################################################
                    [1m Learning iteration 1921/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.523s, learning 0.196s)
               Value function loss: 25.6600
                    Surrogate loss: -0.0135
             Mean action noise std: 0.78
                       Mean reward: 237.05
               Mean episode length: 119.11
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31490048
                    Iteration time: 11.72s
                        Total time: 25212.10s
                               ETA: 1286565.0s

################################################################################
                    [1m Learning iteration 1922/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.778s, learning 0.234s)
               Value function loss: 26.4440
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 236.26
               Mean episode length: 119.08
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31506432
                    Iteration time: 12.01s
                        Total time: 25224.12s
                               ETA: 1286495.5s

################################################################################
                    [1m Learning iteration 1923/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.134s, learning 0.187s)
               Value function loss: 30.0621
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 228.39
               Mean episode length: 114.18
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31522816
                    Iteration time: 12.32s
                        Total time: 25236.44s
                               ETA: 1286441.8s

################################################################################
                    [1m Learning iteration 1924/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.892s, learning 0.174s)
               Value function loss: 23.1585
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 250.19
               Mean episode length: 124.99
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31539200
                    Iteration time: 12.07s
                        Total time: 25248.50s
                               ETA: 1286375.2s

################################################################################
                    [1m Learning iteration 1925/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.858s, learning 0.206s)
               Value function loss: 28.0726
                    Surrogate loss: -0.0055
             Mean action noise std: 0.77
                       Mean reward: 244.61
               Mean episode length: 120.93
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31555584
                    Iteration time: 12.06s
                        Total time: 25260.57s
                               ETA: 1286308.5s

################################################################################
                    [1m Learning iteration 1926/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.570s, learning 0.168s)
               Value function loss: 29.1741
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 246.83
               Mean episode length: 122.39
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31571968
                    Iteration time: 11.74s
                        Total time: 25272.31s
                               ETA: 1286225.3s

################################################################################
                    [1m Learning iteration 1927/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.298s, learning 0.164s)
               Value function loss: 24.2238
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 238.94
               Mean episode length: 120.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31588352
                    Iteration time: 12.46s
                        Total time: 25284.77s
                               ETA: 1286178.9s

################################################################################
                    [1m Learning iteration 1928/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.770s, learning 0.170s)
               Value function loss: 22.7801
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 245.54
               Mean episode length: 120.37
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31604736
                    Iteration time: 11.94s
                        Total time: 25296.71s
                               ETA: 1286106.1s

################################################################################
                    [1m Learning iteration 1929/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.097s, learning 0.223s)
               Value function loss: 28.1176
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 244.56
               Mean episode length: 121.23
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31621120
                    Iteration time: 12.32s
                        Total time: 25309.03s
                               ETA: 1286052.6s

################################################################################
                    [1m Learning iteration 1930/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.164s, learning 0.213s)
               Value function loss: 22.2184
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 234.71
               Mean episode length: 118.21
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31637504
                    Iteration time: 12.38s
                        Total time: 25321.40s
                               ETA: 1286002.1s

################################################################################
                    [1m Learning iteration 1931/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.795s, learning 0.192s)
               Value function loss: 22.7880
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 242.50
               Mean episode length: 120.46
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31653888
                    Iteration time: 11.99s
                        Total time: 25333.39s
                               ETA: 1285931.9s

################################################################################
                    [1m Learning iteration 1932/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.779s, learning 0.160s)
               Value function loss: 22.6305
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 237.14
               Mean episode length: 118.59
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31670272
                    Iteration time: 11.94s
                        Total time: 25345.33s
                               ETA: 1285859.3s

################################################################################
                    [1m Learning iteration 1933/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.358s, learning 0.164s)
               Value function loss: 23.9697
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 242.64
               Mean episode length: 120.35
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31686656
                    Iteration time: 11.52s
                        Total time: 25356.85s
                               ETA: 1285765.5s

################################################################################
                    [1m Learning iteration 1934/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.275s, learning 0.176s)
               Value function loss: 25.6580
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 236.49
               Mean episode length: 118.55
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31703040
                    Iteration time: 11.45s
                        Total time: 25368.30s
                               ETA: 1285668.3s

################################################################################
                    [1m Learning iteration 1935/100000 [0m                    

                       Computation: 1414 steps/s (collection: 11.415s, learning 0.168s)
               Value function loss: 22.7141
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 244.69
               Mean episode length: 119.56
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31719424
                    Iteration time: 11.58s
                        Total time: 25379.89s
                               ETA: 1285577.8s

################################################################################
                    [1m Learning iteration 1936/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.522s, learning 0.171s)
               Value function loss: 21.6748
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 248.35
               Mean episode length: 122.06
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31735808
                    Iteration time: 11.69s
                        Total time: 25391.58s
                               ETA: 1285492.9s

################################################################################
                    [1m Learning iteration 1937/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.550s, learning 0.156s)
               Value function loss: 23.5192
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 238.96
               Mean episode length: 118.75
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31752192
                    Iteration time: 11.71s
                        Total time: 25403.29s
                               ETA: 1285408.9s

################################################################################
                    [1m Learning iteration 1938/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.878s, learning 0.265s)
               Value function loss: 26.1219
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 243.01
               Mean episode length: 121.10
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31768576
                    Iteration time: 12.14s
                        Total time: 25415.43s
                               ETA: 1285347.0s

################################################################################
                    [1m Learning iteration 1939/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.825s, learning 0.172s)
               Value function loss: 25.3729
                    Surrogate loss: -0.0010
             Mean action noise std: 0.77
                       Mean reward: 244.79
               Mean episode length: 121.34
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31784960
                    Iteration time: 12.00s
                        Total time: 25427.43s
                               ETA: 1285277.8s

################################################################################
                    [1m Learning iteration 1940/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.694s, learning 0.240s)
               Value function loss: 21.2892
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 247.21
               Mean episode length: 122.99
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31801344
                    Iteration time: 11.93s
                        Total time: 25439.36s
                               ETA: 1285205.4s

################################################################################
                    [1m Learning iteration 1941/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.242s, learning 0.168s)
               Value function loss: 22.7265
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 244.11
               Mean episode length: 121.68
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31817728
                    Iteration time: 12.41s
                        Total time: 25451.77s
                               ETA: 1285157.1s

################################################################################
                    [1m Learning iteration 1942/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.836s, learning 0.201s)
               Value function loss: 24.9264
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 254.04
               Mean episode length: 124.06
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31834112
                    Iteration time: 12.04s
                        Total time: 25463.81s
                               ETA: 1285090.1s

################################################################################
                    [1m Learning iteration 1943/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.711s, learning 0.169s)
               Value function loss: 23.0312
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 243.50
               Mean episode length: 122.17
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31850496
                    Iteration time: 11.88s
                        Total time: 25475.69s
                               ETA: 1285015.2s

################################################################################
                    [1m Learning iteration 1944/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.731s, learning 0.223s)
               Value function loss: 21.5103
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 247.98
               Mean episode length: 123.06
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31866880
                    Iteration time: 11.95s
                        Total time: 25487.64s
                               ETA: 1284944.0s

################################################################################
                    [1m Learning iteration 1945/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.972s, learning 0.166s)
               Value function loss: 24.7972
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 244.51
               Mean episode length: 121.61
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31883264
                    Iteration time: 12.14s
                        Total time: 25499.78s
                               ETA: 1284882.3s

################################################################################
                    [1m Learning iteration 1946/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.869s, learning 0.247s)
               Value function loss: 21.7448
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 244.37
               Mean episode length: 120.31
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31899648
                    Iteration time: 12.12s
                        Total time: 25511.90s
                               ETA: 1284819.5s

################################################################################
                    [1m Learning iteration 1947/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.842s, learning 0.212s)
               Value function loss: 24.7033
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 242.54
               Mean episode length: 121.10
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31916032
                    Iteration time: 12.05s
                        Total time: 25523.95s
                               ETA: 1284753.6s

################################################################################
                    [1m Learning iteration 1948/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.770s, learning 0.173s)
               Value function loss: 22.7466
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 243.20
               Mean episode length: 120.28
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31932416
                    Iteration time: 11.94s
                        Total time: 25535.89s
                               ETA: 1284682.1s

################################################################################
                    [1m Learning iteration 1949/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.619s, learning 0.185s)
               Value function loss: 28.4313
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 245.81
               Mean episode length: 121.28
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31948800
                    Iteration time: 11.80s
                        Total time: 25547.70s
                               ETA: 1284603.7s

################################################################################
                    [1m Learning iteration 1950/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.809s, learning 0.197s)
               Value function loss: 26.6329
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 240.35
               Mean episode length: 119.11
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31965184
                    Iteration time: 12.01s
                        Total time: 25559.70s
                               ETA: 1284535.6s

################################################################################
                    [1m Learning iteration 1951/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.784s, learning 0.274s)
               Value function loss: 20.3762
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 238.61
               Mean episode length: 118.61
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31981568
                    Iteration time: 12.06s
                        Total time: 25571.76s
                               ETA: 1284470.1s

################################################################################
                    [1m Learning iteration 1952/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.771s, learning 0.184s)
               Value function loss: 23.5854
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 244.59
               Mean episode length: 121.90
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 31997952
                    Iteration time: 11.95s
                        Total time: 25583.72s
                               ETA: 1284399.5s

################################################################################
                    [1m Learning iteration 1953/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.759s, learning 0.165s)
               Value function loss: 26.5117
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 242.70
               Mean episode length: 121.32
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32014336
                    Iteration time: 11.92s
                        Total time: 25595.64s
                               ETA: 1284327.3s

################################################################################
                    [1m Learning iteration 1954/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.868s, learning 0.199s)
               Value function loss: 25.8117
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 248.74
               Mean episode length: 122.81
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32030720
                    Iteration time: 12.07s
                        Total time: 25607.71s
                               ETA: 1284262.5s

################################################################################
                    [1m Learning iteration 1955/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.751s, learning 0.197s)
               Value function loss: 22.5418
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 247.58
               Mean episode length: 121.25
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32047104
                    Iteration time: 11.95s
                        Total time: 25619.65s
                               ETA: 1284191.7s

################################################################################
                    [1m Learning iteration 1956/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.997s, learning 0.206s)
               Value function loss: 23.9065
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 247.89
               Mean episode length: 122.84
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32063488
                    Iteration time: 12.20s
                        Total time: 25631.86s
                               ETA: 1284133.8s

################################################################################
                    [1m Learning iteration 1957/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.060s, learning 0.173s)
               Value function loss: 27.3692
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 243.63
               Mean episode length: 120.08
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32079872
                    Iteration time: 12.23s
                        Total time: 25644.09s
                               ETA: 1284077.4s

################################################################################
                    [1m Learning iteration 1958/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.103s, learning 0.166s)
               Value function loss: 23.0095
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 241.55
               Mean episode length: 119.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32096256
                    Iteration time: 12.27s
                        Total time: 25656.36s
                               ETA: 1284022.9s

################################################################################
                    [1m Learning iteration 1959/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.032s, learning 0.340s)
               Value function loss: 25.2295
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 243.63
               Mean episode length: 119.80
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32112640
                    Iteration time: 12.37s
                        Total time: 25668.73s
                               ETA: 1283973.5s

################################################################################
                    [1m Learning iteration 1960/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.798s, learning 0.162s)
               Value function loss: 27.2400
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 240.90
               Mean episode length: 118.71
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32129024
                    Iteration time: 11.96s
                        Total time: 25680.69s
                               ETA: 1283903.7s

################################################################################
                    [1m Learning iteration 1961/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.984s, learning 0.155s)
               Value function loss: 24.9954
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 248.93
               Mean episode length: 122.91
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32145408
                    Iteration time: 12.14s
                        Total time: 25692.83s
                               ETA: 1283842.8s

################################################################################
                    [1m Learning iteration 1962/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.479s, learning 0.261s)
               Value function loss: 26.4768
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 238.78
               Mean episode length: 116.71
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32161792
                    Iteration time: 11.74s
                        Total time: 25704.57s
                               ETA: 1283762.0s

################################################################################
                    [1m Learning iteration 1963/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.902s, learning 0.192s)
               Value function loss: 22.8674
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 248.77
               Mean episode length: 121.13
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32178176
                    Iteration time: 12.09s
                        Total time: 25716.67s
                               ETA: 1283699.0s

################################################################################
                    [1m Learning iteration 1964/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.529s, learning 0.215s)
               Value function loss: 23.0088
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 244.99
               Mean episode length: 121.14
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32194560
                    Iteration time: 11.74s
                        Total time: 25728.41s
                               ETA: 1283618.5s

################################################################################
                    [1m Learning iteration 1965/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.717s, learning 0.188s)
               Value function loss: 27.6655
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 247.99
               Mean episode length: 121.01
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32210944
                    Iteration time: 11.91s
                        Total time: 25740.31s
                               ETA: 1283546.1s

################################################################################
                    [1m Learning iteration 1966/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.985s, learning 0.216s)
               Value function loss: 28.8039
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 242.67
               Mean episode length: 118.27
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32227328
                    Iteration time: 12.20s
                        Total time: 25752.51s
                               ETA: 1283488.6s

################################################################################
                    [1m Learning iteration 1967/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.017s, learning 0.197s)
               Value function loss: 22.3214
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 240.00
               Mean episode length: 119.63
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32243712
                    Iteration time: 12.21s
                        Total time: 25764.73s
                               ETA: 1283431.7s

################################################################################
                    [1m Learning iteration 1968/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.932s, learning 0.175s)
               Value function loss: 29.7125
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 252.18
               Mean episode length: 122.76
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32260096
                    Iteration time: 12.11s
                        Total time: 25776.84s
                               ETA: 1283369.7s

################################################################################
                    [1m Learning iteration 1969/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.036s, learning 0.171s)
               Value function loss: 27.7937
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 243.03
               Mean episode length: 120.06
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32276480
                    Iteration time: 12.21s
                        Total time: 25789.04s
                               ETA: 1283312.6s

################################################################################
                    [1m Learning iteration 1970/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.670s, learning 0.178s)
               Value function loss: 30.9378
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 247.48
               Mean episode length: 121.09
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32292864
                    Iteration time: 11.85s
                        Total time: 25800.89s
                               ETA: 1283237.6s

################################################################################
                    [1m Learning iteration 1971/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.847s, learning 0.176s)
               Value function loss: 22.4578
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 237.72
               Mean episode length: 116.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32309248
                    Iteration time: 12.02s
                        Total time: 25812.91s
                               ETA: 1283171.5s

################################################################################
                    [1m Learning iteration 1972/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.830s, learning 0.261s)
               Value function loss: 25.5236
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 241.78
               Mean episode length: 119.34
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32325632
                    Iteration time: 12.09s
                        Total time: 25825.01s
                               ETA: 1283108.8s

################################################################################
                    [1m Learning iteration 1973/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.868s, learning 0.220s)
               Value function loss: 26.6620
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 243.27
               Mean episode length: 119.83
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32342016
                    Iteration time: 12.09s
                        Total time: 25837.09s
                               ETA: 1283046.0s

################################################################################
                    [1m Learning iteration 1974/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.009s, learning 0.164s)
               Value function loss: 24.5284
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 246.16
               Mean episode length: 120.88
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32358400
                    Iteration time: 12.17s
                        Total time: 25849.27s
                               ETA: 1282987.4s

################################################################################
                    [1m Learning iteration 1975/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.729s, learning 0.163s)
               Value function loss: 22.1459
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 246.57
               Mean episode length: 121.90
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32374784
                    Iteration time: 11.89s
                        Total time: 25861.16s
                               ETA: 1282915.0s

################################################################################
                    [1m Learning iteration 1976/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.451s, learning 0.219s)
               Value function loss: 23.4779
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 243.37
               Mean episode length: 120.15
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32391168
                    Iteration time: 11.67s
                        Total time: 25872.83s
                               ETA: 1282831.6s

################################################################################
                    [1m Learning iteration 1977/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.980s, learning 0.172s)
               Value function loss: 22.5312
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 241.37
               Mean episode length: 119.75
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32407552
                    Iteration time: 12.15s
                        Total time: 25884.98s
                               ETA: 1282772.2s

################################################################################
                    [1m Learning iteration 1978/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.844s, learning 0.199s)
               Value function loss: 24.0789
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 245.23
               Mean episode length: 120.89
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32423936
                    Iteration time: 12.04s
                        Total time: 25897.02s
                               ETA: 1282707.5s

################################################################################
                    [1m Learning iteration 1979/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.781s, learning 0.213s)
               Value function loss: 24.7383
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 231.76
               Mean episode length: 116.31
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32440320
                    Iteration time: 11.99s
                        Total time: 25909.02s
                               ETA: 1282640.3s

################################################################################
                    [1m Learning iteration 1980/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.086s, learning 0.223s)
               Value function loss: 24.9240
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 245.24
               Mean episode length: 121.39
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32456704
                    Iteration time: 12.31s
                        Total time: 25921.33s
                               ETA: 1282588.8s

################################################################################
                    [1m Learning iteration 1981/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.952s, learning 0.218s)
               Value function loss: 23.9392
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 244.37
               Mean episode length: 119.58
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32473088
                    Iteration time: 12.17s
                        Total time: 25933.50s
                               ETA: 1282530.4s

################################################################################
                    [1m Learning iteration 1982/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.077s, learning 0.155s)
               Value function loss: 23.3399
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 252.84
               Mean episode length: 122.70
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32489472
                    Iteration time: 12.23s
                        Total time: 25945.73s
                               ETA: 1282475.2s

################################################################################
                    [1m Learning iteration 1983/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.632s, learning 0.180s)
               Value function loss: 23.0981
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 238.93
               Mean episode length: 117.32
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32505856
                    Iteration time: 11.81s
                        Total time: 25957.54s
                               ETA: 1282399.3s

################################################################################
                    [1m Learning iteration 1984/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.952s, learning 0.211s)
               Value function loss: 25.9339
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 243.89
               Mean episode length: 119.56
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32522240
                    Iteration time: 12.16s
                        Total time: 25969.70s
                               ETA: 1282340.8s

################################################################################
                    [1m Learning iteration 1985/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.300s, learning 0.160s)
               Value function loss: 30.0191
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 245.65
               Mean episode length: 121.81
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32538624
                    Iteration time: 12.46s
                        Total time: 25982.16s
                               ETA: 1282296.9s

################################################################################
                    [1m Learning iteration 1986/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.294s, learning 0.274s)
               Value function loss: 26.9289
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 243.93
               Mean episode length: 119.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32555008
                    Iteration time: 12.57s
                        Total time: 25994.73s
                               ETA: 1282258.4s

################################################################################
                    [1m Learning iteration 1987/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.826s, learning 0.235s)
               Value function loss: 24.6440
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 243.33
               Mean episode length: 119.61
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32571392
                    Iteration time: 12.06s
                        Total time: 26006.79s
                               ETA: 1282195.0s

################################################################################
                    [1m Learning iteration 1988/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.025s, learning 0.158s)
               Value function loss: 27.9517
                    Surrogate loss: -0.0052
             Mean action noise std: 0.77
                       Mean reward: 240.13
               Mean episode length: 119.16
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32587776
                    Iteration time: 12.18s
                        Total time: 26018.97s
                               ETA: 1282137.6s

################################################################################
                    [1m Learning iteration 1989/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.992s, learning 0.247s)
               Value function loss: 24.0803
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 241.46
               Mean episode length: 118.66
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32604160
                    Iteration time: 12.24s
                        Total time: 26031.21s
                               ETA: 1282083.0s

################################################################################
                    [1m Learning iteration 1990/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.894s, learning 0.157s)
               Value function loss: 22.5312
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 237.53
               Mean episode length: 116.88
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32620544
                    Iteration time: 12.05s
                        Total time: 26043.26s
                               ETA: 1282019.3s

################################################################################
                    [1m Learning iteration 1991/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.128s, learning 0.213s)
               Value function loss: 24.3062
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 250.76
               Mean episode length: 122.59
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32636928
                    Iteration time: 12.34s
                        Total time: 26055.61s
                               ETA: 1281969.8s

################################################################################
                    [1m Learning iteration 1992/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.291s, learning 0.175s)
               Value function loss: 26.9258
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 241.95
               Mean episode length: 118.07
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32653312
                    Iteration time: 12.47s
                        Total time: 26068.07s
                               ETA: 1281926.5s

################################################################################
                    [1m Learning iteration 1993/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.755s, learning 0.155s)
               Value function loss: 23.9805
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 240.81
               Mean episode length: 118.04
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32669696
                    Iteration time: 11.91s
                        Total time: 26079.98s
                               ETA: 1281855.9s

################################################################################
                    [1m Learning iteration 1994/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.936s, learning 0.180s)
               Value function loss: 24.5976
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 240.18
               Mean episode length: 119.39
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32686080
                    Iteration time: 12.12s
                        Total time: 26092.10s
                               ETA: 1281795.5s

################################################################################
                    [1m Learning iteration 1995/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.224s, learning 0.162s)
               Value function loss: 21.6595
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 248.71
               Mean episode length: 121.25
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32702464
                    Iteration time: 12.39s
                        Total time: 26104.48s
                               ETA: 1281748.4s

################################################################################
                    [1m Learning iteration 1996/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.112s, learning 0.200s)
               Value function loss: 24.2948
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 248.72
               Mean episode length: 121.02
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32718848
                    Iteration time: 12.31s
                        Total time: 26116.80s
                               ETA: 1281697.7s

################################################################################
                    [1m Learning iteration 1997/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.790s, learning 0.169s)
               Value function loss: 23.2770
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 238.11
               Mean episode length: 116.86
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32735232
                    Iteration time: 11.96s
                        Total time: 26128.75s
                               ETA: 1281629.8s

################################################################################
                    [1m Learning iteration 1998/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.480s, learning 0.207s)
               Value function loss: 22.6897
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 226.99
               Mean episode length: 112.39
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32751616
                    Iteration time: 11.69s
                        Total time: 26140.44s
                               ETA: 1281548.5s

################################################################################
                    [1m Learning iteration 1999/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.953s, learning 0.192s)
               Value function loss: 24.7170
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 245.06
               Mean episode length: 119.64
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32768000
                    Iteration time: 12.14s
                        Total time: 26152.59s
                               ETA: 1281489.8s

################################################################################
                    [1m Learning iteration 2000/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.707s, learning 0.168s)
               Value function loss: 28.7657
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 251.22
               Mean episode length: 122.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32784384
                    Iteration time: 11.88s
                        Total time: 26164.46s
                               ETA: 1281417.9s

################################################################################
                    [1m Learning iteration 2001/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.853s, learning 0.220s)
               Value function loss: 26.0846
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 240.38
               Mean episode length: 118.25
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32800768
                    Iteration time: 12.07s
                        Total time: 26176.53s
                               ETA: 1281355.7s

################################################################################
                    [1m Learning iteration 2002/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.018s, learning 0.201s)
               Value function loss: 23.6891
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 244.26
               Mean episode length: 119.42
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32817152
                    Iteration time: 12.22s
                        Total time: 26188.75s
                               ETA: 1281300.8s

################################################################################
                    [1m Learning iteration 2003/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.929s, learning 0.218s)
               Value function loss: 30.0413
                    Surrogate loss: 0.0005
             Mean action noise std: 0.77
                       Mean reward: 246.97
               Mean episode length: 119.11
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32833536
                    Iteration time: 12.15s
                        Total time: 26200.90s
                               ETA: 1281242.3s

################################################################################
                    [1m Learning iteration 2004/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.774s, learning 0.178s)
               Value function loss: 31.5353
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 245.44
               Mean episode length: 120.21
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32849920
                    Iteration time: 11.95s
                        Total time: 26212.85s
                               ETA: 1281174.4s

################################################################################
                    [1m Learning iteration 2005/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.740s, learning 0.203s)
               Value function loss: 26.6094
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 238.25
               Mean episode length: 116.11
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32866304
                    Iteration time: 11.94s
                        Total time: 26224.79s
                               ETA: 1281106.1s

################################################################################
                    [1m Learning iteration 2006/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.512s, learning 0.171s)
               Value function loss: 27.6076
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 255.05
               Mean episode length: 122.67
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32882688
                    Iteration time: 11.68s
                        Total time: 26236.48s
                               ETA: 1281025.1s

################################################################################
                    [1m Learning iteration 2007/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.902s, learning 0.191s)
               Value function loss: 30.8886
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 249.35
               Mean episode length: 121.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32899072
                    Iteration time: 12.09s
                        Total time: 26248.57s
                               ETA: 1280964.3s

################################################################################
                    [1m Learning iteration 2008/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.874s, learning 0.209s)
               Value function loss: 33.6270
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 249.72
               Mean episode length: 121.81
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32915456
                    Iteration time: 12.08s
                        Total time: 26260.65s
                               ETA: 1280903.0s

################################################################################
                    [1m Learning iteration 2009/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.712s, learning 0.183s)
               Value function loss: 33.6445
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 243.39
               Mean episode length: 119.10
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32931840
                    Iteration time: 11.89s
                        Total time: 26272.55s
                               ETA: 1280832.5s

################################################################################
                    [1m Learning iteration 2010/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.938s, learning 0.161s)
               Value function loss: 32.9809
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 241.06
               Mean episode length: 118.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32948224
                    Iteration time: 12.10s
                        Total time: 26284.65s
                               ETA: 1280772.1s

################################################################################
                    [1m Learning iteration 2011/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.888s, learning 0.201s)
               Value function loss: 37.0514
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 236.30
               Mean episode length: 116.11
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32964608
                    Iteration time: 12.09s
                        Total time: 26296.74s
                               ETA: 1280711.3s

################################################################################
                    [1m Learning iteration 2012/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.059s, learning 0.158s)
               Value function loss: 35.8851
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 239.40
               Mean episode length: 118.36
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32980992
                    Iteration time: 12.22s
                        Total time: 26308.95s
                               ETA: 1280656.7s

################################################################################
                    [1m Learning iteration 2013/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.959s, learning 0.164s)
               Value function loss: 36.4224
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 242.27
               Mean episode length: 118.19
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 32997376
                    Iteration time: 12.12s
                        Total time: 26321.08s
                               ETA: 1280597.6s

################################################################################
                    [1m Learning iteration 2014/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.746s, learning 0.292s)
               Value function loss: 32.9483
                    Surrogate loss: 0.0132
             Mean action noise std: 0.77
                       Mean reward: 239.72
               Mean episode length: 117.05
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33013760
                    Iteration time: 12.04s
                        Total time: 26333.12s
                               ETA: 1280534.3s

################################################################################
                    [1m Learning iteration 2015/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.902s, learning 0.214s)
               Value function loss: 33.3147
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 251.23
               Mean episode length: 122.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33030144
                    Iteration time: 12.12s
                        Total time: 26345.23s
                               ETA: 1280474.9s

################################################################################
                    [1m Learning iteration 2016/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.345s, learning 0.200s)
               Value function loss: 37.8684
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 255.30
               Mean episode length: 123.14
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33046528
                    Iteration time: 12.55s
                        Total time: 26357.78s
                               ETA: 1280436.5s

################################################################################
                    [1m Learning iteration 2017/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.063s, learning 0.165s)
               Value function loss: 32.8604
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 243.59
               Mean episode length: 119.62
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33062912
                    Iteration time: 12.23s
                        Total time: 26370.00s
                               ETA: 1280382.6s

################################################################################
                    [1m Learning iteration 2018/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.174s, learning 0.262s)
               Value function loss: 27.6700
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 247.72
               Mean episode length: 120.18
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33079296
                    Iteration time: 11.44s
                        Total time: 26381.44s
                               ETA: 1280290.4s

################################################################################
                    [1m Learning iteration 2019/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.925s, learning 0.300s)
               Value function loss: 32.7277
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 243.95
               Mean episode length: 119.65
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33095680
                    Iteration time: 12.23s
                        Total time: 26393.67s
                               ETA: 1280236.5s

################################################################################
                    [1m Learning iteration 2020/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.817s, learning 0.158s)
               Value function loss: 29.4456
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 245.36
               Mean episode length: 120.70
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33112064
                    Iteration time: 11.97s
                        Total time: 26405.64s
                               ETA: 1280170.5s

################################################################################
                    [1m Learning iteration 2021/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.599s, learning 0.166s)
               Value function loss: 24.9525
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 244.61
               Mean episode length: 118.75
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33128448
                    Iteration time: 11.76s
                        Total time: 26417.41s
                               ETA: 1280094.4s

################################################################################
                    [1m Learning iteration 2022/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.439s, learning 0.167s)
               Value function loss: 24.7164
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 242.56
               Mean episode length: 116.79
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33144832
                    Iteration time: 11.61s
                        Total time: 26429.01s
                               ETA: 1280010.7s

################################################################################
                    [1m Learning iteration 2023/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.367s, learning 0.168s)
               Value function loss: 27.8778
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 243.41
               Mean episode length: 118.18
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33161216
                    Iteration time: 12.53s
                        Total time: 26441.55s
                               ETA: 1279972.0s

################################################################################
                    [1m Learning iteration 2024/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.183s, learning 0.207s)
               Value function loss: 25.3998
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: 256.32
               Mean episode length: 123.83
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33177600
                    Iteration time: 12.39s
                        Total time: 26453.94s
                               ETA: 1279926.3s

################################################################################
                    [1m Learning iteration 2025/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.954s, learning 0.163s)
               Value function loss: 27.5442
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 241.75
               Mean episode length: 118.35
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33193984
                    Iteration time: 12.12s
                        Total time: 26466.05s
                               ETA: 1279867.5s

################################################################################
                    [1m Learning iteration 2026/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.581s, learning 0.160s)
               Value function loss: 29.3438
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 239.32
               Mean episode length: 117.18
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33210368
                    Iteration time: 11.74s
                        Total time: 26477.79s
                               ETA: 1279790.5s

################################################################################
                    [1m Learning iteration 2027/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.890s, learning 0.234s)
               Value function loss: 32.2888
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 251.12
               Mean episode length: 120.98
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33226752
                    Iteration time: 12.12s
                        Total time: 26489.92s
                               ETA: 1279732.1s

################################################################################
                    [1m Learning iteration 2028/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.079s, learning 0.157s)
               Value function loss: 28.2590
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 256.87
               Mean episode length: 123.65
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33243136
                    Iteration time: 12.24s
                        Total time: 26502.15s
                               ETA: 1279679.2s

################################################################################
                    [1m Learning iteration 2029/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.923s, learning 0.171s)
               Value function loss: 28.4939
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 242.73
               Mean episode length: 118.35
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33259520
                    Iteration time: 12.09s
                        Total time: 26514.25s
                               ETA: 1279619.5s

################################################################################
                    [1m Learning iteration 2030/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.995s, learning 0.165s)
               Value function loss: 28.3113
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 232.47
               Mean episode length: 113.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33275904
                    Iteration time: 12.16s
                        Total time: 26526.41s
                               ETA: 1279562.9s

################################################################################
                    [1m Learning iteration 2031/100000 [0m                    

                       Computation: 1429 steps/s (collection: 11.297s, learning 0.162s)
               Value function loss: 30.3457
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 250.19
               Mean episode length: 121.18
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33292288
                    Iteration time: 11.46s
                        Total time: 26537.87s
                               ETA: 1279472.6s

################################################################################
                    [1m Learning iteration 2032/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.859s, learning 0.198s)
               Value function loss: 25.8084
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 254.35
               Mean episode length: 122.90
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33308672
                    Iteration time: 12.06s
                        Total time: 26549.93s
                               ETA: 1279411.3s

################################################################################
                    [1m Learning iteration 2033/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.131s, learning 0.200s)
               Value function loss: 31.3187
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 243.56
               Mean episode length: 118.28
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33325056
                    Iteration time: 12.33s
                        Total time: 26562.26s
                               ETA: 1279363.1s

################################################################################
                    [1m Learning iteration 2034/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.871s, learning 0.277s)
               Value function loss: 29.8510
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 243.52
               Mean episode length: 118.19
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33341440
                    Iteration time: 12.15s
                        Total time: 26574.40s
                               ETA: 1279306.2s

################################################################################
                    [1m Learning iteration 2035/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.738s, learning 0.167s)
               Value function loss: 33.5884
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: 237.26
               Mean episode length: 115.31
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33357824
                    Iteration time: 11.90s
                        Total time: 26586.31s
                               ETA: 1279237.6s

################################################################################
                    [1m Learning iteration 2036/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.158s, learning 0.191s)
               Value function loss: 28.4547
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 239.45
               Mean episode length: 117.63
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33374208
                    Iteration time: 12.35s
                        Total time: 26598.66s
                               ETA: 1279190.5s

################################################################################
                    [1m Learning iteration 2037/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.965s, learning 0.161s)
               Value function loss: 30.3451
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 242.86
               Mean episode length: 117.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33390592
                    Iteration time: 12.13s
                        Total time: 26610.79s
                               ETA: 1279132.7s

################################################################################
                    [1m Learning iteration 2038/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.582s, learning 0.170s)
               Value function loss: 28.9053
                    Surrogate loss: 0.0000
             Mean action noise std: 0.77
                       Mean reward: 235.90
               Mean episode length: 114.86
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33406976
                    Iteration time: 11.75s
                        Total time: 26622.54s
                               ETA: 1279056.9s

################################################################################
                    [1m Learning iteration 2039/100000 [0m                    

                       Computation: 1305 steps/s (collection: 12.265s, learning 0.285s)
               Value function loss: 31.7890
                    Surrogate loss: 0.0041
             Mean action noise std: 0.77
                       Mean reward: 249.06
               Mean episode length: 120.70
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33423360
                    Iteration time: 12.55s
                        Total time: 26635.09s
                               ETA: 1279019.5s

################################################################################
                    [1m Learning iteration 2040/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.805s, learning 0.164s)
               Value function loss: 29.1629
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 247.36
               Mean episode length: 120.88
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33439744
                    Iteration time: 11.97s
                        Total time: 26647.06s
                               ETA: 1278954.3s

################################################################################
                    [1m Learning iteration 2041/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.907s, learning 0.169s)
               Value function loss: 33.9049
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 242.62
               Mean episode length: 118.96
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33456128
                    Iteration time: 12.08s
                        Total time: 26659.13s
                               ETA: 1278894.2s

################################################################################
                    [1m Learning iteration 2042/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.970s, learning 0.162s)
               Value function loss: 33.7860
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 261.53
               Mean episode length: 124.58
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33472512
                    Iteration time: 12.13s
                        Total time: 26671.26s
                               ETA: 1278836.9s

################################################################################
                    [1m Learning iteration 2043/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.127s, learning 0.159s)
               Value function loss: 31.9236
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 240.26
               Mean episode length: 117.16
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33488896
                    Iteration time: 12.29s
                        Total time: 26683.55s
                               ETA: 1278787.0s

################################################################################
                    [1m Learning iteration 2044/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.664s, learning 0.165s)
               Value function loss: 32.3884
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 250.21
               Mean episode length: 119.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33505280
                    Iteration time: 11.83s
                        Total time: 26695.38s
                               ETA: 1278715.3s

################################################################################
                    [1m Learning iteration 2045/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.515s, learning 0.295s)
               Value function loss: 30.3718
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 244.83
               Mean episode length: 118.44
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33521664
                    Iteration time: 11.81s
                        Total time: 26707.19s
                               ETA: 1278642.6s

################################################################################
                    [1m Learning iteration 2046/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.128s, learning 0.187s)
               Value function loss: 30.3589
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 242.52
               Mean episode length: 118.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33538048
                    Iteration time: 12.31s
                        Total time: 26719.50s
                               ETA: 1278594.2s

################################################################################
                    [1m Learning iteration 2047/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.690s, learning 0.171s)
               Value function loss: 33.8271
                    Surrogate loss: -0.0040
             Mean action noise std: 0.77
                       Mean reward: 242.94
               Mean episode length: 118.02
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33554432
                    Iteration time: 11.86s
                        Total time: 26731.37s
                               ETA: 1278524.2s

################################################################################
                    [1m Learning iteration 2048/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.753s, learning 0.201s)
               Value function loss: 30.6811
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 252.24
               Mean episode length: 121.71
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33570816
                    Iteration time: 11.95s
                        Total time: 26743.32s
                               ETA: 1278458.7s

################################################################################
                    [1m Learning iteration 2049/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.891s, learning 0.209s)
               Value function loss: 24.4570
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 249.10
               Mean episode length: 120.71
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33587200
                    Iteration time: 12.10s
                        Total time: 26755.42s
                               ETA: 1278400.1s

################################################################################
                    [1m Learning iteration 2050/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.637s, learning 0.188s)
               Value function loss: 28.3474
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 235.26
               Mean episode length: 114.20
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33603584
                    Iteration time: 11.82s
                        Total time: 26767.25s
                               ETA: 1278328.5s

################################################################################
                    [1m Learning iteration 2051/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.109s, learning 0.175s)
               Value function loss: 30.8382
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 249.24
               Mean episode length: 120.76
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33619968
                    Iteration time: 12.28s
                        Total time: 26779.53s
                               ETA: 1278278.8s

################################################################################
                    [1m Learning iteration 2052/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.428s, learning 0.163s)
               Value function loss: 26.7784
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 249.89
               Mean episode length: 120.54
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33636352
                    Iteration time: 11.59s
                        Total time: 26791.12s
                               ETA: 1278196.2s

################################################################################
                    [1m Learning iteration 2053/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.595s, learning 0.162s)
               Value function loss: 26.5176
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 247.59
               Mean episode length: 119.39
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33652736
                    Iteration time: 11.76s
                        Total time: 26802.88s
                               ETA: 1278121.5s

################################################################################
                    [1m Learning iteration 2054/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.726s, learning 0.201s)
               Value function loss: 28.5475
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 240.77
               Mean episode length: 117.77
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33669120
                    Iteration time: 11.93s
                        Total time: 26814.81s
                               ETA: 1278055.0s

################################################################################
                    [1m Learning iteration 2055/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.951s, learning 0.178s)
               Value function loss: 28.4651
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 231.74
               Mean episode length: 113.52
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33685504
                    Iteration time: 12.13s
                        Total time: 26826.94s
                               ETA: 1277998.1s

################################################################################
                    [1m Learning iteration 2056/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.841s, learning 0.305s)
               Value function loss: 31.9990
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 248.28
               Mean episode length: 119.81
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33701888
                    Iteration time: 12.15s
                        Total time: 26839.08s
                               ETA: 1277942.1s

################################################################################
                    [1m Learning iteration 2057/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.707s, learning 0.212s)
               Value function loss: 28.7780
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 255.11
               Mean episode length: 123.93
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33718272
                    Iteration time: 11.92s
                        Total time: 26851.00s
                               ETA: 1277875.4s

################################################################################
                    [1m Learning iteration 2058/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.831s, learning 0.207s)
               Value function loss: 34.6359
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 249.23
               Mean episode length: 120.55
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33734656
                    Iteration time: 12.04s
                        Total time: 26863.04s
                               ETA: 1277814.3s

################################################################################
                    [1m Learning iteration 2059/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.469s, learning 0.168s)
               Value function loss: 36.3019
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 246.38
               Mean episode length: 119.74
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33751040
                    Iteration time: 11.64s
                        Total time: 26874.67s
                               ETA: 1277734.2s

################################################################################
                    [1m Learning iteration 2060/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.893s, learning 0.168s)
               Value function loss: 33.9602
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 256.60
               Mean episode length: 123.74
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33767424
                    Iteration time: 12.06s
                        Total time: 26886.74s
                               ETA: 1277674.4s

################################################################################
                    [1m Learning iteration 2061/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.533s, learning 0.164s)
               Value function loss: 40.7248
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 249.50
               Mean episode length: 121.81
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33783808
                    Iteration time: 11.70s
                        Total time: 26898.43s
                               ETA: 1277597.3s

################################################################################
                    [1m Learning iteration 2062/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.986s, learning 0.205s)
               Value function loss: 35.2998
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 239.89
               Mean episode length: 116.88
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33800192
                    Iteration time: 12.19s
                        Total time: 26910.62s
                               ETA: 1277543.7s

################################################################################
                    [1m Learning iteration 2063/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.895s, learning 0.165s)
               Value function loss: 33.6883
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 248.65
               Mean episode length: 120.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33816576
                    Iteration time: 12.06s
                        Total time: 26922.68s
                               ETA: 1277483.9s

################################################################################
                    [1m Learning iteration 2064/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.840s, learning 0.235s)
               Value function loss: 34.6476
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 244.92
               Mean episode length: 119.37
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33832960
                    Iteration time: 12.07s
                        Total time: 26934.76s
                               ETA: 1277424.9s

################################################################################
                    [1m Learning iteration 2065/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.988s, learning 0.173s)
               Value function loss: 28.7620
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 243.87
               Mean episode length: 118.95
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33849344
                    Iteration time: 12.16s
                        Total time: 26946.92s
                               ETA: 1277370.0s

################################################################################
                    [1m Learning iteration 2066/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.803s, learning 0.250s)
               Value function loss: 40.7118
                    Surrogate loss: 0.0091
             Mean action noise std: 0.77
                       Mean reward: 248.04
               Mean episode length: 120.25
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33865728
                    Iteration time: 12.05s
                        Total time: 26958.97s
                               ETA: 1277310.1s

################################################################################
                    [1m Learning iteration 2067/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.948s, learning 0.168s)
               Value function loss: 39.8105
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 246.75
               Mean episode length: 120.39
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33882112
                    Iteration time: 12.12s
                        Total time: 26971.09s
                               ETA: 1277253.2s

################################################################################
                    [1m Learning iteration 2068/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.763s, learning 0.229s)
               Value function loss: 35.0080
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 248.72
               Mean episode length: 121.08
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33898496
                    Iteration time: 11.99s
                        Total time: 26983.08s
                               ETA: 1277190.5s

################################################################################
                    [1m Learning iteration 2069/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.892s, learning 0.165s)
               Value function loss: 37.5217
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 236.75
               Mean episode length: 115.71
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33914880
                    Iteration time: 12.06s
                        Total time: 26995.14s
                               ETA: 1277130.8s

################################################################################
                    [1m Learning iteration 2070/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.111s, learning 0.214s)
               Value function loss: 36.1745
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 248.78
               Mean episode length: 122.01
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33931264
                    Iteration time: 12.33s
                        Total time: 27007.46s
                               ETA: 1277083.9s

################################################################################
                    [1m Learning iteration 2071/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.002s, learning 0.169s)
               Value function loss: 36.4992
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 243.71
               Mean episode length: 119.60
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33947648
                    Iteration time: 12.17s
                        Total time: 27019.63s
                               ETA: 1277029.8s

################################################################################
                    [1m Learning iteration 2072/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.593s, learning 0.204s)
               Value function loss: 29.6454
                    Surrogate loss: 0.0080
             Mean action noise std: 0.77
                       Mean reward: 252.70
               Mean episode length: 122.94
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33964032
                    Iteration time: 11.80s
                        Total time: 27031.43s
                               ETA: 1276958.0s

################################################################################
                    [1m Learning iteration 2073/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.897s, learning 0.311s)
               Value function loss: 26.9002
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 250.34
               Mean episode length: 122.04
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33980416
                    Iteration time: 12.21s
                        Total time: 27043.64s
                               ETA: 1276905.7s

################################################################################
                    [1m Learning iteration 2074/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.958s, learning 0.156s)
               Value function loss: 28.2136
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 248.02
               Mean episode length: 120.70
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 33996800
                    Iteration time: 12.11s
                        Total time: 27055.75s
                               ETA: 1276848.9s

################################################################################
                    [1m Learning iteration 2075/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.425s, learning 0.183s)
               Value function loss: 31.6283
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 245.72
               Mean episode length: 119.76
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34013184
                    Iteration time: 11.61s
                        Total time: 27067.36s
                               ETA: 1276768.4s

################################################################################
                    [1m Learning iteration 2076/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.145s, learning 0.270s)
               Value function loss: 30.2034
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 246.59
               Mean episode length: 120.79
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34029568
                    Iteration time: 12.42s
                        Total time: 27079.78s
                               ETA: 1276726.0s

################################################################################
                    [1m Learning iteration 2077/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.990s, learning 0.228s)
               Value function loss: 28.5209
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 246.40
               Mean episode length: 120.60
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34045952
                    Iteration time: 12.22s
                        Total time: 27091.99s
                               ETA: 1276674.3s

################################################################################
                    [1m Learning iteration 2078/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.883s, learning 0.183s)
               Value function loss: 32.5852
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 237.33
               Mean episode length: 116.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34062336
                    Iteration time: 12.07s
                        Total time: 27104.06s
                               ETA: 1276615.6s

################################################################################
                    [1m Learning iteration 2079/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.847s, learning 0.161s)
               Value function loss: 28.6452
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 252.88
               Mean episode length: 123.27
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34078720
                    Iteration time: 12.01s
                        Total time: 27116.07s
                               ETA: 1276554.1s

################################################################################
                    [1m Learning iteration 2080/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.334s, learning 0.156s)
               Value function loss: 28.9468
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 250.76
               Mean episode length: 121.34
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34095104
                    Iteration time: 11.49s
                        Total time: 27127.56s
                               ETA: 1276468.2s

################################################################################
                    [1m Learning iteration 2081/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.714s, learning 0.161s)
               Value function loss: 29.0624
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 242.06
               Mean episode length: 118.84
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34111488
                    Iteration time: 11.87s
                        Total time: 27139.43s
                               ETA: 1276400.6s

################################################################################
                    [1m Learning iteration 2082/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.746s, learning 0.269s)
               Value function loss: 37.1462
                    Surrogate loss: 0.0205
             Mean action noise std: 0.77
                       Mean reward: 251.67
               Mean episode length: 121.91
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34127872
                    Iteration time: 12.01s
                        Total time: 27151.45s
                               ETA: 1276339.6s

################################################################################
                    [1m Learning iteration 2083/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.984s, learning 0.160s)
               Value function loss: 27.4578
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 250.29
               Mean episode length: 121.50
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34144256
                    Iteration time: 12.14s
                        Total time: 27163.59s
                               ETA: 1276284.7s

################################################################################
                    [1m Learning iteration 2084/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.881s, learning 0.177s)
               Value function loss: 29.0532
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 247.72
               Mean episode length: 120.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34160640
                    Iteration time: 12.06s
                        Total time: 27175.65s
                               ETA: 1276225.8s

################################################################################
                    [1m Learning iteration 2085/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.751s, learning 0.165s)
               Value function loss: 30.8369
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 247.35
               Mean episode length: 119.93
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34177024
                    Iteration time: 11.92s
                        Total time: 27187.56s
                               ETA: 1276160.3s

################################################################################
                    [1m Learning iteration 2086/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.543s, learning 0.180s)
               Value function loss: 30.9317
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 246.45
               Mean episode length: 118.76
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34193408
                    Iteration time: 11.72s
                        Total time: 27199.29s
                               ETA: 1276085.7s

################################################################################
                    [1m Learning iteration 2087/100000 [0m                    

                       Computation: 1451 steps/s (collection: 11.130s, learning 0.157s)
               Value function loss: 30.5548
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 244.89
               Mean episode length: 118.73
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34209792
                    Iteration time: 11.29s
                        Total time: 27210.57s
                               ETA: 1275990.8s

################################################################################
                    [1m Learning iteration 2088/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.166s)
               Value function loss: 29.3532
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 244.74
               Mean episode length: 119.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34226176
                    Iteration time: 12.07s
                        Total time: 27222.65s
                               ETA: 1275932.8s

################################################################################
                    [1m Learning iteration 2089/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.169s, learning 0.182s)
               Value function loss: 28.0414
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 250.47
               Mean episode length: 121.87
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34242560
                    Iteration time: 12.35s
                        Total time: 27235.00s
                               ETA: 1275887.9s

################################################################################
                    [1m Learning iteration 2090/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.173s, learning 0.223s)
               Value function loss: 28.6934
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 255.57
               Mean episode length: 122.85
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34258944
                    Iteration time: 12.40s
                        Total time: 27247.39s
                               ETA: 1275845.1s

################################################################################
                    [1m Learning iteration 2091/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.682s, learning 0.162s)
               Value function loss: 29.5981
                    Surrogate loss: 0.0157
             Mean action noise std: 0.77
                       Mean reward: 248.43
               Mean episode length: 120.01
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34275328
                    Iteration time: 11.84s
                        Total time: 27259.24s
                               ETA: 1275776.5s

################################################################################
                    [1m Learning iteration 2092/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.936s, learning 0.185s)
               Value function loss: 28.0767
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 249.05
               Mean episode length: 120.70
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34291712
                    Iteration time: 12.12s
                        Total time: 27271.36s
                               ETA: 1275721.0s

################################################################################
                    [1m Learning iteration 2093/100000 [0m                    

                       Computation: 1430 steps/s (collection: 11.271s, learning 0.182s)
               Value function loss: 30.2087
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 250.27
               Mean episode length: 120.53
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34308096
                    Iteration time: 11.45s
                        Total time: 27282.81s
                               ETA: 1275634.2s

################################################################################
                    [1m Learning iteration 2094/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.528s, learning 0.176s)
               Value function loss: 30.5920
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 258.68
               Mean episode length: 124.59
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34324480
                    Iteration time: 11.70s
                        Total time: 27294.51s
                               ETA: 1275559.3s

################################################################################
                    [1m Learning iteration 2095/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.038s, learning 0.173s)
               Value function loss: 29.6719
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 249.99
               Mean episode length: 120.75
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34340864
                    Iteration time: 12.21s
                        Total time: 27306.72s
                               ETA: 1275508.1s

################################################################################
                    [1m Learning iteration 2096/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.621s, learning 0.165s)
               Value function loss: 25.4356
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 256.35
               Mean episode length: 123.14
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34357248
                    Iteration time: 11.79s
                        Total time: 27318.51s
                               ETA: 1275437.0s

################################################################################
                    [1m Learning iteration 2097/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.547s, learning 0.202s)
               Value function loss: 34.0582
                    Surrogate loss: 0.0048
             Mean action noise std: 0.77
                       Mean reward: 252.54
               Mean episode length: 121.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34373632
                    Iteration time: 11.75s
                        Total time: 27330.26s
                               ETA: 1275364.3s

################################################################################
                    [1m Learning iteration 2098/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.132s, learning 0.319s)
               Value function loss: 31.6325
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 249.98
               Mean episode length: 121.83
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34390016
                    Iteration time: 12.45s
                        Total time: 27342.71s
                               ETA: 1275324.4s

################################################################################
                    [1m Learning iteration 2099/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.008s, learning 0.180s)
               Value function loss: 27.8078
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 253.44
               Mean episode length: 122.29
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34406400
                    Iteration time: 12.19s
                        Total time: 27354.90s
                               ETA: 1275272.3s

################################################################################
                    [1m Learning iteration 2100/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.853s, learning 0.170s)
               Value function loss: 26.6629
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 249.12
               Mean episode length: 119.97
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34422784
                    Iteration time: 12.02s
                        Total time: 27366.92s
                               ETA: 1275212.6s

################################################################################
                    [1m Learning iteration 2101/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.969s, learning 0.161s)
               Value function loss: 29.4698
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 243.28
               Mean episode length: 117.91
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34439168
                    Iteration time: 12.13s
                        Total time: 27379.05s
                               ETA: 1275157.8s

################################################################################
                    [1m Learning iteration 2102/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.143s, learning 0.207s)
               Value function loss: 28.4775
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 245.83
               Mean episode length: 120.15
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34455552
                    Iteration time: 12.35s
                        Total time: 27391.40s
                               ETA: 1275113.4s

################################################################################
                    [1m Learning iteration 2103/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.226s, learning 0.217s)
               Value function loss: 28.9805
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 246.20
               Mean episode length: 118.77
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34471936
                    Iteration time: 12.44s
                        Total time: 27403.84s
                               ETA: 1275073.3s

################################################################################
                    [1m Learning iteration 2104/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.686s, learning 0.185s)
               Value function loss: 26.5455
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 242.26
               Mean episode length: 117.73
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34488320
                    Iteration time: 11.87s
                        Total time: 27415.72s
                               ETA: 1275006.6s

################################################################################
                    [1m Learning iteration 2105/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.101s, learning 0.173s)
               Value function loss: 29.6252
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 241.15
               Mean episode length: 116.63
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34504704
                    Iteration time: 12.27s
                        Total time: 27427.99s
                               ETA: 1274958.7s

################################################################################
                    [1m Learning iteration 2106/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.829s, learning 0.157s)
               Value function loss: 29.0915
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 251.80
               Mean episode length: 121.10
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34521088
                    Iteration time: 11.99s
                        Total time: 27439.97s
                               ETA: 1274897.4s

################################################################################
                    [1m Learning iteration 2107/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.522s, learning 0.203s)
               Value function loss: 27.5713
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 248.23
               Mean episode length: 120.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34537472
                    Iteration time: 11.72s
                        Total time: 27451.70s
                               ETA: 1274824.1s

################################################################################
                    [1m Learning iteration 2108/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.582s, learning 0.160s)
               Value function loss: 29.7692
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 248.68
               Mean episode length: 119.04
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34553856
                    Iteration time: 11.74s
                        Total time: 27463.44s
                               ETA: 1274751.6s

################################################################################
                    [1m Learning iteration 2109/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.459s, learning 0.163s)
               Value function loss: 33.2479
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 253.34
               Mean episode length: 122.67
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34570240
                    Iteration time: 11.62s
                        Total time: 27475.06s
                               ETA: 1274673.6s

################################################################################
                    [1m Learning iteration 2110/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.567s, learning 0.164s)
               Value function loss: 28.3873
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 240.48
               Mean episode length: 117.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34586624
                    Iteration time: 11.73s
                        Total time: 27486.79s
                               ETA: 1274600.8s

################################################################################
                    [1m Learning iteration 2111/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.132s, learning 0.168s)
               Value function loss: 31.3670
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 250.30
               Mean episode length: 120.55
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34603008
                    Iteration time: 12.30s
                        Total time: 27499.09s
                               ETA: 1274554.3s

################################################################################
                    [1m Learning iteration 2112/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.667s, learning 0.164s)
               Value function loss: 25.8154
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 244.48
               Mean episode length: 117.89
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34619392
                    Iteration time: 11.83s
                        Total time: 27510.92s
                               ETA: 1274486.2s

################################################################################
                    [1m Learning iteration 2113/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.773s, learning 0.158s)
               Value function loss: 29.0261
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 243.36
               Mean episode length: 117.91
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34635776
                    Iteration time: 11.93s
                        Total time: 27522.85s
                               ETA: 1274422.7s

################################################################################
                    [1m Learning iteration 2114/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.817s, learning 0.164s)
               Value function loss: 26.8733
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 245.19
               Mean episode length: 119.02
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34652160
                    Iteration time: 11.98s
                        Total time: 27534.84s
                               ETA: 1274361.6s

################################################################################
                    [1m Learning iteration 2115/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.135s, learning 0.159s)
               Value function loss: 28.0095
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 250.57
               Mean episode length: 120.64
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34668544
                    Iteration time: 12.29s
                        Total time: 27547.13s
                               ETA: 1274315.1s

################################################################################
                    [1m Learning iteration 2116/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.839s, learning 0.158s)
               Value function loss: 29.8078
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 244.26
               Mean episode length: 118.30
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34684928
                    Iteration time: 12.00s
                        Total time: 27559.13s
                               ETA: 1274254.9s

################################################################################
                    [1m Learning iteration 2117/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.475s, learning 0.255s)
               Value function loss: 30.2217
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 258.02
               Mean episode length: 123.25
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34701312
                    Iteration time: 11.73s
                        Total time: 27570.86s
                               ETA: 1274182.3s

################################################################################
                    [1m Learning iteration 2118/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.030s, learning 0.185s)
               Value function loss: 25.1121
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 256.58
               Mean episode length: 122.51
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34717696
                    Iteration time: 12.22s
                        Total time: 27583.07s
                               ETA: 1274132.3s

################################################################################
                    [1m Learning iteration 2119/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.809s, learning 0.165s)
               Value function loss: 25.2347
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 248.49
               Mean episode length: 120.30
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34734080
                    Iteration time: 11.97s
                        Total time: 27595.05s
                               ETA: 1274071.1s

################################################################################
                    [1m Learning iteration 2120/100000 [0m                    

                       Computation: 1428 steps/s (collection: 11.308s, learning 0.160s)
               Value function loss: 24.6320
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 259.26
               Mean episode length: 123.13
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34750464
                    Iteration time: 11.47s
                        Total time: 27606.51s
                               ETA: 1273986.6s

################################################################################
                    [1m Learning iteration 2121/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.362s, learning 0.167s)
               Value function loss: 27.0882
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 253.97
               Mean episode length: 121.52
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34766848
                    Iteration time: 11.53s
                        Total time: 27618.04s
                               ETA: 1273905.0s

################################################################################
                    [1m Learning iteration 2122/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.358s, learning 0.161s)
               Value function loss: 28.1333
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 256.10
               Mean episode length: 121.96
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34783232
                    Iteration time: 11.52s
                        Total time: 27629.56s
                               ETA: 1273823.0s

################################################################################
                    [1m Learning iteration 2123/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.616s, learning 0.161s)
               Value function loss: 31.4258
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 252.24
               Mean episode length: 121.43
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34799616
                    Iteration time: 11.78s
                        Total time: 27641.34s
                               ETA: 1273753.0s

################################################################################
                    [1m Learning iteration 2124/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.296s, learning 0.197s)
               Value function loss: 27.4859
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 253.41
               Mean episode length: 121.27
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34816000
                    Iteration time: 12.49s
                        Total time: 27653.83s
                               ETA: 1273715.9s

################################################################################
                    [1m Learning iteration 2125/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.583s, learning 0.199s)
               Value function loss: 25.2856
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 254.49
               Mean episode length: 121.71
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34832384
                    Iteration time: 11.78s
                        Total time: 27665.61s
                               ETA: 1273646.2s

################################################################################
                    [1m Learning iteration 2126/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.858s, learning 0.299s)
               Value function loss: 28.5320
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 256.26
               Mean episode length: 122.78
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34848768
                    Iteration time: 12.16s
                        Total time: 27677.77s
                               ETA: 1273593.8s

################################################################################
                    [1m Learning iteration 2127/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.038s, learning 0.201s)
               Value function loss: 25.8477
                    Surrogate loss: 0.0001
             Mean action noise std: 0.77
                       Mean reward: 248.76
               Mean episode length: 118.78
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34865152
                    Iteration time: 12.24s
                        Total time: 27690.01s
                               ETA: 1273545.3s

################################################################################
                    [1m Learning iteration 2128/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.093s, learning 0.189s)
               Value function loss: 27.6485
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 256.03
               Mean episode length: 122.86
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34881536
                    Iteration time: 12.28s
                        Total time: 27702.29s
                               ETA: 1273498.7s

################################################################################
                    [1m Learning iteration 2129/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.657s, learning 0.166s)
               Value function loss: 28.3299
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 241.73
               Mean episode length: 117.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34897920
                    Iteration time: 11.82s
                        Total time: 27714.11s
                               ETA: 1273431.0s

################################################################################
                    [1m Learning iteration 2130/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.005s, learning 0.164s)
               Value function loss: 26.2288
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 252.93
               Mean episode length: 121.42
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34914304
                    Iteration time: 12.17s
                        Total time: 27726.28s
                               ETA: 1273379.3s

################################################################################
                    [1m Learning iteration 2131/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.873s, learning 0.270s)
               Value function loss: 24.9785
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 245.61
               Mean episode length: 118.75
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34930688
                    Iteration time: 12.14s
                        Total time: 27738.43s
                               ETA: 1273326.5s

################################################################################
                    [1m Learning iteration 2132/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.720s, learning 0.200s)
               Value function loss: 28.9176
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 247.85
               Mean episode length: 119.81
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34947072
                    Iteration time: 11.92s
                        Total time: 27750.35s
                               ETA: 1273263.4s

################################################################################
                    [1m Learning iteration 2133/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.797s, learning 0.273s)
               Value function loss: 25.1294
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 251.41
               Mean episode length: 120.44
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34963456
                    Iteration time: 12.07s
                        Total time: 27762.42s
                               ETA: 1273207.3s

################################################################################
                    [1m Learning iteration 2134/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.016s, learning 0.198s)
               Value function loss: 26.5795
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 246.99
               Mean episode length: 119.66
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34979840
                    Iteration time: 12.21s
                        Total time: 27774.63s
                               ETA: 1273157.8s

################################################################################
                    [1m Learning iteration 2135/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.707s, learning 0.169s)
               Value function loss: 25.6042
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 251.15
               Mean episode length: 120.85
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 34996224
                    Iteration time: 11.88s
                        Total time: 27786.51s
                               ETA: 1273092.9s

################################################################################
                    [1m Learning iteration 2136/100000 [0m                    

                       Computation: 1434 steps/s (collection: 11.266s, learning 0.159s)
               Value function loss: 24.6306
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 249.41
               Mean episode length: 120.35
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35012608
                    Iteration time: 11.42s
                        Total time: 27797.93s
                               ETA: 1273007.4s

################################################################################
                    [1m Learning iteration 2137/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.028s, learning 0.178s)
               Value function loss: 23.6758
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 255.47
               Mean episode length: 123.02
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35028992
                    Iteration time: 12.21s
                        Total time: 27810.14s
                               ETA: 1272957.7s

################################################################################
                    [1m Learning iteration 2138/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.097s, learning 0.168s)
               Value function loss: 25.8918
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 248.49
               Mean episode length: 121.15
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35045376
                    Iteration time: 12.27s
                        Total time: 27822.40s
                               ETA: 1272910.7s

################################################################################
                    [1m Learning iteration 2139/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.088s, learning 0.176s)
               Value function loss: 28.8017
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 252.40
               Mean episode length: 122.40
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35061760
                    Iteration time: 12.26s
                        Total time: 27834.67s
                               ETA: 1272863.8s

################################################################################
                    [1m Learning iteration 2140/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.865s, learning 0.204s)
               Value function loss: 24.6874
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 247.68
               Mean episode length: 120.75
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35078144
                    Iteration time: 12.07s
                        Total time: 27846.74s
                               ETA: 1272807.9s

################################################################################
                    [1m Learning iteration 2141/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.157s, learning 0.189s)
               Value function loss: 25.0007
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 248.89
               Mean episode length: 119.72
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35094528
                    Iteration time: 12.35s
                        Total time: 27859.08s
                               ETA: 1272764.7s

################################################################################
                    [1m Learning iteration 2142/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.644s, learning 0.163s)
               Value function loss: 31.4401
                    Surrogate loss: -0.0037
             Mean action noise std: 0.77
                       Mean reward: 250.72
               Mean episode length: 121.44
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35110912
                    Iteration time: 11.81s
                        Total time: 27870.89s
                               ETA: 1272697.0s

################################################################################
                    [1m Learning iteration 2143/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.515s, learning 0.162s)
               Value function loss: 26.2127
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 247.94
               Mean episode length: 120.36
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35127296
                    Iteration time: 11.68s
                        Total time: 27882.57s
                               ETA: 1272623.3s

################################################################################
                    [1m Learning iteration 2144/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.329s, learning 0.168s)
               Value function loss: 29.8473
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 248.17
               Mean episode length: 120.09
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35143680
                    Iteration time: 11.50s
                        Total time: 27894.06s
                               ETA: 1272541.5s

################################################################################
                    [1m Learning iteration 2145/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.911s, learning 0.159s)
               Value function loss: 30.5658
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 250.84
               Mean episode length: 122.09
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35160064
                    Iteration time: 12.07s
                        Total time: 27906.13s
                               ETA: 1272485.8s

################################################################################
                    [1m Learning iteration 2146/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.693s, learning 0.188s)
               Value function loss: 24.7803
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 249.86
               Mean episode length: 121.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35176448
                    Iteration time: 11.88s
                        Total time: 27918.01s
                               ETA: 1272421.7s

################################################################################
                    [1m Learning iteration 2147/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.878s, learning 0.212s)
               Value function loss: 26.9894
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 247.96
               Mean episode length: 121.15
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35192832
                    Iteration time: 12.09s
                        Total time: 27930.10s
                               ETA: 1272367.1s

################################################################################
                    [1m Learning iteration 2148/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.535s, learning 0.192s)
               Value function loss: 31.1634
                    Surrogate loss: 0.0026
             Mean action noise std: 0.77
                       Mean reward: 241.02
               Mean episode length: 117.28
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35209216
                    Iteration time: 11.73s
                        Total time: 27941.83s
                               ETA: 1272296.0s

################################################################################
                    [1m Learning iteration 2149/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.622s, learning 0.177s)
               Value function loss: 24.4237
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 246.18
               Mean episode length: 119.71
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35225600
                    Iteration time: 11.80s
                        Total time: 27953.63s
                               ETA: 1272228.2s

################################################################################
                    [1m Learning iteration 2150/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.390s, learning 0.167s)
               Value function loss: 24.6931
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 253.97
               Mean episode length: 122.55
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35241984
                    Iteration time: 11.56s
                        Total time: 27965.19s
                               ETA: 1272149.5s

################################################################################
                    [1m Learning iteration 2151/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.262s, learning 0.279s)
               Value function loss: 25.2313
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 244.75
               Mean episode length: 118.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35258368
                    Iteration time: 12.54s
                        Total time: 27977.73s
                               ETA: 1272115.6s

################################################################################
                    [1m Learning iteration 2152/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.802s, learning 0.171s)
               Value function loss: 29.5262
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 244.96
               Mean episode length: 119.03
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35274752
                    Iteration time: 11.97s
                        Total time: 27989.70s
                               ETA: 1272055.9s

################################################################################
                    [1m Learning iteration 2153/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.558s, learning 0.232s)
               Value function loss: 27.5756
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 250.66
               Mean episode length: 121.78
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35291136
                    Iteration time: 11.79s
                        Total time: 28001.49s
                               ETA: 1271988.0s

################################################################################
                    [1m Learning iteration 2154/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.955s, learning 0.238s)
               Value function loss: 28.4592
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 250.07
               Mean episode length: 120.71
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35307520
                    Iteration time: 12.19s
                        Total time: 28013.69s
                               ETA: 1271938.3s

################################################################################
                    [1m Learning iteration 2155/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.885s, learning 0.190s)
               Value function loss: 24.2223
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 255.41
               Mean episode length: 123.01
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35323904
                    Iteration time: 12.08s
                        Total time: 28025.76s
                               ETA: 1271883.4s

################################################################################
                    [1m Learning iteration 2156/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.049s, learning 0.167s)
               Value function loss: 24.2246
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 250.60
               Mean episode length: 121.23
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35340288
                    Iteration time: 12.22s
                        Total time: 28037.98s
                               ETA: 1271834.9s

################################################################################
                    [1m Learning iteration 2157/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.582s, learning 0.175s)
               Value function loss: 23.8471
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 258.08
               Mean episode length: 124.90
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35356672
                    Iteration time: 11.76s
                        Total time: 28049.73s
                               ETA: 1271765.6s

################################################################################
                    [1m Learning iteration 2158/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.698s, learning 0.166s)
               Value function loss: 28.1338
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 251.18
               Mean episode length: 121.02
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35373056
                    Iteration time: 11.86s
                        Total time: 28061.60s
                               ETA: 1271701.2s

################################################################################
                    [1m Learning iteration 2159/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.944s, learning 0.231s)
               Value function loss: 27.5093
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 242.96
               Mean episode length: 118.13
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35389440
                    Iteration time: 12.18s
                        Total time: 28073.77s
                               ETA: 1271650.9s

################################################################################
                    [1m Learning iteration 2160/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.979s, learning 0.159s)
               Value function loss: 26.0710
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 249.98
               Mean episode length: 119.50
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35405824
                    Iteration time: 12.14s
                        Total time: 28085.91s
                               ETA: 1271599.0s

################################################################################
                    [1m Learning iteration 2161/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.377s, learning 0.198s)
               Value function loss: 23.9066
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 248.93
               Mean episode length: 119.82
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35422208
                    Iteration time: 11.58s
                        Total time: 28097.49s
                               ETA: 1271521.7s

################################################################################
                    [1m Learning iteration 2162/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.011s, learning 0.335s)
               Value function loss: 25.9249
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 236.31
               Mean episode length: 115.18
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35438592
                    Iteration time: 12.35s
                        Total time: 28109.83s
                               ETA: 1271479.3s

################################################################################
                    [1m Learning iteration 2163/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.687s, learning 0.235s)
               Value function loss: 25.1602
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 255.57
               Mean episode length: 123.02
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35454976
                    Iteration time: 11.92s
                        Total time: 28121.75s
                               ETA: 1271417.8s

################################################################################
                    [1m Learning iteration 2164/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.937s, learning 0.190s)
               Value function loss: 29.6333
                    Surrogate loss: -0.0166
             Mean action noise std: 0.77
                       Mean reward: 252.12
               Mean episode length: 121.83
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35471360
                    Iteration time: 12.13s
                        Total time: 28133.88s
                               ETA: 1271365.6s

################################################################################
                    [1m Learning iteration 2165/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.814s, learning 0.206s)
               Value function loss: 26.1977
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 247.39
               Mean episode length: 119.76
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35487744
                    Iteration time: 12.02s
                        Total time: 28145.90s
                               ETA: 1271308.5s

################################################################################
                    [1m Learning iteration 2166/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.886s, learning 0.156s)
               Value function loss: 23.9059
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 254.49
               Mean episode length: 122.55
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35504128
                    Iteration time: 12.04s
                        Total time: 28157.94s
                               ETA: 1271252.5s

################################################################################
                    [1m Learning iteration 2167/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.610s, learning 0.157s)
               Value function loss: 24.3870
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 253.05
               Mean episode length: 121.73
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35520512
                    Iteration time: 11.77s
                        Total time: 28169.71s
                               ETA: 1271184.2s

################################################################################
                    [1m Learning iteration 2168/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.794s, learning 0.220s)
               Value function loss: 24.8010
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 244.64
               Mean episode length: 117.95
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35536896
                    Iteration time: 12.01s
                        Total time: 28181.72s
                               ETA: 1271127.0s

################################################################################
                    [1m Learning iteration 2169/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.521s, learning 0.175s)
               Value function loss: 29.4855
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 254.06
               Mean episode length: 122.77
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35553280
                    Iteration time: 11.70s
                        Total time: 28193.42s
                               ETA: 1271055.5s

################################################################################
                    [1m Learning iteration 2170/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.735s, learning 0.165s)
               Value function loss: 28.0368
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 251.08
               Mean episode length: 120.33
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35569664
                    Iteration time: 11.90s
                        Total time: 28205.32s
                               ETA: 1270993.3s

################################################################################
                    [1m Learning iteration 2171/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.043s, learning 0.283s)
               Value function loss: 25.0888
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 254.35
               Mean episode length: 123.12
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35586048
                    Iteration time: 12.33s
                        Total time: 28217.65s
                               ETA: 1270950.4s

################################################################################
                    [1m Learning iteration 2172/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.778s, learning 0.187s)
               Value function loss: 19.6494
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 254.40
               Mean episode length: 121.88
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35602432
                    Iteration time: 11.97s
                        Total time: 28229.61s
                               ETA: 1270891.2s

################################################################################
                    [1m Learning iteration 2173/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.757s, learning 0.198s)
               Value function loss: 26.1469
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 253.69
               Mean episode length: 122.99
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35618816
                    Iteration time: 11.95s
                        Total time: 28241.57s
                               ETA: 1270831.6s

################################################################################
                    [1m Learning iteration 2174/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.701s, learning 0.165s)
               Value function loss: 28.3193
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 254.26
               Mean episode length: 122.13
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35635200
                    Iteration time: 11.87s
                        Total time: 28253.43s
                               ETA: 1270768.0s

################################################################################
                    [1m Learning iteration 2175/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.080s, learning 0.165s)
               Value function loss: 30.2635
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 252.29
               Mean episode length: 121.43
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35651584
                    Iteration time: 12.24s
                        Total time: 28265.68s
                               ETA: 1270721.5s

################################################################################
                    [1m Learning iteration 2176/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.959s, learning 0.163s)
               Value function loss: 29.0251
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 257.10
               Mean episode length: 123.03
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35667968
                    Iteration time: 12.12s
                        Total time: 28277.80s
                               ETA: 1270669.5s

################################################################################
                    [1m Learning iteration 2177/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.203s, learning 0.214s)
               Value function loss: 27.2298
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 253.46
               Mean episode length: 122.13
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35684352
                    Iteration time: 12.42s
                        Total time: 28290.22s
                               ETA: 1270630.8s

################################################################################
                    [1m Learning iteration 2178/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.078s, learning 0.209s)
               Value function loss: 28.5825
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 250.03
               Mean episode length: 119.24
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35700736
                    Iteration time: 12.29s
                        Total time: 28302.50s
                               ETA: 1270586.3s

################################################################################
                    [1m Learning iteration 2179/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.825s, learning 0.176s)
               Value function loss: 27.8072
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 254.54
               Mean episode length: 121.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35717120
                    Iteration time: 12.00s
                        Total time: 28314.51s
                               ETA: 1270529.0s

################################################################################
                    [1m Learning iteration 2180/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.694s, learning 0.154s)
               Value function loss: 27.0389
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 252.90
               Mean episode length: 121.03
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35733504
                    Iteration time: 11.85s
                        Total time: 28326.35s
                               ETA: 1270464.9s

################################################################################
                    [1m Learning iteration 2181/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.620s, learning 0.201s)
               Value function loss: 27.4892
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 252.65
               Mean episode length: 120.20
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35749888
                    Iteration time: 11.82s
                        Total time: 28338.17s
                               ETA: 1270399.6s

################################################################################
                    [1m Learning iteration 2182/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.679s, learning 0.308s)
               Value function loss: 21.5158
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 248.08
               Mean episode length: 119.51
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35766272
                    Iteration time: 11.99s
                        Total time: 28350.16s
                               ETA: 1270341.8s

################################################################################
                    [1m Learning iteration 2183/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.473s, learning 0.168s)
               Value function loss: 27.5227
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 253.73
               Mean episode length: 121.40
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35782656
                    Iteration time: 11.64s
                        Total time: 28361.80s
                               ETA: 1270268.5s

################################################################################
                    [1m Learning iteration 2184/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.849s, learning 0.205s)
               Value function loss: 24.7642
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 257.05
               Mean episode length: 122.59
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35799040
                    Iteration time: 12.05s
                        Total time: 28373.86s
                               ETA: 1270213.8s

################################################################################
                    [1m Learning iteration 2185/100000 [0m                    

                       Computation: 1307 steps/s (collection: 12.367s, learning 0.166s)
               Value function loss: 27.6632
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 244.81
               Mean episode length: 119.12
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35815424
                    Iteration time: 12.53s
                        Total time: 28386.39s
                               ETA: 1270180.6s

################################################################################
                    [1m Learning iteration 2186/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.405s, learning 0.164s)
               Value function loss: 27.4584
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 253.14
               Mean episode length: 121.78
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35831808
                    Iteration time: 11.57s
                        Total time: 28397.96s
                               ETA: 1270104.3s

################################################################################
                    [1m Learning iteration 2187/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.462s, learning 0.162s)
               Value function loss: 23.1548
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 251.52
               Mean episode length: 120.91
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35848192
                    Iteration time: 11.62s
                        Total time: 28409.58s
                               ETA: 1270030.5s

################################################################################
                    [1m Learning iteration 2188/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.901s, learning 0.165s)
               Value function loss: 22.2918
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 249.51
               Mean episode length: 119.63
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35864576
                    Iteration time: 12.07s
                        Total time: 28421.65s
                               ETA: 1269976.5s

################################################################################
                    [1m Learning iteration 2189/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.553s, learning 0.162s)
               Value function loss: 30.1908
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 256.18
               Mean episode length: 122.64
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35880960
                    Iteration time: 11.72s
                        Total time: 28433.37s
                               ETA: 1269906.8s

################################################################################
                    [1m Learning iteration 2190/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.624s, learning 0.183s)
               Value function loss: 24.6656
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 249.13
               Mean episode length: 119.06
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35897344
                    Iteration time: 11.81s
                        Total time: 28445.17s
                               ETA: 1269841.3s

################################################################################
                    [1m Learning iteration 2191/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.712s, learning 0.295s)
               Value function loss: 30.5797
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 257.87
               Mean episode length: 122.62
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35913728
                    Iteration time: 12.01s
                        Total time: 28457.18s
                               ETA: 1269784.8s

################################################################################
                    [1m Learning iteration 2192/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.547s, learning 0.165s)
               Value function loss: 29.3776
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 254.48
               Mean episode length: 122.04
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35930112
                    Iteration time: 11.71s
                        Total time: 28468.89s
                               ETA: 1269715.2s

################################################################################
                    [1m Learning iteration 2193/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.849s, learning 0.205s)
               Value function loss: 26.4214
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 242.51
               Mean episode length: 116.35
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35946496
                    Iteration time: 12.05s
                        Total time: 28480.95s
                               ETA: 1269660.8s

################################################################################
                    [1m Learning iteration 2194/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.289s, learning 0.217s)
               Value function loss: 25.9248
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 252.55
               Mean episode length: 120.82
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35962880
                    Iteration time: 12.51s
                        Total time: 28493.45s
                               ETA: 1269626.7s

################################################################################
                    [1m Learning iteration 2195/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.758s, learning 0.164s)
               Value function loss: 27.2300
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 252.85
               Mean episode length: 120.64
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35979264
                    Iteration time: 11.92s
                        Total time: 28505.37s
                               ETA: 1269566.5s

################################################################################
                    [1m Learning iteration 2196/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.596s, learning 0.170s)
               Value function loss: 26.2993
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 253.07
               Mean episode length: 121.47
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 35995648
                    Iteration time: 11.77s
                        Total time: 28517.14s
                               ETA: 1269499.4s

################################################################################
                    [1m Learning iteration 2197/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.799s, learning 0.166s)
               Value function loss: 23.2458
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 255.06
               Mean episode length: 122.92
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36012032
                    Iteration time: 11.97s
                        Total time: 28529.10s
                               ETA: 1269441.3s

################################################################################
                    [1m Learning iteration 2198/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.012s, learning 0.271s)
               Value function loss: 22.5225
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 249.82
               Mean episode length: 120.14
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36028416
                    Iteration time: 12.28s
                        Total time: 28541.39s
                               ETA: 1269397.3s

################################################################################
                    [1m Learning iteration 2199/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.895s, learning 0.159s)
               Value function loss: 23.8224
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 259.44
               Mean episode length: 123.95
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36044800
                    Iteration time: 12.05s
                        Total time: 28553.44s
                               ETA: 1269343.2s

################################################################################
                    [1m Learning iteration 2200/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.946s, learning 0.162s)
               Value function loss: 24.7622
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 257.23
               Mean episode length: 124.00
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36061184
                    Iteration time: 12.11s
                        Total time: 28565.55s
                               ETA: 1269291.5s

################################################################################
                    [1m Learning iteration 2201/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.168s, learning 0.159s)
               Value function loss: 26.8701
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 249.13
               Mean episode length: 119.28
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36077568
                    Iteration time: 12.33s
                        Total time: 28577.87s
                               ETA: 1269249.6s

################################################################################
                    [1m Learning iteration 2202/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.781s, learning 0.197s)
               Value function loss: 26.4362
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 252.31
               Mean episode length: 120.84
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36093952
                    Iteration time: 11.98s
                        Total time: 28589.85s
                               ETA: 1269192.2s

################################################################################
                    [1m Learning iteration 2203/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.028s, learning 0.191s)
               Value function loss: 21.7450
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 250.60
               Mean episode length: 121.01
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36110336
                    Iteration time: 12.22s
                        Total time: 28602.07s
                               ETA: 1269145.6s

################################################################################
                    [1m Learning iteration 2204/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.611s, learning 0.175s)
               Value function loss: 22.4219
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 258.64
               Mean episode length: 124.06
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36126720
                    Iteration time: 11.79s
                        Total time: 28613.86s
                               ETA: 1269079.7s

################################################################################
                    [1m Learning iteration 2205/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.018s, learning 0.189s)
               Value function loss: 26.5774
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 258.53
               Mean episode length: 123.78
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36143104
                    Iteration time: 12.21s
                        Total time: 28626.07s
                               ETA: 1269032.7s

################################################################################
                    [1m Learning iteration 2206/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.697s, learning 0.192s)
               Value function loss: 28.7612
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 255.15
               Mean episode length: 123.26
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36159488
                    Iteration time: 11.89s
                        Total time: 28637.95s
                               ETA: 1268971.5s

################################################################################
                    [1m Learning iteration 2207/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.889s, learning 0.316s)
               Value function loss: 28.9300
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 259.18
               Mean episode length: 123.66
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36175872
                    Iteration time: 12.20s
                        Total time: 28650.16s
                               ETA: 1268924.4s

################################################################################
                    [1m Learning iteration 2208/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.154s, learning 0.223s)
               Value function loss: 24.1297
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 253.91
               Mean episode length: 120.74
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36192256
                    Iteration time: 12.38s
                        Total time: 28662.54s
                               ETA: 1268884.9s

################################################################################
                    [1m Learning iteration 2209/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.964s, learning 0.161s)
               Value function loss: 26.6934
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 255.57
               Mean episode length: 122.17
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36208640
                    Iteration time: 12.12s
                        Total time: 28674.66s
                               ETA: 1268834.3s

################################################################################
                    [1m Learning iteration 2210/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.264s, learning 0.190s)
               Value function loss: 26.0371
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 251.88
               Mean episode length: 120.92
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36225024
                    Iteration time: 12.45s
                        Total time: 28687.11s
                               ETA: 1268798.3s

################################################################################
                    [1m Learning iteration 2211/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.817s, learning 0.165s)
               Value function loss: 28.0633
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 253.83
               Mean episode length: 122.37
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36241408
                    Iteration time: 11.98s
                        Total time: 28699.10s
                               ETA: 1268741.4s

################################################################################
                    [1m Learning iteration 2212/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.170s, learning 0.163s)
               Value function loss: 25.0284
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 255.53
               Mean episode length: 122.31
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36257792
                    Iteration time: 12.33s
                        Total time: 28711.43s
                               ETA: 1268700.1s

################################################################################
                    [1m Learning iteration 2213/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.712s, learning 0.189s)
               Value function loss: 24.5995
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 255.80
               Mean episode length: 122.81
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36274176
                    Iteration time: 11.90s
                        Total time: 28723.33s
                               ETA: 1268639.7s

################################################################################
                    [1m Learning iteration 2214/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.876s, learning 0.225s)
               Value function loss: 22.5805
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 252.20
               Mean episode length: 121.29
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36290560
                    Iteration time: 12.10s
                        Total time: 28735.43s
                               ETA: 1268588.2s

################################################################################
                    [1m Learning iteration 2215/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.839s, learning 0.173s)
               Value function loss: 21.5047
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 260.73
               Mean episode length: 124.13
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36306944
                    Iteration time: 12.01s
                        Total time: 28747.44s
                               ETA: 1268532.8s

################################################################################
                    [1m Learning iteration 2216/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.800s, learning 0.164s)
               Value function loss: 26.7288
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 251.79
               Mean episode length: 120.82
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36323328
                    Iteration time: 11.96s
                        Total time: 28759.41s
                               ETA: 1268475.4s

################################################################################
                    [1m Learning iteration 2217/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.808s, learning 0.248s)
               Value function loss: 27.5217
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 259.58
               Mean episode length: 123.82
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36339712
                    Iteration time: 12.06s
                        Total time: 28771.46s
                               ETA: 1268422.0s

################################################################################
                    [1m Learning iteration 2218/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.008s, learning 0.167s)
               Value function loss: 24.6068
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 246.85
               Mean episode length: 119.02
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36356096
                    Iteration time: 12.17s
                        Total time: 28783.64s
                               ETA: 1268373.9s

################################################################################
                    [1m Learning iteration 2219/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.876s, learning 0.159s)
               Value function loss: 21.8890
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 247.53
               Mean episode length: 118.76
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36372480
                    Iteration time: 12.04s
                        Total time: 28795.67s
                               ETA: 1268319.7s

################################################################################
                    [1m Learning iteration 2220/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.922s, learning 0.175s)
               Value function loss: 26.3365
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 257.87
               Mean episode length: 123.87
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36388864
                    Iteration time: 12.10s
                        Total time: 28807.77s
                               ETA: 1268268.3s

################################################################################
                    [1m Learning iteration 2221/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.195s, learning 0.164s)
               Value function loss: 26.3855
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 249.94
               Mean episode length: 119.75
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36405248
                    Iteration time: 12.36s
                        Total time: 28820.13s
                               ETA: 1268228.4s

################################################################################
                    [1m Learning iteration 2222/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.147s, learning 0.163s)
               Value function loss: 31.5209
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 256.34
               Mean episode length: 123.53
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36421632
                    Iteration time: 12.31s
                        Total time: 28832.44s
                               ETA: 1268186.4s

################################################################################
                    [1m Learning iteration 2223/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.928s, learning 0.235s)
               Value function loss: 31.2970
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 250.75
               Mean episode length: 120.06
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36438016
                    Iteration time: 12.16s
                        Total time: 28844.60s
                               ETA: 1268138.0s

################################################################################
                    [1m Learning iteration 2224/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.159s, learning 0.169s)
               Value function loss: 30.1419
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 255.02
               Mean episode length: 121.93
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36454400
                    Iteration time: 12.33s
                        Total time: 28856.93s
                               ETA: 1268096.8s

################################################################################
                    [1m Learning iteration 2225/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.149s, learning 0.254s)
               Value function loss: 30.0036
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 256.67
               Mean episode length: 122.51
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36470784
                    Iteration time: 12.40s
                        Total time: 28869.33s
                               ETA: 1268058.9s

################################################################################
                    [1m Learning iteration 2226/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.427s, learning 0.165s)
               Value function loss: 33.6432
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 246.52
               Mean episode length: 117.98
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36487168
                    Iteration time: 11.59s
                        Total time: 28880.93s
                               ETA: 1267985.5s

################################################################################
                    [1m Learning iteration 2227/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.491s, learning 0.201s)
               Value function loss: 28.1470
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 244.30
               Mean episode length: 117.32
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36503552
                    Iteration time: 11.69s
                        Total time: 28892.62s
                               ETA: 1267916.5s

################################################################################
                    [1m Learning iteration 2228/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.873s, learning 0.160s)
               Value function loss: 29.7104
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 244.83
               Mean episode length: 118.08
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36519936
                    Iteration time: 12.03s
                        Total time: 28904.65s
                               ETA: 1267862.6s

################################################################################
                    [1m Learning iteration 2229/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.992s, learning 0.278s)
               Value function loss: 26.4852
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 258.58
               Mean episode length: 123.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36536320
                    Iteration time: 12.27s
                        Total time: 28916.92s
                               ETA: 1267819.1s

################################################################################
                    [1m Learning iteration 2230/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.724s, learning 0.166s)
               Value function loss: 28.0396
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 253.43
               Mean episode length: 121.25
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36552704
                    Iteration time: 11.89s
                        Total time: 28928.81s
                               ETA: 1267758.9s

################################################################################
                    [1m Learning iteration 2231/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.616s, learning 0.273s)
               Value function loss: 25.7300
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 252.86
               Mean episode length: 121.23
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36569088
                    Iteration time: 11.89s
                        Total time: 28940.70s
                               ETA: 1267698.7s

################################################################################
                    [1m Learning iteration 2232/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.978s, learning 0.184s)
               Value function loss: 27.6643
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 254.93
               Mean episode length: 122.40
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36585472
                    Iteration time: 12.16s
                        Total time: 28952.86s
                               ETA: 1267650.5s

################################################################################
                    [1m Learning iteration 2233/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.231s, learning 0.192s)
               Value function loss: 26.8802
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 256.19
               Mean episode length: 122.23
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36601856
                    Iteration time: 12.42s
                        Total time: 28965.29s
                               ETA: 1267613.8s

################################################################################
                    [1m Learning iteration 2234/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.746s, learning 0.176s)
               Value function loss: 23.4329
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 253.33
               Mean episode length: 120.72
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36618240
                    Iteration time: 11.92s
                        Total time: 28977.21s
                               ETA: 1267555.1s

################################################################################
                    [1m Learning iteration 2235/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.088s, learning 0.199s)
               Value function loss: 22.5439
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 254.16
               Mean episode length: 120.93
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36634624
                    Iteration time: 12.29s
                        Total time: 28989.49s
                               ETA: 1267512.5s

################################################################################
                    [1m Learning iteration 2236/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.885s, learning 0.302s)
               Value function loss: 27.1826
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 247.36
               Mean episode length: 118.89
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36651008
                    Iteration time: 12.19s
                        Total time: 29001.68s
                               ETA: 1267465.5s

################################################################################
                    [1m Learning iteration 2237/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.995s, learning 0.162s)
               Value function loss: 30.3423
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 250.78
               Mean episode length: 120.23
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36667392
                    Iteration time: 12.16s
                        Total time: 29013.84s
                               ETA: 1267417.3s

################################################################################
                    [1m Learning iteration 2238/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.765s, learning 0.167s)
               Value function loss: 29.3822
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 254.73
               Mean episode length: 123.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36683776
                    Iteration time: 11.93s
                        Total time: 29025.77s
                               ETA: 1267359.2s

################################################################################
                    [1m Learning iteration 2239/100000 [0m                    

                       Computation: 1338 steps/s (collection: 11.963s, learning 0.276s)
               Value function loss: 26.8313
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 257.56
               Mean episode length: 123.00
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36700160
                    Iteration time: 12.24s
                        Total time: 29038.01s
                               ETA: 1267314.6s

################################################################################
                    [1m Learning iteration 2240/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.976s, learning 0.173s)
               Value function loss: 29.0578
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 248.36
               Mean episode length: 120.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36716544
                    Iteration time: 12.15s
                        Total time: 29050.16s
                               ETA: 1267266.1s

################################################################################
                    [1m Learning iteration 2241/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.739s, learning 0.199s)
               Value function loss: 30.0115
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 250.35
               Mean episode length: 120.62
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36732928
                    Iteration time: 11.94s
                        Total time: 29062.10s
                               ETA: 1267208.5s

################################################################################
                    [1m Learning iteration 2242/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.015s, learning 0.160s)
               Value function loss: 32.8240
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 258.81
               Mean episode length: 124.35
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36749312
                    Iteration time: 12.18s
                        Total time: 29074.27s
                               ETA: 1267161.2s

################################################################################
                    [1m Learning iteration 2243/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.214s, learning 0.187s)
               Value function loss: 28.8602
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 250.48
               Mean episode length: 120.54
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36765696
                    Iteration time: 12.40s
                        Total time: 29086.67s
                               ETA: 1267123.8s

################################################################################
                    [1m Learning iteration 2244/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.931s, learning 0.169s)
               Value function loss: 27.9497
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 252.34
               Mean episode length: 121.95
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36782080
                    Iteration time: 12.10s
                        Total time: 29098.77s
                               ETA: 1267073.3s

################################################################################
                    [1m Learning iteration 2245/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.740s, learning 0.176s)
               Value function loss: 26.9377
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 248.62
               Mean episode length: 118.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36798464
                    Iteration time: 11.92s
                        Total time: 29110.69s
                               ETA: 1267014.8s

################################################################################
                    [1m Learning iteration 2246/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.471s, learning 0.197s)
               Value function loss: 25.2404
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 254.42
               Mean episode length: 122.62
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36814848
                    Iteration time: 11.67s
                        Total time: 29122.35s
                               ETA: 1266945.5s

################################################################################
                    [1m Learning iteration 2247/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.366s, learning 0.167s)
               Value function loss: 25.9848
                    Surrogate loss: -0.0071
             Mean action noise std: 0.77
                       Mean reward: 253.23
               Mean episode length: 121.33
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36831232
                    Iteration time: 11.53s
                        Total time: 29133.89s
                               ETA: 1266870.5s

################################################################################
                    [1m Learning iteration 2248/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.733s, learning 0.162s)
               Value function loss: 29.1264
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 249.08
               Mean episode length: 120.76
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36847616
                    Iteration time: 11.89s
                        Total time: 29145.78s
                               ETA: 1266811.2s

################################################################################
                    [1m Learning iteration 2249/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.771s, learning 0.213s)
               Value function loss: 26.8861
                    Surrogate loss: -0.0188
             Mean action noise std: 0.77
                       Mean reward: 246.08
               Mean episode length: 118.83
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36864000
                    Iteration time: 11.98s
                        Total time: 29157.77s
                               ETA: 1266755.9s

################################################################################
                    [1m Learning iteration 2250/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.937s, learning 0.166s)
               Value function loss: 23.7255
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 248.01
               Mean episode length: 119.43
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36880384
                    Iteration time: 12.10s
                        Total time: 29169.87s
                               ETA: 1266705.8s

################################################################################
                    [1m Learning iteration 2251/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.748s, learning 0.167s)
               Value function loss: 26.5370
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 252.99
               Mean episode length: 120.88
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36896768
                    Iteration time: 11.91s
                        Total time: 29181.78s
                               ETA: 1266647.5s

################################################################################
                    [1m Learning iteration 2252/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.557s, learning 0.156s)
               Value function loss: 27.9285
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 255.98
               Mean episode length: 123.39
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36913152
                    Iteration time: 11.71s
                        Total time: 29193.50s
                               ETA: 1266580.6s

################################################################################
                    [1m Learning iteration 2253/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.700s, learning 0.204s)
               Value function loss: 33.8043
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 259.84
               Mean episode length: 123.81
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36929536
                    Iteration time: 11.90s
                        Total time: 29205.40s
                               ETA: 1266521.9s

################################################################################
                    [1m Learning iteration 2254/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.920s, learning 0.199s)
               Value function loss: 30.3005
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 251.65
               Mean episode length: 121.44
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36945920
                    Iteration time: 12.12s
                        Total time: 29217.52s
                               ETA: 1266472.6s

################################################################################
                    [1m Learning iteration 2255/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.532s, learning 0.252s)
               Value function loss: 28.9593
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 253.57
               Mean episode length: 122.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36962304
                    Iteration time: 11.78s
                        Total time: 29229.30s
                               ETA: 1266408.8s

################################################################################
                    [1m Learning iteration 2256/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.886s, learning 0.166s)
               Value function loss: 31.2832
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 246.88
               Mean episode length: 119.58
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36978688
                    Iteration time: 12.05s
                        Total time: 29241.36s
                               ETA: 1266356.7s

################################################################################
                    [1m Learning iteration 2257/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.819s, learning 0.283s)
               Value function loss: 36.7807
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 252.40
               Mean episode length: 120.65
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 36995072
                    Iteration time: 12.10s
                        Total time: 29253.46s
                               ETA: 1266306.8s

################################################################################
                    [1m Learning iteration 2258/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.887s, learning 0.154s)
               Value function loss: 30.2595
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 245.69
               Mean episode length: 118.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37011456
                    Iteration time: 12.04s
                        Total time: 29265.50s
                               ETA: 1266254.2s

################################################################################
                    [1m Learning iteration 2259/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.816s, learning 0.206s)
               Value function loss: 30.3373
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 253.63
               Mean episode length: 122.40
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37027840
                    Iteration time: 12.02s
                        Total time: 29277.52s
                               ETA: 1266200.9s

################################################################################
                    [1m Learning iteration 2260/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.590s, learning 0.190s)
               Value function loss: 28.2707
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 258.68
               Mean episode length: 122.24
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37044224
                    Iteration time: 11.78s
                        Total time: 29289.30s
                               ETA: 1266137.2s

################################################################################
                    [1m Learning iteration 2261/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.771s, learning 0.198s)
               Value function loss: 29.9540
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 258.22
               Mean episode length: 123.64
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37060608
                    Iteration time: 11.97s
                        Total time: 29301.27s
                               ETA: 1266081.7s

################################################################################
                    [1m Learning iteration 2262/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.541s, learning 0.208s)
               Value function loss: 27.2392
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 254.63
               Mean episode length: 122.13
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37076992
                    Iteration time: 11.75s
                        Total time: 29313.02s
                               ETA: 1266016.7s

################################################################################
                    [1m Learning iteration 2263/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.081s, learning 0.172s)
               Value function loss: 27.0294
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 248.70
               Mean episode length: 119.97
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37093376
                    Iteration time: 12.25s
                        Total time: 29325.27s
                               ETA: 1265973.5s

################################################################################
                    [1m Learning iteration 2264/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.725s, learning 0.222s)
               Value function loss: 26.4850
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 251.33
               Mean episode length: 122.53
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37109760
                    Iteration time: 11.95s
                        Total time: 29337.22s
                               ETA: 1265917.1s

################################################################################
                    [1m Learning iteration 2265/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.241s, learning 0.165s)
               Value function loss: 24.9308
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 251.29
               Mean episode length: 121.47
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37126144
                    Iteration time: 12.41s
                        Total time: 29349.62s
                               ETA: 1265880.6s

################################################################################
                    [1m Learning iteration 2266/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.703s, learning 0.173s)
               Value function loss: 25.2286
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 249.74
               Mean episode length: 120.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37142528
                    Iteration time: 11.88s
                        Total time: 29361.50s
                               ETA: 1265821.3s

################################################################################
                    [1m Learning iteration 2267/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.155s, learning 0.205s)
               Value function loss: 30.0662
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 257.48
               Mean episode length: 122.55
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37158912
                    Iteration time: 12.36s
                        Total time: 29373.86s
                               ETA: 1265782.8s

################################################################################
                    [1m Learning iteration 2268/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.678s, learning 0.167s)
               Value function loss: 31.8050
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 256.13
               Mean episode length: 122.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37175296
                    Iteration time: 11.85s
                        Total time: 29385.70s
                               ETA: 1265722.2s

################################################################################
                    [1m Learning iteration 2269/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.454s, learning 0.164s)
               Value function loss: 31.7599
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 252.07
               Mean episode length: 121.82
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37191680
                    Iteration time: 11.62s
                        Total time: 29397.32s
                               ETA: 1265651.8s

################################################################################
                    [1m Learning iteration 2270/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.476s, learning 0.163s)
               Value function loss: 31.4389
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 247.71
               Mean episode length: 119.49
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37208064
                    Iteration time: 11.64s
                        Total time: 29408.96s
                               ETA: 1265582.4s

################################################################################
                    [1m Learning iteration 2271/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.228s, learning 0.369s)
               Value function loss: 27.6061
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 251.07
               Mean episode length: 121.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37224448
                    Iteration time: 12.60s
                        Total time: 29421.56s
                               ETA: 1265554.3s

################################################################################
                    [1m Learning iteration 2272/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.013s, learning 0.196s)
               Value function loss: 31.2691
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 242.49
               Mean episode length: 117.72
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37240832
                    Iteration time: 12.21s
                        Total time: 29433.77s
                               ETA: 1265509.5s

################################################################################
                    [1m Learning iteration 2273/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.680s, learning 0.183s)
               Value function loss: 37.4196
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 254.09
               Mean episode length: 122.79
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37257216
                    Iteration time: 11.86s
                        Total time: 29445.63s
                               ETA: 1265449.9s

################################################################################
                    [1m Learning iteration 2274/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.846s, learning 0.205s)
               Value function loss: 32.9649
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 248.98
               Mean episode length: 120.16
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37273600
                    Iteration time: 12.05s
                        Total time: 29457.68s
                               ETA: 1265398.3s

################################################################################
                    [1m Learning iteration 2275/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.482s, learning 0.200s)
               Value function loss: 27.8464
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 243.70
               Mean episode length: 117.28
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37289984
                    Iteration time: 11.68s
                        Total time: 29469.36s
                               ETA: 1265331.0s

################################################################################
                    [1m Learning iteration 2276/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.808s, learning 0.279s)
               Value function loss: 28.4813
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 231.40
               Mean episode length: 114.21
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37306368
                    Iteration time: 12.09s
                        Total time: 29481.45s
                               ETA: 1265281.1s

################################################################################
                    [1m Learning iteration 2277/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.947s, learning 0.183s)
               Value function loss: 27.7086
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 248.90
               Mean episode length: 120.37
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37322752
                    Iteration time: 12.13s
                        Total time: 29493.58s
                               ETA: 1265233.1s

################################################################################
                    [1m Learning iteration 2278/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.478s, learning 0.183s)
               Value function loss: 25.0973
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 242.04
               Mean episode length: 117.52
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37339136
                    Iteration time: 11.66s
                        Total time: 29505.24s
                               ETA: 1265165.0s

################################################################################
                    [1m Learning iteration 2279/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.781s, learning 0.287s)
               Value function loss: 26.0230
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 250.22
               Mean episode length: 120.10
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37355520
                    Iteration time: 12.07s
                        Total time: 29517.31s
                               ETA: 1265114.4s

################################################################################
                    [1m Learning iteration 2280/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.996s, learning 0.154s)
               Value function loss: 27.6005
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 251.72
               Mean episode length: 121.02
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37371904
                    Iteration time: 12.15s
                        Total time: 29529.46s
                               ETA: 1265067.4s

################################################################################
                    [1m Learning iteration 2281/100000 [0m                    

                       Computation: 1436 steps/s (collection: 11.215s, learning 0.192s)
               Value function loss: 24.0836
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 246.54
               Mean episode length: 119.30
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37388288
                    Iteration time: 11.41s
                        Total time: 29540.87s
                               ETA: 1264988.6s

################################################################################
                    [1m Learning iteration 2282/100000 [0m                    

                       Computation: 1417 steps/s (collection: 11.389s, learning 0.173s)
               Value function loss: 25.0067
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 248.60
               Mean episode length: 120.11
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37404672
                    Iteration time: 11.56s
                        Total time: 29552.43s
                               ETA: 1264916.5s

################################################################################
                    [1m Learning iteration 2283/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.229s, learning 0.163s)
               Value function loss: 26.6371
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 250.93
               Mean episode length: 121.89
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37421056
                    Iteration time: 12.39s
                        Total time: 29564.82s
                               ETA: 1264879.9s

################################################################################
                    [1m Learning iteration 2284/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.719s, learning 0.292s)
               Value function loss: 31.5044
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 250.31
               Mean episode length: 121.58
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37437440
                    Iteration time: 12.01s
                        Total time: 29576.83s
                               ETA: 1264827.0s

################################################################################
                    [1m Learning iteration 2285/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.184s, learning 0.256s)
               Value function loss: 26.9872
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 248.59
               Mean episode length: 119.99
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37453824
                    Iteration time: 11.44s
                        Total time: 29588.27s
                               ETA: 1264749.8s

################################################################################
                    [1m Learning iteration 2286/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.799s, learning 0.163s)
               Value function loss: 26.3273
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 252.30
               Mean episode length: 122.17
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37470208
                    Iteration time: 11.96s
                        Total time: 29600.23s
                               ETA: 1264694.9s

################################################################################
                    [1m Learning iteration 2287/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.570s, learning 0.163s)
               Value function loss: 29.7165
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 248.35
               Mean episode length: 121.35
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37486592
                    Iteration time: 11.73s
                        Total time: 29611.97s
                               ETA: 1264630.3s

################################################################################
                    [1m Learning iteration 2288/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.614s, learning 0.226s)
               Value function loss: 27.9348
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 257.01
               Mean episode length: 123.84
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37502976
                    Iteration time: 11.84s
                        Total time: 29623.81s
                               ETA: 1264570.3s

################################################################################
                    [1m Learning iteration 2289/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.752s, learning 0.185s)
               Value function loss: 30.0835
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 252.19
               Mean episode length: 121.61
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37519360
                    Iteration time: 11.94s
                        Total time: 29635.74s
                               ETA: 1264514.4s

################################################################################
                    [1m Learning iteration 2290/100000 [0m                    

                       Computation: 1418 steps/s (collection: 11.389s, learning 0.165s)
               Value function loss: 28.2952
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 254.15
               Mean episode length: 123.12
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37535744
                    Iteration time: 11.55s
                        Total time: 29647.30s
                               ETA: 1264442.3s

################################################################################
                    [1m Learning iteration 2291/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.242s, learning 0.235s)
               Value function loss: 27.9038
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 248.13
               Mean episode length: 120.34
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37552128
                    Iteration time: 12.48s
                        Total time: 29659.77s
                               ETA: 1264409.6s

################################################################################
                    [1m Learning iteration 2292/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.773s, learning 0.197s)
               Value function loss: 28.3480
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 246.89
               Mean episode length: 120.22
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37568512
                    Iteration time: 11.97s
                        Total time: 29671.74s
                               ETA: 1264355.3s

################################################################################
                    [1m Learning iteration 2293/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.734s, learning 0.183s)
               Value function loss: 28.3904
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 246.67
               Mean episode length: 119.28
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37584896
                    Iteration time: 11.92s
                        Total time: 29683.66s
                               ETA: 1264298.8s

################################################################################
                    [1m Learning iteration 2294/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.791s, learning 0.162s)
               Value function loss: 26.1437
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 251.13
               Mean episode length: 122.73
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37601280
                    Iteration time: 11.95s
                        Total time: 29695.61s
                               ETA: 1264243.8s

################################################################################
                    [1m Learning iteration 2295/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.548s, learning 0.196s)
               Value function loss: 30.7947
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 245.20
               Mean episode length: 119.14
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37617664
                    Iteration time: 11.74s
                        Total time: 29707.36s
                               ETA: 1264180.0s

################################################################################
                    [1m Learning iteration 2296/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.927s, learning 0.157s)
               Value function loss: 27.2789
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 254.98
               Mean episode length: 123.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37634048
                    Iteration time: 12.08s
                        Total time: 29719.44s
                               ETA: 1264130.7s

################################################################################
                    [1m Learning iteration 2297/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.584s, learning 0.251s)
               Value function loss: 25.0530
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 246.75
               Mean episode length: 119.31
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37650432
                    Iteration time: 11.83s
                        Total time: 29731.28s
                               ETA: 1264070.9s

################################################################################
                    [1m Learning iteration 2298/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.742s, learning 0.163s)
               Value function loss: 30.4854
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 247.76
               Mean episode length: 119.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37666816
                    Iteration time: 11.91s
                        Total time: 29743.18s
                               ETA: 1264014.0s

################################################################################
                    [1m Learning iteration 2299/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.689s, learning 0.157s)
               Value function loss: 30.1981
                    Surrogate loss: -0.0185
             Mean action noise std: 0.77
                       Mean reward: 252.48
               Mean episode length: 121.83
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37683200
                    Iteration time: 11.85s
                        Total time: 29755.03s
                               ETA: 1263954.8s

################################################################################
                    [1m Learning iteration 2300/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.455s, learning 0.166s)
               Value function loss: 30.8456
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 257.71
               Mean episode length: 123.99
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37699584
                    Iteration time: 11.62s
                        Total time: 29766.65s
                               ETA: 1263885.9s

################################################################################
                    [1m Learning iteration 2301/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.876s, learning 0.215s)
               Value function loss: 31.7456
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 241.08
               Mean episode length: 117.63
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37715968
                    Iteration time: 12.09s
                        Total time: 29778.74s
                               ETA: 1263837.1s

################################################################################
                    [1m Learning iteration 2302/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.642s, learning 0.268s)
               Value function loss: 30.4536
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 246.05
               Mean episode length: 119.94
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37732352
                    Iteration time: 11.91s
                        Total time: 29790.65s
                               ETA: 1263780.7s

################################################################################
                    [1m Learning iteration 2303/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.730s, learning 0.164s)
               Value function loss: 34.3035
                    Surrogate loss: -0.0173
             Mean action noise std: 0.77
                       Mean reward: 240.40
               Mean episode length: 117.40
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37748736
                    Iteration time: 11.89s
                        Total time: 29802.54s
                               ETA: 1263723.6s

################################################################################
                    [1m Learning iteration 2304/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.697s, learning 0.166s)
               Value function loss: 34.3455
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 252.22
               Mean episode length: 122.45
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37765120
                    Iteration time: 11.86s
                        Total time: 29814.41s
                               ETA: 1263665.2s

################################################################################
                    [1m Learning iteration 2305/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.566s, learning 0.194s)
               Value function loss: 33.0628
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 248.27
               Mean episode length: 120.32
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37781504
                    Iteration time: 11.76s
                        Total time: 29826.17s
                               ETA: 1263602.5s

################################################################################
                    [1m Learning iteration 2306/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.283s, learning 0.242s)
               Value function loss: 31.7049
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 249.93
               Mean episode length: 120.20
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37797888
                    Iteration time: 12.52s
                        Total time: 29838.69s
                               ETA: 1263572.2s

################################################################################
                    [1m Learning iteration 2307/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.428s, learning 0.186s)
               Value function loss: 28.8249
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 251.46
               Mean episode length: 122.21
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37814272
                    Iteration time: 11.61s
                        Total time: 29850.31s
                               ETA: 1263503.5s

################################################################################
                    [1m Learning iteration 2308/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.737s, learning 0.205s)
               Value function loss: 31.9049
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 243.87
               Mean episode length: 118.65
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37830656
                    Iteration time: 11.94s
                        Total time: 29862.25s
                               ETA: 1263448.6s

################################################################################
                    [1m Learning iteration 2309/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.584s, learning 0.163s)
               Value function loss: 27.6573
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 246.11
               Mean episode length: 119.53
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37847040
                    Iteration time: 11.75s
                        Total time: 29874.00s
                               ETA: 1263385.5s

################################################################################
                    [1m Learning iteration 2310/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.825s, learning 0.164s)
               Value function loss: 28.2564
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 258.62
               Mean episode length: 124.42
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37863424
                    Iteration time: 11.99s
                        Total time: 29885.98s
                               ETA: 1263332.7s

################################################################################
                    [1m Learning iteration 2311/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.443s, learning 0.221s)
               Value function loss: 30.2575
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 254.31
               Mean episode length: 123.05
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37879808
                    Iteration time: 11.66s
                        Total time: 29897.65s
                               ETA: 1263266.2s

################################################################################
                    [1m Learning iteration 2312/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.918s, learning 0.161s)
               Value function loss: 24.7039
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 245.07
               Mean episode length: 120.10
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37896192
                    Iteration time: 12.08s
                        Total time: 29909.73s
                               ETA: 1263217.2s

################################################################################
                    [1m Learning iteration 2313/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.892s, learning 0.173s)
               Value function loss: 24.6832
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 246.05
               Mean episode length: 119.12
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37912576
                    Iteration time: 12.06s
                        Total time: 29921.79s
                               ETA: 1263167.7s

################################################################################
                    [1m Learning iteration 2314/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.010s, learning 0.156s)
               Value function loss: 30.4974
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 251.81
               Mean episode length: 122.37
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37928960
                    Iteration time: 12.17s
                        Total time: 29933.96s
                               ETA: 1263122.6s

################################################################################
                    [1m Learning iteration 2315/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.747s, learning 0.210s)
               Value function loss: 31.0144
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 248.51
               Mean episode length: 120.95
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37945344
                    Iteration time: 11.96s
                        Total time: 29945.92s
                               ETA: 1263068.6s

################################################################################
                    [1m Learning iteration 2316/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.928s, learning 0.156s)
               Value function loss: 29.5178
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 247.33
               Mean episode length: 120.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37961728
                    Iteration time: 12.08s
                        Total time: 29958.00s
                               ETA: 1263019.9s

################################################################################
                    [1m Learning iteration 2317/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.452s, learning 0.192s)
               Value function loss: 28.6134
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 253.04
               Mean episode length: 123.36
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37978112
                    Iteration time: 11.64s
                        Total time: 29969.64s
                               ETA: 1262952.9s

################################################################################
                    [1m Learning iteration 2318/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.529s, learning 0.161s)
               Value function loss: 29.6456
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 254.19
               Mean episode length: 122.11
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 37994496
                    Iteration time: 11.69s
                        Total time: 29981.33s
                               ETA: 1262887.7s

################################################################################
                    [1m Learning iteration 2319/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.653s, learning 0.192s)
               Value function loss: 28.6211
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 241.15
               Mean episode length: 118.13
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38010880
                    Iteration time: 11.84s
                        Total time: 29993.18s
                               ETA: 1262829.2s

################################################################################
                    [1m Learning iteration 2320/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.868s, learning 0.212s)
               Value function loss: 35.8608
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 250.86
               Mean episode length: 120.58
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38027264
                    Iteration time: 12.08s
                        Total time: 30005.26s
                               ETA: 1262780.5s

################################################################################
                    [1m Learning iteration 2321/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.211s, learning 0.162s)
               Value function loss: 28.4083
                    Surrogate loss: -0.0205
             Mean action noise std: 0.77
                       Mean reward: 238.63
               Mean episode length: 116.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38043648
                    Iteration time: 12.37s
                        Total time: 30017.63s
                               ETA: 1262744.3s

################################################################################
                    [1m Learning iteration 2322/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.771s, learning 0.198s)
               Value function loss: 28.7119
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 252.64
               Mean episode length: 122.77
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38060032
                    Iteration time: 11.97s
                        Total time: 30029.60s
                               ETA: 1262691.0s

################################################################################
                    [1m Learning iteration 2323/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.677s, learning 0.162s)
               Value function loss: 30.6014
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 247.57
               Mean episode length: 120.70
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38076416
                    Iteration time: 11.84s
                        Total time: 30041.44s
                               ETA: 1262632.4s

################################################################################
                    [1m Learning iteration 2324/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.781s, learning 0.199s)
               Value function loss: 30.0279
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 240.40
               Mean episode length: 117.19
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38092800
                    Iteration time: 11.98s
                        Total time: 30053.42s
                               ETA: 1262579.7s

################################################################################
                    [1m Learning iteration 2325/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.737s, learning 0.172s)
               Value function loss: 27.5056
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 247.94
               Mean episode length: 120.26
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38109184
                    Iteration time: 11.91s
                        Total time: 30065.33s
                               ETA: 1262524.0s

################################################################################
                    [1m Learning iteration 2326/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.729s, learning 0.272s)
               Value function loss: 28.0549
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 246.54
               Mean episode length: 120.28
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38125568
                    Iteration time: 12.00s
                        Total time: 30077.33s
                               ETA: 1262472.3s

################################################################################
                    [1m Learning iteration 2327/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.579s, learning 0.167s)
               Value function loss: 24.2478
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 251.07
               Mean episode length: 121.98
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38141952
                    Iteration time: 11.75s
                        Total time: 30089.07s
                               ETA: 1262409.8s

################################################################################
                    [1m Learning iteration 2328/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.051s, learning 0.190s)
               Value function loss: 25.6587
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 248.49
               Mean episode length: 121.17
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38158336
                    Iteration time: 12.24s
                        Total time: 30101.31s
                               ETA: 1262368.2s

################################################################################
                    [1m Learning iteration 2329/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.380s, learning 0.274s)
               Value function loss: 28.7632
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 244.76
               Mean episode length: 119.27
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38174720
                    Iteration time: 11.65s
                        Total time: 30112.97s
                               ETA: 1262302.0s

################################################################################
                    [1m Learning iteration 2330/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.718s, learning 0.168s)
               Value function loss: 31.2039
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 245.05
               Mean episode length: 118.68
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38191104
                    Iteration time: 11.89s
                        Total time: 30124.85s
                               ETA: 1262245.6s

################################################################################
                    [1m Learning iteration 2331/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.770s, learning 0.217s)
               Value function loss: 32.8708
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 250.42
               Mean episode length: 121.52
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38207488
                    Iteration time: 11.99s
                        Total time: 30136.84s
                               ETA: 1262193.5s

################################################################################
                    [1m Learning iteration 2332/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.951s, learning 0.241s)
               Value function loss: 27.8694
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 244.24
               Mean episode length: 120.52
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38223872
                    Iteration time: 12.19s
                        Total time: 30149.03s
                               ETA: 1262149.9s

################################################################################
                    [1m Learning iteration 2333/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.825s, learning 0.208s)
               Value function loss: 25.4950
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 249.10
               Mean episode length: 122.03
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38240256
                    Iteration time: 12.03s
                        Total time: 30161.07s
                               ETA: 1262099.8s

################################################################################
                    [1m Learning iteration 2334/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.724s, learning 0.225s)
               Value function loss: 31.5075
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 248.01
               Mean episode length: 120.90
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38256640
                    Iteration time: 11.95s
                        Total time: 30173.02s
                               ETA: 1262046.2s

################################################################################
                    [1m Learning iteration 2335/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.822s, learning 0.179s)
               Value function loss: 31.3306
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 248.69
               Mean episode length: 122.36
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38273024
                    Iteration time: 12.00s
                        Total time: 30185.02s
                               ETA: 1261994.7s

################################################################################
                    [1m Learning iteration 2336/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.100s, learning 0.177s)
               Value function loss: 28.7701
                    Surrogate loss: -0.0186
             Mean action noise std: 0.77
                       Mean reward: 236.02
               Mean episode length: 115.62
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38289408
                    Iteration time: 12.28s
                        Total time: 30197.29s
                               ETA: 1261954.9s

################################################################################
                    [1m Learning iteration 2337/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.667s, learning 0.235s)
               Value function loss: 29.0373
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 242.53
               Mean episode length: 119.98
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38305792
                    Iteration time: 11.90s
                        Total time: 30209.20s
                               ETA: 1261899.4s

################################################################################
                    [1m Learning iteration 2338/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.840s, learning 0.175s)
               Value function loss: 25.8679
                    Surrogate loss: -0.0213
             Mean action noise std: 0.77
                       Mean reward: 246.13
               Mean episode length: 120.39
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38322176
                    Iteration time: 12.02s
                        Total time: 30221.21s
                               ETA: 1261848.7s

################################################################################
                    [1m Learning iteration 2339/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.904s, learning 0.185s)
               Value function loss: 28.9252
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 241.96
               Mean episode length: 118.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38338560
                    Iteration time: 12.09s
                        Total time: 30233.30s
                               ETA: 1261801.0s

################################################################################
                    [1m Learning iteration 2340/100000 [0m                    

                       Computation: 679 steps/s (collection: 23.865s, learning 0.233s)
               Value function loss: 26.9060
                    Surrogate loss: -0.0184
             Mean action noise std: 0.77
                       Mean reward: 236.03
               Mean episode length: 116.25
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38354944
                    Iteration time: 24.10s
                        Total time: 30257.40s
                               ETA: 1262254.4s

################################################################################
                    [1m Learning iteration 2341/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.575s, learning 0.214s)
               Value function loss: 29.0375
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 241.42
               Mean episode length: 119.01
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38371328
                    Iteration time: 23.79s
                        Total time: 30281.19s
                               ETA: 1262694.5s

################################################################################
                    [1m Learning iteration 2342/100000 [0m                    

                       Computation: 702 steps/s (collection: 23.145s, learning 0.184s)
               Value function loss: 28.3698
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 242.65
               Mean episode length: 119.64
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38387712
                    Iteration time: 23.33s
                        Total time: 30304.52s
                               ETA: 1263115.0s

################################################################################
                    [1m Learning iteration 2343/100000 [0m                    

                       Computation: 716 steps/s (collection: 22.698s, learning 0.167s)
               Value function loss: 25.8723
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 235.26
               Mean episode length: 116.39
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38404096
                    Iteration time: 22.86s
                        Total time: 30327.38s
                               ETA: 1263515.9s

################################################################################
                    [1m Learning iteration 2344/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.328s, learning 0.280s)
               Value function loss: 23.3396
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 231.85
               Mean episode length: 115.93
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38420480
                    Iteration time: 23.61s
                        Total time: 30350.99s
                               ETA: 1263947.2s

################################################################################
                    [1m Learning iteration 2345/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.375s, learning 0.222s)
               Value function loss: 26.8793
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 243.12
               Mean episode length: 121.17
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38436864
                    Iteration time: 23.60s
                        Total time: 30374.59s
                               ETA: 1264377.8s

################################################################################
                    [1m Learning iteration 2346/100000 [0m                    

                       Computation: 684 steps/s (collection: 23.695s, learning 0.234s)
               Value function loss: 29.4979
                    Surrogate loss: -0.0189
             Mean action noise std: 0.77
                       Mean reward: 238.48
               Mean episode length: 118.23
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38453248
                    Iteration time: 23.93s
                        Total time: 30398.52s
                               ETA: 1264821.8s

################################################################################
                    [1m Learning iteration 2347/100000 [0m                    

                       Computation: 702 steps/s (collection: 23.168s, learning 0.167s)
               Value function loss: 26.6531
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 242.15
               Mean episode length: 119.05
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38469632
                    Iteration time: 23.33s
                        Total time: 30421.85s
                               ETA: 1265240.6s

################################################################################
                    [1m Learning iteration 2348/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.932s, learning 0.192s)
               Value function loss: 26.1425
                    Surrogate loss: -0.0197
             Mean action noise std: 0.77
                       Mean reward: 248.44
               Mean episode length: 121.75
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38486016
                    Iteration time: 23.12s
                        Total time: 30444.98s
                               ETA: 1265650.4s

################################################################################
                    [1m Learning iteration 2349/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.076s, learning 0.202s)
               Value function loss: 24.9730
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 243.61
               Mean episode length: 120.29
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38502400
                    Iteration time: 23.28s
                        Total time: 30468.25s
                               ETA: 1266066.2s

################################################################################
                    [1m Learning iteration 2350/100000 [0m                    

                       Computation: 714 steps/s (collection: 22.780s, learning 0.160s)
               Value function loss: 29.8379
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 239.54
               Mean episode length: 118.77
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38518784
                    Iteration time: 22.94s
                        Total time: 30491.19s
                               ETA: 1266467.5s

################################################################################
                    [1m Learning iteration 2351/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.365s, learning 0.253s)
               Value function loss: 29.9983
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 246.11
               Mean episode length: 120.91
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38535168
                    Iteration time: 23.62s
                        Total time: 30514.81s
                               ETA: 1266896.6s

################################################################################
                    [1m Learning iteration 2352/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.742s, learning 0.239s)
               Value function loss: 26.2165
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 242.32
               Mean episode length: 120.40
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38551552
                    Iteration time: 22.98s
                        Total time: 30537.79s
                               ETA: 1267299.0s

################################################################################
                    [1m Learning iteration 2353/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.461s, learning 0.217s)
               Value function loss: 25.7571
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 236.71
               Mean episode length: 117.54
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38567936
                    Iteration time: 23.68s
                        Total time: 30561.47s
                               ETA: 1267729.8s

################################################################################
                    [1m Learning iteration 2354/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.241s, learning 0.244s)
               Value function loss: 25.6955
                    Surrogate loss: -0.0209
             Mean action noise std: 0.77
                       Mean reward: 242.97
               Mean episode length: 119.55
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38584320
                    Iteration time: 23.48s
                        Total time: 30584.96s
                               ETA: 1268152.3s

################################################################################
                    [1m Learning iteration 2355/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.222s, learning 0.206s)
               Value function loss: 31.3016
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 240.59
               Mean episode length: 117.65
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38600704
                    Iteration time: 23.43s
                        Total time: 30608.38s
                               ETA: 1268572.0s

################################################################################
                    [1m Learning iteration 2356/100000 [0m                    

                       Computation: 686 steps/s (collection: 23.617s, learning 0.236s)
               Value function loss: 23.5066
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 233.10
               Mean episode length: 114.69
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38617088
                    Iteration time: 23.85s
                        Total time: 30632.24s
                               ETA: 1269009.0s

################################################################################
                    [1m Learning iteration 2357/100000 [0m                    

                       Computation: 695 steps/s (collection: 23.390s, learning 0.182s)
               Value function loss: 23.8821
                    Surrogate loss: -0.0211
             Mean action noise std: 0.77
                       Mean reward: 234.56
               Mean episode length: 116.25
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38633472
                    Iteration time: 23.57s
                        Total time: 30655.81s
                               ETA: 1269434.0s

################################################################################
                    [1m Learning iteration 2358/100000 [0m                    

                       Computation: 723 steps/s (collection: 22.402s, learning 0.235s)
               Value function loss: 22.9550
                    Surrogate loss: -0.0216
             Mean action noise std: 0.77
                       Mean reward: 241.41
               Mean episode length: 119.38
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38649856
                    Iteration time: 22.64s
                        Total time: 30678.45s
                               ETA: 1269819.8s

################################################################################
                    [1m Learning iteration 2359/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.065s, learning 0.179s)
               Value function loss: 23.7603
                    Surrogate loss: -0.0228
             Mean action noise std: 0.77
                       Mean reward: 244.76
               Mean episode length: 119.86
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38666240
                    Iteration time: 23.24s
                        Total time: 30701.69s
                               ETA: 1270230.5s

################################################################################
                    [1m Learning iteration 2360/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.353s, learning 0.233s)
               Value function loss: 22.5818
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 236.60
               Mean episode length: 117.12
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38682624
                    Iteration time: 23.59s
                        Total time: 30725.28s
                               ETA: 1270654.9s

################################################################################
                    [1m Learning iteration 2361/100000 [0m                    

                       Computation: 706 steps/s (collection: 22.983s, learning 0.208s)
               Value function loss: 23.4933
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 246.97
               Mean episode length: 120.85
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38699008
                    Iteration time: 23.19s
                        Total time: 30748.47s
                               ETA: 1271062.6s

################################################################################
                    [1m Learning iteration 2362/100000 [0m                    

                       Computation: 681 steps/s (collection: 23.771s, learning 0.264s)
               Value function loss: 26.2001
                    Surrogate loss: -0.0219
             Mean action noise std: 0.77
                       Mean reward: 230.02
               Mean episode length: 113.72
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38715392
                    Iteration time: 24.04s
                        Total time: 30772.50s
                               ETA: 1271504.8s

################################################################################
                    [1m Learning iteration 2363/100000 [0m                    

                       Computation: 693 steps/s (collection: 23.352s, learning 0.281s)
               Value function loss: 23.6185
                    Surrogate loss: -0.0195
             Mean action noise std: 0.77
                       Mean reward: 245.65
               Mean episode length: 120.80
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38731776
                    Iteration time: 23.63s
                        Total time: 30796.14s
                               ETA: 1271930.0s

################################################################################
                    [1m Learning iteration 2364/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.271s, learning 0.257s)
               Value function loss: 22.4122
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 239.55
               Mean episode length: 117.29
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38748160
                    Iteration time: 23.53s
                        Total time: 30819.67s
                               ETA: 1272350.5s

################################################################################
                    [1m Learning iteration 2365/100000 [0m                    

                       Computation: 685 steps/s (collection: 23.746s, learning 0.163s)
               Value function loss: 28.2264
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 241.48
               Mean episode length: 118.17
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38764544
                    Iteration time: 23.91s
                        Total time: 30843.58s
                               ETA: 1272786.4s

################################################################################
                    [1m Learning iteration 2366/100000 [0m                    

                       Computation: 684 steps/s (collection: 23.668s, learning 0.258s)
               Value function loss: 29.1927
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 231.71
               Mean episode length: 114.49
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38780928
                    Iteration time: 23.93s
                        Total time: 30867.50s
                               ETA: 1273222.5s

################################################################################
                    [1m Learning iteration 2367/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.159s, learning 0.231s)
               Value function loss: 27.3248
                    Surrogate loss: -0.0202
             Mean action noise std: 0.77
                       Mean reward: 243.61
               Mean episode length: 119.30
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38797312
                    Iteration time: 23.39s
                        Total time: 30890.89s
                               ETA: 1273636.2s

################################################################################
                    [1m Learning iteration 2368/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.318s, learning 0.174s)
               Value function loss: 27.5978
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 244.02
               Mean episode length: 120.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38813696
                    Iteration time: 23.49s
                        Total time: 30914.38s
                               ETA: 1274053.7s

################################################################################
                    [1m Learning iteration 2369/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.246s, learning 0.209s)
               Value function loss: 24.6127
                    Surrogate loss: -0.0222
             Mean action noise std: 0.77
                       Mean reward: 237.75
               Mean episode length: 117.24
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38830080
                    Iteration time: 23.45s
                        Total time: 30937.84s
                               ETA: 1274469.3s

################################################################################
                    [1m Learning iteration 2370/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.272s, learning 0.196s)
               Value function loss: 27.7279
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 236.44
               Mean episode length: 116.61
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38846464
                    Iteration time: 23.47s
                        Total time: 30961.31s
                               ETA: 1274885.0s

################################################################################
                    [1m Learning iteration 2371/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.122s, learning 0.174s)
               Value function loss: 27.5275
                    Surrogate loss: -0.0192
             Mean action noise std: 0.77
                       Mean reward: 241.61
               Mean episode length: 118.93
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38862848
                    Iteration time: 23.30s
                        Total time: 30984.60s
                               ETA: 1275293.3s

################################################################################
                    [1m Learning iteration 2372/100000 [0m                    

                       Computation: 688 steps/s (collection: 23.542s, learning 0.253s)
               Value function loss: 22.8261
                    Surrogate loss: -0.0228
             Mean action noise std: 0.77
                       Mean reward: 245.11
               Mean episode length: 120.30
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38879232
                    Iteration time: 23.80s
                        Total time: 31008.40s
                               ETA: 1275721.8s

################################################################################
                    [1m Learning iteration 2373/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.194s, learning 0.210s)
               Value function loss: 26.6797
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 243.60
               Mean episode length: 118.40
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38895616
                    Iteration time: 23.40s
                        Total time: 31031.80s
                               ETA: 1276133.8s

################################################################################
                    [1m Learning iteration 2374/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.223s, learning 0.159s)
               Value function loss: 22.1953
                    Surrogate loss: -0.0206
             Mean action noise std: 0.77
                       Mean reward: 239.60
               Mean episode length: 117.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38912000
                    Iteration time: 23.38s
                        Total time: 31055.18s
                               ETA: 1276544.6s

################################################################################
                    [1m Learning iteration 2375/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.347s, learning 0.185s)
               Value function loss: 23.8860
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 238.90
               Mean episode length: 117.54
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38928384
                    Iteration time: 23.53s
                        Total time: 31078.72s
                               ETA: 1276961.1s

################################################################################
                    [1m Learning iteration 2376/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.064s, learning 0.204s)
               Value function loss: 22.7183
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 240.60
               Mean episode length: 117.71
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38944768
                    Iteration time: 23.27s
                        Total time: 31101.98s
                               ETA: 1277366.4s

################################################################################
                    [1m Learning iteration 2377/100000 [0m                    

                       Computation: 849 steps/s (collection: 19.100s, learning 0.186s)
               Value function loss: 26.3751
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 244.78
               Mean episode length: 119.55
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38961152
                    Iteration time: 19.29s
                        Total time: 31121.27s
                               ETA: 1277607.9s

################################################################################
                    [1m Learning iteration 2378/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.541s, learning 0.287s)
               Value function loss: 28.5964
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 243.63
               Mean episode length: 119.34
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38977536
                    Iteration time: 11.83s
                        Total time: 31133.10s
                               ETA: 1277543.2s

################################################################################
                    [1m Learning iteration 2379/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.836s, learning 0.216s)
               Value function loss: 22.9021
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 231.28
               Mean episode length: 113.78
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 38993920
                    Iteration time: 12.05s
                        Total time: 31145.15s
                               ETA: 1277487.7s

################################################################################
                    [1m Learning iteration 2380/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.476s, learning 0.163s)
               Value function loss: 26.3506
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 241.68
               Mean episode length: 117.76
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39010304
                    Iteration time: 11.64s
                        Total time: 31156.79s
                               ETA: 1277415.3s

################################################################################
                    [1m Learning iteration 2381/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.361s, learning 0.164s)
               Value function loss: 29.3638
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 244.86
               Mean episode length: 118.72
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39026688
                    Iteration time: 11.53s
                        Total time: 31168.31s
                               ETA: 1277338.2s

################################################################################
                    [1m Learning iteration 2382/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.523s, learning 0.174s)
               Value function loss: 25.0804
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 249.02
               Mean episode length: 122.02
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39043072
                    Iteration time: 11.70s
                        Total time: 31180.01s
                               ETA: 1277268.3s

################################################################################
                    [1m Learning iteration 2383/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.539s, learning 0.168s)
               Value function loss: 25.6798
                    Surrogate loss: -0.0194
             Mean action noise std: 0.77
                       Mean reward: 245.39
               Mean episode length: 121.08
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39059456
                    Iteration time: 11.71s
                        Total time: 31191.72s
                               ETA: 1277198.8s

################################################################################
                    [1m Learning iteration 2384/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.954s, learning 0.167s)
               Value function loss: 26.2892
                    Surrogate loss: -0.0178
             Mean action noise std: 0.77
                       Mean reward: 242.39
               Mean episode length: 119.56
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39075840
                    Iteration time: 12.12s
                        Total time: 31203.84s
                               ETA: 1277146.3s

################################################################################
                    [1m Learning iteration 2385/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.870s, learning 0.191s)
               Value function loss: 24.2604
                    Surrogate loss: -0.0222
             Mean action noise std: 0.77
                       Mean reward: 245.53
               Mean episode length: 120.28
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39092224
                    Iteration time: 12.06s
                        Total time: 31215.90s
                               ETA: 1277091.4s

################################################################################
                    [1m Learning iteration 2386/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.614s, learning 0.163s)
               Value function loss: 27.2324
                    Surrogate loss: -0.0182
             Mean action noise std: 0.77
                       Mean reward: 245.29
               Mean episode length: 120.22
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39108608
                    Iteration time: 11.78s
                        Total time: 31227.68s
                               ETA: 1277024.9s

################################################################################
                    [1m Learning iteration 2387/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.457s, learning 0.164s)
               Value function loss: 24.1530
                    Surrogate loss: -0.0210
             Mean action noise std: 0.77
                       Mean reward: 240.62
               Mean episode length: 117.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39124992
                    Iteration time: 11.62s
                        Total time: 31239.30s
                               ETA: 1276952.1s

################################################################################
                    [1m Learning iteration 2388/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.703s, learning 0.180s)
               Value function loss: 23.1967
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 241.54
               Mean episode length: 118.43
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39141376
                    Iteration time: 11.88s
                        Total time: 31251.18s
                               ETA: 1276890.1s

################################################################################
                    [1m Learning iteration 2389/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.452s, learning 0.169s)
               Value function loss: 22.7241
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 247.79
               Mean episode length: 121.04
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39157760
                    Iteration time: 11.62s
                        Total time: 31262.80s
                               ETA: 1276817.4s

################################################################################
                    [1m Learning iteration 2390/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.791s, learning 0.238s)
               Value function loss: 24.3222
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 242.83
               Mean episode length: 118.37
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39174144
                    Iteration time: 12.03s
                        Total time: 31274.83s
                               ETA: 1276761.3s

################################################################################
                    [1m Learning iteration 2391/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.708s, learning 0.213s)
               Value function loss: 22.4875
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 244.97
               Mean episode length: 119.52
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39190528
                    Iteration time: 11.92s
                        Total time: 31286.75s
                               ETA: 1276701.0s

################################################################################
                    [1m Learning iteration 2392/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.663s, learning 0.168s)
               Value function loss: 21.9838
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 236.57
               Mean episode length: 117.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39206912
                    Iteration time: 11.83s
                        Total time: 31298.58s
                               ETA: 1276636.9s

################################################################################
                    [1m Learning iteration 2393/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.498s, learning 0.162s)
               Value function loss: 28.7169
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 242.66
               Mean episode length: 119.26
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39223296
                    Iteration time: 11.66s
                        Total time: 31310.24s
                               ETA: 1276566.0s

################################################################################
                    [1m Learning iteration 2394/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.885s, learning 0.258s)
               Value function loss: 26.4015
                    Surrogate loss: -0.0180
             Mean action noise std: 0.77
                       Mean reward: 242.69
               Mean episode length: 119.15
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39239680
                    Iteration time: 12.14s
                        Total time: 31322.39s
                               ETA: 1276514.8s

################################################################################
                    [1m Learning iteration 2395/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.715s, learning 0.175s)
               Value function loss: 24.0285
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 240.75
               Mean episode length: 118.68
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39256064
                    Iteration time: 11.89s
                        Total time: 31334.28s
                               ETA: 1276453.3s

################################################################################
                    [1m Learning iteration 2396/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.892s, learning 0.162s)
               Value function loss: 26.6369
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 244.33
               Mean episode length: 118.36
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39272448
                    Iteration time: 12.05s
                        Total time: 31346.33s
                               ETA: 1276398.5s

################################################################################
                    [1m Learning iteration 2397/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.824s, learning 0.202s)
               Value function loss: 29.2479
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 254.86
               Mean episode length: 122.70
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39288832
                    Iteration time: 12.03s
                        Total time: 31358.36s
                               ETA: 1276342.7s

################################################################################
                    [1m Learning iteration 2398/100000 [0m                    

                       Computation: 1282 steps/s (collection: 12.499s, learning 0.278s)
               Value function loss: 24.2147
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 240.48
               Mean episode length: 117.14
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39305216
                    Iteration time: 12.78s
                        Total time: 31371.13s
                               ETA: 1276317.4s

################################################################################
                    [1m Learning iteration 2399/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.881s, learning 0.211s)
               Value function loss: 27.2693
                    Surrogate loss: -0.0172
             Mean action noise std: 0.77
                       Mean reward: 254.47
               Mean episode length: 123.83
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39321600
                    Iteration time: 12.09s
                        Total time: 31383.23s
                               ETA: 1276264.3s

################################################################################
                    [1m Learning iteration 2400/100000 [0m                    

                       Computation: 1287 steps/s (collection: 12.442s, learning 0.283s)
               Value function loss: 27.0907
                    Surrogate loss: -0.0030
             Mean action noise std: 0.77
                       Mean reward: 246.86
               Mean episode length: 120.19
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39337984
                    Iteration time: 12.72s
                        Total time: 31395.95s
                               ETA: 1276236.9s

################################################################################
                    [1m Learning iteration 2401/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.141s, learning 0.186s)
               Value function loss: 29.2981
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 249.02
               Mean episode length: 119.25
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39354368
                    Iteration time: 12.33s
                        Total time: 31408.28s
                               ETA: 1276193.4s

################################################################################
                    [1m Learning iteration 2402/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.807s, learning 0.162s)
               Value function loss: 29.8009
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 247.95
               Mean episode length: 119.77
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39370752
                    Iteration time: 11.97s
                        Total time: 31420.25s
                               ETA: 1276135.4s

################################################################################
                    [1m Learning iteration 2403/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.865s, learning 0.169s)
               Value function loss: 22.9639
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 244.72
               Mean episode length: 119.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39387136
                    Iteration time: 12.03s
                        Total time: 31432.28s
                               ETA: 1276080.0s

################################################################################
                    [1m Learning iteration 2404/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.691s, learning 0.172s)
               Value function loss: 26.8127
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 251.60
               Mean episode length: 121.33
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39403520
                    Iteration time: 11.86s
                        Total time: 31444.14s
                               ETA: 1276017.8s

################################################################################
                    [1m Learning iteration 2405/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.727s, learning 0.211s)
               Value function loss: 24.2315
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 248.44
               Mean episode length: 119.99
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39419904
                    Iteration time: 11.94s
                        Total time: 31456.08s
                               ETA: 1275958.6s

################################################################################
                    [1m Learning iteration 2406/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.669s, learning 0.267s)
               Value function loss: 27.7848
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 244.89
               Mean episode length: 118.12
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39436288
                    Iteration time: 11.94s
                        Total time: 31468.02s
                               ETA: 1275899.4s

################################################################################
                    [1m Learning iteration 2407/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.789s, learning 0.205s)
               Value function loss: 22.3208
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 249.77
               Mean episode length: 120.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39452672
                    Iteration time: 11.99s
                        Total time: 31480.01s
                               ETA: 1275842.6s

################################################################################
                    [1m Learning iteration 2408/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.320s, learning 0.178s)
               Value function loss: 24.4216
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 254.59
               Mean episode length: 122.20
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39469056
                    Iteration time: 12.50s
                        Total time: 31492.51s
                               ETA: 1275806.2s

################################################################################
                    [1m Learning iteration 2409/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.801s, learning 0.170s)
               Value function loss: 31.8838
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 250.78
               Mean episode length: 120.75
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39485440
                    Iteration time: 11.97s
                        Total time: 31504.48s
                               ETA: 1275748.5s

################################################################################
                    [1m Learning iteration 2410/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.397s, learning 0.230s)
               Value function loss: 23.0774
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 248.72
               Mean episode length: 120.46
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39501824
                    Iteration time: 11.63s
                        Total time: 31516.11s
                               ETA: 1275677.0s

################################################################################
                    [1m Learning iteration 2411/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.064s, learning 0.158s)
               Value function loss: 25.5872
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 252.53
               Mean episode length: 121.35
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39518208
                    Iteration time: 12.22s
                        Total time: 31528.33s
                               ETA: 1275629.5s

################################################################################
                    [1m Learning iteration 2412/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.775s, learning 0.209s)
               Value function loss: 29.5072
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 247.78
               Mean episode length: 120.40
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39534592
                    Iteration time: 11.98s
                        Total time: 31540.32s
                               ETA: 1275572.5s

################################################################################
                    [1m Learning iteration 2413/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.958s, learning 0.193s)
               Value function loss: 26.8366
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 248.55
               Mean episode length: 119.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39550976
                    Iteration time: 12.15s
                        Total time: 31552.47s
                               ETA: 1275522.2s

################################################################################
                    [1m Learning iteration 2414/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.053s, learning 0.366s)
               Value function loss: 24.3953
                    Surrogate loss: -0.0168
             Mean action noise std: 0.77
                       Mean reward: 253.95
               Mean episode length: 122.86
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39567360
                    Iteration time: 12.42s
                        Total time: 31564.89s
                               ETA: 1275482.8s

################################################################################
                    [1m Learning iteration 2415/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.017s, learning 0.161s)
               Value function loss: 28.4607
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 253.10
               Mean episode length: 122.07
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39583744
                    Iteration time: 12.18s
                        Total time: 31577.06s
                               ETA: 1275433.7s

################################################################################
                    [1m Learning iteration 2416/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.658s, learning 0.163s)
               Value function loss: 24.5637
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 248.45
               Mean episode length: 120.89
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39600128
                    Iteration time: 11.82s
                        Total time: 31588.89s
                               ETA: 1275370.2s

################################################################################
                    [1m Learning iteration 2417/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.598s, learning 0.231s)
               Value function loss: 28.0976
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 254.18
               Mean episode length: 122.11
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39616512
                    Iteration time: 11.83s
                        Total time: 31600.71s
                               ETA: 1275307.1s

################################################################################
                    [1m Learning iteration 2418/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.565s, learning 0.158s)
               Value function loss: 28.6797
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 238.70
               Mean episode length: 115.94
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39632896
                    Iteration time: 11.72s
                        Total time: 31612.44s
                               ETA: 1275239.8s

################################################################################
                    [1m Learning iteration 2419/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.661s, learning 0.166s)
               Value function loss: 26.5164
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 245.56
               Mean episode length: 119.13
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39649280
                    Iteration time: 11.83s
                        Total time: 31624.27s
                               ETA: 1275176.6s

################################################################################
                    [1m Learning iteration 2420/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.834s, learning 0.182s)
               Value function loss: 25.2228
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 245.85
               Mean episode length: 118.54
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39665664
                    Iteration time: 12.02s
                        Total time: 31636.28s
                               ETA: 1275121.2s

################################################################################
                    [1m Learning iteration 2421/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.664s, learning 0.189s)
               Value function loss: 25.6280
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 251.69
               Mean episode length: 120.69
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39682048
                    Iteration time: 11.85s
                        Total time: 31648.13s
                               ETA: 1275059.2s

################################################################################
                    [1m Learning iteration 2422/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.160s)
               Value function loss: 26.9023
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 252.14
               Mean episode length: 120.95
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39698432
                    Iteration time: 12.07s
                        Total time: 31660.20s
                               ETA: 1275005.8s

################################################################################
                    [1m Learning iteration 2423/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.713s, learning 0.185s)
               Value function loss: 24.3169
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 248.73
               Mean episode length: 119.89
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39714816
                    Iteration time: 11.90s
                        Total time: 31672.10s
                               ETA: 1274945.7s

################################################################################
                    [1m Learning iteration 2424/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.867s, learning 0.171s)
               Value function loss: 29.8252
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 253.42
               Mean episode length: 121.25
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39731200
                    Iteration time: 12.04s
                        Total time: 31684.14s
                               ETA: 1274891.2s

################################################################################
                    [1m Learning iteration 2425/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.729s, learning 0.167s)
               Value function loss: 28.6901
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 257.10
               Mean episode length: 123.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39747584
                    Iteration time: 11.90s
                        Total time: 31696.03s
                               ETA: 1274831.1s

################################################################################
                    [1m Learning iteration 2426/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.700s, learning 0.162s)
               Value function loss: 28.3557
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 250.51
               Mean episode length: 120.78
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39763968
                    Iteration time: 11.86s
                        Total time: 31707.89s
                               ETA: 1274769.7s

################################################################################
                    [1m Learning iteration 2427/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.005s, learning 0.202s)
               Value function loss: 28.2030
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 250.80
               Mean episode length: 120.59
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39780352
                    Iteration time: 12.21s
                        Total time: 31720.10s
                               ETA: 1274722.1s

################################################################################
                    [1m Learning iteration 2428/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.734s, learning 0.159s)
               Value function loss: 29.4385
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 255.97
               Mean episode length: 123.23
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39796736
                    Iteration time: 11.89s
                        Total time: 31731.99s
                               ETA: 1274662.0s

################################################################################
                    [1m Learning iteration 2429/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.807s, learning 0.165s)
               Value function loss: 25.1791
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 246.43
               Mean episode length: 119.74
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39813120
                    Iteration time: 11.97s
                        Total time: 31743.96s
                               ETA: 1274605.1s

################################################################################
                    [1m Learning iteration 2430/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.562s, learning 0.160s)
               Value function loss: 26.2390
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 249.83
               Mean episode length: 120.17
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39829504
                    Iteration time: 11.72s
                        Total time: 31755.69s
                               ETA: 1274538.2s

################################################################################
                    [1m Learning iteration 2431/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.722s, learning 0.154s)
               Value function loss: 30.5531
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 250.02
               Mean episode length: 120.91
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39845888
                    Iteration time: 11.88s
                        Total time: 31767.56s
                               ETA: 1274477.5s

################################################################################
                    [1m Learning iteration 2432/100000 [0m                    

                       Computation: 1290 steps/s (collection: 12.437s, learning 0.257s)
               Value function loss: 28.0335
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 253.43
               Mean episode length: 121.90
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39862272
                    Iteration time: 12.69s
                        Total time: 31780.26s
                               ETA: 1274449.7s

################################################################################
                    [1m Learning iteration 2433/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.667s, learning 0.191s)
               Value function loss: 31.4643
                    Surrogate loss: 0.0015
             Mean action noise std: 0.77
                       Mean reward: 249.54
               Mean episode length: 119.81
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39878656
                    Iteration time: 11.86s
                        Total time: 31792.11s
                               ETA: 1274388.3s

################################################################################
                    [1m Learning iteration 2434/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.595s, learning 0.187s)
               Value function loss: 26.7484
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 253.94
               Mean episode length: 121.46
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39895040
                    Iteration time: 11.78s
                        Total time: 31803.90s
                               ETA: 1274324.0s

################################################################################
                    [1m Learning iteration 2435/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.083s, learning 0.218s)
               Value function loss: 24.9270
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 244.84
               Mean episode length: 119.04
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39911424
                    Iteration time: 12.30s
                        Total time: 31816.20s
                               ETA: 1274280.5s

################################################################################
                    [1m Learning iteration 2436/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.128s, learning 0.179s)
               Value function loss: 23.2435
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 253.74
               Mean episode length: 123.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39927808
                    Iteration time: 12.31s
                        Total time: 31828.50s
                               ETA: 1274237.2s

################################################################################
                    [1m Learning iteration 2437/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.957s, learning 0.171s)
               Value function loss: 26.0450
                    Surrogate loss: -0.0028
             Mean action noise std: 0.77
                       Mean reward: 251.76
               Mean episode length: 121.76
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39944192
                    Iteration time: 12.13s
                        Total time: 31840.63s
                               ETA: 1274186.9s

################################################################################
                    [1m Learning iteration 2438/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.929s, learning 0.163s)
               Value function loss: 22.0211
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 244.45
               Mean episode length: 118.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39960576
                    Iteration time: 12.09s
                        Total time: 31852.72s
                               ETA: 1274135.1s

################################################################################
                    [1m Learning iteration 2439/100000 [0m                    

                       Computation: 1318 steps/s (collection: 12.125s, learning 0.298s)
               Value function loss: 21.7447
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 247.33
               Mean episode length: 119.87
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39976960
                    Iteration time: 12.42s
                        Total time: 31865.15s
                               ETA: 1274096.6s

################################################################################
                    [1m Learning iteration 2440/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.495s, learning 0.212s)
               Value function loss: 30.5983
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 242.32
               Mean episode length: 118.76
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 39993344
                    Iteration time: 11.71s
                        Total time: 31876.86s
                               ETA: 1274029.5s

################################################################################
                    [1m Learning iteration 2441/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.126s, learning 0.158s)
               Value function loss: 22.0684
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 251.39
               Mean episode length: 121.86
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40009728
                    Iteration time: 12.28s
                        Total time: 31889.14s
                               ETA: 1273985.5s

################################################################################
                    [1m Learning iteration 2442/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.886s, learning 0.162s)
               Value function loss: 22.5979
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 255.07
               Mean episode length: 123.90
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40026112
                    Iteration time: 12.05s
                        Total time: 31901.19s
                               ETA: 1273932.1s

################################################################################
                    [1m Learning iteration 2443/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.653s, learning 0.172s)
               Value function loss: 27.1490
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 245.30
               Mean episode length: 119.38
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40042496
                    Iteration time: 11.83s
                        Total time: 31913.01s
                               ETA: 1273869.8s

################################################################################
                    [1m Learning iteration 2444/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.040s, learning 0.187s)
               Value function loss: 30.7229
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 248.29
               Mean episode length: 121.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40058880
                    Iteration time: 12.23s
                        Total time: 31925.24s
                               ETA: 1273823.6s

################################################################################
                    [1m Learning iteration 2445/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.126s, learning 0.171s)
               Value function loss: 24.1400
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 241.76
               Mean episode length: 119.01
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40075264
                    Iteration time: 12.30s
                        Total time: 31937.54s
                               ETA: 1273780.2s

################################################################################
                    [1m Learning iteration 2446/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.025s, learning 0.239s)
               Value function loss: 27.9571
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 240.84
               Mean episode length: 117.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40091648
                    Iteration time: 12.26s
                        Total time: 31949.80s
                               ETA: 1273735.5s

################################################################################
                    [1m Learning iteration 2447/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.775s, learning 0.168s)
               Value function loss: 28.8978
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 240.06
               Mean episode length: 118.24
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40108032
                    Iteration time: 11.94s
                        Total time: 31961.74s
                               ETA: 1273678.1s

################################################################################
                    [1m Learning iteration 2448/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.775s, learning 0.161s)
               Value function loss: 25.6765
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 247.43
               Mean episode length: 120.85
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40124416
                    Iteration time: 11.94s
                        Total time: 31973.68s
                               ETA: 1273620.4s

################################################################################
                    [1m Learning iteration 2449/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.815s, learning 0.211s)
               Value function loss: 29.7045
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 236.60
               Mean episode length: 116.47
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40140800
                    Iteration time: 12.03s
                        Total time: 31985.71s
                               ETA: 1273566.3s

################################################################################
                    [1m Learning iteration 2450/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.148s, learning 0.214s)
               Value function loss: 25.7295
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 244.98
               Mean episode length: 120.45
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40157184
                    Iteration time: 12.36s
                        Total time: 31998.07s
                               ETA: 1273525.7s

################################################################################
                    [1m Learning iteration 2451/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.696s, learning 0.198s)
               Value function loss: 24.8414
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 251.29
               Mean episode length: 121.61
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40173568
                    Iteration time: 11.89s
                        Total time: 32009.96s
                               ETA: 1273466.4s

################################################################################
                    [1m Learning iteration 2452/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.038s, learning 0.208s)
               Value function loss: 23.7050
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 252.48
               Mean episode length: 122.99
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40189952
                    Iteration time: 12.25s
                        Total time: 32022.21s
                               ETA: 1273421.3s

################################################################################
                    [1m Learning iteration 2453/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.884s, learning 0.189s)
               Value function loss: 27.4341
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 242.02
               Mean episode length: 118.30
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40206336
                    Iteration time: 12.07s
                        Total time: 32034.28s
                               ETA: 1273369.2s

################################################################################
                    [1m Learning iteration 2454/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.810s, learning 0.179s)
               Value function loss: 22.4463
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 246.51
               Mean episode length: 119.65
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40222720
                    Iteration time: 11.99s
                        Total time: 32046.27s
                               ETA: 1273313.8s

################################################################################
                    [1m Learning iteration 2455/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.546s, learning 0.260s)
               Value function loss: 28.1855
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 245.62
               Mean episode length: 120.43
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40239104
                    Iteration time: 11.81s
                        Total time: 32058.08s
                               ETA: 1273251.2s

################################################################################
                    [1m Learning iteration 2456/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.933s, learning 0.242s)
               Value function loss: 36.0583
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 253.36
               Mean episode length: 122.96
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40255488
                    Iteration time: 12.18s
                        Total time: 32070.25s
                               ETA: 1273203.4s

################################################################################
                    [1m Learning iteration 2457/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.625s, learning 0.164s)
               Value function loss: 22.5569
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 238.83
               Mean episode length: 116.67
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40271872
                    Iteration time: 11.79s
                        Total time: 32082.04s
                               ETA: 1273140.2s

################################################################################
                    [1m Learning iteration 2458/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.859s, learning 0.161s)
               Value function loss: 26.9921
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 245.93
               Mean episode length: 121.02
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40288256
                    Iteration time: 12.02s
                        Total time: 32094.06s
                               ETA: 1273086.2s

################################################################################
                    [1m Learning iteration 2459/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.931s, learning 0.253s)
               Value function loss: 34.4466
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 251.35
               Mean episode length: 121.79
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40304640
                    Iteration time: 12.18s
                        Total time: 32106.25s
                               ETA: 1273038.7s

################################################################################
                    [1m Learning iteration 2460/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.841s, learning 0.200s)
               Value function loss: 29.2259
                    Surrogate loss: 0.0038
             Mean action noise std: 0.77
                       Mean reward: 241.63
               Mean episode length: 119.05
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40321024
                    Iteration time: 12.04s
                        Total time: 32118.29s
                               ETA: 1272985.6s

################################################################################
                    [1m Learning iteration 2461/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.685s, learning 0.163s)
               Value function loss: 26.1957
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 243.13
               Mean episode length: 118.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40337408
                    Iteration time: 11.85s
                        Total time: 32130.13s
                               ETA: 1272924.9s

################################################################################
                    [1m Learning iteration 2462/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.640s, learning 0.164s)
               Value function loss: 37.1814
                    Surrogate loss: 0.0122
             Mean action noise std: 0.77
                       Mean reward: 249.84
               Mean episode length: 121.36
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40353792
                    Iteration time: 11.80s
                        Total time: 32141.94s
                               ETA: 1272862.5s

################################################################################
                    [1m Learning iteration 2463/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.754s, learning 0.226s)
               Value function loss: 30.5204
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 250.82
               Mean episode length: 122.14
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40370176
                    Iteration time: 11.98s
                        Total time: 32153.92s
                               ETA: 1272807.1s

################################################################################
                    [1m Learning iteration 2464/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.892s, learning 0.254s)
               Value function loss: 40.6307
                    Surrogate loss: 0.0010
             Mean action noise std: 0.77
                       Mean reward: 239.25
               Mean episode length: 116.85
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40386560
                    Iteration time: 12.15s
                        Total time: 32166.06s
                               ETA: 1272758.3s

################################################################################
                    [1m Learning iteration 2465/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.806s, learning 0.160s)
               Value function loss: 37.2979
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 246.92
               Mean episode length: 120.26
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40402944
                    Iteration time: 11.97s
                        Total time: 32178.03s
                               ETA: 1272702.4s

################################################################################
                    [1m Learning iteration 2466/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.544s, learning 0.177s)
               Value function loss: 31.1063
                    Surrogate loss: -0.0038
             Mean action noise std: 0.77
                       Mean reward: 248.83
               Mean episode length: 121.26
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40419328
                    Iteration time: 11.72s
                        Total time: 32189.75s
                               ETA: 1272636.9s

################################################################################
                    [1m Learning iteration 2467/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.698s, learning 0.181s)
               Value function loss: 28.1359
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 247.75
               Mean episode length: 121.07
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40435712
                    Iteration time: 11.88s
                        Total time: 32201.63s
                               ETA: 1272577.6s

################################################################################
                    [1m Learning iteration 2468/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.634s, learning 0.182s)
               Value function loss: 29.3378
                    Surrogate loss: 0.0004
             Mean action noise std: 0.77
                       Mean reward: 251.70
               Mean episode length: 121.98
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40452096
                    Iteration time: 11.82s
                        Total time: 32213.45s
                               ETA: 1272515.9s

################################################################################
                    [1m Learning iteration 2469/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.151s, learning 0.214s)
               Value function loss: 26.7545
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 247.64
               Mean episode length: 121.20
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40468480
                    Iteration time: 12.36s
                        Total time: 32225.81s
                               ETA: 1272475.9s

################################################################################
                    [1m Learning iteration 2470/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.793s, learning 0.170s)
               Value function loss: 27.8462
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 240.38
               Mean episode length: 117.46
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40484864
                    Iteration time: 11.96s
                        Total time: 32237.77s
                               ETA: 1272420.1s

################################################################################
                    [1m Learning iteration 2471/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.777s, learning 0.225s)
               Value function loss: 34.2710
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 237.53
               Mean episode length: 116.69
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40501248
                    Iteration time: 12.00s
                        Total time: 32249.78s
                               ETA: 1272365.8s

################################################################################
                    [1m Learning iteration 2472/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.619s, learning 0.198s)
               Value function loss: 34.3271
                    Surrogate loss: -0.0004
             Mean action noise std: 0.77
                       Mean reward: 245.80
               Mean episode length: 120.34
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40517632
                    Iteration time: 11.82s
                        Total time: 32261.59s
                               ETA: 1272304.3s

################################################################################
                    [1m Learning iteration 2473/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.742s, learning 0.182s)
               Value function loss: 29.4407
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: 241.77
               Mean episode length: 117.85
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40534016
                    Iteration time: 11.92s
                        Total time: 32273.52s
                               ETA: 1272247.1s

################################################################################
                    [1m Learning iteration 2474/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.779s, learning 0.171s)
               Value function loss: 32.3684
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 249.08
               Mean episode length: 120.90
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40550400
                    Iteration time: 11.95s
                        Total time: 32285.47s
                               ETA: 1272190.9s

################################################################################
                    [1m Learning iteration 2475/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.054s, learning 0.160s)
               Value function loss: 42.9515
                    Surrogate loss: 0.0023
             Mean action noise std: 0.77
                       Mean reward: 245.99
               Mean episode length: 118.77
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40566784
                    Iteration time: 12.21s
                        Total time: 32297.68s
                               ETA: 1272145.2s

################################################################################
                    [1m Learning iteration 2476/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.834s, learning 0.210s)
               Value function loss: 28.7928
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 228.33
               Mean episode length: 112.72
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40583168
                    Iteration time: 12.04s
                        Total time: 32309.73s
                               ETA: 1272092.8s

################################################################################
                    [1m Learning iteration 2477/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.933s, learning 0.262s)
               Value function loss: 34.1805
                    Surrogate loss: 0.0005
             Mean action noise std: 0.77
                       Mean reward: 245.30
               Mean episode length: 118.44
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40599552
                    Iteration time: 12.20s
                        Total time: 32321.92s
                               ETA: 1272046.3s

################################################################################
                    [1m Learning iteration 2478/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.029s, learning 0.186s)
               Value function loss: 36.3197
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 246.96
               Mean episode length: 120.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40615936
                    Iteration time: 12.22s
                        Total time: 32334.14s
                               ETA: 1272000.7s

################################################################################
                    [1m Learning iteration 2479/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.855s, learning 0.230s)
               Value function loss: 33.4925
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 249.73
               Mean episode length: 121.33
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40632320
                    Iteration time: 12.08s
                        Total time: 32346.22s
                               ETA: 1271950.0s

################################################################################
                    [1m Learning iteration 2480/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.860s, learning 0.189s)
               Value function loss: 42.5949
                    Surrogate loss: 0.0153
             Mean action noise std: 0.77
                       Mean reward: 250.54
               Mean episode length: 121.18
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40648704
                    Iteration time: 12.05s
                        Total time: 32358.27s
                               ETA: 1271897.8s

################################################################################
                    [1m Learning iteration 2481/100000 [0m                    

                       Computation: 1299 steps/s (collection: 12.404s, learning 0.205s)
               Value function loss: 35.7059
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: 239.65
               Mean episode length: 116.82
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40665088
                    Iteration time: 12.61s
                        Total time: 32370.88s
                               ETA: 1271867.8s

################################################################################
                    [1m Learning iteration 2482/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.855s, learning 0.161s)
               Value function loss: 37.3392
                    Surrogate loss: 0.0026
             Mean action noise std: 0.77
                       Mean reward: 248.30
               Mean episode length: 119.93
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40681472
                    Iteration time: 12.02s
                        Total time: 32382.90s
                               ETA: 1271814.4s

################################################################################
                    [1m Learning iteration 2483/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.816s, learning 0.160s)
               Value function loss: 30.0853
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 252.05
               Mean episode length: 121.11
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40697856
                    Iteration time: 11.98s
                        Total time: 32394.87s
                               ETA: 1271759.6s

################################################################################
                    [1m Learning iteration 2484/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.971s, learning 0.165s)
               Value function loss: 37.9530
                    Surrogate loss: 0.0051
             Mean action noise std: 0.77
                       Mean reward: 247.33
               Mean episode length: 120.43
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40714240
                    Iteration time: 12.14s
                        Total time: 32407.01s
                               ETA: 1271711.0s

################################################################################
                    [1m Learning iteration 2485/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.843s, learning 0.342s)
               Value function loss: 31.2466
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 244.64
               Mean episode length: 119.03
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40730624
                    Iteration time: 12.19s
                        Total time: 32419.19s
                               ETA: 1271664.4s

################################################################################
                    [1m Learning iteration 2486/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.984s, learning 0.190s)
               Value function loss: 34.7882
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: 250.19
               Mean episode length: 122.12
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40747008
                    Iteration time: 12.17s
                        Total time: 32431.37s
                               ETA: 1271617.4s

################################################################################
                    [1m Learning iteration 2487/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.890s, learning 0.319s)
               Value function loss: 45.3740
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 245.65
               Mean episode length: 119.62
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40763392
                    Iteration time: 12.21s
                        Total time: 32443.58s
                               ETA: 1271571.7s

################################################################################
                    [1m Learning iteration 2488/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.883s, learning 0.157s)
               Value function loss: 38.1422
                    Surrogate loss: -0.0008
             Mean action noise std: 0.77
                       Mean reward: 255.55
               Mean episode length: 123.25
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40779776
                    Iteration time: 12.04s
                        Total time: 32455.62s
                               ETA: 1271519.5s

################################################################################
                    [1m Learning iteration 2489/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.908s, learning 0.167s)
               Value function loss: 32.0651
                    Surrogate loss: -0.0005
             Mean action noise std: 0.77
                       Mean reward: 245.55
               Mean episode length: 119.69
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40796160
                    Iteration time: 12.07s
                        Total time: 32467.69s
                               ETA: 1271468.6s

################################################################################
                    [1m Learning iteration 2490/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.743s, learning 0.198s)
               Value function loss: 35.4570
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 243.85
               Mean episode length: 119.02
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40812544
                    Iteration time: 11.94s
                        Total time: 32479.63s
                               ETA: 1271412.6s

################################################################################
                    [1m Learning iteration 2491/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.295s, learning 0.159s)
               Value function loss: 45.6838
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 250.84
               Mean episode length: 121.67
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40828928
                    Iteration time: 12.45s
                        Total time: 32492.09s
                               ETA: 1271376.7s

################################################################################
                    [1m Learning iteration 2492/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.241s, learning 0.164s)
               Value function loss: 35.8326
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 256.40
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40845312
                    Iteration time: 12.40s
                        Total time: 32504.49s
                               ETA: 1271338.8s

################################################################################
                    [1m Learning iteration 2493/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.791s, learning 0.192s)
               Value function loss: 40.0110
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 253.40
               Mean episode length: 123.17
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40861696
                    Iteration time: 11.98s
                        Total time: 32516.47s
                               ETA: 1271284.6s

################################################################################
                    [1m Learning iteration 2494/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.706s, learning 0.162s)
               Value function loss: 39.5178
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 240.12
               Mean episode length: 117.64
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40878080
                    Iteration time: 11.87s
                        Total time: 32528.34s
                               ETA: 1271225.8s

################################################################################
                    [1m Learning iteration 2495/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.804s, learning 0.159s)
               Value function loss: 49.1899
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 251.74
               Mean episode length: 122.93
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40894464
                    Iteration time: 11.96s
                        Total time: 32540.30s
                               ETA: 1271170.8s

################################################################################
                    [1m Learning iteration 2496/100000 [0m                    

                       Computation: 1432 steps/s (collection: 11.215s, learning 0.226s)
               Value function loss: 36.7949
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 251.72
               Mean episode length: 123.05
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40910848
                    Iteration time: 11.44s
                        Total time: 32551.74s
                               ETA: 1271095.4s

################################################################################
                    [1m Learning iteration 2497/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.957s, learning 0.189s)
               Value function loss: 33.6572
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 251.54
               Mean episode length: 122.83
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40927232
                    Iteration time: 12.15s
                        Total time: 32563.89s
                               ETA: 1271047.6s

################################################################################
                    [1m Learning iteration 2498/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.603s, learning 0.166s)
               Value function loss: 32.4978
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 255.31
               Mean episode length: 124.01
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40943616
                    Iteration time: 11.77s
                        Total time: 32575.66s
                               ETA: 1270985.1s

################################################################################
                    [1m Learning iteration 2499/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.129s, learning 0.247s)
               Value function loss: 28.8946
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 246.47
               Mean episode length: 121.09
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40960000
                    Iteration time: 12.38s
                        Total time: 32588.03s
                               ETA: 1270946.4s

################################################################################
                    [1m Learning iteration 2500/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.639s, learning 0.164s)
               Value function loss: 29.3822
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 242.60
               Mean episode length: 119.27
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40976384
                    Iteration time: 11.80s
                        Total time: 32599.84s
                               ETA: 1270885.3s

################################################################################
                    [1m Learning iteration 2501/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.864s, learning 0.157s)
               Value function loss: 32.7463
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 248.08
               Mean episode length: 121.43
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 40992768
                    Iteration time: 12.02s
                        Total time: 32611.86s
                               ETA: 1270832.8s

################################################################################
                    [1m Learning iteration 2502/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.835s, learning 0.158s)
               Value function loss: 33.8516
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 248.59
               Mean episode length: 121.66
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41009152
                    Iteration time: 11.99s
                        Total time: 32623.85s
                               ETA: 1270779.1s

################################################################################
                    [1m Learning iteration 2503/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.054s, learning 0.203s)
               Value function loss: 37.2673
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 248.16
               Mean episode length: 121.77
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41025536
                    Iteration time: 12.26s
                        Total time: 32636.11s
                               ETA: 1270735.9s

################################################################################
                    [1m Learning iteration 2504/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.816s, learning 0.179s)
               Value function loss: 37.4708
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 249.31
               Mean episode length: 121.86
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41041920
                    Iteration time: 11.99s
                        Total time: 32648.10s
                               ETA: 1270682.4s

################################################################################
                    [1m Learning iteration 2505/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.813s, learning 0.174s)
               Value function loss: 44.8679
                    Surrogate loss: 0.0201
             Mean action noise std: 0.77
                       Mean reward: 249.88
               Mean episode length: 122.55
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41058304
                    Iteration time: 11.99s
                        Total time: 32660.09s
                               ETA: 1270628.6s

################################################################################
                    [1m Learning iteration 2506/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.595s, learning 0.230s)
               Value function loss: 35.3146
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 245.09
               Mean episode length: 120.49
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41074688
                    Iteration time: 11.82s
                        Total time: 32671.91s
                               ETA: 1270568.6s

################################################################################
                    [1m Learning iteration 2507/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.706s, learning 0.163s)
               Value function loss: 30.4027
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 251.65
               Mean episode length: 122.55
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41091072
                    Iteration time: 11.87s
                        Total time: 32683.78s
                               ETA: 1270510.3s

################################################################################
                    [1m Learning iteration 2508/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.608s, learning 0.163s)
               Value function loss: 28.7489
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 242.30
               Mean episode length: 117.66
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41107456
                    Iteration time: 11.77s
                        Total time: 32695.55s
                               ETA: 1270448.3s

################################################################################
                    [1m Learning iteration 2509/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.064s, learning 0.212s)
               Value function loss: 34.6005
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 250.91
               Mean episode length: 122.38
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41123840
                    Iteration time: 12.28s
                        Total time: 32707.83s
                               ETA: 1270405.9s

################################################################################
                    [1m Learning iteration 2510/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.979s, learning 0.178s)
               Value function loss: 34.3697
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 248.79
               Mean episode length: 121.19
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41140224
                    Iteration time: 12.16s
                        Total time: 32719.99s
                               ETA: 1270359.0s

################################################################################
                    [1m Learning iteration 2511/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.744s, learning 0.170s)
               Value function loss: 45.5480
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 251.84
               Mean episode length: 122.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41156608
                    Iteration time: 11.91s
                        Total time: 32731.90s
                               ETA: 1270302.6s

################################################################################
                    [1m Learning iteration 2512/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.119s, learning 0.317s)
               Value function loss: 46.9651
                    Surrogate loss: -0.0007
             Mean action noise std: 0.77
                       Mean reward: 256.46
               Mean episode length: 124.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41172992
                    Iteration time: 12.44s
                        Total time: 32744.34s
                               ETA: 1270266.5s

################################################################################
                    [1m Learning iteration 2513/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.862s, learning 0.265s)
               Value function loss: 34.8187
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: 254.94
               Mean episode length: 123.90
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41189376
                    Iteration time: 12.13s
                        Total time: 32756.46s
                               ETA: 1270218.5s

################################################################################
                    [1m Learning iteration 2514/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.023s, learning 0.302s)
               Value function loss: 31.6676
                    Surrogate loss: 0.0049
             Mean action noise std: 0.77
                       Mean reward: 240.95
               Mean episode length: 117.88
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41205760
                    Iteration time: 12.32s
                        Total time: 32768.79s
                               ETA: 1270178.1s

################################################################################
                    [1m Learning iteration 2515/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.804s, learning 0.230s)
               Value function loss: 33.4577
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 252.53
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41222144
                    Iteration time: 12.03s
                        Total time: 32780.82s
                               ETA: 1270126.5s

################################################################################
                    [1m Learning iteration 2516/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.924s, learning 0.181s)
               Value function loss: 34.4108
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 245.87
               Mean episode length: 120.73
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41238528
                    Iteration time: 12.11s
                        Total time: 32792.93s
                               ETA: 1270077.7s

################################################################################
                    [1m Learning iteration 2517/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.809s, learning 0.192s)
               Value function loss: 33.7858
                    Surrogate loss: -0.0001
             Mean action noise std: 0.77
                       Mean reward: 253.28
               Mean episode length: 123.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41254912
                    Iteration time: 12.00s
                        Total time: 32804.93s
                               ETA: 1270024.9s

################################################################################
                    [1m Learning iteration 2518/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.642s, learning 0.257s)
               Value function loss: 41.1116
                    Surrogate loss: -0.0010
             Mean action noise std: 0.77
                       Mean reward: 249.49
               Mean episode length: 122.27
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41271296
                    Iteration time: 11.90s
                        Total time: 32816.83s
                               ETA: 1269968.1s

################################################################################
                    [1m Learning iteration 2519/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.705s, learning 0.179s)
               Value function loss: 29.6399
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 246.63
               Mean episode length: 120.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41287680
                    Iteration time: 11.88s
                        Total time: 32828.71s
                               ETA: 1269910.9s

################################################################################
                    [1m Learning iteration 2520/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.895s, learning 0.279s)
               Value function loss: 29.1930
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 245.83
               Mean episode length: 120.60
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41304064
                    Iteration time: 12.17s
                        Total time: 32840.88s
                               ETA: 1269864.9s

################################################################################
                    [1m Learning iteration 2521/100000 [0m                    

                       Computation: 1435 steps/s (collection: 11.245s, learning 0.165s)
               Value function loss: 31.0221
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 248.66
               Mean episode length: 121.52
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41320448
                    Iteration time: 11.41s
                        Total time: 32852.29s
                               ETA: 1269789.4s

################################################################################
                    [1m Learning iteration 2522/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.044s, learning 0.193s)
               Value function loss: 35.7942
                    Surrogate loss: 0.0154
             Mean action noise std: 0.77
                       Mean reward: 252.74
               Mean episode length: 123.26
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41336832
                    Iteration time: 12.24s
                        Total time: 32864.53s
                               ETA: 1269745.9s

################################################################################
                    [1m Learning iteration 2523/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.947s, learning 0.178s)
               Value function loss: 24.4428
                    Surrogate loss: 0.0009
             Mean action noise std: 0.77
                       Mean reward: 252.63
               Mean episode length: 124.15
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41353216
                    Iteration time: 12.13s
                        Total time: 32876.66s
                               ETA: 1269698.1s

################################################################################
                    [1m Learning iteration 2524/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.591s, learning 0.191s)
               Value function loss: 28.0201
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 249.48
               Mean episode length: 122.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41369600
                    Iteration time: 11.78s
                        Total time: 32888.44s
                               ETA: 1269637.1s

################################################################################
                    [1m Learning iteration 2525/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.994s, learning 0.185s)
               Value function loss: 29.4402
                    Surrogate loss: -0.0005
             Mean action noise std: 0.77
                       Mean reward: 250.85
               Mean episode length: 122.98
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41385984
                    Iteration time: 12.18s
                        Total time: 32900.62s
                               ETA: 1269591.4s

################################################################################
                    [1m Learning iteration 2526/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.695s, learning 0.170s)
               Value function loss: 30.0375
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 253.58
               Mean episode length: 122.37
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41402368
                    Iteration time: 11.86s
                        Total time: 32912.48s
                               ETA: 1269533.6s

################################################################################
                    [1m Learning iteration 2527/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.910s, learning 0.239s)
               Value function loss: 31.6022
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 244.22
               Mean episode length: 118.61
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41418752
                    Iteration time: 12.15s
                        Total time: 32924.63s
                               ETA: 1269486.8s

################################################################################
                    [1m Learning iteration 2528/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.731s, learning 0.167s)
               Value function loss: 26.4481
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 255.43
               Mean episode length: 124.03
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41435136
                    Iteration time: 11.90s
                        Total time: 32936.53s
                               ETA: 1269430.4s

################################################################################
                    [1m Learning iteration 2529/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.585s, learning 0.236s)
               Value function loss: 29.1891
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 246.54
               Mean episode length: 120.48
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41451520
                    Iteration time: 11.82s
                        Total time: 32948.35s
                               ETA: 1269371.1s

################################################################################
                    [1m Learning iteration 2530/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.618s, learning 0.164s)
               Value function loss: 31.0062
                    Surrogate loss: 0.0018
             Mean action noise std: 0.77
                       Mean reward: 243.10
               Mean episode length: 119.50
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41467904
                    Iteration time: 11.78s
                        Total time: 32960.13s
                               ETA: 1269310.2s

################################################################################
                    [1m Learning iteration 2531/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.972s, learning 0.176s)
               Value function loss: 29.4557
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 253.67
               Mean episode length: 122.56
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41484288
                    Iteration time: 12.15s
                        Total time: 32972.28s
                               ETA: 1269263.5s

################################################################################
                    [1m Learning iteration 2532/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.699s, learning 0.189s)
               Value function loss: 26.8072
                    Surrogate loss: -0.0031
             Mean action noise std: 0.77
                       Mean reward: 251.99
               Mean episode length: 123.18
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41500672
                    Iteration time: 11.89s
                        Total time: 32984.17s
                               ETA: 1269206.9s

################################################################################
                    [1m Learning iteration 2533/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.336s, learning 0.182s)
               Value function loss: 24.9580
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: 243.54
               Mean episode length: 119.44
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41517056
                    Iteration time: 11.52s
                        Total time: 32995.69s
                               ETA: 1269136.0s

################################################################################
                    [1m Learning iteration 2534/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.573s, learning 0.202s)
               Value function loss: 33.2017
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 251.34
               Mean episode length: 121.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41533440
                    Iteration time: 11.77s
                        Total time: 33007.46s
                               ETA: 1269075.1s

################################################################################
                    [1m Learning iteration 2535/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.903s, learning 0.181s)
               Value function loss: 23.5616
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 247.69
               Mean episode length: 120.75
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41549824
                    Iteration time: 12.08s
                        Total time: 33019.55s
                               ETA: 1269026.1s

################################################################################
                    [1m Learning iteration 2536/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.528s, learning 0.211s)
               Value function loss: 23.9167
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 244.70
               Mean episode length: 119.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41566208
                    Iteration time: 11.74s
                        Total time: 33031.29s
                               ETA: 1268963.8s

################################################################################
                    [1m Learning iteration 2537/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.712s, learning 0.175s)
               Value function loss: 27.6377
                    Surrogate loss: -0.0030
             Mean action noise std: 0.77
                       Mean reward: 252.08
               Mean episode length: 122.34
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41582592
                    Iteration time: 11.89s
                        Total time: 33043.17s
                               ETA: 1268907.3s

################################################################################
                    [1m Learning iteration 2538/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.832s, learning 0.181s)
               Value function loss: 25.4530
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 250.29
               Mean episode length: 121.71
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41598976
                    Iteration time: 12.01s
                        Total time: 33055.19s
                               ETA: 1268855.6s

################################################################################
                    [1m Learning iteration 2539/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.851s, learning 0.325s)
               Value function loss: 23.1868
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 246.39
               Mean episode length: 119.79
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41615360
                    Iteration time: 12.18s
                        Total time: 33067.36s
                               ETA: 1268810.3s

################################################################################
                    [1m Learning iteration 2540/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.115s, learning 0.180s)
               Value function loss: 28.1507
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 249.57
               Mean episode length: 122.38
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41631744
                    Iteration time: 12.30s
                        Total time: 33079.66s
                               ETA: 1268769.5s

################################################################################
                    [1m Learning iteration 2541/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.963s, learning 0.187s)
               Value function loss: 27.3032
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 248.75
               Mean episode length: 121.46
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41648128
                    Iteration time: 12.15s
                        Total time: 33091.81s
                               ETA: 1268723.2s

################################################################################
                    [1m Learning iteration 2542/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.556s, learning 0.197s)
               Value function loss: 32.0998
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 242.46
               Mean episode length: 119.13
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41664512
                    Iteration time: 11.75s
                        Total time: 33103.56s
                               ETA: 1268661.7s

################################################################################
                    [1m Learning iteration 2543/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.886s, learning 0.242s)
               Value function loss: 29.1668
                    Surrogate loss: -0.0015
             Mean action noise std: 0.77
                       Mean reward: 236.22
               Mean episode length: 116.09
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41680896
                    Iteration time: 12.13s
                        Total time: 33115.69s
                               ETA: 1268614.6s

################################################################################
                    [1m Learning iteration 2544/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.943s, learning 0.192s)
               Value function loss: 30.2104
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 233.62
               Mean episode length: 115.93
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41697280
                    Iteration time: 12.13s
                        Total time: 33127.82s
                               ETA: 1268567.8s

################################################################################
                    [1m Learning iteration 2545/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.711s, learning 0.206s)
               Value function loss: 28.5580
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 241.13
               Mean episode length: 119.53
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41713664
                    Iteration time: 11.92s
                        Total time: 33139.74s
                               ETA: 1268512.7s

################################################################################
                    [1m Learning iteration 2546/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.699s, learning 0.181s)
               Value function loss: 29.4033
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 236.08
               Mean episode length: 116.52
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41730048
                    Iteration time: 11.88s
                        Total time: 33151.62s
                               ETA: 1268456.2s

################################################################################
                    [1m Learning iteration 2547/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.565s, learning 0.171s)
               Value function loss: 29.4968
                    Surrogate loss: 0.0039
             Mean action noise std: 0.77
                       Mean reward: 251.53
               Mean episode length: 122.07
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41746432
                    Iteration time: 11.74s
                        Total time: 33163.35s
                               ETA: 1268394.2s

################################################################################
                    [1m Learning iteration 2548/100000 [0m                    

                       Computation: 1420 steps/s (collection: 11.357s, learning 0.176s)
               Value function loss: 28.8989
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 213.63
               Mean episode length: 108.05
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41762816
                    Iteration time: 11.53s
                        Total time: 33174.89s
                               ETA: 1268324.4s

################################################################################
                    [1m Learning iteration 2549/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.947s, learning 0.280s)
               Value function loss: 34.9598
                    Surrogate loss: -0.0018
             Mean action noise std: 0.77
                       Mean reward: 233.74
               Mean episode length: 114.83
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41779200
                    Iteration time: 12.23s
                        Total time: 33187.11s
                               ETA: 1268281.3s

################################################################################
                    [1m Learning iteration 2550/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.826s, learning 0.166s)
               Value function loss: 33.0078
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 237.56
               Mean episode length: 118.61
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41795584
                    Iteration time: 11.99s
                        Total time: 33199.11s
                               ETA: 1268229.3s

################################################################################
                    [1m Learning iteration 2551/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.636s, learning 0.178s)
               Value function loss: 29.2703
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 248.34
               Mean episode length: 121.79
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41811968
                    Iteration time: 11.81s
                        Total time: 33210.92s
                               ETA: 1268170.4s

################################################################################
                    [1m Learning iteration 2552/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.769s, learning 0.185s)
               Value function loss: 29.5098
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 246.25
               Mean episode length: 121.41
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41828352
                    Iteration time: 11.95s
                        Total time: 33222.87s
                               ETA: 1268117.0s

################################################################################
                    [1m Learning iteration 2553/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.648s, learning 0.195s)
               Value function loss: 29.2388
                    Surrogate loss: -0.0019
             Mean action noise std: 0.77
                       Mean reward: 243.82
               Mean episode length: 120.10
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41844736
                    Iteration time: 11.84s
                        Total time: 33234.72s
                               ETA: 1268059.3s

################################################################################
                    [1m Learning iteration 2554/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.628s, learning 0.212s)
               Value function loss: 25.1896
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 232.08
               Mean episode length: 114.92
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41861120
                    Iteration time: 11.84s
                        Total time: 33246.56s
                               ETA: 1268001.6s

################################################################################
                    [1m Learning iteration 2555/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.738s, learning 0.208s)
               Value function loss: 26.4133
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 237.59
               Mean episode length: 117.29
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41877504
                    Iteration time: 11.95s
                        Total time: 33258.50s
                               ETA: 1267947.9s

################################################################################
                    [1m Learning iteration 2556/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.485s, learning 0.177s)
               Value function loss: 28.1394
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 247.09
               Mean episode length: 119.95
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41893888
                    Iteration time: 11.66s
                        Total time: 33270.17s
                               ETA: 1267883.4s

################################################################################
                    [1m Learning iteration 2557/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.000s, learning 0.255s)
               Value function loss: 26.7972
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 236.52
               Mean episode length: 117.61
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41910272
                    Iteration time: 12.26s
                        Total time: 33282.42s
                               ETA: 1267841.6s

################################################################################
                    [1m Learning iteration 2558/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.557s, learning 0.192s)
               Value function loss: 27.8153
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 244.70
               Mean episode length: 120.06
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41926656
                    Iteration time: 11.75s
                        Total time: 33294.17s
                               ETA: 1267780.5s

################################################################################
                    [1m Learning iteration 2559/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.882s, learning 0.165s)
               Value function loss: 23.9458
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 240.05
               Mean episode length: 117.70
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41943040
                    Iteration time: 12.05s
                        Total time: 33306.22s
                               ETA: 1267730.9s

################################################################################
                    [1m Learning iteration 2560/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.613s, learning 0.172s)
               Value function loss: 25.8046
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 246.61
               Mean episode length: 120.13
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41959424
                    Iteration time: 11.79s
                        Total time: 33318.00s
                               ETA: 1267671.3s

################################################################################
                    [1m Learning iteration 2561/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.810s, learning 0.189s)
               Value function loss: 25.6883
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 237.16
               Mean episode length: 116.46
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41975808
                    Iteration time: 12.00s
                        Total time: 33330.00s
                               ETA: 1267619.8s

################################################################################
                    [1m Learning iteration 2562/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.074s, learning 0.181s)
               Value function loss: 25.0756
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 232.35
               Mean episode length: 114.64
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 41992192
                    Iteration time: 12.26s
                        Total time: 33342.26s
                               ETA: 1267578.2s

################################################################################
                    [1m Learning iteration 2563/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.750s, learning 0.190s)
               Value function loss: 25.8613
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 242.05
               Mean episode length: 117.49
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42008576
                    Iteration time: 11.94s
                        Total time: 33354.20s
                               ETA: 1267524.5s

################################################################################
                    [1m Learning iteration 2564/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.363s, learning 0.258s)
               Value function loss: 23.8007
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 247.79
               Mean episode length: 120.20
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42024960
                    Iteration time: 12.62s
                        Total time: 33366.82s
                               ETA: 1267496.8s

################################################################################
                    [1m Learning iteration 2565/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.982s, learning 0.231s)
               Value function loss: 29.4791
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 242.63
               Mean episode length: 117.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42041344
                    Iteration time: 12.21s
                        Total time: 33379.03s
                               ETA: 1267453.6s

################################################################################
                    [1m Learning iteration 2566/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.625s, learning 0.249s)
               Value function loss: 23.2499
                    Surrogate loss: -0.0183
             Mean action noise std: 0.77
                       Mean reward: 242.54
               Mean episode length: 118.41
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42057728
                    Iteration time: 11.87s
                        Total time: 33390.91s
                               ETA: 1267397.5s

################################################################################
                    [1m Learning iteration 2567/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.708s, learning 0.197s)
               Value function loss: 26.5251
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 242.41
               Mean episode length: 117.64
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42074112
                    Iteration time: 11.91s
                        Total time: 33402.81s
                               ETA: 1267342.7s

################################################################################
                    [1m Learning iteration 2568/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.834s, learning 0.168s)
               Value function loss: 25.9193
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 238.46
               Mean episode length: 116.59
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42090496
                    Iteration time: 12.00s
                        Total time: 33414.81s
                               ETA: 1267291.5s

################################################################################
                    [1m Learning iteration 2569/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.472s, learning 0.201s)
               Value function loss: 28.7431
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 253.76
               Mean episode length: 124.38
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42106880
                    Iteration time: 11.67s
                        Total time: 33426.49s
                               ETA: 1267228.0s

################################################################################
                    [1m Learning iteration 2570/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.657s, learning 0.163s)
               Value function loss: 22.1712
                    Surrogate loss: -0.0190
             Mean action noise std: 0.77
                       Mean reward: 246.54
               Mean episode length: 120.04
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42123264
                    Iteration time: 11.82s
                        Total time: 33438.31s
                               ETA: 1267170.0s

################################################################################
                    [1m Learning iteration 2571/100000 [0m                    

                       Computation: 1453 steps/s (collection: 11.104s, learning 0.166s)
               Value function loss: 24.9895
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 246.65
               Mean episode length: 119.96
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42139648
                    Iteration time: 11.27s
                        Total time: 33449.58s
                               ETA: 1267091.3s

################################################################################
                    [1m Learning iteration 2572/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.225s, learning 0.225s)
               Value function loss: 25.1372
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 245.24
               Mean episode length: 119.36
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42156032
                    Iteration time: 12.45s
                        Total time: 33462.03s
                               ETA: 1267057.2s

################################################################################
                    [1m Learning iteration 2573/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.802s, learning 0.170s)
               Value function loss: 30.4286
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 250.42
               Mean episode length: 121.11
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42172416
                    Iteration time: 11.97s
                        Total time: 33474.00s
                               ETA: 1267005.1s

################################################################################
                    [1m Learning iteration 2574/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.789s, learning 0.184s)
               Value function loss: 28.7610
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 246.88
               Mean episode length: 118.76
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42188800
                    Iteration time: 11.97s
                        Total time: 33485.97s
                               ETA: 1266953.1s

################################################################################
                    [1m Learning iteration 2575/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.468s, learning 0.158s)
               Value function loss: 26.2472
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 241.59
               Mean episode length: 116.53
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42205184
                    Iteration time: 11.63s
                        Total time: 33497.60s
                               ETA: 1266888.0s

################################################################################
                    [1m Learning iteration 2576/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.755s, learning 0.168s)
               Value function loss: 28.8783
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 236.24
               Mean episode length: 114.25
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42221568
                    Iteration time: 11.92s
                        Total time: 33509.52s
                               ETA: 1266834.1s

################################################################################
                    [1m Learning iteration 2577/100000 [0m                    

                       Computation: 1438 steps/s (collection: 11.224s, learning 0.162s)
               Value function loss: 26.2474
                    Surrogate loss: -0.0032
             Mean action noise std: 0.77
                       Mean reward: 237.80
               Mean episode length: 114.58
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42237952
                    Iteration time: 11.39s
                        Total time: 33520.91s
                               ETA: 1266760.0s

################################################################################
                    [1m Learning iteration 2578/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.764s, learning 0.168s)
               Value function loss: 27.4419
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 243.21
               Mean episode length: 117.86
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42254336
                    Iteration time: 11.93s
                        Total time: 33532.84s
                               ETA: 1266706.6s

################################################################################
                    [1m Learning iteration 2579/100000 [0m                    

                       Computation: 1339 steps/s (collection: 11.985s, learning 0.246s)
               Value function loss: 27.3006
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 239.70
               Mean episode length: 116.39
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42270720
                    Iteration time: 12.23s
                        Total time: 33545.07s
                               ETA: 1266664.4s

################################################################################
                    [1m Learning iteration 2580/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.723s, learning 0.196s)
               Value function loss: 27.2358
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 245.98
               Mean episode length: 119.40
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42287104
                    Iteration time: 11.92s
                        Total time: 33556.99s
                               ETA: 1266610.5s

################################################################################
                    [1m Learning iteration 2581/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.544s, learning 0.262s)
               Value function loss: 30.9357
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 246.80
               Mean episode length: 118.41
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.46
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42303488
                    Iteration time: 11.81s
                        Total time: 33568.79s
                               ETA: 1266552.4s

################################################################################
                    [1m Learning iteration 2582/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.797s, learning 0.166s)
               Value function loss: 22.1516
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 226.45
               Mean episode length: 110.76
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42319872
                    Iteration time: 11.96s
                        Total time: 33580.76s
                               ETA: 1266500.2s

################################################################################
                    [1m Learning iteration 2583/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.696s, learning 0.243s)
               Value function loss: 25.4416
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 252.80
               Mean episode length: 121.53
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42336256
                    Iteration time: 11.94s
                        Total time: 33592.70s
                               ETA: 1266447.2s

################################################################################
                    [1m Learning iteration 2584/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.429s, learning 0.199s)
               Value function loss: 29.6213
                    Surrogate loss: -0.0163
             Mean action noise std: 0.77
                       Mean reward: 248.99
               Mean episode length: 120.40
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42352640
                    Iteration time: 11.63s
                        Total time: 33604.32s
                               ETA: 1266382.5s

################################################################################
                    [1m Learning iteration 2585/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.953s, learning 0.166s)
               Value function loss: 22.3300
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 254.66
               Mean episode length: 121.32
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42369024
                    Iteration time: 12.12s
                        Total time: 33616.44s
                               ETA: 1266336.3s

################################################################################
                    [1m Learning iteration 2586/100000 [0m                    

                       Computation: 1408 steps/s (collection: 11.466s, learning 0.170s)
               Value function loss: 23.0908
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 251.52
               Mean episode length: 120.91
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42385408
                    Iteration time: 11.64s
                        Total time: 33628.08s
                               ETA: 1266272.0s

################################################################################
                    [1m Learning iteration 2587/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.685s, learning 0.159s)
               Value function loss: 25.0160
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 245.17
               Mean episode length: 118.48
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42401792
                    Iteration time: 11.84s
                        Total time: 33639.92s
                               ETA: 1266215.5s

################################################################################
                    [1m Learning iteration 2588/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.764s, learning 0.272s)
               Value function loss: 22.5559
                    Surrogate loss: -0.0156
             Mean action noise std: 0.77
                       Mean reward: 251.62
               Mean episode length: 121.39
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42418176
                    Iteration time: 12.04s
                        Total time: 33651.96s
                               ETA: 1266166.3s

################################################################################
                    [1m Learning iteration 2589/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.651s, learning 0.161s)
               Value function loss: 26.6023
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 243.88
               Mean episode length: 116.94
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42434560
                    Iteration time: 11.81s
                        Total time: 33663.77s
                               ETA: 1266108.7s

################################################################################
                    [1m Learning iteration 2590/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.676s, learning 0.200s)
               Value function loss: 26.3291
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 246.29
               Mean episode length: 117.91
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42450944
                    Iteration time: 11.88s
                        Total time: 33675.65s
                               ETA: 1266053.5s

################################################################################
                    [1m Learning iteration 2591/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.053s, learning 0.166s)
               Value function loss: 24.4556
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 250.02
               Mean episode length: 120.30
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42467328
                    Iteration time: 12.22s
                        Total time: 33687.87s
                               ETA: 1266011.3s

################################################################################
                    [1m Learning iteration 2592/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.817s, learning 0.158s)
               Value function loss: 27.2819
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 252.67
               Mean episode length: 120.95
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42483712
                    Iteration time: 11.97s
                        Total time: 33699.84s
                               ETA: 1265959.9s

################################################################################
                    [1m Learning iteration 2593/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.649s, learning 0.167s)
               Value function loss: 26.4100
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 244.48
               Mean episode length: 118.38
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42500096
                    Iteration time: 11.82s
                        Total time: 33711.66s
                               ETA: 1265902.6s

################################################################################
                    [1m Learning iteration 2594/100000 [0m                    

                       Computation: 1301 steps/s (collection: 12.398s, learning 0.188s)
               Value function loss: 24.5924
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 237.23
               Mean episode length: 114.68
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42516480
                    Iteration time: 12.59s
                        Total time: 33724.24s
                               ETA: 1265874.2s

################################################################################
                    [1m Learning iteration 2595/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.786s, learning 0.246s)
               Value function loss: 24.8141
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 245.76
               Mean episode length: 117.95
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42532864
                    Iteration time: 12.03s
                        Total time: 33736.28s
                               ETA: 1265825.1s

################################################################################
                    [1m Learning iteration 2596/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.798s, learning 0.167s)
               Value function loss: 30.8632
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 253.73
               Mean episode length: 120.94
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42549248
                    Iteration time: 11.96s
                        Total time: 33748.24s
                               ETA: 1265773.4s

################################################################################
                    [1m Learning iteration 2597/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.075s, learning 0.166s)
               Value function loss: 27.2136
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 251.39
               Mean episode length: 121.76
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42565632
                    Iteration time: 12.24s
                        Total time: 33760.48s
                               ETA: 1265732.2s

################################################################################
                    [1m Learning iteration 2598/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.738s, learning 0.168s)
               Value function loss: 23.2948
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 246.61
               Mean episode length: 118.17
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42582016
                    Iteration time: 11.91s
                        Total time: 33772.39s
                               ETA: 1265678.4s

################################################################################
                    [1m Learning iteration 2599/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.819s, learning 0.172s)
               Value function loss: 23.9433
                    Surrogate loss: -0.0170
             Mean action noise std: 0.77
                       Mean reward: 242.99
               Mean episode length: 116.80
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42598400
                    Iteration time: 11.99s
                        Total time: 33784.38s
                               ETA: 1265627.8s

################################################################################
                    [1m Learning iteration 2600/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.615s, learning 0.168s)
               Value function loss: 32.9900
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: 248.37
               Mean episode length: 119.16
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42614784
                    Iteration time: 11.78s
                        Total time: 33796.16s
                               ETA: 1265569.5s

################################################################################
                    [1m Learning iteration 2601/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.845s, learning 0.172s)
               Value function loss: 23.9235
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 250.70
               Mean episode length: 119.36
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42631168
                    Iteration time: 12.02s
                        Total time: 33808.18s
                               ETA: 1265519.9s

################################################################################
                    [1m Learning iteration 2602/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.801s, learning 0.172s)
               Value function loss: 26.6543
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 246.06
               Mean episode length: 118.21
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42647552
                    Iteration time: 11.97s
                        Total time: 33820.15s
                               ETA: 1265468.8s

################################################################################
                    [1m Learning iteration 2603/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.728s, learning 0.223s)
               Value function loss: 25.2107
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 234.52
               Mean episode length: 114.42
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42663936
                    Iteration time: 11.95s
                        Total time: 33832.10s
                               ETA: 1265416.8s

################################################################################
                    [1m Learning iteration 2604/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.494s, learning 0.169s)
               Value function loss: 26.6906
                    Surrogate loss: -0.0150
             Mean action noise std: 0.77
                       Mean reward: 244.38
               Mean episode length: 118.04
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42680320
                    Iteration time: 11.66s
                        Total time: 33843.77s
                               ETA: 1265354.1s

################################################################################
                    [1m Learning iteration 2605/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.096s, learning 0.192s)
               Value function loss: 29.9473
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 243.23
               Mean episode length: 117.23
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42696704
                    Iteration time: 12.29s
                        Total time: 33856.05s
                               ETA: 1265314.8s

################################################################################
                    [1m Learning iteration 2606/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.195s, learning 0.170s)
               Value function loss: 28.7395
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 247.97
               Mean episode length: 119.06
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42713088
                    Iteration time: 12.37s
                        Total time: 33868.42s
                               ETA: 1265278.5s

################################################################################
                    [1m Learning iteration 2607/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.700s, learning 0.195s)
               Value function loss: 30.2787
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 251.11
               Mean episode length: 119.55
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42729472
                    Iteration time: 11.90s
                        Total time: 33880.32s
                               ETA: 1265224.5s

################################################################################
                    [1m Learning iteration 2608/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.103s, learning 0.203s)
               Value function loss: 30.3338
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 249.82
               Mean episode length: 119.02
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42745856
                    Iteration time: 12.31s
                        Total time: 33892.62s
                               ETA: 1265186.0s

################################################################################
                    [1m Learning iteration 2609/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.787s, learning 0.168s)
               Value function loss: 30.1677
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 253.98
               Mean episode length: 121.14
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42762240
                    Iteration time: 11.95s
                        Total time: 33904.58s
                               ETA: 1265134.3s

################################################################################
                    [1m Learning iteration 2610/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.918s, learning 0.160s)
               Value function loss: 27.2973
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 247.86
               Mean episode length: 118.30
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42778624
                    Iteration time: 12.08s
                        Total time: 33916.66s
                               ETA: 1265087.3s

################################################################################
                    [1m Learning iteration 2611/100000 [0m                    

                       Computation: 1397 steps/s (collection: 11.519s, learning 0.201s)
               Value function loss: 29.5957
                    Surrogate loss: 0.0014
             Mean action noise std: 0.77
                       Mean reward: 251.67
               Mean episode length: 119.64
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42795008
                    Iteration time: 11.72s
                        Total time: 33928.38s
                               ETA: 1265027.0s

################################################################################
                    [1m Learning iteration 2612/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.822s, learning 0.200s)
               Value function loss: 30.1244
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 248.31
               Mean episode length: 119.50
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42811392
                    Iteration time: 12.02s
                        Total time: 33940.40s
                               ETA: 1264978.0s

################################################################################
                    [1m Learning iteration 2613/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.857s, learning 0.172s)
               Value function loss: 24.8304
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 251.92
               Mean episode length: 119.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42827776
                    Iteration time: 12.03s
                        Total time: 33952.43s
                               ETA: 1264929.2s

################################################################################
                    [1m Learning iteration 2614/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.947s, learning 0.163s)
               Value function loss: 24.0364
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 244.98
               Mean episode length: 118.30
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42844160
                    Iteration time: 12.11s
                        Total time: 33964.54s
                               ETA: 1264883.5s

################################################################################
                    [1m Learning iteration 2615/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.861s, learning 0.189s)
               Value function loss: 28.1272
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 254.88
               Mean episode length: 121.15
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42860544
                    Iteration time: 12.05s
                        Total time: 33976.59s
                               ETA: 1264835.6s

################################################################################
                    [1m Learning iteration 2616/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.466s, learning 0.185s)
               Value function loss: 27.6972
                    Surrogate loss: -0.0043
             Mean action noise std: 0.77
                       Mean reward: 242.28
               Mean episode length: 116.16
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42876928
                    Iteration time: 11.65s
                        Total time: 33988.24s
                               ETA: 1264772.9s

################################################################################
                    [1m Learning iteration 2617/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.052s, learning 0.161s)
               Value function loss: 23.2594
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 244.05
               Mean episode length: 118.12
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42893312
                    Iteration time: 12.21s
                        Total time: 34000.45s
                               ETA: 1264731.1s

################################################################################
                    [1m Learning iteration 2618/100000 [0m                    

                       Computation: 1421 steps/s (collection: 11.367s, learning 0.157s)
               Value function loss: 26.6909
                    Surrogate loss: -0.0061
             Mean action noise std: 0.77
                       Mean reward: 241.56
               Mean episode length: 116.48
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42909696
                    Iteration time: 11.52s
                        Total time: 34011.98s
                               ETA: 1264663.7s

################################################################################
                    [1m Learning iteration 2619/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.868s, learning 0.163s)
               Value function loss: 24.8825
                    Surrogate loss: -0.0161
             Mean action noise std: 0.77
                       Mean reward: 251.69
               Mean episode length: 121.34
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42926080
                    Iteration time: 12.03s
                        Total time: 34024.01s
                               ETA: 1264615.2s

################################################################################
                    [1m Learning iteration 2620/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.621s, learning 0.155s)
               Value function loss: 26.0615
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 251.77
               Mean episode length: 119.93
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42942464
                    Iteration time: 11.78s
                        Total time: 34035.78s
                               ETA: 1264557.2s

################################################################################
                    [1m Learning iteration 2621/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.755s, learning 0.161s)
               Value function loss: 28.7065
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 249.48
               Mean episode length: 120.13
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42958848
                    Iteration time: 11.92s
                        Total time: 34047.70s
                               ETA: 1264504.5s

################################################################################
                    [1m Learning iteration 2622/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.490s, learning 0.197s)
               Value function loss: 23.7333
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 250.71
               Mean episode length: 120.20
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42975232
                    Iteration time: 11.69s
                        Total time: 34059.38s
                               ETA: 1264443.3s

################################################################################
                    [1m Learning iteration 2623/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.147s, learning 0.213s)
               Value function loss: 25.4388
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 248.26
               Mean episode length: 119.80
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 42991616
                    Iteration time: 12.36s
                        Total time: 34071.74s
                               ETA: 1264407.1s

################################################################################
                    [1m Learning iteration 2624/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.279s, learning 0.166s)
               Value function loss: 27.1206
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 250.30
               Mean episode length: 120.22
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43008000
                    Iteration time: 12.44s
                        Total time: 34084.19s
                               ETA: 1264374.1s

################################################################################
                    [1m Learning iteration 2625/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.827s, learning 0.169s)
               Value function loss: 27.4706
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 242.01
               Mean episode length: 118.42
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43024384
                    Iteration time: 12.00s
                        Total time: 34096.19s
                               ETA: 1264324.5s

################################################################################
                    [1m Learning iteration 2626/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.554s, learning 0.281s)
               Value function loss: 27.3011
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 249.18
               Mean episode length: 120.46
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43040768
                    Iteration time: 11.84s
                        Total time: 34108.02s
                               ETA: 1264268.9s

################################################################################
                    [1m Learning iteration 2627/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.305s, learning 0.158s)
               Value function loss: 29.5229
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 253.85
               Mean episode length: 122.04
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43057152
                    Iteration time: 12.46s
                        Total time: 34120.48s
                               ETA: 1264236.7s

################################################################################
                    [1m Learning iteration 2628/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.034s, learning 0.192s)
               Value function loss: 27.7915
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 248.74
               Mean episode length: 119.85
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43073536
                    Iteration time: 12.23s
                        Total time: 34132.71s
                               ETA: 1264195.6s

################################################################################
                    [1m Learning iteration 2629/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.893s, learning 0.163s)
               Value function loss: 23.7518
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 253.77
               Mean episode length: 121.77
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43089920
                    Iteration time: 12.06s
                        Total time: 34144.77s
                               ETA: 1264148.3s

################################################################################
                    [1m Learning iteration 2630/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.983s, learning 0.170s)
               Value function loss: 25.5984
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 246.19
               Mean episode length: 119.04
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43106304
                    Iteration time: 12.15s
                        Total time: 34156.92s
                               ETA: 1264104.6s

################################################################################
                    [1m Learning iteration 2631/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.786s, learning 0.169s)
               Value function loss: 26.9465
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 257.59
               Mean episode length: 122.68
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43122688
                    Iteration time: 11.96s
                        Total time: 34168.87s
                               ETA: 1264053.6s

################################################################################
                    [1m Learning iteration 2632/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.538s, learning 0.168s)
               Value function loss: 21.8778
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 246.73
               Mean episode length: 118.48
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43139072
                    Iteration time: 11.71s
                        Total time: 34180.58s
                               ETA: 1263993.5s

################################################################################
                    [1m Learning iteration 2633/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.925s, learning 0.161s)
               Value function loss: 20.4948
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 244.42
               Mean episode length: 117.95
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43155456
                    Iteration time: 12.09s
                        Total time: 34192.67s
                               ETA: 1263947.4s

################################################################################
                    [1m Learning iteration 2634/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.762s, learning 0.203s)
               Value function loss: 23.5964
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 261.93
               Mean episode length: 123.85
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43171840
                    Iteration time: 11.97s
                        Total time: 34204.63s
                               ETA: 1263896.8s

################################################################################
                    [1m Learning iteration 2635/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.776s, learning 0.214s)
               Value function loss: 20.4654
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 250.24
               Mean episode length: 120.11
                  Mean reward/step: 2.10
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43188224
                    Iteration time: 11.99s
                        Total time: 34216.62s
                               ETA: 1263847.3s

################################################################################
                    [1m Learning iteration 2636/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.039s, learning 0.164s)
               Value function loss: 23.3252
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 254.37
               Mean episode length: 122.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43204608
                    Iteration time: 12.20s
                        Total time: 34228.83s
                               ETA: 1263805.6s

################################################################################
                    [1m Learning iteration 2637/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.841s, learning 0.165s)
               Value function loss: 22.9383
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 246.00
               Mean episode length: 118.67
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43220992
                    Iteration time: 12.01s
                        Total time: 34240.83s
                               ETA: 1263756.7s

################################################################################
                    [1m Learning iteration 2638/100000 [0m                    

                       Computation: 1347 steps/s (collection: 12.003s, learning 0.159s)
               Value function loss: 24.5368
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 253.99
               Mean episode length: 122.16
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43237376
                    Iteration time: 12.16s
                        Total time: 34252.99s
                               ETA: 1263713.5s

################################################################################
                    [1m Learning iteration 2639/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.673s, learning 0.257s)
               Value function loss: 22.3455
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 248.81
               Mean episode length: 118.92
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43253760
                    Iteration time: 11.93s
                        Total time: 34264.92s
                               ETA: 1263661.8s

################################################################################
                    [1m Learning iteration 2640/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.799s, learning 0.160s)
               Value function loss: 27.4267
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 253.58
               Mean episode length: 122.14
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43270144
                    Iteration time: 11.96s
                        Total time: 34276.88s
                               ETA: 1263611.2s

################################################################################
                    [1m Learning iteration 2641/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.641s, learning 0.186s)
               Value function loss: 23.2611
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 248.02
               Mean episode length: 118.87
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43286528
                    Iteration time: 11.83s
                        Total time: 34288.71s
                               ETA: 1263555.8s

################################################################################
                    [1m Learning iteration 2642/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.125s, learning 0.163s)
               Value function loss: 22.5670
                    Surrogate loss: -0.0167
             Mean action noise std: 0.77
                       Mean reward: 250.93
               Mean episode length: 119.75
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43302912
                    Iteration time: 12.29s
                        Total time: 34301.00s
                               ETA: 1263517.4s

################################################################################
                    [1m Learning iteration 2643/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.948s, learning 0.271s)
               Value function loss: 25.5901
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 246.97
               Mean episode length: 118.45
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43319296
                    Iteration time: 12.22s
                        Total time: 34313.22s
                               ETA: 1263476.4s

################################################################################
                    [1m Learning iteration 2644/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.940s, learning 0.214s)
               Value function loss: 24.7702
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 249.24
               Mean episode length: 119.38
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43335680
                    Iteration time: 12.15s
                        Total time: 34325.37s
                               ETA: 1263433.2s

################################################################################
                    [1m Learning iteration 2645/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.849s, learning 0.268s)
               Value function loss: 25.5315
                    Surrogate loss: -0.0016
             Mean action noise std: 0.77
                       Mean reward: 247.87
               Mean episode length: 119.37
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43352064
                    Iteration time: 12.12s
                        Total time: 34337.49s
                               ETA: 1263388.5s

################################################################################
                    [1m Learning iteration 2646/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.874s, learning 0.162s)
               Value function loss: 29.7299
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 253.95
               Mean episode length: 120.32
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43368448
                    Iteration time: 12.04s
                        Total time: 34349.52s
                               ETA: 1263340.9s

################################################################################
                    [1m Learning iteration 2647/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.991s, learning 0.177s)
               Value function loss: 26.8521
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 252.37
               Mean episode length: 120.06
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43384832
                    Iteration time: 12.17s
                        Total time: 34361.69s
                               ETA: 1263298.2s

################################################################################
                    [1m Learning iteration 2648/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.909s, learning 0.161s)
               Value function loss: 21.1398
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 247.22
               Mean episode length: 119.20
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43401216
                    Iteration time: 12.07s
                        Total time: 34373.76s
                               ETA: 1263252.0s

################################################################################
                    [1m Learning iteration 2649/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.914s, learning 0.187s)
               Value function loss: 23.4072
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 252.48
               Mean episode length: 121.37
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43417600
                    Iteration time: 12.10s
                        Total time: 34385.86s
                               ETA: 1263206.8s

################################################################################
                    [1m Learning iteration 2650/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.180s, learning 0.159s)
               Value function loss: 24.5562
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 244.73
               Mean episode length: 117.81
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43433984
                    Iteration time: 12.34s
                        Total time: 34398.20s
                               ETA: 1263170.5s

################################################################################
                    [1m Learning iteration 2651/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.610s, learning 0.230s)
               Value function loss: 25.4607
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 248.70
               Mean episode length: 119.77
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43450368
                    Iteration time: 11.84s
                        Total time: 34410.04s
                               ETA: 1263115.9s

################################################################################
                    [1m Learning iteration 2652/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.587s, learning 0.160s)
               Value function loss: 35.0018
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 252.91
               Mean episode length: 121.85
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43466752
                    Iteration time: 11.75s
                        Total time: 34421.79s
                               ETA: 1263057.8s

################################################################################
                    [1m Learning iteration 2653/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.944s, learning 0.190s)
               Value function loss: 31.2704
                    Surrogate loss: 0.0010
             Mean action noise std: 0.77
                       Mean reward: 258.14
               Mean episode length: 123.27
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43483136
                    Iteration time: 12.13s
                        Total time: 34433.92s
                               ETA: 1263014.0s

################################################################################
                    [1m Learning iteration 2654/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.053s, learning 0.209s)
               Value function loss: 29.8303
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 249.01
               Mean episode length: 121.03
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43499520
                    Iteration time: 12.26s
                        Total time: 34446.19s
                               ETA: 1262974.9s

################################################################################
                    [1m Learning iteration 2655/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.878s, learning 0.156s)
               Value function loss: 30.3207
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 258.05
               Mean episode length: 123.03
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43515904
                    Iteration time: 12.03s
                        Total time: 34458.22s
                               ETA: 1262927.5s

################################################################################
                    [1m Learning iteration 2656/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.483s, learning 0.185s)
               Value function loss: 32.9993
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 251.48
               Mean episode length: 121.01
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43532288
                    Iteration time: 11.67s
                        Total time: 34469.89s
                               ETA: 1262866.7s

################################################################################
                    [1m Learning iteration 2657/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.801s, learning 0.209s)
               Value function loss: 30.6645
                    Surrogate loss: 0.0011
             Mean action noise std: 0.77
                       Mean reward: 253.08
               Mean episode length: 121.64
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43548672
                    Iteration time: 12.01s
                        Total time: 34481.90s
                               ETA: 1262818.4s

################################################################################
                    [1m Learning iteration 2658/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.163s, learning 0.160s)
               Value function loss: 26.3870
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 252.99
               Mean episode length: 121.93
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43565056
                    Iteration time: 12.32s
                        Total time: 34494.22s
                               ETA: 1262781.6s

################################################################################
                    [1m Learning iteration 2659/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.617s, learning 0.161s)
               Value function loss: 30.3038
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 258.31
               Mean episode length: 124.90
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43581440
                    Iteration time: 11.78s
                        Total time: 34506.00s
                               ETA: 1262724.9s

################################################################################
                    [1m Learning iteration 2660/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.862s, learning 0.159s)
               Value function loss: 26.2391
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 254.44
               Mean episode length: 122.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43597824
                    Iteration time: 12.02s
                        Total time: 34518.02s
                               ETA: 1262677.1s

################################################################################
                    [1m Learning iteration 2661/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.756s, learning 0.211s)
               Value function loss: 29.1199
                    Surrogate loss: -0.0030
             Mean action noise std: 0.77
                       Mean reward: 245.63
               Mean episode length: 118.36
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43614208
                    Iteration time: 11.97s
                        Total time: 34529.99s
                               ETA: 1262627.4s

################################################################################
                    [1m Learning iteration 2662/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.007s, learning 0.164s)
               Value function loss: 29.3348
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 255.36
               Mean episode length: 121.74
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43630592
                    Iteration time: 12.17s
                        Total time: 34542.16s
                               ETA: 1262585.2s

################################################################################
                    [1m Learning iteration 2663/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.788s, learning 0.335s)
               Value function loss: 27.8194
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 256.01
               Mean episode length: 122.49
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43646976
                    Iteration time: 12.12s
                        Total time: 34554.28s
                               ETA: 1262541.2s

################################################################################
                    [1m Learning iteration 2664/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.214s, learning 0.161s)
               Value function loss: 24.4580
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 247.76
               Mean episode length: 117.86
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43663360
                    Iteration time: 12.38s
                        Total time: 34566.65s
                               ETA: 1262506.5s

################################################################################
                    [1m Learning iteration 2665/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.926s, learning 0.225s)
               Value function loss: 27.5734
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 245.42
               Mean episode length: 118.06
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43679744
                    Iteration time: 12.15s
                        Total time: 34578.81s
                               ETA: 1262463.6s

################################################################################
                    [1m Learning iteration 2666/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.754s, learning 0.168s)
               Value function loss: 25.7804
                    Surrogate loss: 0.0010
             Mean action noise std: 0.77
                       Mean reward: 250.62
               Mean episode length: 122.05
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43696128
                    Iteration time: 11.92s
                        Total time: 34590.73s
                               ETA: 1262412.4s

################################################################################
                    [1m Learning iteration 2667/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.655s, learning 0.161s)
               Value function loss: 27.1408
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 246.77
               Mean episode length: 119.40
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43712512
                    Iteration time: 11.82s
                        Total time: 34602.54s
                               ETA: 1262357.3s

################################################################################
                    [1m Learning iteration 2668/100000 [0m                    

                       Computation: 1346 steps/s (collection: 11.965s, learning 0.207s)
               Value function loss: 27.0829
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 253.83
               Mean episode length: 122.48
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43728896
                    Iteration time: 12.17s
                        Total time: 34614.71s
                               ETA: 1262315.2s

################################################################################
                    [1m Learning iteration 2669/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.792s, learning 0.158s)
               Value function loss: 29.2532
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 240.09
               Mean episode length: 115.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43745280
                    Iteration time: 11.95s
                        Total time: 34626.66s
                               ETA: 1262265.1s

################################################################################
                    [1m Learning iteration 2670/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.178s, learning 0.270s)
               Value function loss: 27.6664
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 252.45
               Mean episode length: 121.57
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43761664
                    Iteration time: 12.45s
                        Total time: 34639.11s
                               ETA: 1262233.2s

################################################################################
                    [1m Learning iteration 2671/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.503s, learning 0.161s)
               Value function loss: 31.2458
                    Surrogate loss: -0.0082
             Mean action noise std: 0.77
                       Mean reward: 249.93
               Mean episode length: 121.39
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43778048
                    Iteration time: 11.66s
                        Total time: 34650.78s
                               ETA: 1262172.7s

################################################################################
                    [1m Learning iteration 2672/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.786s, learning 0.158s)
               Value function loss: 26.7956
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 253.17
               Mean episode length: 121.50
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43794432
                    Iteration time: 11.94s
                        Total time: 34662.72s
                               ETA: 1262122.4s

################################################################################
                    [1m Learning iteration 2673/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.976s, learning 0.161s)
               Value function loss: 25.8075
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 244.12
               Mean episode length: 118.57
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43810816
                    Iteration time: 12.14s
                        Total time: 34674.86s
                               ETA: 1262079.2s

################################################################################
                    [1m Learning iteration 2674/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.544s, learning 0.162s)
               Value function loss: 26.8360
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 239.28
               Mean episode length: 116.02
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43827200
                    Iteration time: 11.71s
                        Total time: 34686.56s
                               ETA: 1262020.4s

################################################################################
                    [1m Learning iteration 2675/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.093s, learning 0.213s)
               Value function loss: 32.2422
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 252.27
               Mean episode length: 121.98
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43843584
                    Iteration time: 12.31s
                        Total time: 34698.87s
                               ETA: 1261983.4s

################################################################################
                    [1m Learning iteration 2676/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.873s, learning 0.166s)
               Value function loss: 28.5222
                    Surrogate loss: -0.0017
             Mean action noise std: 0.77
                       Mean reward: 248.63
               Mean episode length: 119.70
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43859968
                    Iteration time: 12.04s
                        Total time: 34710.91s
                               ETA: 1261936.7s

################################################################################
                    [1m Learning iteration 2677/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.665s, learning 0.207s)
               Value function loss: 28.3552
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 244.52
               Mean episode length: 119.07
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43876352
                    Iteration time: 11.87s
                        Total time: 34722.78s
                               ETA: 1261884.0s

################################################################################
                    [1m Learning iteration 2678/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.545s, learning 0.160s)
               Value function loss: 28.8138
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 246.77
               Mean episode length: 119.72
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43892736
                    Iteration time: 11.70s
                        Total time: 34734.49s
                               ETA: 1261825.2s

################################################################################
                    [1m Learning iteration 2679/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.845s, learning 0.167s)
               Value function loss: 25.5836
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 250.33
               Mean episode length: 122.02
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43909120
                    Iteration time: 12.01s
                        Total time: 34746.50s
                               ETA: 1261777.6s

################################################################################
                    [1m Learning iteration 2680/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.707s, learning 0.166s)
               Value function loss: 26.2030
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 253.33
               Mean episode length: 122.25
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43925504
                    Iteration time: 11.87s
                        Total time: 34758.37s
                               ETA: 1261725.0s

################################################################################
                    [1m Learning iteration 2681/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.756s, learning 0.191s)
               Value function loss: 29.0801
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 250.84
               Mean episode length: 120.74
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43941888
                    Iteration time: 11.95s
                        Total time: 34770.32s
                               ETA: 1261675.1s

################################################################################
                    [1m Learning iteration 2682/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.662s, learning 0.161s)
               Value function loss: 26.6521
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 234.77
               Mean episode length: 116.49
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43958272
                    Iteration time: 11.82s
                        Total time: 34782.14s
                               ETA: 1261620.7s

################################################################################
                    [1m Learning iteration 2683/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.711s, learning 0.180s)
               Value function loss: 29.3958
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 255.49
               Mean episode length: 124.12
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43974656
                    Iteration time: 11.89s
                        Total time: 34794.03s
                               ETA: 1261568.8s

################################################################################
                    [1m Learning iteration 2684/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.076s, learning 0.170s)
               Value function loss: 27.9875
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 247.05
               Mean episode length: 120.27
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 43991040
                    Iteration time: 12.25s
                        Total time: 34806.28s
                               ETA: 1261529.9s

################################################################################
                    [1m Learning iteration 2685/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.505s, learning 0.207s)
               Value function loss: 32.7492
                    Surrogate loss: -0.0007
             Mean action noise std: 0.77
                       Mean reward: 248.27
               Mean episode length: 121.99
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44007424
                    Iteration time: 11.71s
                        Total time: 34817.99s
                               ETA: 1261471.6s

################################################################################
                    [1m Learning iteration 2686/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.834s, learning 0.170s)
               Value function loss: 24.1038
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 249.41
               Mean episode length: 122.41
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44023808
                    Iteration time: 12.00s
                        Total time: 34829.99s
                               ETA: 1261423.9s

################################################################################
                    [1m Learning iteration 2687/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.651s, learning 0.159s)
               Value function loss: 34.6038
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 233.28
               Mean episode length: 115.25
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44040192
                    Iteration time: 11.81s
                        Total time: 34841.80s
                               ETA: 1261369.2s

################################################################################
                    [1m Learning iteration 2688/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.947s, learning 0.167s)
               Value function loss: 34.8117
                    Surrogate loss: -0.0158
             Mean action noise std: 0.77
                       Mean reward: 253.44
               Mean episode length: 123.09
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44056576
                    Iteration time: 12.11s
                        Total time: 34853.92s
                               ETA: 1261325.5s

################################################################################
                    [1m Learning iteration 2689/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.680s, learning 0.195s)
               Value function loss: 34.7050
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 240.22
               Mean episode length: 118.53
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44072960
                    Iteration time: 11.87s
                        Total time: 34865.79s
                               ETA: 1261273.3s

################################################################################
                    [1m Learning iteration 2690/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.742s, learning 0.164s)
               Value function loss: 34.3192
                    Surrogate loss: -0.0021
             Mean action noise std: 0.77
                       Mean reward: 243.02
               Mean episode length: 120.73
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44089344
                    Iteration time: 11.91s
                        Total time: 34877.70s
                               ETA: 1261222.1s

################################################################################
                    [1m Learning iteration 2691/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.662s, learning 0.255s)
               Value function loss: 28.5752
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 236.71
               Mean episode length: 119.27
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44105728
                    Iteration time: 11.92s
                        Total time: 34889.61s
                               ETA: 1261171.4s

################################################################################
                    [1m Learning iteration 2692/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.086s, learning 0.164s)
               Value function loss: 31.4480
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 243.38
               Mean episode length: 121.02
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44122112
                    Iteration time: 12.25s
                        Total time: 34901.87s
                               ETA: 1261132.8s

################################################################################
                    [1m Learning iteration 2693/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.543s, learning 0.165s)
               Value function loss: 36.4732
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 243.59
               Mean episode length: 121.00
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44138496
                    Iteration time: 11.71s
                        Total time: 34913.57s
                               ETA: 1261074.6s

################################################################################
                    [1m Learning iteration 2694/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.669s, learning 0.201s)
               Value function loss: 29.6760
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 241.48
               Mean episode length: 120.23
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44154880
                    Iteration time: 11.87s
                        Total time: 34925.44s
                               ETA: 1261022.3s

################################################################################
                    [1m Learning iteration 2695/100000 [0m                    

                       Computation: 1433 steps/s (collection: 11.270s, learning 0.156s)
               Value function loss: 24.4484
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 241.47
               Mean episode length: 122.08
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44171264
                    Iteration time: 11.43s
                        Total time: 34936.87s
                               ETA: 1260954.0s

################################################################################
                    [1m Learning iteration 2696/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.711s, learning 0.228s)
               Value function loss: 23.0597
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 230.85
               Mean episode length: 118.17
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44187648
                    Iteration time: 11.94s
                        Total time: 34948.81s
                               ETA: 1260904.3s

################################################################################
                    [1m Learning iteration 2697/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.760s, learning 0.167s)
               Value function loss: 26.3346
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 241.59
               Mean episode length: 119.93
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44204032
                    Iteration time: 11.93s
                        Total time: 34960.74s
                               ETA: 1260854.1s

################################################################################
                    [1m Learning iteration 2698/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.811s, learning 0.188s)
               Value function loss: 24.7562
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 231.81
               Mean episode length: 117.12
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44220416
                    Iteration time: 12.00s
                        Total time: 34972.73s
                               ETA: 1260806.6s

################################################################################
                    [1m Learning iteration 2699/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.857s, learning 0.239s)
               Value function loss: 28.0055
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 229.36
               Mean episode length: 115.27
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44236800
                    Iteration time: 12.10s
                        Total time: 34984.83s
                               ETA: 1260762.6s

################################################################################
                    [1m Learning iteration 2700/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.713s, learning 0.165s)
               Value function loss: 24.9412
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 240.90
               Mean episode length: 121.23
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44253184
                    Iteration time: 11.88s
                        Total time: 34996.71s
                               ETA: 1260710.7s

################################################################################
                    [1m Learning iteration 2701/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.857s, learning 0.172s)
               Value function loss: 27.1711
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 243.29
               Mean episode length: 120.96
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44269568
                    Iteration time: 12.03s
                        Total time: 35008.74s
                               ETA: 1260664.4s

################################################################################
                    [1m Learning iteration 2702/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.454s, learning 0.197s)
               Value function loss: 27.7033
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 241.33
               Mean episode length: 120.62
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44285952
                    Iteration time: 11.65s
                        Total time: 35020.39s
                               ETA: 1260604.4s

################################################################################
                    [1m Learning iteration 2703/100000 [0m                    

                       Computation: 1448 steps/s (collection: 11.156s, learning 0.158s)
               Value function loss: 28.1667
                    Surrogate loss: -0.0142
             Mean action noise std: 0.77
                       Mean reward: 243.62
               Mean episode length: 121.31
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44302336
                    Iteration time: 11.31s
                        Total time: 35031.70s
                               ETA: 1260532.4s

################################################################################
                    [1m Learning iteration 2704/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.826s, learning 0.263s)
               Value function loss: 24.8323
                    Surrogate loss: -0.0164
             Mean action noise std: 0.77
                       Mean reward: 245.76
               Mean episode length: 122.70
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44318720
                    Iteration time: 12.09s
                        Total time: 35043.79s
                               ETA: 1260488.2s

################################################################################
                    [1m Learning iteration 2705/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.820s, learning 0.287s)
               Value function loss: 25.9966
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 236.79
               Mean episode length: 118.45
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44335104
                    Iteration time: 12.11s
                        Total time: 35055.90s
                               ETA: 1260444.8s

################################################################################
                    [1m Learning iteration 2706/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.872s, learning 0.232s)
               Value function loss: 29.4400
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 237.41
               Mean episode length: 118.34
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44351488
                    Iteration time: 12.10s
                        Total time: 35068.00s
                               ETA: 1260401.3s

################################################################################
                    [1m Learning iteration 2707/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.855s, learning 0.156s)
               Value function loss: 23.4351
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 244.09
               Mean episode length: 122.49
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44367872
                    Iteration time: 12.01s
                        Total time: 35080.01s
                               ETA: 1260354.4s

################################################################################
                    [1m Learning iteration 2708/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.643s, learning 0.201s)
               Value function loss: 25.2274
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 247.16
               Mean episode length: 123.69
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44384256
                    Iteration time: 11.84s
                        Total time: 35091.86s
                               ETA: 1260301.6s

################################################################################
                    [1m Learning iteration 2709/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.782s, learning 0.324s)
               Value function loss: 26.6561
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 240.87
               Mean episode length: 118.98
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44400640
                    Iteration time: 12.11s
                        Total time: 35103.96s
                               ETA: 1260258.2s

################################################################################
                    [1m Learning iteration 2710/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.797s, learning 0.268s)
               Value function loss: 24.1420
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 231.35
               Mean episode length: 116.74
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44417024
                    Iteration time: 12.06s
                        Total time: 35116.03s
                               ETA: 1260213.3s

################################################################################
                    [1m Learning iteration 2711/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.696s, learning 0.165s)
               Value function loss: 21.0588
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 237.93
               Mean episode length: 119.47
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44433408
                    Iteration time: 11.86s
                        Total time: 35127.89s
                               ETA: 1260161.2s

################################################################################
                    [1m Learning iteration 2712/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.495s, learning 0.164s)
               Value function loss: 27.0541
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 245.72
               Mean episode length: 122.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44449792
                    Iteration time: 11.66s
                        Total time: 35139.55s
                               ETA: 1260101.9s

################################################################################
                    [1m Learning iteration 2713/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.160s, learning 0.231s)
               Value function loss: 24.7194
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 243.93
               Mean episode length: 121.87
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44466176
                    Iteration time: 12.39s
                        Total time: 35151.94s
                               ETA: 1260068.8s

################################################################################
                    [1m Learning iteration 2714/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.900s, learning 0.211s)
               Value function loss: 23.2816
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 240.00
               Mean episode length: 120.73
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44482560
                    Iteration time: 12.11s
                        Total time: 35164.05s
                               ETA: 1260025.7s

################################################################################
                    [1m Learning iteration 2715/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.677s, learning 0.161s)
               Value function loss: 29.2056
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 241.88
               Mean episode length: 120.48
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44498944
                    Iteration time: 11.84s
                        Total time: 35175.89s
                               ETA: 1259972.9s

################################################################################
                    [1m Learning iteration 2716/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.857s, learning 0.174s)
               Value function loss: 26.4159
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 248.98
               Mean episode length: 122.63
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44515328
                    Iteration time: 12.03s
                        Total time: 35187.92s
                               ETA: 1259927.0s

################################################################################
                    [1m Learning iteration 2717/100000 [0m                    

                       Computation: 1284 steps/s (collection: 12.593s, learning 0.164s)
               Value function loss: 24.7373
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 237.75
               Mean episode length: 117.60
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44531712
                    Iteration time: 12.76s
                        Total time: 35200.68s
                               ETA: 1259907.1s

################################################################################
                    [1m Learning iteration 2718/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.109s, learning 0.193s)
               Value function loss: 29.7152
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 249.84
               Mean episode length: 123.22
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44548096
                    Iteration time: 11.30s
                        Total time: 35211.98s
                               ETA: 1259835.2s

################################################################################
                    [1m Learning iteration 2719/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.661s, learning 0.156s)
               Value function loss: 27.0982
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 248.56
               Mean episode length: 122.41
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44564480
                    Iteration time: 11.82s
                        Total time: 35223.80s
                               ETA: 1259781.7s

################################################################################
                    [1m Learning iteration 2720/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.354s, learning 0.286s)
               Value function loss: 26.6641
                    Surrogate loss: -0.0171
             Mean action noise std: 0.77
                       Mean reward: 240.46
               Mean episode length: 120.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44580864
                    Iteration time: 11.64s
                        Total time: 35235.44s
                               ETA: 1259721.9s

################################################################################
                    [1m Learning iteration 2721/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.734s, learning 0.170s)
               Value function loss: 31.9911
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 242.99
               Mean episode length: 122.60
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44597248
                    Iteration time: 11.90s
                        Total time: 35247.34s
                               ETA: 1259671.6s

################################################################################
                    [1m Learning iteration 2722/100000 [0m                    

                       Computation: 1394 steps/s (collection: 11.559s, learning 0.191s)
               Value function loss: 30.0280
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 244.04
               Mean episode length: 122.16
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44613632
                    Iteration time: 11.75s
                        Total time: 35259.09s
                               ETA: 1259615.8s

################################################################################
                    [1m Learning iteration 2723/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.805s, learning 0.168s)
               Value function loss: 26.9498
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 239.56
               Mean episode length: 120.84
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44630016
                    Iteration time: 11.97s
                        Total time: 35271.06s
                               ETA: 1259568.1s

################################################################################
                    [1m Learning iteration 2724/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.727s, learning 0.161s)
               Value function loss: 29.4759
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 251.71
               Mean episode length: 124.71
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44646400
                    Iteration time: 11.89s
                        Total time: 35282.95s
                               ETA: 1259517.2s

################################################################################
                    [1m Learning iteration 2725/100000 [0m                    

                       Computation: 1462 steps/s (collection: 11.002s, learning 0.200s)
               Value function loss: 28.4575
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 230.74
               Mean episode length: 114.90
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44662784
                    Iteration time: 11.20s
                        Total time: 35294.15s
                               ETA: 1259442.0s

################################################################################
                    [1m Learning iteration 2726/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.354s, learning 0.160s)
               Value function loss: 22.0379
                    Surrogate loss: -0.0176
             Mean action noise std: 0.77
                       Mean reward: 238.11
               Mean episode length: 120.40
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44679168
                    Iteration time: 11.51s
                        Total time: 35305.67s
                               ETA: 1259377.9s

################################################################################
                    [1m Learning iteration 2727/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.499s, learning 0.160s)
               Value function loss: 27.1533
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 243.70
               Mean episode length: 121.09
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44695552
                    Iteration time: 11.66s
                        Total time: 35317.33s
                               ETA: 1259319.1s

################################################################################
                    [1m Learning iteration 2728/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.869s, learning 0.327s)
               Value function loss: 27.8035
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 247.12
               Mean episode length: 122.16
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44711936
                    Iteration time: 12.20s
                        Total time: 35329.52s
                               ETA: 1259279.4s

################################################################################
                    [1m Learning iteration 2729/100000 [0m                    

                       Computation: 1303 steps/s (collection: 12.372s, learning 0.197s)
               Value function loss: 24.8748
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 232.07
               Mean episode length: 117.18
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44728320
                    Iteration time: 12.57s
                        Total time: 35342.09s
                               ETA: 1259253.0s

################################################################################
                    [1m Learning iteration 2730/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.071s, learning 0.209s)
               Value function loss: 28.4565
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 236.83
               Mean episode length: 117.97
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44744704
                    Iteration time: 12.28s
                        Total time: 35354.37s
                               ETA: 1259216.4s

################################################################################
                    [1m Learning iteration 2731/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.498s, learning 0.205s)
               Value function loss: 27.0199
                    Surrogate loss: -0.0181
             Mean action noise std: 0.77
                       Mean reward: 246.26
               Mean episode length: 121.35
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44761088
                    Iteration time: 11.70s
                        Total time: 35366.08s
                               ETA: 1259159.2s

################################################################################
                    [1m Learning iteration 2732/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.921s, learning 0.156s)
               Value function loss: 29.0598
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 239.82
               Mean episode length: 119.33
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44777472
                    Iteration time: 12.08s
                        Total time: 35378.15s
                               ETA: 1259115.4s

################################################################################
                    [1m Learning iteration 2733/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.717s, learning 0.196s)
               Value function loss: 27.2279
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 244.59
               Mean episode length: 121.38
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44793856
                    Iteration time: 11.91s
                        Total time: 35390.07s
                               ETA: 1259065.7s

################################################################################
                    [1m Learning iteration 2734/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.714s, learning 0.262s)
               Value function loss: 31.5697
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 250.71
               Mean episode length: 123.13
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44810240
                    Iteration time: 11.98s
                        Total time: 35402.04s
                               ETA: 1259018.3s

################################################################################
                    [1m Learning iteration 2735/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.752s, learning 0.164s)
               Value function loss: 27.5545
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 244.17
               Mean episode length: 122.50
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44826624
                    Iteration time: 11.92s
                        Total time: 35413.96s
                               ETA: 1258968.8s

################################################################################
                    [1m Learning iteration 2736/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.753s, learning 0.165s)
               Value function loss: 25.8796
                    Surrogate loss: -0.0187
             Mean action noise std: 0.77
                       Mean reward: 248.03
               Mean episode length: 123.33
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44843008
                    Iteration time: 11.92s
                        Total time: 35425.88s
                               ETA: 1258919.4s

################################################################################
                    [1m Learning iteration 2737/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.464s, learning 0.189s)
               Value function loss: 29.1790
                    Surrogate loss: -0.0174
             Mean action noise std: 0.77
                       Mean reward: 247.74
               Mean episode length: 123.77
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44859392
                    Iteration time: 11.65s
                        Total time: 35437.53s
                               ETA: 1258860.7s

################################################################################
                    [1m Learning iteration 2738/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.669s, learning 0.154s)
               Value function loss: 22.7711
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 250.26
               Mean episode length: 123.37
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44875776
                    Iteration time: 11.82s
                        Total time: 35449.35s
                               ETA: 1258808.0s

################################################################################
                    [1m Learning iteration 2739/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.680s, learning 0.207s)
               Value function loss: 26.0331
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 242.42
               Mean episode length: 119.72
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44892160
                    Iteration time: 11.89s
                        Total time: 35461.24s
                               ETA: 1258757.6s

################################################################################
                    [1m Learning iteration 2740/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.656s, learning 0.190s)
               Value function loss: 27.3864
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 243.84
               Mean episode length: 120.90
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44908544
                    Iteration time: 11.85s
                        Total time: 35473.09s
                               ETA: 1258705.7s

################################################################################
                    [1m Learning iteration 2741/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.615s, learning 0.164s)
               Value function loss: 28.7975
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 243.88
               Mean episode length: 120.44
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44924928
                    Iteration time: 11.78s
                        Total time: 35484.87s
                               ETA: 1258651.5s

################################################################################
                    [1m Learning iteration 2742/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.674s, learning 0.264s)
               Value function loss: 23.8087
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 243.42
               Mean episode length: 121.85
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44941312
                    Iteration time: 11.94s
                        Total time: 35496.80s
                               ETA: 1258603.0s

################################################################################
                    [1m Learning iteration 2743/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.159s, learning 0.159s)
               Value function loss: 28.3585
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 247.28
               Mean episode length: 122.26
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44957696
                    Iteration time: 12.32s
                        Total time: 35509.12s
                               ETA: 1258568.0s

################################################################################
                    [1m Learning iteration 2744/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.631s, learning 0.197s)
               Value function loss: 24.2981
                    Surrogate loss: -0.0199
             Mean action noise std: 0.77
                       Mean reward: 236.61
               Mean episode length: 119.58
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44974080
                    Iteration time: 11.83s
                        Total time: 35520.95s
                               ETA: 1258515.6s

################################################################################
                    [1m Learning iteration 2745/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.641s, learning 0.261s)
               Value function loss: 25.7306
                    Surrogate loss: -0.0177
             Mean action noise std: 0.77
                       Mean reward: 240.72
               Mean episode length: 120.90
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 44990464
                    Iteration time: 11.90s
                        Total time: 35532.85s
                               ETA: 1258465.9s

################################################################################
                    [1m Learning iteration 2746/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.892s, learning 0.202s)
               Value function loss: 29.3789
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 248.58
               Mean episode length: 123.41
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45006848
                    Iteration time: 12.09s
                        Total time: 35544.94s
                               ETA: 1258423.0s

################################################################################
                    [1m Learning iteration 2747/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.704s, learning 0.239s)
               Value function loss: 28.7233
                    Surrogate loss: -0.0191
             Mean action noise std: 0.77
                       Mean reward: 240.50
               Mean episode length: 120.69
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45023232
                    Iteration time: 11.94s
                        Total time: 35556.89s
                               ETA: 1258374.8s

################################################################################
                    [1m Learning iteration 2748/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.738s, learning 0.239s)
               Value function loss: 26.1840
                    Surrogate loss: -0.0179
             Mean action noise std: 0.77
                       Mean reward: 245.60
               Mean episode length: 121.70
                  Mean reward/step: 1.99
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45039616
                    Iteration time: 11.98s
                        Total time: 35568.86s
                               ETA: 1258327.8s

################################################################################
                    [1m Learning iteration 2749/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.833s, learning 0.213s)
               Value function loss: 28.9435
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 238.97
               Mean episode length: 121.08
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45056000
                    Iteration time: 12.05s
                        Total time: 35580.91s
                               ETA: 1258283.3s

################################################################################
                    [1m Learning iteration 2750/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.923s, learning 0.205s)
               Value function loss: 27.7606
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 237.56
               Mean episode length: 119.16
                  Mean reward/step: 2.00
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45072384
                    Iteration time: 12.13s
                        Total time: 35593.04s
                               ETA: 1258241.7s

################################################################################
                    [1m Learning iteration 2751/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.584s, learning 0.154s)
               Value function loss: 26.4094
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 245.26
               Mean episode length: 123.17
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45088768
                    Iteration time: 11.74s
                        Total time: 35604.78s
                               ETA: 1258186.4s

################################################################################
                    [1m Learning iteration 2752/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.938s, learning 0.165s)
               Value function loss: 30.0383
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 242.65
               Mean episode length: 120.75
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45105152
                    Iteration time: 12.10s
                        Total time: 35616.88s
                               ETA: 1258144.0s

################################################################################
                    [1m Learning iteration 2753/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.633s, learning 0.258s)
               Value function loss: 30.0714
                    Surrogate loss: 0.0006
             Mean action noise std: 0.77
                       Mean reward: 243.83
               Mean episode length: 121.89
                  Mean reward/step: 2.01
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45121536
                    Iteration time: 11.89s
                        Total time: 35628.77s
                               ETA: 1258094.1s

################################################################################
                    [1m Learning iteration 2754/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.973s, learning 0.163s)
               Value function loss: 26.6075
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 234.95
               Mean episode length: 117.75
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45137920
                    Iteration time: 12.14s
                        Total time: 35640.91s
                               ETA: 1258052.9s

################################################################################
                    [1m Learning iteration 2755/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.850s, learning 0.199s)
               Value function loss: 30.9518
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 246.32
               Mean episode length: 122.35
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45154304
                    Iteration time: 12.05s
                        Total time: 35652.96s
                               ETA: 1258008.6s

################################################################################
                    [1m Learning iteration 2756/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.211s, learning 0.297s)
               Value function loss: 30.7204
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 248.96
               Mean episode length: 122.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45170688
                    Iteration time: 12.51s
                        Total time: 35665.46s
                               ETA: 1257980.6s

################################################################################
                    [1m Learning iteration 2757/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.893s, learning 0.158s)
               Value function loss: 26.8847
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 245.01
               Mean episode length: 122.73
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45187072
                    Iteration time: 12.05s
                        Total time: 35677.52s
                               ETA: 1257936.4s

################################################################################
                    [1m Learning iteration 2758/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.399s, learning 0.166s)
               Value function loss: 27.6417
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 243.56
               Mean episode length: 120.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45203456
                    Iteration time: 11.57s
                        Total time: 35689.08s
                               ETA: 1257875.2s

################################################################################
                    [1m Learning iteration 2759/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.613s, learning 0.164s)
               Value function loss: 29.9640
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 244.44
               Mean episode length: 120.47
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45219840
                    Iteration time: 11.78s
                        Total time: 35700.86s
                               ETA: 1257821.4s

################################################################################
                    [1m Learning iteration 2760/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.784s, learning 0.165s)
               Value function loss: 25.9890
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 244.55
               Mean episode length: 119.96
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45236224
                    Iteration time: 11.95s
                        Total time: 35712.81s
                               ETA: 1257773.7s

################################################################################
                    [1m Learning iteration 2761/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.109s, learning 0.208s)
               Value function loss: 27.7290
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 246.50
               Mean episode length: 122.73
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45252608
                    Iteration time: 12.32s
                        Total time: 35725.12s
                               ETA: 1257739.1s

################################################################################
                    [1m Learning iteration 2762/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.525s, learning 0.155s)
               Value function loss: 26.9394
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 240.12
               Mean episode length: 119.18
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45268992
                    Iteration time: 11.68s
                        Total time: 35736.80s
                               ETA: 1257682.0s

################################################################################
                    [1m Learning iteration 2763/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.744s, learning 0.253s)
               Value function loss: 28.3244
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 250.85
               Mean episode length: 123.98
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45285376
                    Iteration time: 12.00s
                        Total time: 35748.80s
                               ETA: 1257636.1s

################################################################################
                    [1m Learning iteration 2764/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.496s, learning 0.162s)
               Value function loss: 25.5817
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 246.14
               Mean episode length: 121.77
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45301760
                    Iteration time: 11.66s
                        Total time: 35760.46s
                               ETA: 1257578.3s

################################################################################
                    [1m Learning iteration 2765/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.152s, learning 0.165s)
               Value function loss: 32.4385
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 239.52
               Mean episode length: 118.97
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45318144
                    Iteration time: 12.32s
                        Total time: 35772.78s
                               ETA: 1257543.7s

################################################################################
                    [1m Learning iteration 2766/100000 [0m                    

                       Computation: 1449 steps/s (collection: 11.141s, learning 0.161s)
               Value function loss: 26.5391
                    Surrogate loss: -0.0138
             Mean action noise std: 0.77
                       Mean reward: 247.34
               Mean episode length: 122.66
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45334528
                    Iteration time: 11.30s
                        Total time: 35784.08s
                               ETA: 1257473.4s

################################################################################
                    [1m Learning iteration 2767/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.700s, learning 0.166s)
               Value function loss: 27.3765
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 251.00
               Mean episode length: 124.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45350912
                    Iteration time: 11.87s
                        Total time: 35795.94s
                               ETA: 1257423.0s

################################################################################
                    [1m Learning iteration 2768/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.094s, learning 0.158s)
               Value function loss: 32.4286
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 248.69
               Mean episode length: 122.36
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45367296
                    Iteration time: 12.25s
                        Total time: 35808.20s
                               ETA: 1257386.2s

################################################################################
                    [1m Learning iteration 2769/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.535s, learning 0.161s)
               Value function loss: 23.1737
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 247.65
               Mean episode length: 121.24
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45383680
                    Iteration time: 11.70s
                        Total time: 35819.89s
                               ETA: 1257329.9s

################################################################################
                    [1m Learning iteration 2770/100000 [0m                    

                       Computation: 1393 steps/s (collection: 11.585s, learning 0.172s)
               Value function loss: 28.9510
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 240.39
               Mean episode length: 119.06
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45400064
                    Iteration time: 11.76s
                        Total time: 35831.65s
                               ETA: 1257275.8s

################################################################################
                    [1m Learning iteration 2771/100000 [0m                    

                       Computation: 1331 steps/s (collection: 12.114s, learning 0.190s)
               Value function loss: 30.9122
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 245.40
               Mean episode length: 121.89
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45416448
                    Iteration time: 12.30s
                        Total time: 35843.95s
                               ETA: 1257240.8s

################################################################################
                    [1m Learning iteration 2772/100000 [0m                    

                       Computation: 1395 steps/s (collection: 11.574s, learning 0.171s)
               Value function loss: 30.5773
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 248.00
               Mean episode length: 123.57
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45432832
                    Iteration time: 11.74s
                        Total time: 35855.70s
                               ETA: 1257186.3s

################################################################################
                    [1m Learning iteration 2773/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.779s, learning 0.162s)
               Value function loss: 23.2704
                    Surrogate loss: -0.0175
             Mean action noise std: 0.77
                       Mean reward: 239.32
               Mean episode length: 116.88
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45449216
                    Iteration time: 11.94s
                        Total time: 35867.64s
                               ETA: 1257138.7s

################################################################################
                    [1m Learning iteration 2774/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.556s, learning 0.159s)
               Value function loss: 25.2727
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 249.72
               Mean episode length: 122.79
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45465600
                    Iteration time: 11.71s
                        Total time: 35879.35s
                               ETA: 1257083.2s

################################################################################
                    [1m Learning iteration 2775/100000 [0m                    

                       Computation: 1343 steps/s (collection: 12.043s, learning 0.156s)
               Value function loss: 28.8149
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 247.73
               Mean episode length: 121.95
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45481984
                    Iteration time: 12.20s
                        Total time: 35891.55s
                               ETA: 1257044.7s

################################################################################
                    [1m Learning iteration 2776/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.892s, learning 0.156s)
               Value function loss: 28.1878
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 239.31
               Mean episode length: 119.93
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45498368
                    Iteration time: 12.05s
                        Total time: 35903.60s
                               ETA: 1257000.9s

################################################################################
                    [1m Learning iteration 2777/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.754s, learning 0.158s)
               Value function loss: 35.5295
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 253.46
               Mean episode length: 124.08
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45514752
                    Iteration time: 11.91s
                        Total time: 35915.51s
                               ETA: 1256952.4s

################################################################################
                    [1m Learning iteration 2778/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.782s, learning 0.165s)
               Value function loss: 29.5979
                    Surrogate loss: 0.0005
             Mean action noise std: 0.77
                       Mean reward: 251.44
               Mean episode length: 121.79
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45531136
                    Iteration time: 11.95s
                        Total time: 35927.46s
                               ETA: 1256905.1s

################################################################################
                    [1m Learning iteration 2779/100000 [0m                    

                       Computation: 1084 steps/s (collection: 14.878s, learning 0.233s)
               Value function loss: 28.0429
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 247.98
               Mean episode length: 121.53
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45547520
                    Iteration time: 15.11s
                        Total time: 35942.57s
                               ETA: 1256968.5s

################################################################################
                    [1m Learning iteration 2780/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.334s, learning 0.169s)
               Value function loss: 29.3007
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 247.53
               Mean episode length: 121.52
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45563904
                    Iteration time: 23.50s
                        Total time: 35966.07s
                               ETA: 1257325.3s

################################################################################
                    [1m Learning iteration 2781/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.265s, learning 0.177s)
               Value function loss: 29.5180
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 248.98
               Mean episode length: 122.31
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45580288
                    Iteration time: 23.44s
                        Total time: 35989.51s
                               ETA: 1257679.6s

################################################################################
                    [1m Learning iteration 2782/100000 [0m                    

                       Computation: 692 steps/s (collection: 23.380s, learning 0.285s)
               Value function loss: 30.0487
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 243.25
               Mean episode length: 118.70
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45596672
                    Iteration time: 23.66s
                        Total time: 36013.18s
                               ETA: 1258041.4s

################################################################################
                    [1m Learning iteration 2783/100000 [0m                    

                       Computation: 716 steps/s (collection: 22.720s, learning 0.162s)
               Value function loss: 32.6501
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 254.85
               Mean episode length: 124.08
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45613056
                    Iteration time: 22.88s
                        Total time: 36036.06s
                               ETA: 1258375.6s

################################################################################
                    [1m Learning iteration 2784/100000 [0m                    

                       Computation: 722 steps/s (collection: 22.504s, learning 0.165s)
               Value function loss: 33.8489
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 251.83
               Mean episode length: 123.11
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45629440
                    Iteration time: 22.67s
                        Total time: 36058.73s
                               ETA: 1258702.2s

################################################################################
                    [1m Learning iteration 2785/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.183s, learning 0.197s)
               Value function loss: 25.5504
                    Surrogate loss: -0.0148
             Mean action noise std: 0.77
                       Mean reward: 246.23
               Mean episode length: 119.64
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45645824
                    Iteration time: 23.38s
                        Total time: 36082.11s
                               ETA: 1259053.3s

################################################################################
                    [1m Learning iteration 2786/100000 [0m                    

                       Computation: 706 steps/s (collection: 23.025s, learning 0.172s)
               Value function loss: 31.3179
                    Surrogate loss: -0.0068
             Mean action noise std: 0.77
                       Mean reward: 248.15
               Mean episode length: 121.25
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45662208
                    Iteration time: 23.20s
                        Total time: 36105.31s
                               ETA: 1259397.7s

################################################################################
                    [1m Learning iteration 2787/100000 [0m                    

                       Computation: 716 steps/s (collection: 22.668s, learning 0.191s)
               Value function loss: 29.9233
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 255.23
               Mean episode length: 123.78
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45678592
                    Iteration time: 22.86s
                        Total time: 36128.17s
                               ETA: 1259730.1s

################################################################################
                    [1m Learning iteration 2788/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.297s, learning 0.158s)
               Value function loss: 28.0673
                    Surrogate loss: -0.0096
             Mean action noise std: 0.77
                       Mean reward: 253.53
               Mean episode length: 123.47
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45694976
                    Iteration time: 23.46s
                        Total time: 36151.62s
                               ETA: 1260083.0s

################################################################################
                    [1m Learning iteration 2789/100000 [0m                    

                       Computation: 708 steps/s (collection: 22.954s, learning 0.158s)
               Value function loss: 28.1545
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 245.34
               Mean episode length: 119.35
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45711360
                    Iteration time: 23.11s
                        Total time: 36174.73s
                               ETA: 1260423.6s

################################################################################
                    [1m Learning iteration 2790/100000 [0m                    

                       Computation: 697 steps/s (collection: 23.337s, learning 0.155s)
               Value function loss: 28.6270
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 251.75
               Mean episode length: 122.11
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45727744
                    Iteration time: 23.49s
                        Total time: 36198.22s
                               ETA: 1260777.3s

################################################################################
                    [1m Learning iteration 2791/100000 [0m                    

                       Computation: 698 steps/s (collection: 23.259s, learning 0.182s)
               Value function loss: 25.6990
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 243.52
               Mean episode length: 118.04
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45744128
                    Iteration time: 23.44s
                        Total time: 36221.67s
                               ETA: 1261128.9s

################################################################################
                    [1m Learning iteration 2792/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.268s, learning 0.163s)
               Value function loss: 31.2362
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 254.83
               Mean episode length: 123.95
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45760512
                    Iteration time: 23.43s
                        Total time: 36245.10s
                               ETA: 1261479.9s

################################################################################
                    [1m Learning iteration 2793/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.327s, learning 0.199s)
               Value function loss: 32.5445
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 254.35
               Mean episode length: 123.04
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45776896
                    Iteration time: 23.53s
                        Total time: 36268.62s
                               ETA: 1261833.9s

################################################################################
                    [1m Learning iteration 2794/100000 [0m                    

                       Computation: 683 steps/s (collection: 23.740s, learning 0.229s)
               Value function loss: 30.7108
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 255.09
               Mean episode length: 123.39
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45793280
                    Iteration time: 23.97s
                        Total time: 36292.59s
                               ETA: 1262203.1s

################################################################################
                    [1m Learning iteration 2795/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.153s, learning 0.229s)
               Value function loss: 30.2542
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 249.08
               Mean episode length: 121.15
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45809664
                    Iteration time: 23.38s
                        Total time: 36315.97s
                               ETA: 1262551.6s

################################################################################
                    [1m Learning iteration 2796/100000 [0m                    

                       Computation: 706 steps/s (collection: 22.962s, learning 0.224s)
               Value function loss: 33.3979
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 248.37
               Mean episode length: 121.43
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45826048
                    Iteration time: 23.19s
                        Total time: 36339.16s
                               ETA: 1262893.0s

################################################################################
                    [1m Learning iteration 2797/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.544s, learning 0.164s)
               Value function loss: 30.6502
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 245.70
               Mean episode length: 119.05
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45842432
                    Iteration time: 23.71s
                        Total time: 36362.87s
                               ETA: 1263252.3s

################################################################################
                    [1m Learning iteration 2798/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.352s, learning 0.238s)
               Value function loss: 28.3488
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 248.96
               Mean episode length: 120.24
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45858816
                    Iteration time: 23.59s
                        Total time: 36386.46s
                               ETA: 1263607.2s

################################################################################
                    [1m Learning iteration 2799/100000 [0m                    

                       Computation: 695 steps/s (collection: 23.391s, learning 0.176s)
               Value function loss: 33.7985
                    Surrogate loss: -0.0093
             Mean action noise std: 0.77
                       Mean reward: 252.65
               Mean episode length: 121.14
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45875200
                    Iteration time: 23.57s
                        Total time: 36410.02s
                               ETA: 1263961.0s

################################################################################
                    [1m Learning iteration 2800/100000 [0m                    

                       Computation: 699 steps/s (collection: 23.234s, learning 0.192s)
               Value function loss: 29.5399
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 241.70
               Mean episode length: 117.02
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45891584
                    Iteration time: 23.43s
                        Total time: 36433.45s
                               ETA: 1264309.7s

################################################################################
                    [1m Learning iteration 2801/100000 [0m                    

                       Computation: 703 steps/s (collection: 23.123s, learning 0.166s)
               Value function loss: 28.7818
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 249.98
               Mean episode length: 121.05
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45907968
                    Iteration time: 23.29s
                        Total time: 36456.74s
                               ETA: 1264653.3s

################################################################################
                    [1m Learning iteration 2802/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.238s, learning 0.160s)
               Value function loss: 31.0112
                    Surrogate loss: -0.0101
             Mean action noise std: 0.77
                       Mean reward: 251.33
               Mean episode length: 121.18
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45924352
                    Iteration time: 23.40s
                        Total time: 36480.14s
                               ETA: 1265000.5s

################################################################################
                    [1m Learning iteration 2803/100000 [0m                    

                       Computation: 696 steps/s (collection: 23.299s, learning 0.239s)
               Value function loss: 27.0352
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 253.63
               Mean episode length: 122.40
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45940736
                    Iteration time: 23.54s
                        Total time: 36503.68s
                               ETA: 1265352.3s

################################################################################
                    [1m Learning iteration 2804/100000 [0m                    

                       Computation: 706 steps/s (collection: 23.027s, learning 0.167s)
               Value function loss: 22.4854
                    Surrogate loss: -0.0022
             Mean action noise std: 0.77
                       Mean reward: 250.76
               Mean episode length: 121.38
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45957120
                    Iteration time: 23.19s
                        Total time: 36526.87s
                               ETA: 1265691.9s

################################################################################
                    [1m Learning iteration 2805/100000 [0m                    

                       Computation: 700 steps/s (collection: 23.219s, learning 0.168s)
               Value function loss: 27.6367
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 254.62
               Mean episode length: 123.07
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45973504
                    Iteration time: 23.39s
                        Total time: 36550.26s
                               ETA: 1266037.9s

################################################################################
                    [1m Learning iteration 2806/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.506s, learning 0.187s)
               Value function loss: 26.9478
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 253.45
               Mean episode length: 123.09
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 45989888
                    Iteration time: 23.69s
                        Total time: 36573.95s
                               ETA: 1266394.2s

################################################################################
                    [1m Learning iteration 2807/100000 [0m                    

                       Computation: 716 steps/s (collection: 22.680s, learning 0.176s)
               Value function loss: 26.4955
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 254.14
               Mean episode length: 122.30
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46006272
                    Iteration time: 22.86s
                        Total time: 36596.81s
                               ETA: 1266721.3s

################################################################################
                    [1m Learning iteration 2808/100000 [0m                    

                       Computation: 701 steps/s (collection: 23.178s, learning 0.162s)
               Value function loss: 29.4730
                    Surrogate loss: -0.0100
             Mean action noise std: 0.77
                       Mean reward: 247.37
               Mean episode length: 119.38
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46022656
                    Iteration time: 23.34s
                        Total time: 36620.15s
                               ETA: 1267064.9s

################################################################################
                    [1m Learning iteration 2809/100000 [0m                    

                       Computation: 709 steps/s (collection: 22.945s, learning 0.160s)
               Value function loss: 26.5651
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 254.54
               Mean episode length: 122.18
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46039040
                    Iteration time: 23.11s
                        Total time: 36643.25s
                               ETA: 1267400.1s

################################################################################
                    [1m Learning iteration 2810/100000 [0m                    

                       Computation: 715 steps/s (collection: 22.725s, learning 0.185s)
               Value function loss: 28.0277
                    Surrogate loss: -0.0029
             Mean action noise std: 0.77
                       Mean reward: 255.18
               Mean episode length: 122.41
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46055424
                    Iteration time: 22.91s
                        Total time: 36666.16s
                               ETA: 1267728.3s

################################################################################
                    [1m Learning iteration 2811/100000 [0m                    

                       Computation: 694 steps/s (collection: 23.377s, learning 0.201s)
               Value function loss: 25.4783
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 253.41
               Mean episode length: 121.63
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46071808
                    Iteration time: 23.58s
                        Total time: 36689.74s
                               ETA: 1268079.3s

################################################################################
                    [1m Learning iteration 2812/100000 [0m                    

                       Computation: 712 steps/s (collection: 22.782s, learning 0.210s)
               Value function loss: 33.6714
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 253.48
               Mean episode length: 122.17
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46088192
                    Iteration time: 22.99s
                        Total time: 36712.73s
                               ETA: 1268409.9s

################################################################################
                    [1m Learning iteration 2813/100000 [0m                    

                       Computation: 704 steps/s (collection: 23.061s, learning 0.188s)
               Value function loss: 27.0560
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 256.71
               Mean episode length: 122.55
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46104576
                    Iteration time: 23.25s
                        Total time: 36735.98s
                               ETA: 1268749.0s

################################################################################
                    [1m Learning iteration 2814/100000 [0m                    

                       Computation: 691 steps/s (collection: 23.429s, learning 0.254s)
               Value function loss: 27.0062
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 255.03
               Mean episode length: 122.06
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46120960
                    Iteration time: 23.68s
                        Total time: 36759.66s
                               ETA: 1269102.9s

################################################################################
                    [1m Learning iteration 2815/100000 [0m                    

                       Computation: 715 steps/s (collection: 22.644s, learning 0.262s)
               Value function loss: 33.1346
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 254.06
               Mean episode length: 121.45
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46137344
                    Iteration time: 22.91s
                        Total time: 36782.57s
                               ETA: 1269429.7s

################################################################################
                    [1m Learning iteration 2816/100000 [0m                    

                       Computation: 715 steps/s (collection: 22.718s, learning 0.180s)
               Value function loss: 26.2888
                    Surrogate loss: 0.0025
             Mean action noise std: 0.77
                       Mean reward: 260.08
               Mean episode length: 124.11
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46153728
                    Iteration time: 22.90s
                        Total time: 36805.47s
                               ETA: 1269756.0s

################################################################################
                    [1m Learning iteration 2817/100000 [0m                    

                       Computation: 1019 steps/s (collection: 15.839s, learning 0.235s)
               Value function loss: 28.3136
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 255.45
               Mean episode length: 121.96
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46170112
                    Iteration time: 16.07s
                        Total time: 36821.54s
                               ETA: 1269846.7s

################################################################################
                    [1m Learning iteration 2818/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.841s, learning 0.293s)
               Value function loss: 34.0714
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 247.26
               Mean episode length: 118.48
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46186496
                    Iteration time: 12.13s
                        Total time: 36833.68s
                               ETA: 1269801.5s

################################################################################
                    [1m Learning iteration 2819/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.051s, learning 0.187s)
               Value function loss: 30.8737
                    Surrogate loss: 0.0034
             Mean action noise std: 0.77
                       Mean reward: 253.51
               Mean episode length: 122.19
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46202880
                    Iteration time: 12.24s
                        Total time: 36845.91s
                               ETA: 1269759.9s

################################################################################
                    [1m Learning iteration 2820/100000 [0m                    

                       Computation: 1323 steps/s (collection: 12.206s, learning 0.173s)
               Value function loss: 28.5218
                    Surrogate loss: -0.0069
             Mean action noise std: 0.77
                       Mean reward: 244.55
               Mean episode length: 117.28
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46219264
                    Iteration time: 12.38s
                        Total time: 36858.29s
                               ETA: 1269723.1s

################################################################################
                    [1m Learning iteration 2821/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.671s, learning 0.218s)
               Value function loss: 25.9979
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 257.90
               Mean episode length: 124.01
                  Mean reward/step: 2.09
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46235648
                    Iteration time: 11.89s
                        Total time: 36870.18s
                               ETA: 1269669.5s

################################################################################
                    [1m Learning iteration 2822/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.555s, learning 0.266s)
               Value function loss: 28.2561
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 254.04
               Mean episode length: 121.51
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46252032
                    Iteration time: 11.82s
                        Total time: 36882.00s
                               ETA: 1269613.7s

################################################################################
                    [1m Learning iteration 2823/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.662s, learning 0.167s)
               Value function loss: 33.1529
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 259.13
               Mean episode length: 123.90
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46268416
                    Iteration time: 11.83s
                        Total time: 36893.83s
                               ETA: 1269558.1s

################################################################################
                    [1m Learning iteration 2824/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.743s, learning 0.205s)
               Value function loss: 31.1014
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 252.40
               Mean episode length: 120.91
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46284800
                    Iteration time: 11.95s
                        Total time: 36905.78s
                               ETA: 1269506.6s

################################################################################
                    [1m Learning iteration 2825/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.806s, learning 0.181s)
               Value function loss: 27.5589
                    Surrogate loss: 0.0093
             Mean action noise std: 0.77
                       Mean reward: 258.10
               Mean episode length: 123.69
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46301184
                    Iteration time: 11.99s
                        Total time: 36917.77s
                               ETA: 1269456.5s

################################################################################
                    [1m Learning iteration 2826/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.755s, learning 0.190s)
               Value function loss: 25.4614
                    Surrogate loss: -0.0075
             Mean action noise std: 0.77
                       Mean reward: 248.85
               Mean episode length: 119.52
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46317568
                    Iteration time: 11.94s
                        Total time: 36929.71s
                               ETA: 1269405.0s

################################################################################
                    [1m Learning iteration 2827/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.747s, learning 0.198s)
               Value function loss: 28.1689
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 250.08
               Mean episode length: 120.97
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46333952
                    Iteration time: 11.95s
                        Total time: 36941.66s
                               ETA: 1269353.5s

################################################################################
                    [1m Learning iteration 2828/100000 [0m                    

                       Computation: 1314 steps/s (collection: 12.220s, learning 0.243s)
               Value function loss: 27.9314
                    Surrogate loss: -0.0072
             Mean action noise std: 0.77
                       Mean reward: 256.62
               Mean episode length: 122.98
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46350336
                    Iteration time: 12.46s
                        Total time: 36954.12s
                               ETA: 1269319.9s

################################################################################
                    [1m Learning iteration 2829/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.003s, learning 0.161s)
               Value function loss: 24.7559
                    Surrogate loss: -0.0083
             Mean action noise std: 0.77
                       Mean reward: 257.52
               Mean episode length: 123.65
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46366720
                    Iteration time: 12.16s
                        Total time: 36966.29s
                               ETA: 1269276.0s

################################################################################
                    [1m Learning iteration 2830/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.672s, learning 0.342s)
               Value function loss: 24.6250
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 250.44
               Mean episode length: 120.21
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46383104
                    Iteration time: 12.01s
                        Total time: 36978.30s
                               ETA: 1269227.0s

################################################################################
                    [1m Learning iteration 2831/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.918s, learning 0.193s)
               Value function loss: 30.5201
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 251.79
               Mean episode length: 120.92
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46399488
                    Iteration time: 12.11s
                        Total time: 36990.41s
                               ETA: 1269181.3s

################################################################################
                    [1m Learning iteration 2832/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.824s, learning 0.282s)
               Value function loss: 23.7254
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 257.99
               Mean episode length: 124.75
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46415872
                    Iteration time: 12.11s
                        Total time: 37002.52s
                               ETA: 1269135.4s

################################################################################
                    [1m Learning iteration 2833/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.815s, learning 0.162s)
               Value function loss: 28.2891
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 254.03
               Mean episode length: 123.07
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46432256
                    Iteration time: 11.98s
                        Total time: 37014.50s
                               ETA: 1269085.2s

################################################################################
                    [1m Learning iteration 2834/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.506s, learning 0.198s)
               Value function loss: 27.6086
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 251.86
               Mean episode length: 121.32
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46448640
                    Iteration time: 11.70s
                        Total time: 37026.20s
                               ETA: 1269025.6s

################################################################################
                    [1m Learning iteration 2835/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.536s, learning 0.171s)
               Value function loss: 22.3122
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 249.56
               Mean episode length: 121.05
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46465024
                    Iteration time: 11.71s
                        Total time: 37037.91s
                               ETA: 1268966.2s

################################################################################
                    [1m Learning iteration 2836/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.804s, learning 0.299s)
               Value function loss: 23.7280
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 255.28
               Mean episode length: 123.10
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46481408
                    Iteration time: 12.10s
                        Total time: 37050.01s
                               ETA: 1268920.4s

################################################################################
                    [1m Learning iteration 2837/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.011s, learning 0.158s)
               Value function loss: 26.4419
                    Surrogate loss: -0.0076
             Mean action noise std: 0.77
                       Mean reward: 254.82
               Mean episode length: 123.23
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46497792
                    Iteration time: 12.17s
                        Total time: 37062.18s
                               ETA: 1268876.8s

################################################################################
                    [1m Learning iteration 2838/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.749s, learning 0.210s)
               Value function loss: 22.2658
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 245.07
               Mean episode length: 119.34
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46514176
                    Iteration time: 11.96s
                        Total time: 37074.14s
                               ETA: 1268826.1s

################################################################################
                    [1m Learning iteration 2839/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.969s, learning 0.190s)
               Value function loss: 25.8480
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 247.15
               Mean episode length: 120.24
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46530560
                    Iteration time: 12.16s
                        Total time: 37086.30s
                               ETA: 1268782.2s

################################################################################
                    [1m Learning iteration 2840/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.691s, learning 0.238s)
               Value function loss: 25.1152
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 250.23
               Mean episode length: 120.16
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46546944
                    Iteration time: 11.93s
                        Total time: 37098.22s
                               ETA: 1268730.5s

################################################################################
                    [1m Learning iteration 2841/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.449s, learning 0.171s)
               Value function loss: 23.5731
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 253.78
               Mean episode length: 123.28
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46563328
                    Iteration time: 11.62s
                        Total time: 37109.84s
                               ETA: 1268668.3s

################################################################################
                    [1m Learning iteration 2842/100000 [0m                    

                       Computation: 1422 steps/s (collection: 11.318s, learning 0.196s)
               Value function loss: 22.3934
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 253.62
               Mean episode length: 121.74
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46579712
                    Iteration time: 11.51s
                        Total time: 37121.36s
                               ETA: 1268602.5s

################################################################################
                    [1m Learning iteration 2843/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.700s, learning 0.199s)
               Value function loss: 29.1833
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 249.90
               Mean episode length: 120.59
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46596096
                    Iteration time: 11.90s
                        Total time: 37133.26s
                               ETA: 1268549.9s

################################################################################
                    [1m Learning iteration 2844/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.860s, learning 0.217s)
               Value function loss: 23.6949
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 252.27
               Mean episode length: 121.24
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46612480
                    Iteration time: 12.08s
                        Total time: 37145.33s
                               ETA: 1268503.4s

################################################################################
                    [1m Learning iteration 2845/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.104s, learning 0.178s)
               Value function loss: 23.4253
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 256.12
               Mean episode length: 123.05
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46628864
                    Iteration time: 12.28s
                        Total time: 37157.62s
                               ETA: 1268463.9s

################################################################################
                    [1m Learning iteration 2846/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.045s, learning 0.194s)
               Value function loss: 25.6758
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 253.35
               Mean episode length: 122.17
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46645248
                    Iteration time: 12.24s
                        Total time: 37169.85s
                               ETA: 1268422.9s

################################################################################
                    [1m Learning iteration 2847/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.286s, learning 0.213s)
               Value function loss: 23.8473
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 252.74
               Mean episode length: 122.35
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46661632
                    Iteration time: 12.50s
                        Total time: 37182.35s
                               ETA: 1268390.9s

################################################################################
                    [1m Learning iteration 2848/100000 [0m                    

                       Computation: 1329 steps/s (collection: 12.159s, learning 0.165s)
               Value function loss: 24.6196
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 254.47
               Mean episode length: 122.13
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46678016
                    Iteration time: 12.32s
                        Total time: 37194.68s
                               ETA: 1268352.9s

################################################################################
                    [1m Learning iteration 2849/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.179s, learning 0.163s)
               Value function loss: 31.1271
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 251.00
               Mean episode length: 120.92
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46694400
                    Iteration time: 12.34s
                        Total time: 37207.02s
                               ETA: 1268315.5s

################################################################################
                    [1m Learning iteration 2850/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.739s, learning 0.209s)
               Value function loss: 27.0485
                    Surrogate loss: 0.0023
             Mean action noise std: 0.77
                       Mean reward: 251.77
               Mean episode length: 122.47
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46710784
                    Iteration time: 11.95s
                        Total time: 37218.97s
                               ETA: 1268264.8s

################################################################################
                    [1m Learning iteration 2851/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.715s, learning 0.189s)
               Value function loss: 21.8218
                    Surrogate loss: -0.0135
             Mean action noise std: 0.77
                       Mean reward: 246.72
               Mean episode length: 119.06
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46727168
                    Iteration time: 11.90s
                        Total time: 37230.87s
                               ETA: 1268212.5s

################################################################################
                    [1m Learning iteration 2852/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.520s, learning 0.353s)
               Value function loss: 20.9279
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 241.56
               Mean episode length: 115.82
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46743552
                    Iteration time: 11.87s
                        Total time: 37242.75s
                               ETA: 1268159.2s

################################################################################
                    [1m Learning iteration 2853/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.043s, learning 0.163s)
               Value function loss: 22.8668
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 258.47
               Mean episode length: 124.02
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46759936
                    Iteration time: 12.21s
                        Total time: 37254.95s
                               ETA: 1268117.3s

################################################################################
                    [1m Learning iteration 2854/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.818s, learning 0.181s)
               Value function loss: 20.9108
                    Surrogate loss: -0.0151
             Mean action noise std: 0.77
                       Mean reward: 255.34
               Mean episode length: 123.06
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46776320
                    Iteration time: 12.00s
                        Total time: 37266.95s
                               ETA: 1268068.4s

################################################################################
                    [1m Learning iteration 2855/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.653s, learning 0.230s)
               Value function loss: 24.8627
                    Surrogate loss: 0.0022
             Mean action noise std: 0.77
                       Mean reward: 257.81
               Mean episode length: 124.09
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46792704
                    Iteration time: 11.88s
                        Total time: 37278.84s
                               ETA: 1268015.6s

################################################################################
                    [1m Learning iteration 2856/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.972s, learning 0.217s)
               Value function loss: 21.0956
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 252.76
               Mean episode length: 122.05
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46809088
                    Iteration time: 12.19s
                        Total time: 37291.02s
                               ETA: 1267973.1s

################################################################################
                    [1m Learning iteration 2857/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.020s, learning 0.235s)
               Value function loss: 25.5150
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 247.51
               Mean episode length: 121.15
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46825472
                    Iteration time: 12.26s
                        Total time: 37303.28s
                               ETA: 1267933.0s

################################################################################
                    [1m Learning iteration 2858/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.174s, learning 0.211s)
               Value function loss: 23.1804
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 257.32
               Mean episode length: 124.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46841856
                    Iteration time: 12.38s
                        Total time: 37315.66s
                               ETA: 1267897.2s

################################################################################
                    [1m Learning iteration 2859/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.727s, learning 0.158s)
               Value function loss: 26.6644
                    Surrogate loss: -0.0105
             Mean action noise std: 0.77
                       Mean reward: 255.53
               Mean episode length: 122.30
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46858240
                    Iteration time: 11.88s
                        Total time: 37327.55s
                               ETA: 1267844.5s

################################################################################
                    [1m Learning iteration 2860/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.681s, learning 0.164s)
               Value function loss: 22.9655
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 249.88
               Mean episode length: 120.65
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46874624
                    Iteration time: 11.84s
                        Total time: 37339.39s
                               ETA: 1267790.5s

################################################################################
                    [1m Learning iteration 2861/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.755s, learning 0.237s)
               Value function loss: 21.1114
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 253.43
               Mean episode length: 122.72
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46891008
                    Iteration time: 11.99s
                        Total time: 37351.39s
                               ETA: 1267741.5s

################################################################################
                    [1m Learning iteration 2862/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.999s, learning 0.193s)
               Value function loss: 25.4215
                    Surrogate loss: -0.0035
             Mean action noise std: 0.77
                       Mean reward: 253.25
               Mean episode length: 123.50
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46907392
                    Iteration time: 12.19s
                        Total time: 37363.58s
                               ETA: 1267699.3s

################################################################################
                    [1m Learning iteration 2863/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.838s, learning 0.323s)
               Value function loss: 19.2988
                    Surrogate loss: -0.0144
             Mean action noise std: 0.77
                       Mean reward: 246.64
               Mean episode length: 121.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46923776
                    Iteration time: 12.16s
                        Total time: 37375.74s
                               ETA: 1267656.1s

################################################################################
                    [1m Learning iteration 2864/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.605s, learning 0.179s)
               Value function loss: 23.3813
                    Surrogate loss: -0.0087
             Mean action noise std: 0.77
                       Mean reward: 248.80
               Mean episode length: 121.43
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46940160
                    Iteration time: 11.78s
                        Total time: 37387.52s
                               ETA: 1267600.2s

################################################################################
                    [1m Learning iteration 2865/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.748s, learning 0.160s)
               Value function loss: 24.2797
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 255.49
               Mean episode length: 123.72
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.50
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46956544
                    Iteration time: 11.91s
                        Total time: 37399.43s
                               ETA: 1267548.4s

################################################################################
                    [1m Learning iteration 2866/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.723s, learning 0.192s)
               Value function loss: 25.9253
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 247.48
               Mean episode length: 121.69
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46972928
                    Iteration time: 11.91s
                        Total time: 37411.35s
                               ETA: 1267496.9s

################################################################################
                    [1m Learning iteration 2867/100000 [0m                    

                       Computation: 1341 steps/s (collection: 12.060s, learning 0.154s)
               Value function loss: 23.4539
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 248.14
               Mean episode length: 121.32
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 46989312
                    Iteration time: 12.21s
                        Total time: 37423.56s
                               ETA: 1267455.6s

################################################################################
                    [1m Learning iteration 2868/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.043s, learning 0.187s)
               Value function loss: 24.3020
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 239.31
               Mean episode length: 117.82
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47005696
                    Iteration time: 12.23s
                        Total time: 37435.79s
                               ETA: 1267414.8s

################################################################################
                    [1m Learning iteration 2869/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.210s, learning 0.265s)
               Value function loss: 23.0825
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 248.64
               Mean episode length: 120.62
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47022080
                    Iteration time: 12.48s
                        Total time: 37448.27s
                               ETA: 1267382.4s

################################################################################
                    [1m Learning iteration 2870/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.956s, learning 0.156s)
               Value function loss: 29.9446
                    Surrogate loss: 0.0007
             Mean action noise std: 0.77
                       Mean reward: 240.82
               Mean episode length: 118.09
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47038464
                    Iteration time: 12.11s
                        Total time: 37460.38s
                               ETA: 1267337.7s

################################################################################
                    [1m Learning iteration 2871/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.466s, learning 0.162s)
               Value function loss: 25.9037
                    Surrogate loss: -0.0048
             Mean action noise std: 0.77
                       Mean reward: 247.30
               Mean episode length: 120.21
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47054848
                    Iteration time: 11.63s
                        Total time: 37472.01s
                               ETA: 1267276.6s

################################################################################
                    [1m Learning iteration 2872/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.669s, learning 0.167s)
               Value function loss: 23.6346
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 246.80
               Mean episode length: 121.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47071232
                    Iteration time: 11.84s
                        Total time: 37483.84s
                               ETA: 1267222.6s

################################################################################
                    [1m Learning iteration 2873/100000 [0m                    

                       Computation: 1333 steps/s (collection: 12.030s, learning 0.252s)
               Value function loss: 23.2595
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 237.79
               Mean episode length: 116.68
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47087616
                    Iteration time: 12.28s
                        Total time: 37496.12s
                               ETA: 1267183.7s

################################################################################
                    [1m Learning iteration 2874/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.637s, learning 0.188s)
               Value function loss: 26.6293
                    Surrogate loss: -0.0062
             Mean action noise std: 0.77
                       Mean reward: 244.63
               Mean episode length: 120.69
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47104000
                    Iteration time: 11.82s
                        Total time: 37507.95s
                               ETA: 1267129.4s

################################################################################
                    [1m Learning iteration 2875/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.624s, learning 0.187s)
               Value function loss: 26.0348
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 251.66
               Mean episode length: 122.91
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47120384
                    Iteration time: 11.81s
                        Total time: 37519.76s
                               ETA: 1267074.6s

################################################################################
                    [1m Learning iteration 2876/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.729s, learning 0.265s)
               Value function loss: 22.0616
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 245.90
               Mean episode length: 121.27
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47136768
                    Iteration time: 11.99s
                        Total time: 37531.75s
                               ETA: 1267026.0s

################################################################################
                    [1m Learning iteration 2877/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.895s, learning 0.159s)
               Value function loss: 22.4743
                    Surrogate loss: -0.0136
             Mean action noise std: 0.77
                       Mean reward: 248.60
               Mean episode length: 121.79
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47153152
                    Iteration time: 12.05s
                        Total time: 37543.81s
                               ETA: 1266979.5s

################################################################################
                    [1m Learning iteration 2878/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.922s, learning 0.193s)
               Value function loss: 26.5480
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 252.36
               Mean episode length: 122.26
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47169536
                    Iteration time: 12.12s
                        Total time: 37555.92s
                               ETA: 1266935.1s

################################################################################
                    [1m Learning iteration 2879/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.963s, learning 0.236s)
               Value function loss: 19.0065
                    Surrogate loss: -0.0165
             Mean action noise std: 0.77
                       Mean reward: 251.64
               Mean episode length: 123.13
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.66
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47185920
                    Iteration time: 12.20s
                        Total time: 37568.12s
                               ETA: 1266893.6s

################################################################################
                    [1m Learning iteration 2880/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.988s, learning 0.167s)
               Value function loss: 25.5222
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 251.18
               Mean episode length: 121.26
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47202304
                    Iteration time: 12.16s
                        Total time: 37580.28s
                               ETA: 1266850.6s

################################################################################
                    [1m Learning iteration 2881/100000 [0m                    

                       Computation: 1317 steps/s (collection: 12.269s, learning 0.164s)
               Value function loss: 24.4744
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 248.91
               Mean episode length: 121.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47218688
                    Iteration time: 12.43s
                        Total time: 37592.71s
                               ETA: 1266816.9s

################################################################################
                    [1m Learning iteration 2882/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.835s, learning 0.261s)
               Value function loss: 22.6403
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 251.79
               Mean episode length: 122.51
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47235072
                    Iteration time: 12.10s
                        Total time: 37604.80s
                               ETA: 1266771.9s

################################################################################
                    [1m Learning iteration 2883/100000 [0m                    

                       Computation: 1321 steps/s (collection: 12.225s, learning 0.176s)
               Value function loss: 25.9401
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 242.34
               Mean episode length: 119.14
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47251456
                    Iteration time: 12.40s
                        Total time: 37617.21s
                               ETA: 1266737.2s

################################################################################
                    [1m Learning iteration 2884/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.875s, learning 0.157s)
               Value function loss: 24.3238
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 248.29
               Mean episode length: 121.18
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47267840
                    Iteration time: 12.03s
                        Total time: 37629.24s
                               ETA: 1266690.1s

################################################################################
                    [1m Learning iteration 2885/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.645s, learning 0.165s)
               Value function loss: 22.7813
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 249.79
               Mean episode length: 122.57
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47284224
                    Iteration time: 11.81s
                        Total time: 37641.05s
                               ETA: 1266635.6s

################################################################################
                    [1m Learning iteration 2886/100000 [0m                    

                       Computation: 1386 steps/s (collection: 11.651s, learning 0.167s)
               Value function loss: 23.7611
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 247.53
               Mean episode length: 121.78
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47300608
                    Iteration time: 11.82s
                        Total time: 37652.87s
                               ETA: 1266581.4s

################################################################################
                    [1m Learning iteration 2887/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.584s, learning 0.209s)
               Value function loss: 24.2245
                    Surrogate loss: 0.0013
             Mean action noise std: 0.77
                       Mean reward: 241.45
               Mean episode length: 119.63
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47316992
                    Iteration time: 11.79s
                        Total time: 37664.66s
                               ETA: 1266526.3s

################################################################################
                    [1m Learning iteration 2888/100000 [0m                    

                       Computation: 1410 steps/s (collection: 11.452s, learning 0.168s)
               Value function loss: 25.1485
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 250.74
               Mean episode length: 122.38
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47333376
                    Iteration time: 11.62s
                        Total time: 37676.28s
                               ETA: 1266465.5s

################################################################################
                    [1m Learning iteration 2889/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.753s, learning 0.168s)
               Value function loss: 22.8943
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 247.36
               Mean episode length: 120.69
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47349760
                    Iteration time: 11.92s
                        Total time: 37688.20s
                               ETA: 1266414.8s

################################################################################
                    [1m Learning iteration 2890/100000 [0m                    

                       Computation: 1342 steps/s (collection: 11.985s, learning 0.223s)
               Value function loss: 28.0495
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 250.63
               Mean episode length: 123.35
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.48
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47366144
                    Iteration time: 12.21s
                        Total time: 37700.41s
                               ETA: 1266373.8s

################################################################################
                    [1m Learning iteration 2891/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.782s, learning 0.299s)
               Value function loss: 25.8207
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 246.75
               Mean episode length: 121.62
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47382528
                    Iteration time: 12.08s
                        Total time: 37712.49s
                               ETA: 1266328.5s

################################################################################
                    [1m Learning iteration 2892/100000 [0m                    

                       Computation: 1383 steps/s (collection: 11.601s, learning 0.246s)
               Value function loss: 23.4470
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 244.14
               Mean episode length: 120.44
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47398912
                    Iteration time: 11.85s
                        Total time: 37724.34s
                               ETA: 1266275.4s

################################################################################
                    [1m Learning iteration 2893/100000 [0m                    

                       Computation: 1308 steps/s (collection: 12.255s, learning 0.262s)
               Value function loss: 22.6791
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 255.51
               Mean episode length: 123.10
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47415296
                    Iteration time: 12.52s
                        Total time: 37736.85s
                               ETA: 1266244.8s

################################################################################
                    [1m Learning iteration 2894/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.906s, learning 0.160s)
               Value function loss: 21.3349
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 246.71
               Mean episode length: 121.04
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47431680
                    Iteration time: 12.07s
                        Total time: 37748.92s
                               ETA: 1266199.2s

################################################################################
                    [1m Learning iteration 2895/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.474s, learning 0.183s)
               Value function loss: 25.2435
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 247.74
               Mean episode length: 121.20
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47448064
                    Iteration time: 11.66s
                        Total time: 37760.58s
                               ETA: 1266139.8s

################################################################################
                    [1m Learning iteration 2896/100000 [0m                    

                       Computation: 1324 steps/s (collection: 12.178s, learning 0.191s)
               Value function loss: 27.3247
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 248.10
               Mean episode length: 122.21
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47464448
                    Iteration time: 12.37s
                        Total time: 37772.95s
                               ETA: 1266104.3s

################################################################################
                    [1m Learning iteration 2897/100000 [0m                    

                       Computation: 1298 steps/s (collection: 12.415s, learning 0.203s)
               Value function loss: 25.6757
                    Surrogate loss: -0.0036
             Mean action noise std: 0.77
                       Mean reward: 253.65
               Mean episode length: 124.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47480832
                    Iteration time: 12.62s
                        Total time: 37785.56s
                               ETA: 1266077.1s

################################################################################
                    [1m Learning iteration 2898/100000 [0m                    

                       Computation: 1358 steps/s (collection: 11.898s, learning 0.165s)
               Value function loss: 20.5799
                    Surrogate loss: -0.0126
             Mean action noise std: 0.77
                       Mean reward: 247.60
               Mean episode length: 120.50
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47497216
                    Iteration time: 12.06s
                        Total time: 37797.63s
                               ETA: 1266031.5s

################################################################################
                    [1m Learning iteration 2899/100000 [0m                    

                       Computation: 1325 steps/s (collection: 12.117s, learning 0.246s)
               Value function loss: 24.8096
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 252.20
               Mean episode length: 123.14
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47513600
                    Iteration time: 12.36s
                        Total time: 37809.99s
                               ETA: 1265995.8s

################################################################################
                    [1m Learning iteration 2900/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.023s, learning 0.162s)
               Value function loss: 24.6036
                    Surrogate loss: -0.0147
             Mean action noise std: 0.77
                       Mean reward: 255.07
               Mean episode length: 123.94
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47529984
                    Iteration time: 12.18s
                        Total time: 37822.17s
                               ETA: 1265954.2s

################################################################################
                    [1m Learning iteration 2901/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.520s, learning 0.246s)
               Value function loss: 24.6505
                    Surrogate loss: 0.0013
             Mean action noise std: 0.77
                       Mean reward: 250.51
               Mean episode length: 121.37
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47546368
                    Iteration time: 11.77s
                        Total time: 37833.94s
                               ETA: 1265898.6s

################################################################################
                    [1m Learning iteration 2902/100000 [0m                    

                       Computation: 1319 steps/s (collection: 12.254s, learning 0.164s)
               Value function loss: 25.8783
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 252.65
               Mean episode length: 123.05
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47562752
                    Iteration time: 12.42s
                        Total time: 37846.36s
                               ETA: 1265864.9s

################################################################################
                    [1m Learning iteration 2903/100000 [0m                    

                       Computation: 1345 steps/s (collection: 12.017s, learning 0.161s)
               Value function loss: 23.4567
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 252.23
               Mean episode length: 122.78
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47579136
                    Iteration time: 12.18s
                        Total time: 37858.54s
                               ETA: 1265823.2s

################################################################################
                    [1m Learning iteration 2904/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.490s, learning 0.156s)
               Value function loss: 25.0245
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 243.66
               Mean episode length: 118.09
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47595520
                    Iteration time: 11.65s
                        Total time: 37870.18s
                               ETA: 1265763.7s

################################################################################
                    [1m Learning iteration 2905/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.536s, learning 0.161s)
               Value function loss: 23.4129
                    Surrogate loss: -0.0143
             Mean action noise std: 0.77
                       Mean reward: 254.64
               Mean episode length: 123.42
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47611904
                    Iteration time: 11.70s
                        Total time: 37881.88s
                               ETA: 1265705.9s

################################################################################
                    [1m Learning iteration 2906/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.151s, learning 0.197s)
               Value function loss: 28.1980
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 250.76
               Mean episode length: 121.24
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47628288
                    Iteration time: 12.35s
                        Total time: 37894.23s
                               ETA: 1265669.8s

################################################################################
                    [1m Learning iteration 2907/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.856s, learning 0.158s)
               Value function loss: 25.2798
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 250.85
               Mean episode length: 121.16
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47644672
                    Iteration time: 12.01s
                        Total time: 37906.24s
                               ETA: 1265622.7s

################################################################################
                    [1m Learning iteration 2908/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.749s, learning 0.174s)
               Value function loss: 20.0495
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 253.22
               Mean episode length: 122.17
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47661056
                    Iteration time: 11.92s
                        Total time: 37918.16s
                               ETA: 1265572.5s

################################################################################
                    [1m Learning iteration 2909/100000 [0m                    

                       Computation: 1375 steps/s (collection: 11.646s, learning 0.267s)
               Value function loss: 24.3193
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 254.68
               Mean episode length: 121.99
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47677440
                    Iteration time: 11.91s
                        Total time: 37930.08s
                               ETA: 1265522.1s

################################################################################
                    [1m Learning iteration 2910/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.839s, learning 0.280s)
               Value function loss: 20.3172
                    Surrogate loss: -0.0160
             Mean action noise std: 0.77
                       Mean reward: 249.36
               Mean episode length: 120.75
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47693824
                    Iteration time: 12.12s
                        Total time: 37942.20s
                               ETA: 1265478.5s

################################################################################
                    [1m Learning iteration 2911/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.787s, learning 0.175s)
               Value function loss: 22.5716
                    Surrogate loss: -0.0140
             Mean action noise std: 0.77
                       Mean reward: 254.97
               Mean episode length: 123.06
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47710208
                    Iteration time: 11.96s
                        Total time: 37954.16s
                               ETA: 1265429.7s

################################################################################
                    [1m Learning iteration 2912/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.083s, learning 0.196s)
               Value function loss: 24.8216
                    Surrogate loss: -0.0117
             Mean action noise std: 0.77
                       Mean reward: 252.15
               Mean episode length: 121.19
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47726592
                    Iteration time: 12.28s
                        Total time: 37966.44s
                               ETA: 1265391.5s

################################################################################
                    [1m Learning iteration 2913/100000 [0m                    

                       Computation: 1342 steps/s (collection: 12.028s, learning 0.174s)
               Value function loss: 22.7652
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 248.02
               Mean episode length: 120.96
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47742976
                    Iteration time: 12.20s
                        Total time: 37978.64s
                               ETA: 1265350.8s

################################################################################
                    [1m Learning iteration 2914/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.838s, learning 0.199s)
               Value function loss: 23.6085
                    Surrogate loss: -0.0091
             Mean action noise std: 0.77
                       Mean reward: 246.29
               Mean episode length: 119.03
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47759360
                    Iteration time: 12.04s
                        Total time: 37990.68s
                               ETA: 1265304.6s

################################################################################
                    [1m Learning iteration 2915/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.677s, learning 0.198s)
               Value function loss: 23.8423
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 251.22
               Mean episode length: 121.86
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47775744
                    Iteration time: 11.87s
                        Total time: 38002.55s
                               ETA: 1265253.0s

################################################################################
                    [1m Learning iteration 2916/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.271s, learning 0.205s)
               Value function loss: 20.6353
                    Surrogate loss: -0.0169
             Mean action noise std: 0.77
                       Mean reward: 247.11
               Mean episode length: 120.21
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47792128
                    Iteration time: 12.48s
                        Total time: 38015.03s
                               ETA: 1265221.4s

################################################################################
                    [1m Learning iteration 2917/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.982s, learning 0.174s)
               Value function loss: 26.3156
                    Surrogate loss: -0.0104
             Mean action noise std: 0.77
                       Mean reward: 257.86
               Mean episode length: 124.05
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47808512
                    Iteration time: 12.16s
                        Total time: 38027.18s
                               ETA: 1265179.2s

################################################################################
                    [1m Learning iteration 2918/100000 [0m                    

                       Computation: 1401 steps/s (collection: 11.514s, learning 0.175s)
               Value function loss: 26.3007
                    Surrogate loss: -0.0024
             Mean action noise std: 0.77
                       Mean reward: 246.05
               Mean episode length: 120.55
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.49
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47824896
                    Iteration time: 11.69s
                        Total time: 38038.87s
                               ETA: 1265121.5s

################################################################################
                    [1m Learning iteration 2919/100000 [0m                    

                       Computation: 1376 steps/s (collection: 11.716s, learning 0.187s)
               Value function loss: 25.3478
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 248.28
               Mean episode length: 120.22
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47841280
                    Iteration time: 11.90s
                        Total time: 38050.78s
                               ETA: 1265071.0s

################################################################################
                    [1m Learning iteration 2920/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.905s, learning 0.313s)
               Value function loss: 25.9962
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 248.93
               Mean episode length: 120.88
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47857664
                    Iteration time: 12.22s
                        Total time: 38062.99s
                               ETA: 1265030.9s

################################################################################
                    [1m Learning iteration 2921/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.963s, learning 0.160s)
               Value function loss: 25.5009
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 253.79
               Mean episode length: 122.98
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47874048
                    Iteration time: 12.12s
                        Total time: 38075.12s
                               ETA: 1264987.7s

################################################################################
                    [1m Learning iteration 2922/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.499s, learning 0.158s)
               Value function loss: 25.6412
                    Surrogate loss: -0.0153
             Mean action noise std: 0.77
                       Mean reward: 253.70
               Mean episode length: 122.88
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47890432
                    Iteration time: 11.66s
                        Total time: 38086.77s
                               ETA: 1264929.1s

################################################################################
                    [1m Learning iteration 2923/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.837s, learning 0.192s)
               Value function loss: 22.6368
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 251.67
               Mean episode length: 121.19
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47906816
                    Iteration time: 12.03s
                        Total time: 38098.80s
                               ETA: 1264882.8s

################################################################################
                    [1m Learning iteration 2924/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.898s, learning 0.168s)
               Value function loss: 21.3421
                    Surrogate loss: -0.0154
             Mean action noise std: 0.77
                       Mean reward: 249.19
               Mean episode length: 121.81
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47923200
                    Iteration time: 12.07s
                        Total time: 38110.87s
                               ETA: 1264837.8s

################################################################################
                    [1m Learning iteration 2925/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.707s, learning 0.169s)
               Value function loss: 26.0516
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 246.79
               Mean episode length: 120.12
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47939584
                    Iteration time: 11.88s
                        Total time: 38122.74s
                               ETA: 1264786.5s

################################################################################
                    [1m Learning iteration 2926/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.715s, learning 0.158s)
               Value function loss: 23.5972
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 245.94
               Mean episode length: 119.22
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47955968
                    Iteration time: 11.87s
                        Total time: 38134.62s
                               ETA: 1264735.2s

################################################################################
                    [1m Learning iteration 2927/100000 [0m                    

                       Computation: 1404 steps/s (collection: 11.437s, learning 0.225s)
               Value function loss: 24.8370
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 243.05
               Mean episode length: 119.05
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47972352
                    Iteration time: 11.66s
                        Total time: 38146.28s
                               ETA: 1264676.8s

################################################################################
                    [1m Learning iteration 2928/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.453s, learning 0.158s)
               Value function loss: 20.7974
                    Surrogate loss: -0.0133
             Mean action noise std: 0.77
                       Mean reward: 252.16
               Mean episode length: 121.97
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 47988736
                    Iteration time: 11.61s
                        Total time: 38157.89s
                               ETA: 1264616.8s

################################################################################
                    [1m Learning iteration 2929/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.030s, learning 0.233s)
               Value function loss: 20.3150
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 250.68
               Mean episode length: 121.73
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48005120
                    Iteration time: 12.26s
                        Total time: 38170.15s
                               ETA: 1264578.5s

################################################################################
                    [1m Learning iteration 2930/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.657s, learning 0.165s)
               Value function loss: 22.1018
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 250.57
               Mean episode length: 120.31
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48021504
                    Iteration time: 11.82s
                        Total time: 38181.98s
                               ETA: 1264525.6s

################################################################################
                    [1m Learning iteration 2931/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.524s, learning 0.161s)
               Value function loss: 22.0135
                    Surrogate loss: -0.0125
             Mean action noise std: 0.77
                       Mean reward: 247.84
               Mean episode length: 120.89
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48037888
                    Iteration time: 11.68s
                        Total time: 38193.66s
                               ETA: 1264468.1s

################################################################################
                    [1m Learning iteration 2932/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.797s, learning 0.210s)
               Value function loss: 20.4628
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 251.76
               Mean episode length: 121.15
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48054272
                    Iteration time: 12.01s
                        Total time: 38205.67s
                               ETA: 1264421.3s

################################################################################
                    [1m Learning iteration 2933/100000 [0m                    

                       Computation: 1341 steps/s (collection: 11.950s, learning 0.263s)
               Value function loss: 23.9187
                    Surrogate loss: -0.0044
             Mean action noise std: 0.77
                       Mean reward: 245.13
               Mean episode length: 119.08
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48070656
                    Iteration time: 12.21s
                        Total time: 38217.88s
                               ETA: 1264381.4s

################################################################################
                    [1m Learning iteration 2934/100000 [0m                    

                       Computation: 1350 steps/s (collection: 11.826s, learning 0.309s)
               Value function loss: 22.4968
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 257.89
               Mean episode length: 124.58
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48087040
                    Iteration time: 12.13s
                        Total time: 38230.01s
                               ETA: 1264338.9s

################################################################################
                    [1m Learning iteration 2935/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.982s, learning 0.194s)
               Value function loss: 24.8748
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 256.30
               Mean episode length: 124.31
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48103424
                    Iteration time: 12.18s
                        Total time: 38242.19s
                               ETA: 1264297.8s

################################################################################
                    [1m Learning iteration 2936/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.090s, learning 0.160s)
               Value function loss: 20.6366
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 247.53
               Mean episode length: 120.21
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48119808
                    Iteration time: 12.25s
                        Total time: 38254.44s
                               ETA: 1264259.1s

################################################################################
                    [1m Learning iteration 2937/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.735s, learning 0.188s)
               Value function loss: 27.3586
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 250.53
               Mean episode length: 121.84
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.45
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48136192
                    Iteration time: 11.92s
                        Total time: 38266.36s
                               ETA: 1264209.7s

################################################################################
                    [1m Learning iteration 2938/100000 [0m                    

                       Computation: 1322 steps/s (collection: 12.189s, learning 0.199s)
               Value function loss: 21.2058
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 249.67
               Mean episode length: 121.36
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48152576
                    Iteration time: 12.39s
                        Total time: 38278.75s
                               ETA: 1264175.7s

################################################################################
                    [1m Learning iteration 2939/100000 [0m                    

                       Computation: 1372 steps/s (collection: 11.756s, learning 0.177s)
               Value function loss: 21.3076
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 250.29
               Mean episode length: 121.00
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48168960
                    Iteration time: 11.93s
                        Total time: 38290.69s
                               ETA: 1264126.6s

################################################################################
                    [1m Learning iteration 2940/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.011s, learning 0.212s)
               Value function loss: 24.7497
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 249.48
               Mean episode length: 120.36
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48185344
                    Iteration time: 12.22s
                        Total time: 38302.91s
                               ETA: 1264087.2s

################################################################################
                    [1m Learning iteration 2941/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.675s, learning 0.183s)
               Value function loss: 23.8705
                    Surrogate loss: 0.0086
             Mean action noise std: 0.77
                       Mean reward: 240.05
               Mean episode length: 117.24
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48201728
                    Iteration time: 11.86s
                        Total time: 38314.77s
                               ETA: 1264035.7s

################################################################################
                    [1m Learning iteration 2942/100000 [0m                    

                       Computation: 1311 steps/s (collection: 12.124s, learning 0.366s)
               Value function loss: 24.7401
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 250.85
               Mean episode length: 121.40
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48218112
                    Iteration time: 12.49s
                        Total time: 38327.26s
                               ETA: 1264005.0s

################################################################################
                    [1m Learning iteration 2943/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.080s, learning 0.184s)
               Value function loss: 25.8498
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 246.70
               Mean episode length: 120.76
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48234496
                    Iteration time: 12.26s
                        Total time: 38339.52s
                               ETA: 1263967.0s

################################################################################
                    [1m Learning iteration 2944/100000 [0m                    

                       Computation: 1349 steps/s (collection: 11.950s, learning 0.190s)
               Value function loss: 21.9021
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 252.94
               Mean episode length: 121.42
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48250880
                    Iteration time: 12.14s
                        Total time: 38351.66s
                               ETA: 1263924.8s

################################################################################
                    [1m Learning iteration 2945/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.840s, learning 0.226s)
               Value function loss: 23.9877
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 247.32
               Mean episode length: 119.46
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48267264
                    Iteration time: 12.07s
                        Total time: 38363.73s
                               ETA: 1263880.3s

################################################################################
                    [1m Learning iteration 2946/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.102s, learning 0.168s)
               Value function loss: 23.5700
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 254.37
               Mean episode length: 123.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48283648
                    Iteration time: 12.27s
                        Total time: 38376.00s
                               ETA: 1263842.5s

################################################################################
                    [1m Learning iteration 2947/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.679s, learning 0.199s)
               Value function loss: 21.7745
                    Surrogate loss: -0.0053
             Mean action noise std: 0.77
                       Mean reward: 253.46
               Mean episode length: 121.60
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48300032
                    Iteration time: 11.88s
                        Total time: 38387.87s
                               ETA: 1263791.8s

################################################################################
                    [1m Learning iteration 2948/100000 [0m                    

                       Computation: 1454 steps/s (collection: 11.074s, learning 0.192s)
               Value function loss: 25.1376
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 250.24
               Mean episode length: 121.12
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48316416
                    Iteration time: 11.27s
                        Total time: 38399.14s
                               ETA: 1263721.0s

################################################################################
                    [1m Learning iteration 2949/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.024s, learning 0.208s)
               Value function loss: 27.4171
                    Surrogate loss: -0.0039
             Mean action noise std: 0.77
                       Mean reward: 254.97
               Mean episode length: 123.83
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48332800
                    Iteration time: 12.23s
                        Total time: 38411.37s
                               ETA: 1263682.0s

################################################################################
                    [1m Learning iteration 2950/100000 [0m                    

                       Computation: 1334 steps/s (collection: 12.105s, learning 0.170s)
               Value function loss: 25.8907
                    Surrogate loss: -0.0089
             Mean action noise std: 0.77
                       Mean reward: 256.30
               Mean episode length: 122.08
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48349184
                    Iteration time: 12.28s
                        Total time: 38423.65s
                               ETA: 1263644.5s

################################################################################
                    [1m Learning iteration 2951/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.952s, learning 0.208s)
               Value function loss: 24.2175
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 258.60
               Mean episode length: 123.62
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48365568
                    Iteration time: 12.16s
                        Total time: 38435.81s
                               ETA: 1263603.2s

################################################################################
                    [1m Learning iteration 2952/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.944s, learning 0.216s)
               Value function loss: 24.7302
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 244.36
               Mean episode length: 117.92
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48381952
                    Iteration time: 12.16s
                        Total time: 38447.97s
                               ETA: 1263561.9s

################################################################################
                    [1m Learning iteration 2953/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.855s, learning 0.301s)
               Value function loss: 24.3775
                    Surrogate loss: -0.0157
             Mean action noise std: 0.77
                       Mean reward: 248.97
               Mean episode length: 119.97
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48398336
                    Iteration time: 12.16s
                        Total time: 38460.12s
                               ETA: 1263520.5s

################################################################################
                    [1m Learning iteration 2954/100000 [0m                    

                       Computation: 1327 steps/s (collection: 12.168s, learning 0.179s)
               Value function loss: 23.5641
                    Surrogate loss: -0.0059
             Mean action noise std: 0.77
                       Mean reward: 258.58
               Mean episode length: 123.04
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48414720
                    Iteration time: 12.35s
                        Total time: 38472.47s
                               ETA: 1263485.4s

################################################################################
                    [1m Learning iteration 2955/100000 [0m                    

                       Computation: 1355 steps/s (collection: 11.922s, learning 0.164s)
               Value function loss: 23.0756
                    Surrogate loss: -0.0056
             Mean action noise std: 0.77
                       Mean reward: 248.98
               Mean episode length: 120.54
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48431104
                    Iteration time: 12.09s
                        Total time: 38484.56s
                               ETA: 1263441.7s

################################################################################
                    [1m Learning iteration 2956/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.935s, learning 0.175s)
               Value function loss: 29.0547
                    Surrogate loss: -0.0045
             Mean action noise std: 0.77
                       Mean reward: 252.85
               Mean episode length: 122.18
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48447488
                    Iteration time: 12.11s
                        Total time: 38496.66s
                               ETA: 1263398.8s

################################################################################
                    [1m Learning iteration 2957/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.771s, learning 0.198s)
               Value function loss: 25.7495
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 253.20
               Mean episode length: 122.31
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48463872
                    Iteration time: 11.97s
                        Total time: 38508.63s
                               ETA: 1263351.4s

################################################################################
                    [1m Learning iteration 2958/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.870s, learning 0.160s)
               Value function loss: 30.0360
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 256.25
               Mean episode length: 123.05
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48480256
                    Iteration time: 12.03s
                        Total time: 38520.66s
                               ETA: 1263305.9s

################################################################################
                    [1m Learning iteration 2959/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.632s, learning 0.158s)
               Value function loss: 28.2361
                    Surrogate loss: 0.0015
             Mean action noise std: 0.77
                       Mean reward: 251.54
               Mean episode length: 121.13
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48496640
                    Iteration time: 11.79s
                        Total time: 38532.45s
                               ETA: 1263252.6s

################################################################################
                    [1m Learning iteration 2960/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.698s, learning 0.176s)
               Value function loss: 24.3826
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 253.17
               Mean episode length: 122.26
                  Mean reward/step: 2.08
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48513024
                    Iteration time: 11.87s
                        Total time: 38544.33s
                               ETA: 1263202.1s

################################################################################
                    [1m Learning iteration 2961/100000 [0m                    

                       Computation: 1425 steps/s (collection: 11.294s, learning 0.201s)
               Value function loss: 30.6025
                    Surrogate loss: -0.0008
             Mean action noise std: 0.77
                       Mean reward: 257.99
               Mean episode length: 123.23
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48529408
                    Iteration time: 11.50s
                        Total time: 38555.82s
                               ETA: 1263139.2s

################################################################################
                    [1m Learning iteration 2962/100000 [0m                    

                       Computation: 1391 steps/s (collection: 11.613s, learning 0.164s)
               Value function loss: 27.7660
                    Surrogate loss: 0.0053
             Mean action noise std: 0.77
                       Mean reward: 247.51
               Mean episode length: 119.44
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48545792
                    Iteration time: 11.78s
                        Total time: 38567.60s
                               ETA: 1263085.6s

################################################################################
                    [1m Learning iteration 2963/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.630s, learning 0.158s)
               Value function loss: 25.7300
                    Surrogate loss: -0.0042
             Mean action noise std: 0.77
                       Mean reward: 250.08
               Mean episode length: 120.86
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48562176
                    Iteration time: 11.79s
                        Total time: 38579.39s
                               ETA: 1263032.4s

################################################################################
                    [1m Learning iteration 2964/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.787s, learning 0.161s)
               Value function loss: 31.7275
                    Surrogate loss: -0.0033
             Mean action noise std: 0.77
                       Mean reward: 258.35
               Mean episode length: 124.07
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48578560
                    Iteration time: 11.95s
                        Total time: 38591.34s
                               ETA: 1262984.4s

################################################################################
                    [1m Learning iteration 2965/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.657s, learning 0.197s)
               Value function loss: 27.6896
                    Surrogate loss: -0.0034
             Mean action noise std: 0.77
                       Mean reward: 252.95
               Mean episode length: 122.10
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48594944
                    Iteration time: 11.85s
                        Total time: 38603.19s
                               ETA: 1262933.4s

################################################################################
                    [1m Learning iteration 2966/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.630s, learning 0.163s)
               Value function loss: 26.6166
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 248.66
               Mean episode length: 119.99
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48611328
                    Iteration time: 11.79s
                        Total time: 38614.98s
                               ETA: 1262880.4s

################################################################################
                    [1m Learning iteration 2967/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.031s, learning 0.239s)
               Value function loss: 27.5562
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 251.43
               Mean episode length: 121.21
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48627712
                    Iteration time: 12.27s
                        Total time: 38627.25s
                               ETA: 1262843.1s

################################################################################
                    [1m Learning iteration 2968/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.839s, learning 0.168s)
               Value function loss: 29.7616
                    Surrogate loss: -0.0107
             Mean action noise std: 0.77
                       Mean reward: 254.17
               Mean episode length: 122.51
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48644096
                    Iteration time: 12.01s
                        Total time: 38639.26s
                               ETA: 1262797.1s

################################################################################
                    [1m Learning iteration 2969/100000 [0m                    

                       Computation: 1351 steps/s (collection: 11.918s, learning 0.206s)
               Value function loss: 23.2475
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 252.41
               Mean episode length: 122.93
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48660480
                    Iteration time: 12.12s
                        Total time: 38651.38s
                               ETA: 1262755.0s

################################################################################
                    [1m Learning iteration 2970/100000 [0m                    

                       Computation: 1354 steps/s (collection: 11.941s, learning 0.157s)
               Value function loss: 24.4513
                    Surrogate loss: -0.0054
             Mean action noise std: 0.77
                       Mean reward: 256.16
               Mean episode length: 123.56
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.63
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48676864
                    Iteration time: 12.10s
                        Total time: 38663.48s
                               ETA: 1262712.1s

################################################################################
                    [1m Learning iteration 2971/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.543s, learning 0.171s)
               Value function loss: 25.4123
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 246.90
               Mean episode length: 120.53
                  Mean reward/step: 2.07
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48693248
                    Iteration time: 11.71s
                        Total time: 38675.20s
                               ETA: 1262656.7s

################################################################################
                    [1m Learning iteration 2972/100000 [0m                    

                       Computation: 1315 steps/s (collection: 12.249s, learning 0.207s)
               Value function loss: 30.5917
                    Surrogate loss: -0.0070
             Mean action noise std: 0.77
                       Mean reward: 253.70
               Mean episode length: 122.72
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48709632
                    Iteration time: 12.46s
                        Total time: 38687.65s
                               ETA: 1262625.5s

################################################################################
                    [1m Learning iteration 2973/100000 [0m                    

                       Computation: 1379 steps/s (collection: 11.713s, learning 0.161s)
               Value function loss: 28.7874
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 252.04
               Mean episode length: 121.49
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48726016
                    Iteration time: 11.87s
                        Total time: 38699.53s
                               ETA: 1262575.3s

################################################################################
                    [1m Learning iteration 2974/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.429s, learning 0.164s)
               Value function loss: 35.7911
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 258.99
               Mean episode length: 124.53
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48742400
                    Iteration time: 11.59s
                        Total time: 38711.12s
                               ETA: 1262516.0s

################################################################################
                    [1m Learning iteration 2975/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.850s, learning 0.166s)
               Value function loss: 36.1740
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 244.00
               Mean episode length: 118.76
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48758784
                    Iteration time: 12.02s
                        Total time: 38723.13s
                               ETA: 1262470.5s

################################################################################
                    [1m Learning iteration 2976/100000 [0m                    

                       Computation: 1406 steps/s (collection: 11.395s, learning 0.258s)
               Value function loss: 32.4387
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 258.72
               Mean episode length: 123.60
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48775168
                    Iteration time: 11.65s
                        Total time: 38734.79s
                               ETA: 1262413.1s

################################################################################
                    [1m Learning iteration 2977/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.622s, learning 0.165s)
               Value function loss: 31.4042
                    Surrogate loss: -0.0088
             Mean action noise std: 0.77
                       Mean reward: 249.53
               Mean episode length: 120.86
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48791552
                    Iteration time: 11.79s
                        Total time: 38746.57s
                               ETA: 1262360.2s

################################################################################
                    [1m Learning iteration 2978/100000 [0m                    

                       Computation: 1439 steps/s (collection: 11.191s, learning 0.193s)
               Value function loss: 26.2863
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 252.47
               Mean episode length: 122.28
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48807936
                    Iteration time: 11.38s
                        Total time: 38757.96s
                               ETA: 1262294.2s

################################################################################
                    [1m Learning iteration 2979/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.154s, learning 0.254s)
               Value function loss: 26.4634
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 250.22
               Mean episode length: 121.23
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48824320
                    Iteration time: 12.41s
                        Total time: 38770.37s
                               ETA: 1262261.6s

################################################################################
                    [1m Learning iteration 2980/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.783s, learning 0.240s)
               Value function loss: 28.5003
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 246.07
               Mean episode length: 120.35
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48840704
                    Iteration time: 12.02s
                        Total time: 38782.39s
                               ETA: 1262216.5s

################################################################################
                    [1m Learning iteration 2981/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.971s, learning 0.201s)
               Value function loss: 26.7968
                    Surrogate loss: -0.0011
             Mean action noise std: 0.77
                       Mean reward: 254.68
               Mean episode length: 123.79
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48857088
                    Iteration time: 12.17s
                        Total time: 38794.56s
                               ETA: 1262176.2s

################################################################################
                    [1m Learning iteration 2982/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.931s, learning 0.229s)
               Value function loss: 26.3187
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 255.28
               Mean episode length: 122.86
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48873472
                    Iteration time: 12.16s
                        Total time: 38806.72s
                               ETA: 1262135.6s

################################################################################
                    [1m Learning iteration 2983/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.561s, learning 0.158s)
               Value function loss: 24.6074
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 252.66
               Mean episode length: 122.88
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48889856
                    Iteration time: 11.72s
                        Total time: 38818.44s
                               ETA: 1262080.6s

################################################################################
                    [1m Learning iteration 2984/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.879s, learning 0.163s)
               Value function loss: 28.3042
                    Surrogate loss: 0.0131
             Mean action noise std: 0.77
                       Mean reward: 247.29
               Mean episode length: 120.19
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.47
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48906240
                    Iteration time: 12.04s
                        Total time: 38830.48s
                               ETA: 1262036.2s

################################################################################
                    [1m Learning iteration 2985/100000 [0m                    

                       Computation: 1362 steps/s (collection: 11.867s, learning 0.159s)
               Value function loss: 21.8948
                    Surrogate loss: -0.0097
             Mean action noise std: 0.77
                       Mean reward: 251.43
               Mean episode length: 121.75
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48922624
                    Iteration time: 12.03s
                        Total time: 38842.51s
                               ETA: 1261991.3s

################################################################################
                    [1m Learning iteration 2986/100000 [0m                    

                       Computation: 1396 steps/s (collection: 11.527s, learning 0.208s)
               Value function loss: 21.2859
                    Surrogate loss: -0.0025
             Mean action noise std: 0.77
                       Mean reward: 257.04
               Mean episode length: 124.38
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48939008
                    Iteration time: 11.74s
                        Total time: 38854.24s
                               ETA: 1261936.9s

################################################################################
                    [1m Learning iteration 2987/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.601s, learning 0.162s)
               Value function loss: 23.5359
                    Surrogate loss: -0.0085
             Mean action noise std: 0.77
                       Mean reward: 251.03
               Mean episode length: 124.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48955392
                    Iteration time: 11.76s
                        Total time: 38866.01s
                               ETA: 1261883.5s

################################################################################
                    [1m Learning iteration 2988/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.989s, learning 0.204s)
               Value function loss: 21.6431
                    Surrogate loss: -0.0041
             Mean action noise std: 0.77
                       Mean reward: 251.50
               Mean episode length: 124.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48971776
                    Iteration time: 12.19s
                        Total time: 38878.20s
                               ETA: 1261844.1s

################################################################################
                    [1m Learning iteration 2989/100000 [0m                    

                       Computation: 1378 steps/s (collection: 11.722s, learning 0.162s)
               Value function loss: 22.2420
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 251.90
               Mean episode length: 122.28
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 48988160
                    Iteration time: 11.88s
                        Total time: 38890.08s
                               ETA: 1261794.6s

################################################################################
                    [1m Learning iteration 2990/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.070s, learning 0.194s)
               Value function loss: 22.3374
                    Surrogate loss: -0.0065
             Mean action noise std: 0.77
                       Mean reward: 248.69
               Mean episode length: 121.37
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49004544
                    Iteration time: 12.26s
                        Total time: 38902.35s
                               ETA: 1261757.5s

################################################################################
                    [1m Learning iteration 2991/100000 [0m                    

                       Computation: 1338 steps/s (collection: 12.038s, learning 0.199s)
               Value function loss: 18.9506
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 237.07
               Mean episode length: 117.16
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49020928
                    Iteration time: 12.24s
                        Total time: 38914.58s
                               ETA: 1261719.5s

################################################################################
                    [1m Learning iteration 2992/100000 [0m                    

                       Computation: 1335 steps/s (collection: 12.098s, learning 0.169s)
               Value function loss: 23.1269
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 251.33
               Mean episode length: 123.46
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49037312
                    Iteration time: 12.27s
                        Total time: 38926.85s
                               ETA: 1261682.6s

################################################################################
                    [1m Learning iteration 2993/100000 [0m                    

                       Computation: 1367 steps/s (collection: 11.777s, learning 0.206s)
               Value function loss: 23.7213
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 246.13
               Mean episode length: 122.54
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49053696
                    Iteration time: 11.98s
                        Total time: 38938.83s
                               ETA: 1261636.4s

################################################################################
                    [1m Learning iteration 2994/100000 [0m                    

                       Computation: 1347 steps/s (collection: 11.992s, learning 0.163s)
               Value function loss: 21.2733
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 243.75
               Mean episode length: 121.46
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49070080
                    Iteration time: 12.16s
                        Total time: 38950.99s
                               ETA: 1261595.9s

################################################################################
                    [1m Learning iteration 2995/100000 [0m                    

                       Computation: 1399 steps/s (collection: 11.544s, learning 0.160s)
               Value function loss: 23.1924
                    Surrogate loss: -0.0020
             Mean action noise std: 0.77
                       Mean reward: 243.76
               Mean episode length: 120.02
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49086464
                    Iteration time: 11.70s
                        Total time: 38962.69s
                               ETA: 1261540.7s

################################################################################
                    [1m Learning iteration 2996/100000 [0m                    

                       Computation: 1411 steps/s (collection: 11.416s, learning 0.194s)
               Value function loss: 23.5773
                    Surrogate loss: -0.0073
             Mean action noise std: 0.77
                       Mean reward: 252.28
               Mean episode length: 123.98
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49102848
                    Iteration time: 11.61s
                        Total time: 38974.30s
                               ETA: 1261482.6s

################################################################################
                    [1m Learning iteration 2997/100000 [0m                    

                       Computation: 1403 steps/s (collection: 11.472s, learning 0.201s)
               Value function loss: 24.0843
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 250.59
               Mean episode length: 122.09
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49119232
                    Iteration time: 11.67s
                        Total time: 38985.98s
                               ETA: 1261426.5s

################################################################################
                    [1m Learning iteration 2998/100000 [0m                    

                       Computation: 1310 steps/s (collection: 12.217s, learning 0.287s)
               Value function loss: 19.9561
                    Surrogate loss: -0.0155
             Mean action noise std: 0.77
                       Mean reward: 250.38
               Mean episode length: 122.96
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49135616
                    Iteration time: 12.50s
                        Total time: 38998.48s
                               ETA: 1261397.3s

################################################################################
                    [1m Learning iteration 2999/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.035s, learning 0.259s)
               Value function loss: 21.3440
                    Surrogate loss: -0.0118
             Mean action noise std: 0.77
                       Mean reward: 244.03
               Mean episode length: 122.58
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49152000
                    Iteration time: 12.29s
                        Total time: 39010.77s
                               ETA: 1261361.4s

################################################################################
                    [1m Learning iteration 3000/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.835s, learning 0.160s)
               Value function loss: 23.4993
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 240.13
               Mean episode length: 120.05
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49168384
                    Iteration time: 12.00s
                        Total time: 39022.77s
                               ETA: 1261315.8s

################################################################################
                    [1m Learning iteration 3001/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.784s, learning 0.188s)
               Value function loss: 21.1313
                    Surrogate loss: -0.0049
             Mean action noise std: 0.77
                       Mean reward: 254.41
               Mean episode length: 124.20
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49184768
                    Iteration time: 11.97s
                        Total time: 39034.74s
                               ETA: 1261269.4s

################################################################################
                    [1m Learning iteration 3002/100000 [0m                    

                       Computation: 1398 steps/s (collection: 11.561s, learning 0.155s)
               Value function loss: 20.7159
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 239.58
               Mean episode length: 119.62
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49201152
                    Iteration time: 11.72s
                        Total time: 39046.46s
                               ETA: 1261214.9s

################################################################################
                    [1m Learning iteration 3003/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.698s, learning 0.174s)
               Value function loss: 27.0406
                    Surrogate loss: -0.0046
             Mean action noise std: 0.77
                       Mean reward: 246.91
               Mean episode length: 121.64
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49217536
                    Iteration time: 11.87s
                        Total time: 39058.33s
                               ETA: 1261165.3s

################################################################################
                    [1m Learning iteration 3004/100000 [0m                    

                       Computation: 1416 steps/s (collection: 11.311s, learning 0.258s)
               Value function loss: 20.2641
                    Surrogate loss: -0.0063
             Mean action noise std: 0.77
                       Mean reward: 250.74
               Mean episode length: 124.02
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.64
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49233920
                    Iteration time: 11.57s
                        Total time: 39069.90s
                               ETA: 1261106.1s

################################################################################
                    [1m Learning iteration 3005/100000 [0m                    

                       Computation: 1337 steps/s (collection: 12.014s, learning 0.239s)
               Value function loss: 24.6660
                    Surrogate loss: -0.0067
             Mean action noise std: 0.77
                       Mean reward: 238.71
               Mean episode length: 120.01
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49250304
                    Iteration time: 12.25s
                        Total time: 39082.15s
                               ETA: 1261068.9s

################################################################################
                    [1m Learning iteration 3006/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.756s, learning 0.160s)
               Value function loss: 23.8625
                    Surrogate loss: -0.0006
             Mean action noise std: 0.77
                       Mean reward: 247.56
               Mean episode length: 123.42
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49266688
                    Iteration time: 11.92s
                        Total time: 39094.07s
                               ETA: 1261021.0s

################################################################################
                    [1m Learning iteration 3007/100000 [0m                    

                       Computation: 1363 steps/s (collection: 11.802s, learning 0.212s)
               Value function loss: 21.3962
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 242.44
               Mean episode length: 119.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49283072
                    Iteration time: 12.01s
                        Total time: 39106.08s
                               ETA: 1260976.1s

################################################################################
                    [1m Learning iteration 3008/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.759s, learning 0.185s)
               Value function loss: 26.7737
                    Surrogate loss: 0.0051
             Mean action noise std: 0.77
                       Mean reward: 251.25
               Mean episode length: 124.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49299456
                    Iteration time: 11.94s
                        Total time: 39118.03s
                               ETA: 1260929.1s

################################################################################
                    [1m Learning iteration 3009/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.768s, learning 0.201s)
               Value function loss: 23.5212
                    Surrogate loss: -0.0009
             Mean action noise std: 0.77
                       Mean reward: 244.58
               Mean episode length: 120.92
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49315840
                    Iteration time: 11.97s
                        Total time: 39129.99s
                               ETA: 1260882.8s

################################################################################
                    [1m Learning iteration 3010/100000 [0m                    

                       Computation: 1390 steps/s (collection: 11.583s, learning 0.198s)
               Value function loss: 20.5319
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 252.09
               Mean episode length: 123.99
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49332224
                    Iteration time: 11.78s
                        Total time: 39141.78s
                               ETA: 1260830.5s

################################################################################
                    [1m Learning iteration 3011/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.818s, learning 0.156s)
               Value function loss: 24.8027
                    Surrogate loss: -0.0094
             Mean action noise std: 0.77
                       Mean reward: 247.80
               Mean episode length: 122.52
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49348608
                    Iteration time: 11.97s
                        Total time: 39153.75s
                               ETA: 1260784.5s

################################################################################
                    [1m Learning iteration 3012/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.626s, learning 0.223s)
               Value function loss: 22.1346
                    Surrogate loss: -0.0113
             Mean action noise std: 0.77
                       Mean reward: 246.32
               Mean episode length: 122.89
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49364992
                    Iteration time: 11.85s
                        Total time: 39165.60s
                               ETA: 1260734.5s

################################################################################
                    [1m Learning iteration 3013/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.802s, learning 0.203s)
               Value function loss: 22.8625
                    Surrogate loss: -0.0060
             Mean action noise std: 0.77
                       Mean reward: 248.96
               Mean episode length: 122.72
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49381376
                    Iteration time: 12.01s
                        Total time: 39177.60s
                               ETA: 1260689.5s

################################################################################
                    [1m Learning iteration 3014/100000 [0m                    

                       Computation: 1400 steps/s (collection: 11.532s, learning 0.165s)
               Value function loss: 23.6485
                    Surrogate loss: -0.0110
             Mean action noise std: 0.77
                       Mean reward: 246.95
               Mean episode length: 122.73
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49397760
                    Iteration time: 11.70s
                        Total time: 39189.30s
                               ETA: 1260634.6s

################################################################################
                    [1m Learning iteration 3015/100000 [0m                    

                       Computation: 1344 steps/s (collection: 11.990s, learning 0.195s)
               Value function loss: 24.6877
                    Surrogate loss: -0.0066
             Mean action noise std: 0.77
                       Mean reward: 252.82
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49414144
                    Iteration time: 12.19s
                        Total time: 39201.48s
                               ETA: 1260595.5s

################################################################################
                    [1m Learning iteration 3016/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.985s, learning 0.165s)
               Value function loss: 20.4872
                    Surrogate loss: -0.0159
             Mean action noise std: 0.77
                       Mean reward: 243.20
               Mean episode length: 121.82
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49430528
                    Iteration time: 12.15s
                        Total time: 39213.63s
                               ETA: 1260555.2s

################################################################################
                    [1m Learning iteration 3017/100000 [0m                    

                       Computation: 1402 steps/s (collection: 11.489s, learning 0.191s)
               Value function loss: 19.7050
                    Surrogate loss: 0.0007
             Mean action noise std: 0.77
                       Mean reward: 249.33
               Mean episode length: 121.81
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49446912
                    Iteration time: 11.68s
                        Total time: 39225.31s
                               ETA: 1260499.9s

################################################################################
                    [1m Learning iteration 3018/100000 [0m                    

                       Computation: 1335 steps/s (collection: 11.971s, learning 0.296s)
               Value function loss: 21.3597
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 250.57
               Mean episode length: 123.86
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49463296
                    Iteration time: 12.27s
                        Total time: 39237.58s
                               ETA: 1260463.4s

################################################################################
                    [1m Learning iteration 3019/100000 [0m                    

                       Computation: 1377 steps/s (collection: 11.736s, learning 0.156s)
               Value function loss: 21.1863
                    Surrogate loss: -0.0031
             Mean action noise std: 0.77
                       Mean reward: 255.08
               Mean episode length: 125.00
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49479680
                    Iteration time: 11.89s
                        Total time: 39249.47s
                               ETA: 1260414.9s

################################################################################
                    [1m Learning iteration 3020/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.788s, learning 0.208s)
               Value function loss: 21.6708
                    Surrogate loss: -0.0077
             Mean action noise std: 0.77
                       Mean reward: 250.21
               Mean episode length: 122.29
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49496064
                    Iteration time: 12.00s
                        Total time: 39261.47s
                               ETA: 1260369.8s

################################################################################
                    [1m Learning iteration 3021/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.672s, learning 0.164s)
               Value function loss: 22.5660
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 253.43
               Mean episode length: 123.86
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49512448
                    Iteration time: 11.84s
                        Total time: 39273.30s
                               ETA: 1260319.6s

################################################################################
                    [1m Learning iteration 3022/100000 [0m                    

                       Computation: 1405 steps/s (collection: 11.493s, learning 0.163s)
               Value function loss: 23.3945
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 247.68
               Mean episode length: 121.65
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49528832
                    Iteration time: 11.66s
                        Total time: 39284.96s
                               ETA: 1260263.6s

################################################################################
                    [1m Learning iteration 3023/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.645s, learning 0.167s)
               Value function loss: 23.4667
                    Surrogate loss: 0.0039
             Mean action noise std: 0.77
                       Mean reward: 248.87
               Mean episode length: 121.77
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49545216
                    Iteration time: 11.81s
                        Total time: 39296.77s
                               ETA: 1260212.6s

################################################################################
                    [1m Learning iteration 3024/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.404s, learning 0.196s)
               Value function loss: 23.5659
                    Surrogate loss: -0.0099
             Mean action noise std: 0.77
                       Mean reward: 248.65
               Mean episode length: 123.06
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49561600
                    Iteration time: 12.60s
                        Total time: 39309.37s
                               ETA: 1260187.0s

################################################################################
                    [1m Learning iteration 3025/100000 [0m                    

                       Computation: 1371 steps/s (collection: 11.715s, learning 0.233s)
               Value function loss: 21.6334
                    Surrogate loss: -0.0114
             Mean action noise std: 0.77
                       Mean reward: 251.57
               Mean episode length: 125.00
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49577984
                    Iteration time: 11.95s
                        Total time: 39321.32s
                               ETA: 1260140.4s

################################################################################
                    [1m Learning iteration 3026/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.616s, learning 0.173s)
               Value function loss: 24.6399
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 245.25
               Mean episode length: 122.38
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49594368
                    Iteration time: 11.79s
                        Total time: 39333.11s
                               ETA: 1260088.8s

################################################################################
                    [1m Learning iteration 3027/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.828s, learning 0.169s)
               Value function loss: 28.5243
                    Surrogate loss: -0.0152
             Mean action noise std: 0.77
                       Mean reward: 244.86
               Mean episode length: 122.52
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49610752
                    Iteration time: 12.00s
                        Total time: 39345.10s
                               ETA: 1260043.9s

################################################################################
                    [1m Learning iteration 3028/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.897s, learning 0.174s)
               Value function loss: 29.1089
                    Surrogate loss: -0.0103
             Mean action noise std: 0.77
                       Mean reward: 250.31
               Mean episode length: 124.09
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49627136
                    Iteration time: 12.07s
                        Total time: 39357.18s
                               ETA: 1260001.4s

################################################################################
                    [1m Learning iteration 3029/100000 [0m                    

                       Computation: 1368 steps/s (collection: 11.801s, learning 0.170s)
               Value function loss: 28.3183
                    Surrogate loss: -0.0047
             Mean action noise std: 0.77
                       Mean reward: 249.79
               Mean episode length: 122.99
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49643520
                    Iteration time: 11.97s
                        Total time: 39369.15s
                               ETA: 1259955.6s

################################################################################
                    [1m Learning iteration 3030/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.632s, learning 0.166s)
               Value function loss: 26.1820
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 250.46
               Mean episode length: 123.05
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49659904
                    Iteration time: 11.80s
                        Total time: 39380.95s
                               ETA: 1259904.4s

################################################################################
                    [1m Learning iteration 3031/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.073s, learning 0.162s)
               Value function loss: 28.5368
                    Surrogate loss: -0.0145
             Mean action noise std: 0.77
                       Mean reward: 243.39
               Mean episode length: 122.14
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49676288
                    Iteration time: 12.23s
                        Total time: 39393.18s
                               ETA: 1259867.2s

################################################################################
                    [1m Learning iteration 3032/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.669s, learning 0.196s)
               Value function loss: 24.7809
                    Surrogate loss: 0.0072
             Mean action noise std: 0.77
                       Mean reward: 250.55
               Mean episode length: 123.21
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49692672
                    Iteration time: 11.87s
                        Total time: 39405.05s
                               ETA: 1259818.2s

################################################################################
                    [1m Learning iteration 3033/100000 [0m                    

                       Computation: 1346 steps/s (collection: 12.001s, learning 0.169s)
               Value function loss: 22.1623
                    Surrogate loss: -0.0092
             Mean action noise std: 0.77
                       Mean reward: 249.56
               Mean episode length: 122.73
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49709056
                    Iteration time: 12.17s
                        Total time: 39417.22s
                               ETA: 1259778.9s

################################################################################
                    [1m Learning iteration 3034/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.794s, learning 0.196s)
               Value function loss: 27.8170
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 255.57
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49725440
                    Iteration time: 11.99s
                        Total time: 39429.21s
                               ETA: 1259733.9s

################################################################################
                    [1m Learning iteration 3035/100000 [0m                    

                       Computation: 1374 steps/s (collection: 11.749s, learning 0.171s)
               Value function loss: 21.9684
                    Surrogate loss: -0.0078
             Mean action noise std: 0.77
                       Mean reward: 253.51
               Mean episode length: 123.30
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49741824
                    Iteration time: 11.92s
                        Total time: 39441.13s
                               ETA: 1259686.7s

################################################################################
                    [1m Learning iteration 3036/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.465s, learning 0.162s)
               Value function loss: 22.4603
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 253.96
               Mean episode length: 124.49
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49758208
                    Iteration time: 11.63s
                        Total time: 39452.76s
                               ETA: 1259630.2s

################################################################################
                    [1m Learning iteration 3037/100000 [0m                    

                       Computation: 1344 steps/s (collection: 12.024s, learning 0.166s)
               Value function loss: 25.5659
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 256.86
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49774592
                    Iteration time: 12.19s
                        Total time: 39464.95s
                               ETA: 1259591.7s

################################################################################
                    [1m Learning iteration 3038/100000 [0m                    

                       Computation: 1306 steps/s (collection: 12.371s, learning 0.170s)
               Value function loss: 26.0967
                    Surrogate loss: -0.0131
             Mean action noise std: 0.77
                       Mean reward: 251.85
               Mean episode length: 123.74
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49790976
                    Iteration time: 12.54s
                        Total time: 39477.49s
                               ETA: 1259564.3s

################################################################################
                    [1m Learning iteration 3039/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.708s, learning 0.163s)
               Value function loss: 31.0739
                    Surrogate loss: -0.0127
             Mean action noise std: 0.77
                       Mean reward: 246.26
               Mean episode length: 120.29
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49807360
                    Iteration time: 11.87s
                        Total time: 39489.36s
                               ETA: 1259515.6s

################################################################################
                    [1m Learning iteration 3040/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.630s, learning 0.163s)
               Value function loss: 26.6632
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 250.69
               Mean episode length: 122.58
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49823744
                    Iteration time: 11.79s
                        Total time: 39501.15s
                               ETA: 1259464.5s

################################################################################
                    [1m Learning iteration 3041/100000 [0m                    

                       Computation: 1359 steps/s (collection: 11.889s, learning 0.162s)
               Value function loss: 28.0025
                    Surrogate loss: -0.0128
             Mean action noise std: 0.77
                       Mean reward: 254.24
               Mean episode length: 123.95
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49840128
                    Iteration time: 12.05s
                        Total time: 39513.20s
                               ETA: 1259421.6s

################################################################################
                    [1m Learning iteration 3042/100000 [0m                    

                       Computation: 1304 steps/s (collection: 12.405s, learning 0.158s)
               Value function loss: 30.1832
                    Surrogate loss: -0.0108
             Mean action noise std: 0.77
                       Mean reward: 248.77
               Mean episode length: 122.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49856512
                    Iteration time: 12.56s
                        Total time: 39525.76s
                               ETA: 1259395.0s

################################################################################
                    [1m Learning iteration 3043/100000 [0m                    

                       Computation: 1339 steps/s (collection: 12.007s, learning 0.229s)
               Value function loss: 29.7870
                    Surrogate loss: -0.0134
             Mean action noise std: 0.77
                       Mean reward: 247.72
               Mean episode length: 122.64
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49872896
                    Iteration time: 12.24s
                        Total time: 39538.00s
                               ETA: 1259358.0s

################################################################################
                    [1m Learning iteration 3044/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.797s, learning 0.201s)
               Value function loss: 28.2157
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 253.17
               Mean episode length: 124.09
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49889280
                    Iteration time: 12.00s
                        Total time: 39550.00s
                               ETA: 1259313.5s

################################################################################
                    [1m Learning iteration 3045/100000 [0m                    

                       Computation: 1365 steps/s (collection: 11.838s, learning 0.163s)
               Value function loss: 28.7819
                    Surrogate loss: -0.0064
             Mean action noise std: 0.77
                       Mean reward: 255.50
               Mean episode length: 124.07
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49905664
                    Iteration time: 12.00s
                        Total time: 39562.00s
                               ETA: 1259269.0s

################################################################################
                    [1m Learning iteration 3046/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.083s, learning 0.249s)
               Value function loss: 27.1100
                    Surrogate loss: -0.0080
             Mean action noise std: 0.77
                       Mean reward: 255.41
               Mean episode length: 124.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49922048
                    Iteration time: 12.33s
                        Total time: 39574.33s
                               ETA: 1259235.2s

################################################################################
                    [1m Learning iteration 3047/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.124s, learning 0.212s)
               Value function loss: 28.0567
                    Surrogate loss: -0.0102
             Mean action noise std: 0.77
                       Mean reward: 252.67
               Mean episode length: 122.43
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49938432
                    Iteration time: 12.34s
                        Total time: 39586.67s
                               ETA: 1259201.4s

################################################################################
                    [1m Learning iteration 3048/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.120s, learning 0.175s)
               Value function loss: 23.9069
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 251.10
               Mean episode length: 123.31
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49954816
                    Iteration time: 12.29s
                        Total time: 39598.96s
                               ETA: 1259166.4s

################################################################################
                    [1m Learning iteration 3049/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.124s, learning 0.175s)
               Value function loss: 25.8338
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 257.20
               Mean episode length: 124.84
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49971200
                    Iteration time: 12.30s
                        Total time: 39611.26s
                               ETA: 1259131.5s

################################################################################
                    [1m Learning iteration 3050/100000 [0m                    

                       Computation: 1387 steps/s (collection: 11.645s, learning 0.167s)
               Value function loss: 27.8891
                    Surrogate loss: -0.0074
             Mean action noise std: 0.77
                       Mean reward: 252.89
               Mean episode length: 123.92
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 49987584
                    Iteration time: 11.81s
                        Total time: 39623.07s
                               ETA: 1259081.2s

################################################################################
                    [1m Learning iteration 3051/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.764s, learning 0.201s)
               Value function loss: 22.5420
                    Surrogate loss: -0.0149
             Mean action noise std: 0.77
                       Mean reward: 252.25
               Mean episode length: 123.66
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.65
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50003968
                    Iteration time: 11.96s
                        Total time: 39635.04s
                               ETA: 1259035.7s

################################################################################
                    [1m Learning iteration 3052/100000 [0m                    

                       Computation: 1326 steps/s (collection: 12.181s, learning 0.167s)
               Value function loss: 25.1522
                    Surrogate loss: -0.0106
             Mean action noise std: 0.77
                       Mean reward: 254.42
               Mean episode length: 121.93
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50020352
                    Iteration time: 12.35s
                        Total time: 39647.38s
                               ETA: 1259002.5s

################################################################################
                    [1m Learning iteration 3053/100000 [0m                    

                       Computation: 1366 steps/s (collection: 11.796s, learning 0.195s)
               Value function loss: 25.6996
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 253.94
               Mean episode length: 123.25
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50036736
                    Iteration time: 11.99s
                        Total time: 39659.37s
                               ETA: 1258957.9s

################################################################################
                    [1m Learning iteration 3054/100000 [0m                    

                       Computation: 1320 steps/s (collection: 12.176s, learning 0.232s)
               Value function loss: 25.7640
                    Surrogate loss: -0.0137
             Mean action noise std: 0.77
                       Mean reward: 251.04
               Mean episode length: 123.16
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50053120
                    Iteration time: 12.41s
                        Total time: 39671.78s
                               ETA: 1258926.6s

################################################################################
                    [1m Learning iteration 3055/100000 [0m                    

                       Computation: 1369 steps/s (collection: 11.709s, learning 0.259s)
               Value function loss: 29.1489
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 256.98
               Mean episode length: 123.95
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50069504
                    Iteration time: 11.97s
                        Total time: 39683.75s
                               ETA: 1258881.3s

################################################################################
                    [1m Learning iteration 3056/100000 [0m                    

                       Computation: 1330 steps/s (collection: 12.113s, learning 0.202s)
               Value function loss: 26.0745
                    Surrogate loss: -0.0130
             Mean action noise std: 0.77
                       Mean reward: 257.00
               Mean episode length: 124.36
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50085888
                    Iteration time: 12.31s
                        Total time: 39696.07s
                               ETA: 1258847.0s

################################################################################
                    [1m Learning iteration 3057/100000 [0m                    

                       Computation: 1385 steps/s (collection: 11.635s, learning 0.190s)
               Value function loss: 28.2265
                    Surrogate loss: -0.0023
             Mean action noise std: 0.77
                       Mean reward: 257.14
               Mean episode length: 125.00
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50102272
                    Iteration time: 11.82s
                        Total time: 39707.89s
                               ETA: 1258797.2s

################################################################################
                    [1m Learning iteration 3058/100000 [0m                    

                       Computation: 1373 steps/s (collection: 11.760s, learning 0.165s)
               Value function loss: 30.6145
                    Surrogate loss: -0.0111
             Mean action noise std: 0.77
                       Mean reward: 258.04
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50118656
                    Iteration time: 11.92s
                        Total time: 39719.81s
                               ETA: 1258750.7s

################################################################################
                    [1m Learning iteration 3059/100000 [0m                    

                       Computation: 1356 steps/s (collection: 11.916s, learning 0.166s)
               Value function loss: 27.5145
                    Surrogate loss: -0.0146
             Mean action noise std: 0.77
                       Mean reward: 252.01
               Mean episode length: 121.74
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50135040
                    Iteration time: 12.08s
                        Total time: 39731.90s
                               ETA: 1258709.1s

################################################################################
                    [1m Learning iteration 3060/100000 [0m                    

                       Computation: 1328 steps/s (collection: 12.146s, learning 0.184s)
               Value function loss: 29.3376
                    Surrogate loss: -0.0112
             Mean action noise std: 0.77
                       Mean reward: 255.30
               Mean episode length: 124.08
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50151424
                    Iteration time: 12.33s
                        Total time: 39744.23s
                               ETA: 1258675.4s

################################################################################
                    [1m Learning iteration 3061/100000 [0m                    

                       Computation: 1413 steps/s (collection: 11.416s, learning 0.174s)
               Value function loss: 33.2803
                    Surrogate loss: -0.0116
             Mean action noise std: 0.77
                       Mean reward: 251.03
               Mean episode length: 123.06
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50167808
                    Iteration time: 11.59s
                        Total time: 39755.82s
                               ETA: 1258618.3s

################################################################################
                    [1m Learning iteration 3062/100000 [0m                    

                       Computation: 1313 steps/s (collection: 12.290s, learning 0.182s)
               Value function loss: 46.1619
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 254.79
               Mean episode length: 124.08
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.52
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50184192
                    Iteration time: 12.47s
                        Total time: 39768.29s
                               ETA: 1258589.1s

################################################################################
                    [1m Learning iteration 3063/100000 [0m                    

                       Computation: 1336 steps/s (collection: 12.016s, learning 0.240s)
               Value function loss: 33.8303
                    Surrogate loss: -0.0058
             Mean action noise std: 0.77
                       Mean reward: 254.63
               Mean episode length: 123.78
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50200576
                    Iteration time: 12.26s
                        Total time: 39780.55s
                               ETA: 1258553.1s

################################################################################
                    [1m Learning iteration 3064/100000 [0m                    

                       Computation: 1384 steps/s (collection: 11.644s, learning 0.193s)
               Value function loss: 28.9495
                    Surrogate loss: -0.0090
             Mean action noise std: 0.77
                       Mean reward: 253.04
               Mean episode length: 123.89
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50216960
                    Iteration time: 11.84s
                        Total time: 39792.38s
                               ETA: 1258503.9s

################################################################################
                    [1m Learning iteration 3065/100000 [0m                    

                       Computation: 1332 steps/s (collection: 12.053s, learning 0.246s)
               Value function loss: 32.2376
                    Surrogate loss: -0.0013
             Mean action noise std: 0.77
                       Mean reward: 254.73
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50233344
                    Iteration time: 12.30s
                        Total time: 39804.68s
                               ETA: 1258469.3s

################################################################################
                    [1m Learning iteration 3066/100000 [0m                    

                       Computation: 1388 steps/s (collection: 11.632s, learning 0.167s)
               Value function loss: 25.4957
                    Surrogate loss: -0.0098
             Mean action noise std: 0.77
                       Mean reward: 252.53
               Mean episode length: 123.73
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50249728
                    Iteration time: 11.80s
                        Total time: 39816.48s
                               ETA: 1258418.9s

################################################################################
                    [1m Learning iteration 3067/100000 [0m                    

                       Computation: 1340 steps/s (collection: 11.905s, learning 0.318s)
               Value function loss: 28.2284
                    Surrogate loss: -0.0115
             Mean action noise std: 0.77
                       Mean reward: 249.74
               Mean episode length: 122.39
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50266112
                    Iteration time: 12.22s
                        Total time: 39828.70s
                               ETA: 1258382.0s

################################################################################
                    [1m Learning iteration 3068/100000 [0m                    

                       Computation: 1360 steps/s (collection: 11.880s, learning 0.165s)
               Value function loss: 28.4872
                    Surrogate loss: -0.0162
             Mean action noise std: 0.77
                       Mean reward: 256.51
               Mean episode length: 125.00
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50282496
                    Iteration time: 12.05s
                        Total time: 39840.75s
                               ETA: 1258339.4s

################################################################################
                    [1m Learning iteration 3069/100000 [0m                    

                       Computation: 1392 steps/s (collection: 11.588s, learning 0.174s)
               Value function loss: 30.4247
                    Surrogate loss: -0.0119
             Mean action noise std: 0.77
                       Mean reward: 249.04
               Mean episode length: 122.42
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50298880
                    Iteration time: 11.76s
                        Total time: 39852.51s
                               ETA: 1258287.9s

################################################################################
                    [1m Learning iteration 3070/100000 [0m                    

                       Computation: 1357 steps/s (collection: 11.856s, learning 0.211s)
               Value function loss: 32.1325
                    Surrogate loss: -0.0051
             Mean action noise std: 0.77
                       Mean reward: 254.30
               Mean episode length: 123.61
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.55
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50315264
                    Iteration time: 12.07s
                        Total time: 39864.58s
                               ETA: 1258246.0s

################################################################################
                    [1m Learning iteration 3071/100000 [0m                    

                       Computation: 1309 steps/s (collection: 12.305s, learning 0.204s)
               Value function loss: 27.4159
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 249.49
               Mean episode length: 122.15
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50331648
                    Iteration time: 12.51s
                        Total time: 39877.09s
                               ETA: 1258218.2s

################################################################################
                    [1m Learning iteration 3072/100000 [0m                    

                       Computation: 1348 steps/s (collection: 11.952s, learning 0.195s)
               Value function loss: 25.4805
                    Surrogate loss: 0.0216
             Mean action noise std: 0.77
                       Mean reward: 255.51
               Mean episode length: 125.00
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50348032
                    Iteration time: 12.15s
                        Total time: 39889.23s
                               ETA: 1258178.9s

################################################################################
                    [1m Learning iteration 3073/100000 [0m                    

                       Computation: 1364 steps/s (collection: 11.843s, learning 0.168s)
               Value function loss: 30.9037
                    Surrogate loss: -0.0026
             Mean action noise std: 0.77
                       Mean reward: 250.23
               Mean episode length: 123.04
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50364416
                    Iteration time: 12.01s
                        Total time: 39901.24s
                               ETA: 1258135.3s

################################################################################
                    [1m Learning iteration 3074/100000 [0m                    

                       Computation: 1352 steps/s (collection: 11.890s, learning 0.222s)
               Value function loss: 31.4357
                    Surrogate loss: -0.0027
             Mean action noise std: 0.77
                       Mean reward: 247.71
               Mean episode length: 121.46
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.53
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50380800
                    Iteration time: 12.11s
                        Total time: 39913.36s
                               ETA: 1258095.0s

################################################################################
                    [1m Learning iteration 3075/100000 [0m                    

                       Computation: 1340 steps/s (collection: 12.033s, learning 0.187s)
               Value function loss: 28.7968
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 255.34
               Mean episode length: 124.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50397184
                    Iteration time: 12.22s
                        Total time: 39925.58s
                               ETA: 1258058.1s

################################################################################
                    [1m Learning iteration 3076/100000 [0m                    

                       Computation: 1316 steps/s (collection: 12.249s, learning 0.201s)
               Value function loss: 27.6212
                    Surrogate loss: -0.0079
             Mean action noise std: 0.77
                       Mean reward: 251.29
               Mean episode length: 122.63
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50413568
                    Iteration time: 12.45s
                        Total time: 39938.03s
                               ETA: 1258028.4s

################################################################################
                    [1m Learning iteration 3077/100000 [0m                    

                       Computation: 1380 steps/s (collection: 11.673s, learning 0.191s)
               Value function loss: 28.8886
                    Surrogate loss: -0.0132
             Mean action noise std: 0.77
                       Mean reward: 251.87
               Mean episode length: 123.13
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50429952
                    Iteration time: 11.86s
                        Total time: 39949.89s
                               ETA: 1257980.3s

################################################################################
                    [1m Learning iteration 3078/100000 [0m                    

                       Computation: 1300 steps/s (collection: 12.423s, learning 0.178s)
               Value function loss: 28.9060
                    Surrogate loss: -0.0109
             Mean action noise std: 0.77
                       Mean reward: 251.18
               Mean episode length: 123.13
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50446336
                    Iteration time: 12.60s
                        Total time: 39962.49s
                               ETA: 1257955.4s

################################################################################
                    [1m Learning iteration 3079/100000 [0m                    

                       Computation: 1382 steps/s (collection: 11.667s, learning 0.184s)
               Value function loss: 22.6789
                    Surrogate loss: -0.0095
             Mean action noise std: 0.77
                       Mean reward: 246.80
               Mean episode length: 122.27
                  Mean reward/step: 2.02
       Mean episode length/episode: 7.61
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50462720
                    Iteration time: 11.85s
                        Total time: 39974.34s
                               ETA: 1257906.9s

################################################################################
                    [1m Learning iteration 3080/100000 [0m                    

                       Computation: 1343 steps/s (collection: 11.936s, learning 0.257s)
               Value function loss: 22.5734
                    Surrogate loss: -0.0123
             Mean action noise std: 0.77
                       Mean reward: 245.13
               Mean episode length: 121.96
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.60
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50479104
                    Iteration time: 12.19s
                        Total time: 39986.54s
                               ETA: 1257869.2s

################################################################################
                    [1m Learning iteration 3081/100000 [0m                    

                       Computation: 1381 steps/s (collection: 11.682s, learning 0.178s)
               Value function loss: 27.7530
                    Surrogate loss: -0.0121
             Mean action noise std: 0.77
                       Mean reward: 252.74
               Mean episode length: 123.95
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50495488
                    Iteration time: 11.86s
                        Total time: 39998.40s
                               ETA: 1257821.1s

################################################################################
                    [1m Learning iteration 3082/100000 [0m                    

                       Computation: 1361 steps/s (collection: 11.844s, learning 0.191s)
               Value function loss: 20.9486
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 240.44
               Mean episode length: 120.36
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50511872
                    Iteration time: 12.04s
                        Total time: 40010.43s
                               ETA: 1257778.4s

################################################################################
                    [1m Learning iteration 3083/100000 [0m                    

                       Computation: 1389 steps/s (collection: 11.592s, learning 0.200s)
               Value function loss: 28.0480
                    Surrogate loss: -0.0086
             Mean action noise std: 0.77
                       Mean reward: 249.34
               Mean episode length: 123.32
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50528256
                    Iteration time: 11.79s
                        Total time: 40022.22s
                               ETA: 1257728.2s

################################################################################
                    [1m Learning iteration 3084/100000 [0m                    

                       Computation: 1370 steps/s (collection: 11.766s, learning 0.187s)
               Value function loss: 26.9409
                    Surrogate loss: -0.0141
             Mean action noise std: 0.77
                       Mean reward: 249.19
               Mean episode length: 122.44
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.58
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50544640
                    Iteration time: 11.95s
                        Total time: 40034.17s
                               ETA: 1257683.0s

################################################################################
                    [1m Learning iteration 3085/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.452s, learning 0.187s)
               Value function loss: 27.0012
                    Surrogate loss: -0.0081
             Mean action noise std: 0.77
                       Mean reward: 252.12
               Mean episode length: 124.14
                  Mean reward/step: 2.03
       Mean episode length/episode: 7.56
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50561024
                    Iteration time: 11.64s
                        Total time: 40045.81s
                               ETA: 1257628.0s

################################################################################
                    [1m Learning iteration 3086/100000 [0m                    

                       Computation: 1407 steps/s (collection: 11.408s, learning 0.232s)
               Value function loss: 27.7248
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 248.75
               Mean episode length: 123.67
                  Mean reward/step: 2.04
       Mean episode length/episode: 7.51
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50577408
                    Iteration time: 11.64s
                        Total time: 40057.45s
                               ETA: 1257573.0s

################################################################################
                    [1m Learning iteration 3087/100000 [0m                    

                       Computation: 1482 steps/s (collection: 10.887s, learning 0.161s)
               Value function loss: 25.0471
                    Surrogate loss: -0.0122
             Mean action noise std: 0.77
                       Mean reward: 245.52
               Mean episode length: 120.68
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50593792
                    Iteration time: 11.05s
                        Total time: 40068.50s
                               ETA: 1257499.6s

################################################################################
                    [1m Learning iteration 3088/100000 [0m                    

                       Computation: 1415 steps/s (collection: 11.409s, learning 0.163s)
               Value function loss: 24.0952
                    Surrogate loss: -0.0124
             Mean action noise std: 0.77
                       Mean reward: 254.34
               Mean episode length: 124.89
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.62
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50610176
                    Iteration time: 11.57s
                        Total time: 40080.07s
                               ETA: 1257442.6s

################################################################################
                    [1m Learning iteration 3089/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.855s, learning 0.247s)
               Value function loss: 28.9389
                    Surrogate loss: -0.0120
             Mean action noise std: 0.77
                       Mean reward: 249.99
               Mean episode length: 123.21
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50626560
                    Iteration time: 12.10s
                        Total time: 40092.18s
                               ETA: 1257402.2s

################################################################################
                    [1m Learning iteration 3090/100000 [0m                    

                       Computation: 1409 steps/s (collection: 11.460s, learning 0.164s)
               Value function loss: 27.2521
                    Surrogate loss: -0.0129
             Mean action noise std: 0.77
                       Mean reward: 241.93
               Mean episode length: 119.96
                  Mean reward/step: 2.05
       Mean episode length/episode: 7.54
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50642944
                    Iteration time: 11.62s
                        Total time: 40103.80s
                               ETA: 1257346.9s

################################################################################
                    [1m Learning iteration 3091/100000 [0m                    

                       Computation: 1353 steps/s (collection: 11.928s, learning 0.175s)
               Value function loss: 28.1587
                    Surrogate loss: -0.0139
             Mean action noise std: 0.77
                       Mean reward: 254.73
               Mean episode length: 123.87
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.57
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50659328
                    Iteration time: 12.10s
                        Total time: 40115.90s
                               ETA: 1257306.6s

################################################################################
                    [1m Learning iteration 3092/100000 [0m                    

                       Computation: 1345 steps/s (collection: 11.897s, learning 0.275s)
               Value function loss: 28.7433
                    Surrogate loss: -0.0084
             Mean action noise std: 0.77
                       Mean reward: 253.05
               Mean episode length: 123.10
                  Mean reward/step: 2.06
       Mean episode length/episode: 7.59
            Mean episode successes: 0.0000
Mean episode consecutive_successes: 0.0000
--------------------------------------------------------------------------------
                   Total timesteps: 50675712
                    Iteration time: 12.17s
                        Total time: 40128.07s
                               ETA: 1257268.5s
